{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Notes \u00b6 Some things worth saving. NOTE: Currently in process of migrating these notes into obsidian so things will be slowly reorganised not per courses. The compiled version available at notes.deiveris.lv","title":"Notes"},{"location":"#notes","text":"Some things worth saving. NOTE: Currently in process of migrating these notes into obsidian so things will be slowly reorganised not per courses. The compiled version available at notes.deiveris.lv","title":"Notes"},{"location":"AWS/API%20Gateway/API%20Gateway%20Integrations/","text":"API Gateway integrations \u00b6 Outside of VPC Summary : AWS Lambda (most popular / powerful) Endpoints on AWS EC2 Programming/AWS/EC2/Elastic LoadBalancer/Load Balancing Any AWS Service External and publicly accessible HTTP endpoints Inside of VPC AWS Lambda in your VPC Summary [[AWS EC2 Endpoint]]s in your VPC Summary","title":"API Gateway Integrations"},{"location":"AWS/API%20Gateway/API%20Gateway%20Integrations/#api-gateway-integrations","text":"Outside of VPC Summary : AWS Lambda (most popular / powerful) Endpoints on AWS EC2 Programming/AWS/EC2/Elastic LoadBalancer/Load Balancing Any AWS Service External and publicly accessible HTTP endpoints Inside of VPC AWS Lambda in your VPC Summary [[AWS EC2 Endpoint]]s in your VPC Summary","title":"API Gateway integrations"},{"location":"AWS/API%20Gateway/API%20Gateway%20Security/","text":"Security \u00b6 IAM Permissions Create an IAM Policy authorisation and attach to User / Role AWS API Gateway verifies IAM permissions passed by the calling application Good to provide access within your own infrastructure Leverages \"[[Sig v4]]\" capability where IAM credential are in headers Great for users / roles already within your AWS account [[Lambda Authorized]] (formerly Custom Authorisers) Uses AWS Lambda to validate the token in header being passed Option to cache result of authentication Helps to use [[OAuth]] / [[SAML]] / 3rd party type of authentication Lambda must return an IAM Policy for the user Pay per Lambda invocation Programming/AWS/Cognito/AWS Cognito User Pools Cognito fully manages [[user lifecycle]] AWS API Gateway verifies identity automatically from Programming/AWS/Cognito/AWS Cognito No custom implementation required Cognito only helps with [[authentication]], not [[authorisation]]","title":"API Gateway Security"},{"location":"AWS/API%20Gateway/API%20Gateway%20Security/#security","text":"IAM Permissions Create an IAM Policy authorisation and attach to User / Role AWS API Gateway verifies IAM permissions passed by the calling application Good to provide access within your own infrastructure Leverages \"[[Sig v4]]\" capability where IAM credential are in headers Great for users / roles already within your AWS account [[Lambda Authorized]] (formerly Custom Authorisers) Uses AWS Lambda to validate the token in header being passed Option to cache result of authentication Helps to use [[OAuth]] / [[SAML]] / 3rd party type of authentication Lambda must return an IAM Policy for the user Pay per Lambda invocation Programming/AWS/Cognito/AWS Cognito User Pools Cognito fully manages [[user lifecycle]] AWS API Gateway verifies identity automatically from Programming/AWS/Cognito/AWS Cognito No custom implementation required Cognito only helps with [[authentication]], not [[authorisation]]","title":"Security"},{"location":"AWS/API%20Gateway/AWS%20API%20Gateway/","text":"API Gateway \u00b6 AWS Lambda + AWS API Gateway : no infrastructure to manage Handle [[API versioning]] (v1, v2) Handle different environments (dev, test, prod) Handle security ([[Authentication and Authorization]]) Create [[API keys]], handle [[request throttling]] [[Swagger]] / [[Open API]] import to quickly define APIs Transform and validate requests and responses Generate [[SDK and API specifications]] [[Cache API]] responses","title":"API Gateway"},{"location":"AWS/API%20Gateway/AWS%20API%20Gateway/#api-gateway","text":"AWS Lambda + AWS API Gateway : no infrastructure to manage Handle [[API versioning]] (v1, v2) Handle different environments (dev, test, prod) Handle security ([[Authentication and Authorization]]) Create [[API keys]], handle [[request throttling]] [[Swagger]] / [[Open API]] import to quickly define APIs Transform and validate requests and responses Generate [[SDK and API specifications]] [[Cache API]] responses","title":"API Gateway"},{"location":"AWS/AWS%20Serverless%20APIs/","text":"Learning AWS Serverless \u00b6 Learning on how to create my first serverless application on AWS. Sources: - AWS Serverless APIs & Apps - A Complete Introduction","title":"Learning AWS Serverless"},{"location":"AWS/AWS%20Serverless%20APIs/#learning-aws-serverless","text":"Learning on how to create my first serverless application on AWS. Sources: - AWS Serverless APIs & Apps - A Complete Introduction","title":"Learning AWS Serverless"},{"location":"AWS/AWS%20Serverless%20APIs/01-getting-started/01-what-is-serverless/","text":"What is serverless? \u00b6 On traditional apps we might have a web app or a mobile app that needs a back-end REST API that hosts the business logic and provides data. The REST API is made from a server written in a language like php, node, python, ruby etc. As the traffic grows, we need to scale this server up and have multiple instances of it. With this approach we have several issues: We have to reinvent the wheel - we have to figure out the infrastructure We have to figure out on how many servers we need, have a danger of under/over provisioning them. We have to keep these servers updated, as we have to make sure we don't break everything. We can use a special AWS service called Lambda , that executes code on demand, when it needs to run. This provides the following: 1. The code is run on-demand 2. We don't have to pay any idle time 3. Service on creating APIs, don't have to create all that logic 4. Scales automatically 5. Code runs in up-to-date and secure environment When using lambda we can use following languages: - Node.js - Java - Python - C#","title":"What is serverless?"},{"location":"AWS/AWS%20Serverless%20APIs/01-getting-started/01-what-is-serverless/#what-is-serverless","text":"On traditional apps we might have a web app or a mobile app that needs a back-end REST API that hosts the business logic and provides data. The REST API is made from a server written in a language like php, node, python, ruby etc. As the traffic grows, we need to scale this server up and have multiple instances of it. With this approach we have several issues: We have to reinvent the wheel - we have to figure out the infrastructure We have to figure out on how many servers we need, have a danger of under/over provisioning them. We have to keep these servers updated, as we have to make sure we don't break everything. We can use a special AWS service called Lambda , that executes code on demand, when it needs to run. This provides the following: 1. The code is run on-demand 2. We don't have to pay any idle time 3. Service on creating APIs, don't have to create all that logic 4. Scales automatically 5. Code runs in up-to-date and secure environment When using lambda we can use following languages: - Node.js - Java - Python - C#","title":"What is serverless?"},{"location":"AWS/AWS%20Serverless%20APIs/01-getting-started/02-aws-signup-first-serverless-api/","text":"AWS Signup and First serverless API \u00b6 You can go to https://console.aws.amazon.com/console/home to sign up for AWS, this will require a valid credit card. When the signup is done and we're in the console, we can open up services menu and searchn for API Gateway . Click on Get started button. We want a new REST API . Not click on Create API and you will have this API. We can click on Actions -> Create Resource to create a new resource We can't call this yet, because we haven't provided request methods. We can do this by clicking on Actions -> Create Method . Now, we can provide an integration for the method like a: - Lamda function - HTTP endpoint - Mock - AWS service - VPC link After choosing Mock , we can click on the Integration Response And set a Mapping template . Now we are ready to deploy it by going to Actions -> Deploy API . Select a new deployment stage. Then, we will receive an URL like https://j12yugstuf.execute-api.eu-central-1.amazonaws.com/development . When visiting it, we will receive an error message: But, if we go to the actual endpoint at https://j12yugstuf.execute-api.eu-central-1.amazonaws.com/development/first-api-test , we are going to see the mock data.","title":"AWS Signup and First serverless API"},{"location":"AWS/AWS%20Serverless%20APIs/01-getting-started/02-aws-signup-first-serverless-api/#aws-signup-and-first-serverless-api","text":"You can go to https://console.aws.amazon.com/console/home to sign up for AWS, this will require a valid credit card. When the signup is done and we're in the console, we can open up services menu and searchn for API Gateway . Click on Get started button. We want a new REST API . Not click on Create API and you will have this API. We can click on Actions -> Create Resource to create a new resource We can't call this yet, because we haven't provided request methods. We can do this by clicking on Actions -> Create Method . Now, we can provide an integration for the method like a: - Lamda function - HTTP endpoint - Mock - AWS service - VPC link After choosing Mock , we can click on the Integration Response And set a Mapping template . Now we are ready to deploy it by going to Actions -> Deploy API . Select a new deployment stage. Then, we will receive an URL like https://j12yugstuf.execute-api.eu-central-1.amazonaws.com/development . When visiting it, we will receive an error message: But, if we go to the actual endpoint at https://j12yugstuf.execute-api.eu-central-1.amazonaws.com/development/first-api-test , we are going to see the mock data.","title":"AWS Signup and First serverless API"},{"location":"AWS/AWS%20Serverless%20APIs/01-getting-started/03-why-aws/","text":"Why would I use AWS? \u00b6 There are multiple platforms available like Amazon Web services (AWS) Microsoft Azure Google Cloud Platform The advantages of using AWS are: - It is the market leader - It has aggressive pricing - Most serverless services - Rapidly innovating new features and services","title":"Why would I use AWS?"},{"location":"AWS/AWS%20Serverless%20APIs/01-getting-started/03-why-aws/#why-would-i-use-aws","text":"There are multiple platforms available like Amazon Web services (AWS) Microsoft Azure Google Cloud Platform The advantages of using AWS are: - It is the market leader - It has aggressive pricing - Most serverless services - Rapidly innovating new features and services","title":"Why would I use AWS?"},{"location":"AWS/AWS%20Serverless%20APIs/01-getting-started/04-course-structure/","text":"Course Structure \u00b6 Getting started Core Serverless services Business logic with Lamda and API gateway Data storage with DynamoDB Authentication with Cognito Content delivery & hosting with S3, CloudFront, Route53 Beyond the basics (outlook) Roundup","title":"Course Structure"},{"location":"AWS/AWS%20Serverless%20APIs/01-getting-started/04-course-structure/#course-structure","text":"Getting started Core Serverless services Business logic with Lamda and API gateway Data storage with DynamoDB Authentication with Cognito Content delivery & hosting with S3, CloudFront, Route53 Beyond the basics (outlook) Roundup","title":"Course Structure"},{"location":"AWS/AWS%20Serverless%20APIs/02-core-serverless-services/01-overview-over-the-core-serverless-services/","text":"Overview over the Core Serverless Services \u00b6 What services do we need? \u00b6 App - Serve Static App (like angular app) - S3 (Simple Storage Service) API - REST API - API Gateway Logic - Execute code on demand - Lambda Data - Store & retrieve data (database) - DynamoDB Auth - Authenticate users - Cognito DNS - Translate URL - Route 53 Cache - Improve performance - CloudFront","title":"Overview over the Core Serverless Services"},{"location":"AWS/AWS%20Serverless%20APIs/02-core-serverless-services/01-overview-over-the-core-serverless-services/#overview-over-the-core-serverless-services","text":"","title":"Overview over the Core Serverless Services"},{"location":"AWS/AWS%20Serverless%20APIs/02-core-serverless-services/01-overview-over-the-core-serverless-services/#what-services-do-we-need","text":"App - Serve Static App (like angular app) - S3 (Simple Storage Service) API - REST API - API Gateway Logic - Execute code on demand - Lambda Data - Store & retrieve data (database) - DynamoDB Auth - Authenticate users - Cognito DNS - Translate URL - Route 53 Cache - Improve performance - CloudFront","title":"What services do we need?"},{"location":"AWS/AWS%20Serverless%20APIs/02-core-serverless-services/02-course-project/","text":"The \"Compare Yourself\" app \u00b6 We are going to host a single page application on S3 where you can submit some data that calls API gateway endpoints to compare yourself. These endpoints will be auth protected, will implement business logic written on lambda . The data will be stored in DynamoDB database. We will use Route 53 for routing dns, CloudFront for caching.","title":"The \"Compare Yourself\" app"},{"location":"AWS/AWS%20Serverless%20APIs/02-core-serverless-services/02-course-project/#the-compare-yourself-app","text":"We are going to host a single page application on S3 where you can submit some data that calls API gateway endpoints to compare yourself. These endpoints will be auth protected, will implement business logic written on lambda . The data will be stored in DynamoDB database. We will use Route 53 for routing dns, CloudFront for caching.","title":"The \"Compare Yourself\" app"},{"location":"AWS/AWS%20Serverless%20APIs/02-core-serverless-services%202/01-overview-over-the-core-serverless-services/","text":"Overview over the Core Serverless Services \u00b6 What services do we need? \u00b6 App - Serve Static App (like angular app) - S3 (Simple Storage Service) API - REST API - API Gateway Logic - Execute code on demand - Lambda Data - Store & retrieve data (database) - DynamoDB Auth - Authenticate users - Cognito DNS - Translate URL - Route 53 Cache - Improve performance - CloudFront","title":"Overview over the Core Serverless Services"},{"location":"AWS/AWS%20Serverless%20APIs/02-core-serverless-services%202/01-overview-over-the-core-serverless-services/#overview-over-the-core-serverless-services","text":"","title":"Overview over the Core Serverless Services"},{"location":"AWS/AWS%20Serverless%20APIs/02-core-serverless-services%202/01-overview-over-the-core-serverless-services/#what-services-do-we-need","text":"App - Serve Static App (like angular app) - S3 (Simple Storage Service) API - REST API - API Gateway Logic - Execute code on demand - Lambda Data - Store & retrieve data (database) - DynamoDB Auth - Authenticate users - Cognito DNS - Translate URL - Route 53 Cache - Improve performance - CloudFront","title":"What services do we need?"},{"location":"AWS/AWS%20Serverless%20APIs/02-core-serverless-services%202/02-course-project/","text":"The \"Compare Yourself\" app \u00b6 We are going to host a single page application on S3 where you can submit some data that calls API gateway endpoints to compare yourself. These endpoints will be auth protected, will implement business logic written on lambda . The data will be stored in DynamoDB database. We will use Route 53 for routing dns, CloudFront for caching.","title":"The \"Compare Yourself\" app"},{"location":"AWS/AWS%20Serverless%20APIs/02-core-serverless-services%202/02-course-project/#the-compare-yourself-app","text":"We are going to host a single page application on S3 where you can submit some data that calls API gateway endpoints to compare yourself. These endpoints will be auth protected, will implement business logic written on lambda . The data will be stored in DynamoDB database. We will use Route 53 for routing dns, CloudFront for caching.","title":"The \"Compare Yourself\" app"},{"location":"AWS/AWS%20Serverless%20APIs/03-creating-an-API-gateway-and-AWS-lamda/01-what-is-API-gateway/","text":"What is API Gateway \u00b6 How it works \u00b6 With API Gateway , we can build RESTful APIs . These can be for Web applications, mobile apps or other sources. You can define API endpoints and HTTP methods that trigger some action that directly accesses some AWS services like lamda that runs code on demand. Amazon API Gateway overview API Gateway Developer Documentation","title":"What is API Gateway"},{"location":"AWS/AWS%20Serverless%20APIs/03-creating-an-API-gateway-and-AWS-lamda/01-what-is-API-gateway/#what-is-api-gateway","text":"","title":"What is API Gateway"},{"location":"AWS/AWS%20Serverless%20APIs/03-creating-an-API-gateway-and-AWS-lamda/01-what-is-API-gateway/#how-it-works","text":"With API Gateway , we can build RESTful APIs . These can be for Web applications, mobile apps or other sources. You can define API endpoints and HTTP methods that trigger some action that directly accesses some AWS services like lamda that runs code on demand. Amazon API Gateway overview API Gateway Developer Documentation","title":"How it works"},{"location":"AWS/AWS%20Serverless%20APIs/03-creating-an-API-gateway-and-AWS-lamda/02-general-api-gateway-features/","text":"General API Gateway features \u00b6 There are multiple features in the API Gateway : We previously looked at the APIs section where you can define your APIs and their endpoints. There are other features as well like: - Usage Plans - you can create plans on limiting gateway access for throttling or quotas. - API Keys - for limiting access to API - Custom Domain names - for giving your own domain name for the API - Client Certificates - used to prove ownership of the api - Settings - manage roles and permissions service has","title":"General API Gateway features"},{"location":"AWS/AWS%20Serverless%20APIs/03-creating-an-API-gateway-and-AWS-lamda/02-general-api-gateway-features/#general-api-gateway-features","text":"There are multiple features in the API Gateway : We previously looked at the APIs section where you can define your APIs and their endpoints. There are other features as well like: - Usage Plans - you can create plans on limiting gateway access for throttling or quotas. - API Keys - for limiting access to API - Custom Domain names - for giving your own domain name for the API - Client Certificates - used to prove ownership of the api - Settings - manage roles and permissions service has","title":"General API Gateway features"},{"location":"AWS/AWS%20Serverless%20APIs/03-creating-an-API-gateway-and-AWS-lamda/03-API-specific-features-and-options/","text":"API-specific features and options \u00b6 Previously we created a resource and it's method. The resource defines a path in the url. One thing to note is that when we are working on these things, they aren't live. We need to deploy them under the actions. Only then we will able to see them on the web. The actual work happends under resources section. Then under stages section we can we can get information about the deployment. Under the Authorizers section we can add authentication to our APi. For certain resources that needs certain authentication, we can manage it here. The Models section, we can define the shape of the data we are working on. It is provided in JSON format. In the Documentation section we can create documentation for the API so people can know on how to use it. The Binary support section is used when we are sending files in the requests. The Dashboard shows shows stats about the API. The Resource Policy section is used to control how the API is accessed. We can whitelist and/or blacklist resources.","title":"API-specific features and options"},{"location":"AWS/AWS%20Serverless%20APIs/03-creating-an-API-gateway-and-AWS-lamda/03-API-specific-features-and-options/#api-specific-features-and-options","text":"Previously we created a resource and it's method. The resource defines a path in the url. One thing to note is that when we are working on these things, they aren't live. We need to deploy them under the actions. Only then we will able to see them on the web. The actual work happends under resources section. Then under stages section we can we can get information about the deployment. Under the Authorizers section we can add authentication to our APi. For certain resources that needs certain authentication, we can manage it here. The Models section, we can define the shape of the data we are working on. It is provided in JSON format. In the Documentation section we can create documentation for the API so people can know on how to use it. The Binary support section is used when we are sending files in the requests. The Dashboard shows shows stats about the API. The Resource Policy section is used to control how the API is accessed. We can whitelist and/or blacklist resources.","title":"API-specific features and options"},{"location":"AWS/AWS%20Serverless%20APIs/03-creating-an-API-gateway-and-AWS-lamda/04-request-response-cycle/","text":"Request-Response cycle \u00b6 When we previously created an endpoint, we saw this flowchart. It is represents the request-response cycle . We have a client that sends the request. Method request \u00b6 The Method Request depicts on how the request should actually look like. When clicking on it, we can modify multiple parameters like setting up authorization, add validation for query string, headers, body for fitting some schema. We can also require API key. Integration request \u00b6 In previous step on the cycle, we transformed data we want to use. Now we are going to trigger action we want to do with that data. Here we provide integration type - action type that will be triggered, as well ass the data we are going to extract from the request and pass it to the action. Integration reponse \u00b6 Allows us to configure what response we are going to send back. Method response \u00b6 Defines the shape of our response - which status codes, models and headers we are sending back.","title":"Request-Response cycle"},{"location":"AWS/AWS%20Serverless%20APIs/03-creating-an-API-gateway-and-AWS-lamda/04-request-response-cycle/#request-response-cycle","text":"When we previously created an endpoint, we saw this flowchart. It is represents the request-response cycle . We have a client that sends the request.","title":"Request-Response cycle"},{"location":"AWS/AWS%20Serverless%20APIs/03-creating-an-API-gateway-and-AWS-lamda/04-request-response-cycle/#method-request","text":"The Method Request depicts on how the request should actually look like. When clicking on it, we can modify multiple parameters like setting up authorization, add validation for query string, headers, body for fitting some schema. We can also require API key.","title":"Method request"},{"location":"AWS/AWS%20Serverless%20APIs/03-creating-an-API-gateway-and-AWS-lamda/04-request-response-cycle/#integration-request","text":"In previous step on the cycle, we transformed data we want to use. Now we are going to trigger action we want to do with that data. Here we provide integration type - action type that will be triggered, as well ass the data we are going to extract from the request and pass it to the action.","title":"Integration request"},{"location":"AWS/AWS%20Serverless%20APIs/03-creating-an-API-gateway-and-AWS-lamda/04-request-response-cycle/#integration-reponse","text":"Allows us to configure what response we are going to send back.","title":"Integration reponse"},{"location":"AWS/AWS%20Serverless%20APIs/03-creating-an-API-gateway-and-AWS-lamda/04-request-response-cycle/#method-response","text":"Defines the shape of our response - which status codes, models and headers we are sending back.","title":"Method response"},{"location":"AWS/AWS%20Serverless%20APIs/03-creating-an-API-gateway-and-AWS-lamda/05-creating-a-new-API/","text":"Creating a new API \u00b6 We are going to create a new API with a resource compare-yourself and 3 methods - POST , GET and DELETE . Creating an API \u00b6 When creating a new API we have multiple options on how to go about this: - We can create either a REST or WebSockets API. - We can create a new API, import an existing one, import it from a text file in swagger/open api format, create an preset example API. You can click on the example API to see on how the swagger format looks like. Creating resources \u00b6 Now we have to create the compare-yourself resource, we can do that under Action -> Create resource . Note that the resources are hierarchical and we need to make sure we have selected the right resource before we are doing this. Currently we have only root resource, so we are ok. You can define the resource name and path which can be different. The configure as proxy resource option means that this will be a catch-all resource that catches all methods and paths under it and pass it to other services. The CORS option is a security option. This option will allow you to access the API from a different server as well introduce OPTIONS endpoint that is sent to check endpoint availablility. Creating an HTTP method \u00b6 Now, while making sure we have selected the compare-yourself reource, we can click on Actions -> Create Method . Then you can choose the method type, we want to select POST . Then it will bring up setup wizard, where we can choose what integration type it has: Lambda function - run some code on demand HTTP - we can pass trough to another HTTP service Mock - return a dummy response AWS service - pass trough to another AWS service We want to select the lambda function . The option Use lambda proxy will take the incoming requests and it's headers, pass that as json object to the lambda function. You will need to extract what you need and return a response. Creating a lambda function \u00b6 Let's click on the Create a lambda Function link, which will bring us to the lambda console. Once we click on Create Function it will open up the lambda function. The designer section will list all the triggers that calls the function as well all the access permissions it has. By default it has no triggers and has access to write on cloudwatch logs. In the monitoring section we can see statistics, logs about the function. On the bottom you can see the function code, which is the code that is being run when executing the function. We can also set environment variables, tags as well as other settings on this page. Connecting lamda functions to API Gateway \u00b6 When we have created a lamda function, we are going to change the lambda region to other one and back to refresh the list, start writing the lamda function name, select the correct one. Now we can click on the test button which is over the client . This will open up a page where we can test the endpoint. Accessing the API from the WEB & Fixing CORS issues \u00b6 In order to access the API from the web, we need to deploy it, we can check this in the stages section. So, we are going to call Actions -> Deploy API . And this is going to create a new stage that gives us an URL, configuration for the stage Now, if we would request the POST method from our website, it would return a CORS error: To fix this, we are going to resurces -> post method -> method response and under Response headers for 200 add Access-Control-Allow-Origin . And then under Integration Response , we can provide it's value. It will be a wildcard in this case. Body Mapping templates \u00b6 Under Integration Request we can set Body Mapping templates . This will give us a configuration where we can configure on what data is sent to the lambda function as an event. For reference on how to use the template we can look up http://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-mapping-template-reference.html We can customize with something like this. { \"age\" : $input.json('$.personData.age'), } The $ refers to request body. We can also use Body mapping templates in the Integration Response as well. Using Models & Validating requests \u00b6 We can go to the Models section and create a model. When created, we can go to Method Request and add the model. This will ensure validating all the data that is set in the model according to it's schema. The json schema can be referenced from here: - https://json-schema.org/ - https://json-schema.org/understanding-json-schema/index.html Also now, if we go back to Body mapping templates , we can set model templates: The template can be modified as well Adding a DELETE method \u00b6 We are going to do the same steps as previously for creating a DELETE method and creating lambda function for it. Using path parameters \u00b6 We can create an new child resource with a variable type . Under that, we can create a get method with a lambda function assigned to it. And in the Body mapping templates we can put something like this: Accessing the API from the Web the right way \u00b6 When selecting a resource, we can go to Actions -> Enable CORS to setup CORS.","title":"Creating a new API"},{"location":"AWS/AWS%20Serverless%20APIs/03-creating-an-API-gateway-and-AWS-lamda/05-creating-a-new-API/#creating-a-new-api","text":"We are going to create a new API with a resource compare-yourself and 3 methods - POST , GET and DELETE .","title":"Creating a new API"},{"location":"AWS/AWS%20Serverless%20APIs/03-creating-an-API-gateway-and-AWS-lamda/05-creating-a-new-API/#creating-an-api","text":"When creating a new API we have multiple options on how to go about this: - We can create either a REST or WebSockets API. - We can create a new API, import an existing one, import it from a text file in swagger/open api format, create an preset example API. You can click on the example API to see on how the swagger format looks like.","title":"Creating an API"},{"location":"AWS/AWS%20Serverless%20APIs/03-creating-an-API-gateway-and-AWS-lamda/05-creating-a-new-API/#creating-resources","text":"Now we have to create the compare-yourself resource, we can do that under Action -> Create resource . Note that the resources are hierarchical and we need to make sure we have selected the right resource before we are doing this. Currently we have only root resource, so we are ok. You can define the resource name and path which can be different. The configure as proxy resource option means that this will be a catch-all resource that catches all methods and paths under it and pass it to other services. The CORS option is a security option. This option will allow you to access the API from a different server as well introduce OPTIONS endpoint that is sent to check endpoint availablility.","title":"Creating resources"},{"location":"AWS/AWS%20Serverless%20APIs/03-creating-an-API-gateway-and-AWS-lamda/05-creating-a-new-API/#creating-an-http-method","text":"Now, while making sure we have selected the compare-yourself reource, we can click on Actions -> Create Method . Then you can choose the method type, we want to select POST . Then it will bring up setup wizard, where we can choose what integration type it has: Lambda function - run some code on demand HTTP - we can pass trough to another HTTP service Mock - return a dummy response AWS service - pass trough to another AWS service We want to select the lambda function . The option Use lambda proxy will take the incoming requests and it's headers, pass that as json object to the lambda function. You will need to extract what you need and return a response.","title":"Creating an HTTP method"},{"location":"AWS/AWS%20Serverless%20APIs/03-creating-an-API-gateway-and-AWS-lamda/05-creating-a-new-API/#creating-a-lambda-function","text":"Let's click on the Create a lambda Function link, which will bring us to the lambda console. Once we click on Create Function it will open up the lambda function. The designer section will list all the triggers that calls the function as well all the access permissions it has. By default it has no triggers and has access to write on cloudwatch logs. In the monitoring section we can see statistics, logs about the function. On the bottom you can see the function code, which is the code that is being run when executing the function. We can also set environment variables, tags as well as other settings on this page.","title":"Creating a lambda function"},{"location":"AWS/AWS%20Serverless%20APIs/03-creating-an-API-gateway-and-AWS-lamda/05-creating-a-new-API/#connecting-lamda-functions-to-api-gateway","text":"When we have created a lamda function, we are going to change the lambda region to other one and back to refresh the list, start writing the lamda function name, select the correct one. Now we can click on the test button which is over the client . This will open up a page where we can test the endpoint.","title":"Connecting lamda functions to API Gateway"},{"location":"AWS/AWS%20Serverless%20APIs/03-creating-an-API-gateway-and-AWS-lamda/05-creating-a-new-API/#accessing-the-api-from-the-web-fixing-cors-issues","text":"In order to access the API from the web, we need to deploy it, we can check this in the stages section. So, we are going to call Actions -> Deploy API . And this is going to create a new stage that gives us an URL, configuration for the stage Now, if we would request the POST method from our website, it would return a CORS error: To fix this, we are going to resurces -> post method -> method response and under Response headers for 200 add Access-Control-Allow-Origin . And then under Integration Response , we can provide it's value. It will be a wildcard in this case.","title":"Accessing the API from the WEB &amp; Fixing CORS issues"},{"location":"AWS/AWS%20Serverless%20APIs/03-creating-an-API-gateway-and-AWS-lamda/05-creating-a-new-API/#body-mapping-templates","text":"Under Integration Request we can set Body Mapping templates . This will give us a configuration where we can configure on what data is sent to the lambda function as an event. For reference on how to use the template we can look up http://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-mapping-template-reference.html We can customize with something like this. { \"age\" : $input.json('$.personData.age'), } The $ refers to request body. We can also use Body mapping templates in the Integration Response as well.","title":"Body Mapping templates"},{"location":"AWS/AWS%20Serverless%20APIs/03-creating-an-API-gateway-and-AWS-lamda/05-creating-a-new-API/#using-models-validating-requests","text":"We can go to the Models section and create a model. When created, we can go to Method Request and add the model. This will ensure validating all the data that is set in the model according to it's schema. The json schema can be referenced from here: - https://json-schema.org/ - https://json-schema.org/understanding-json-schema/index.html Also now, if we go back to Body mapping templates , we can set model templates: The template can be modified as well","title":"Using Models &amp; Validating requests"},{"location":"AWS/AWS%20Serverless%20APIs/03-creating-an-API-gateway-and-AWS-lamda/05-creating-a-new-API/#adding-a-delete-method","text":"We are going to do the same steps as previously for creating a DELETE method and creating lambda function for it.","title":"Adding a DELETE method"},{"location":"AWS/AWS%20Serverless%20APIs/03-creating-an-API-gateway-and-AWS-lamda/05-creating-a-new-API/#using-path-parameters","text":"We can create an new child resource with a variable type . Under that, we can create a get method with a lambda function assigned to it. And in the Body mapping templates we can put something like this:","title":"Using path parameters"},{"location":"AWS/AWS%20Serverless%20APIs/03-creating-an-API-gateway-and-AWS-lamda/05-creating-a-new-API/#accessing-the-api-from-the-web-the-right-way","text":"When selecting a resource, we can go to Actions -> Enable CORS to setup CORS.","title":"Accessing the API from the Web the right way"},{"location":"AWS/AWS%20Serverless%20APIs/03-creating-an-API-gateway-and-AWS-lamda/06-what-is-aws-lamda/","text":"What is AWS lamda? \u00b6 AWS lamda is an AWS service that runs code on demand based on some event like: S3 (file gets uploaded) CloudWatch (scheduled event) API Gateway (HTTP request) etc When event is triggered, we run the code and return a result, interact with other services. Accessing logs \u00b6 We can see logs like console.log output in node.js by going to CloudWatch -> Logs and selecting our lamda function.","title":"What is AWS lamda?"},{"location":"AWS/AWS%20Serverless%20APIs/03-creating-an-API-gateway-and-AWS-lamda/06-what-is-aws-lamda/#what-is-aws-lamda","text":"AWS lamda is an AWS service that runs code on demand based on some event like: S3 (file gets uploaded) CloudWatch (scheduled event) API Gateway (HTTP request) etc When event is triggered, we run the code and return a result, interact with other services.","title":"What is AWS lamda?"},{"location":"AWS/AWS%20Serverless%20APIs/03-creating-an-API-gateway-and-AWS-lamda/06-what-is-aws-lamda/#accessing-logs","text":"We can see logs like console.log output in node.js by going to CloudWatch -> Logs and selecting our lamda function.","title":"Accessing logs"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb/01-what-is-dynamodb/","text":"What is DynamoDB \u00b6 DynamoDB is a fully managed noSQL database provided by AWS. There are no relations in the database. The data format for this is key-value pairs, without a schema. Overview of the service is available at https://aws.amazon.com/dynamodb/ . As well as the documentation at https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html .","title":"What is DynamoDB"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb/01-what-is-dynamodb/#what-is-dynamodb","text":"DynamoDB is a fully managed noSQL database provided by AWS. There are no relations in the database. The data format for this is key-value pairs, without a schema. Overview of the service is available at https://aws.amazon.com/dynamodb/ . As well as the documentation at https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html .","title":"What is DynamoDB"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb/02-how-dynamodb-organizes-data/","text":"How DynamoDB organizes data \u00b6 In our DynamoDB table, we are always required to provide a partition key on each item, which has to be unique. This key is related to a way dynamodb stores data in paritions. You can then set up a primary key which can be used for querying data. As well as up to 5 indices for a table. Adding multiple databases \u00b6","title":"How DynamoDB organizes data"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb/02-how-dynamodb-organizes-data/#how-dynamodb-organizes-data","text":"In our DynamoDB table, we are always required to provide a partition key on each item, which has to be unique. This key is related to a way dynamodb stores data in paritions. You can then set up a primary key which can be used for querying data. As well as up to 5 indices for a table.","title":"How DynamoDB organizes data"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb/02-how-dynamodb-organizes-data/#adding-multiple-databases","text":"","title":"Adding multiple databases"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb/03-creating-a-table-in-dynamodb/","text":"Creating a table in DynamoDB \u00b6 In AWS services we can search for DynamoDB and open it up. Then simply click on creating a table. Under settings we can set indices and other settings as well. When creating is finished, it will open up a dashboard for the table.","title":"Creating a table in DynamoDB"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb/03-creating-a-table-in-dynamodb/#creating-a-table-in-dynamodb","text":"In AWS services we can search for DynamoDB and open it up. Then simply click on creating a table. Under settings we can set indices and other settings as well. When creating is finished, it will open up a dashboard for the table.","title":"Creating a table in DynamoDB"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb/04-creating-and-scanning-items/","text":"Creating and scanning items \u00b6 Under the Items tab we can see and create new items. Then we can view the item and scan them by using additional filters.","title":"Creating and scanning items"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb/04-creating-and-scanning-items/#creating-and-scanning-items","text":"Under the Items tab we can see and create new items. Then we can view the item and scan them by using additional filters.","title":"Creating and scanning items"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb/05-accessing-dynamodb-from-lambda/","text":"Accessing DynamoDB from Lambda \u00b6 We can open up a lambda function and use aws-sdk integrated DynamoDB class to connect to it. const AWS = require('aws-sdk'); const dynamodb = new AWS.DynamoDB({ region: 'eu-central-1', apiVersion: '2012-08-10' }); Setting roles \u00b6 We need to go to IAM -> Roles and search for our lambda function. Attach dynamoDB policy. Setting up lambda function \u00b6 We then can put logic to post logic. const AWS = require('aws-sdk'); const dynamodb = new AWS.DynamoDB({ region: 'eu-central-1', apiVersion: '2012-08-10' }); exports.fn = (event, context, callback) => { const params = { Item: { \"UserId\": { S: \"user_\" + Math.random() }, \"Age\": { N: \"\" + event.age }, \"Height\": { N: \"\" + event.height }, \"Income\": { N: \"\" + event.income } }, TableName: \"compare-yourself\" } dynamodb.putItem(params, function(err, data) { if(err) { console.log(err) callback() } else { console.log(data) callback() } }); } Hit on save and trigger a test. And we can check the dynamoDB, the item has been added","title":"Accessing DynamoDB from Lambda"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb/05-accessing-dynamodb-from-lambda/#accessing-dynamodb-from-lambda","text":"We can open up a lambda function and use aws-sdk integrated DynamoDB class to connect to it. const AWS = require('aws-sdk'); const dynamodb = new AWS.DynamoDB({ region: 'eu-central-1', apiVersion: '2012-08-10' });","title":"Accessing DynamoDB from Lambda"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb/05-accessing-dynamodb-from-lambda/#setting-roles","text":"We need to go to IAM -> Roles and search for our lambda function. Attach dynamoDB policy.","title":"Setting roles"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb/05-accessing-dynamodb-from-lambda/#setting-up-lambda-function","text":"We then can put logic to post logic. const AWS = require('aws-sdk'); const dynamodb = new AWS.DynamoDB({ region: 'eu-central-1', apiVersion: '2012-08-10' }); exports.fn = (event, context, callback) => { const params = { Item: { \"UserId\": { S: \"user_\" + Math.random() }, \"Age\": { N: \"\" + event.age }, \"Height\": { N: \"\" + event.height }, \"Income\": { N: \"\" + event.income } }, TableName: \"compare-yourself\" } dynamodb.putItem(params, function(err, data) { if(err) { console.log(err) callback() } else { console.log(data) callback() } }); } Hit on save and trigger a test. And we can check the dynamoDB, the item has been added","title":"Setting up lambda function"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb/06-scanning-data-in-dynamodb-from-lambda/","text":"Scanning data in DynamoDB from lambda \u00b6 We can do the same IAM steps from the last section and configure the ct-get-data lambda function: const AWS = require('aws-sdk'); const dynamodb = new AWS.DynamoDB({ region: 'eu-central-1', apiVersion: '2012-08-10' }); exports.fn = (event, context, callback) => { const type = event.type; if (type === 'all') { const params = { TableName: 'compare-myself' } dynamodb.scan(params, function(err, data) { if (err) { console.log(err) callback(err) } else { console.log(data) const items = data.Items.map((dataField) => { return { age: +dataField.Age.N, height: +dataField.Height.N, income: +dataField.Income.N } }); callback(null, items) } }) callback(null, '') } else if (type === 'single') { const params = { Key: { \"UserId\": { S: \"user_0.06768033925361383\" } }, TableName: 'compare-myself' } dynamodb.getItem(params, function(err, data) { if (err) { console.log(err); callback(err); } else { console.log(data); callback(null, [{ age: +data.Age.N, height: +data.Height.N, income: +data.Income.N }]) } }) } else { callback('Something went wrong!') } }","title":"Scanning data in DynamoDB from lambda"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb/06-scanning-data-in-dynamodb-from-lambda/#scanning-data-in-dynamodb-from-lambda","text":"We can do the same IAM steps from the last section and configure the ct-get-data lambda function: const AWS = require('aws-sdk'); const dynamodb = new AWS.DynamoDB({ region: 'eu-central-1', apiVersion: '2012-08-10' }); exports.fn = (event, context, callback) => { const type = event.type; if (type === 'all') { const params = { TableName: 'compare-myself' } dynamodb.scan(params, function(err, data) { if (err) { console.log(err) callback(err) } else { console.log(data) const items = data.Items.map((dataField) => { return { age: +dataField.Age.N, height: +dataField.Height.N, income: +dataField.Income.N } }); callback(null, items) } }) callback(null, '') } else if (type === 'single') { const params = { Key: { \"UserId\": { S: \"user_0.06768033925361383\" } }, TableName: 'compare-myself' } dynamodb.getItem(params, function(err, data) { if (err) { console.log(err); callback(err); } else { console.log(data); callback(null, [{ age: +data.Age.N, height: +data.Height.N, income: +data.Income.N }]) } }) } else { callback('Something went wrong!') } }","title":"Scanning data in DynamoDB from lambda"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb/07-deleting-data-from-lambda/","text":"Deleting data in DynamoDB from lambda \u00b6 We are going to set everything up like the previous sections and set up code for the cy-delete-data like so: const AWS = require('aws-sdk'); const dynamodb = new AWS.DynamoDB({ region: 'eu-central-1', apiVersion: '2012-08-10' }); exports.fn = (event, context, callback) => { const params = { Key: { \"UserId\": { S: \"user_0.06768033925361383\" } }, TableName: 'compare-myself' } dynamodb.deleteItem(params, function(err, data) { if (err) { console.log(err); callback(err); } else { console.log(data); callback(null, data) } }) }","title":"Deleting data in DynamoDB from lambda"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb/07-deleting-data-from-lambda/#deleting-data-in-dynamodb-from-lambda","text":"We are going to set everything up like the previous sections and set up code for the cy-delete-data like so: const AWS = require('aws-sdk'); const dynamodb = new AWS.DynamoDB({ region: 'eu-central-1', apiVersion: '2012-08-10' }); exports.fn = (event, context, callback) => { const params = { Key: { \"UserId\": { S: \"user_0.06768033925361383\" } }, TableName: 'compare-myself' } dynamodb.deleteItem(params, function(err, data) { if (err) { console.log(err); callback(err); } else { console.log(data); callback(null, data) } }) }","title":"Deleting data in DynamoDB from lambda"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb/08-mapping-responses/","text":"Mapping responses \u00b6 Wea are going to make a new Model CompareDataArray in the API Gateway like so: { \"$schema\": \"http://json-schema.org/draft-04/schema#\", \"title\": \"CompareData\", \"type\": \"array\", \"items\": { \"type\": \"object\", \"properties\": { \"age\": {\"type\": \"integer\"}, \"height\": {\"type\": \"integer\"}, \"income\": {\"type\": \"integer\"} }, \"required\": [\"age\", \"height\", \"income\"] } } And then going to get method's Integration Response -> Mapping Templates . #set($inputRoot = $input.path('$')) [ ##TODO: Update this foreach loop to reference array from input json #foreach($elem in $inputRoot.TODO) { \"age\" : $elem.age, \"height\" : $elem.height, \"income\" : $elem.income } #if($foreach.hasNext),#end #end ]","title":"Mapping responses"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb/08-mapping-responses/#mapping-responses","text":"Wea are going to make a new Model CompareDataArray in the API Gateway like so: { \"$schema\": \"http://json-schema.org/draft-04/schema#\", \"title\": \"CompareData\", \"type\": \"array\", \"items\": { \"type\": \"object\", \"properties\": { \"age\": {\"type\": \"integer\"}, \"height\": {\"type\": \"integer\"}, \"income\": {\"type\": \"integer\"} }, \"required\": [\"age\", \"height\", \"income\"] } } And then going to get method's Integration Response -> Mapping Templates . #set($inputRoot = $input.path('$')) [ ##TODO: Update this foreach loop to reference array from input json #foreach($elem in $inputRoot.TODO) { \"age\" : $elem.age, \"height\" : $elem.height, \"income\" : $elem.income } #if($foreach.hasNext),#end #end ]","title":"Mapping responses"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb%202/01-what-is-dynamodb/","text":"What is DynamoDB \u00b6 DynamoDB is a fully managed noSQL database provided by AWS. There are no relations in the database. The data format for this is key-value pairs, without a schema. Overview of the service is available at https://aws.amazon.com/dynamodb/ . As well as the documentation at https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html .","title":"What is DynamoDB"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb%202/01-what-is-dynamodb/#what-is-dynamodb","text":"DynamoDB is a fully managed noSQL database provided by AWS. There are no relations in the database. The data format for this is key-value pairs, without a schema. Overview of the service is available at https://aws.amazon.com/dynamodb/ . As well as the documentation at https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html .","title":"What is DynamoDB"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb%202/02-how-dynamodb-organizes-data/","text":"How DynamoDB organizes data \u00b6 In our DynamoDB table, we are always required to provide a partition key on each item, which has to be unique. This key is related to a way dynamodb stores data in paritions. You can then set up a primary key which can be used for querying data. As well as up to 5 indices for a table. Adding multiple databases \u00b6","title":"How DynamoDB organizes data"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb%202/02-how-dynamodb-organizes-data/#how-dynamodb-organizes-data","text":"In our DynamoDB table, we are always required to provide a partition key on each item, which has to be unique. This key is related to a way dynamodb stores data in paritions. You can then set up a primary key which can be used for querying data. As well as up to 5 indices for a table.","title":"How DynamoDB organizes data"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb%202/02-how-dynamodb-organizes-data/#adding-multiple-databases","text":"","title":"Adding multiple databases"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb%202/03-creating-a-table-in-dynamodb/","text":"Creating a table in DynamoDB \u00b6 In AWS services we can search for DynamoDB and open it up. Then simply click on creating a table. Under settings we can set indices and other settings as well. When creating is finished, it will open up a dashboard for the table.","title":"Creating a table in DynamoDB"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb%202/03-creating-a-table-in-dynamodb/#creating-a-table-in-dynamodb","text":"In AWS services we can search for DynamoDB and open it up. Then simply click on creating a table. Under settings we can set indices and other settings as well. When creating is finished, it will open up a dashboard for the table.","title":"Creating a table in DynamoDB"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb%202/04-creating-and-scanning-items/","text":"Creating and scanning items \u00b6 Under the Items tab we can see and create new items. Then we can view the item and scan them by using additional filters.","title":"Creating and scanning items"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb%202/04-creating-and-scanning-items/#creating-and-scanning-items","text":"Under the Items tab we can see and create new items. Then we can view the item and scan them by using additional filters.","title":"Creating and scanning items"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb%202/05-accessing-dynamodb-from-lambda/","text":"Accessing DynamoDB from Lambda \u00b6 We can open up a lambda function and use aws-sdk integrated DynamoDB class to connect to it. const AWS = require('aws-sdk'); const dynamodb = new AWS.DynamoDB({ region: 'eu-central-1', apiVersion: '2012-08-10' }); Setting roles \u00b6 We need to go to IAM -> Roles and search for our lambda function. Attach dynamoDB policy. Setting up lambda function \u00b6 We then can put logic to post logic. const AWS = require('aws-sdk'); const dynamodb = new AWS.DynamoDB({ region: 'eu-central-1', apiVersion: '2012-08-10' }); exports.fn = (event, context, callback) => { const params = { Item: { \"UserId\": { S: \"user_\" + Math.random() }, \"Age\": { N: \"\" + event.age }, \"Height\": { N: \"\" + event.height }, \"Income\": { N: \"\" + event.income } }, TableName: \"compare-yourself\" } dynamodb.putItem(params, function(err, data) { if(err) { console.log(err) callback() } else { console.log(data) callback() } }); } Hit on save and trigger a test. And we can check the dynamoDB, the item has been added","title":"Accessing DynamoDB from Lambda"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb%202/05-accessing-dynamodb-from-lambda/#accessing-dynamodb-from-lambda","text":"We can open up a lambda function and use aws-sdk integrated DynamoDB class to connect to it. const AWS = require('aws-sdk'); const dynamodb = new AWS.DynamoDB({ region: 'eu-central-1', apiVersion: '2012-08-10' });","title":"Accessing DynamoDB from Lambda"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb%202/05-accessing-dynamodb-from-lambda/#setting-roles","text":"We need to go to IAM -> Roles and search for our lambda function. Attach dynamoDB policy.","title":"Setting roles"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb%202/05-accessing-dynamodb-from-lambda/#setting-up-lambda-function","text":"We then can put logic to post logic. const AWS = require('aws-sdk'); const dynamodb = new AWS.DynamoDB({ region: 'eu-central-1', apiVersion: '2012-08-10' }); exports.fn = (event, context, callback) => { const params = { Item: { \"UserId\": { S: \"user_\" + Math.random() }, \"Age\": { N: \"\" + event.age }, \"Height\": { N: \"\" + event.height }, \"Income\": { N: \"\" + event.income } }, TableName: \"compare-yourself\" } dynamodb.putItem(params, function(err, data) { if(err) { console.log(err) callback() } else { console.log(data) callback() } }); } Hit on save and trigger a test. And we can check the dynamoDB, the item has been added","title":"Setting up lambda function"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb%202/06-scanning-data-in-dynamodb-from-lambda/","text":"Scanning data in DynamoDB from lambda \u00b6 We can do the same IAM steps from the last section and configure the ct-get-data lambda function: const AWS = require('aws-sdk'); const dynamodb = new AWS.DynamoDB({ region: 'eu-central-1', apiVersion: '2012-08-10' }); exports.fn = (event, context, callback) => { const type = event.type; if (type === 'all') { const params = { TableName: 'compare-myself' } dynamodb.scan(params, function(err, data) { if (err) { console.log(err) callback(err) } else { console.log(data) const items = data.Items.map((dataField) => { return { age: +dataField.Age.N, height: +dataField.Height.N, income: +dataField.Income.N } }); callback(null, items) } }) callback(null, '') } else if (type === 'single') { const params = { Key: { \"UserId\": { S: \"user_0.06768033925361383\" } }, TableName: 'compare-myself' } dynamodb.getItem(params, function(err, data) { if (err) { console.log(err); callback(err); } else { console.log(data); callback(null, [{ age: +data.Age.N, height: +data.Height.N, income: +data.Income.N }]) } }) } else { callback('Something went wrong!') } }","title":"Scanning data in DynamoDB from lambda"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb%202/06-scanning-data-in-dynamodb-from-lambda/#scanning-data-in-dynamodb-from-lambda","text":"We can do the same IAM steps from the last section and configure the ct-get-data lambda function: const AWS = require('aws-sdk'); const dynamodb = new AWS.DynamoDB({ region: 'eu-central-1', apiVersion: '2012-08-10' }); exports.fn = (event, context, callback) => { const type = event.type; if (type === 'all') { const params = { TableName: 'compare-myself' } dynamodb.scan(params, function(err, data) { if (err) { console.log(err) callback(err) } else { console.log(data) const items = data.Items.map((dataField) => { return { age: +dataField.Age.N, height: +dataField.Height.N, income: +dataField.Income.N } }); callback(null, items) } }) callback(null, '') } else if (type === 'single') { const params = { Key: { \"UserId\": { S: \"user_0.06768033925361383\" } }, TableName: 'compare-myself' } dynamodb.getItem(params, function(err, data) { if (err) { console.log(err); callback(err); } else { console.log(data); callback(null, [{ age: +data.Age.N, height: +data.Height.N, income: +data.Income.N }]) } }) } else { callback('Something went wrong!') } }","title":"Scanning data in DynamoDB from lambda"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb%202/07-deleting-data-from-lambda/","text":"Deleting data in DynamoDB from lambda \u00b6 We are going to set everything up like the previous sections and set up code for the cy-delete-data like so: const AWS = require('aws-sdk'); const dynamodb = new AWS.DynamoDB({ region: 'eu-central-1', apiVersion: '2012-08-10' }); exports.fn = (event, context, callback) => { const params = { Key: { \"UserId\": { S: \"user_0.06768033925361383\" } }, TableName: 'compare-myself' } dynamodb.deleteItem(params, function(err, data) { if (err) { console.log(err); callback(err); } else { console.log(data); callback(null, data) } }) }","title":"Deleting data in DynamoDB from lambda"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb%202/07-deleting-data-from-lambda/#deleting-data-in-dynamodb-from-lambda","text":"We are going to set everything up like the previous sections and set up code for the cy-delete-data like so: const AWS = require('aws-sdk'); const dynamodb = new AWS.DynamoDB({ region: 'eu-central-1', apiVersion: '2012-08-10' }); exports.fn = (event, context, callback) => { const params = { Key: { \"UserId\": { S: \"user_0.06768033925361383\" } }, TableName: 'compare-myself' } dynamodb.deleteItem(params, function(err, data) { if (err) { console.log(err); callback(err); } else { console.log(data); callback(null, data) } }) }","title":"Deleting data in DynamoDB from lambda"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb%202/08-mapping-responses/","text":"Mapping responses \u00b6 Wea are going to make a new Model CompareDataArray in the API Gateway like so: { \"$schema\": \"http://json-schema.org/draft-04/schema#\", \"title\": \"CompareData\", \"type\": \"array\", \"items\": { \"type\": \"object\", \"properties\": { \"age\": {\"type\": \"integer\"}, \"height\": {\"type\": \"integer\"}, \"income\": {\"type\": \"integer\"} }, \"required\": [\"age\", \"height\", \"income\"] } } And then going to get method's Integration Response -> Mapping Templates . #set($inputRoot = $input.path('$')) [ ##TODO: Update this foreach loop to reference array from input json #foreach($elem in $inputRoot.TODO) { \"age\" : $elem.age, \"height\" : $elem.height, \"income\" : $elem.income } #if($foreach.hasNext),#end #end ]","title":"Mapping responses"},{"location":"AWS/AWS%20Serverless%20APIs/04-data-storage-with-dynamodb%202/08-mapping-responses/#mapping-responses","text":"Wea are going to make a new Model CompareDataArray in the API Gateway like so: { \"$schema\": \"http://json-schema.org/draft-04/schema#\", \"title\": \"CompareData\", \"type\": \"array\", \"items\": { \"type\": \"object\", \"properties\": { \"age\": {\"type\": \"integer\"}, \"height\": {\"type\": \"integer\"}, \"income\": {\"type\": \"integer\"} }, \"required\": [\"age\", \"height\", \"income\"] } } And then going to get method's Integration Response -> Mapping Templates . #set($inputRoot = $input.path('$')) [ ##TODO: Update this foreach loop to reference array from input json #foreach($elem in $inputRoot.TODO) { \"age\" : $elem.age, \"height\" : $elem.height, \"income\" : $elem.income } #if($foreach.hasNext),#end #end ]","title":"Mapping responses"},{"location":"AWS/AWS%20Serverless%20APIs/05-authenticating-users-with-cognito-and-api-gateway/01-how-to-add-authorization-to-api-gateway/","text":"How to add authorization to API Gateway \u00b6 We can add an authorizer in the Authorizers section, this will call a custom lambda or cognito function. For this we are going to create a new lambda function cy-custom-auth . exports.handler = (event, context, callback) => { const token = event.authorizationToken; if (token === 'allow') { callback(null, { principalId: 'afwfwa332afwe', policyDocument: genPolicy('Allow', event.methodArn), context: { simpleAuth: true } }) } else if (token == 'deny') { callback(null, { principalId: 'afwfwa332afwe', policyDocument: genPolicy('Deny', event.methodArn), context: { simpleAuth: true } }) } else { callback('Unauthorized'); } }; function genPolicy(effect, resource) { return { Version: '2012-10-17', Statement: [{ Action: 'execute-api:Invoke', Effect: effect, Resource: resource, }] } } Now we can create an ew Authorizer. Now we can click on test and enter allow or deny since that's what we're looking for in the code.","title":"How to add authorization to API Gateway"},{"location":"AWS/AWS%20Serverless%20APIs/05-authenticating-users-with-cognito-and-api-gateway/01-how-to-add-authorization-to-api-gateway/#how-to-add-authorization-to-api-gateway","text":"We can add an authorizer in the Authorizers section, this will call a custom lambda or cognito function. For this we are going to create a new lambda function cy-custom-auth . exports.handler = (event, context, callback) => { const token = event.authorizationToken; if (token === 'allow') { callback(null, { principalId: 'afwfwa332afwe', policyDocument: genPolicy('Allow', event.methodArn), context: { simpleAuth: true } }) } else if (token == 'deny') { callback(null, { principalId: 'afwfwa332afwe', policyDocument: genPolicy('Deny', event.methodArn), context: { simpleAuth: true } }) } else { callback('Unauthorized'); } }; function genPolicy(effect, resource) { return { Version: '2012-10-17', Statement: [{ Action: 'execute-api:Invoke', Effect: effect, Resource: resource, }] } } Now we can create an ew Authorizer. Now we can click on test and enter allow or deny since that's what we're looking for in the code.","title":"How to add authorization to API Gateway"},{"location":"AWS/AWS%20Serverless%20APIs/05-authenticating-users-with-cognito-and-api-gateway/02-retrieving-users-from-custom-authorizers/","text":"Retrieving users from custom authorizers \u00b6 We can modify the API's POS mapping template to retrieve the user from custom authorizer. #set($inputRoot = $input.path('$')) { \"age\" : \"$inputRoot.age\", \"height\" : \"$inputRoot.height\", \"income\" : \"$inputRoot.income\", \"userId\" : \"$context.authorizer.principalId\", } Then we can go back to the POST lambda file and change the hardcoded user id from: S: \"user_\" + Math.random() to: S: event.userId","title":"Retrieving users from custom authorizers"},{"location":"AWS/AWS%20Serverless%20APIs/05-authenticating-users-with-cognito-and-api-gateway/02-retrieving-users-from-custom-authorizers/#retrieving-users-from-custom-authorizers","text":"We can modify the API's POS mapping template to retrieve the user from custom authorizer. #set($inputRoot = $input.path('$')) { \"age\" : \"$inputRoot.age\", \"height\" : \"$inputRoot.height\", \"income\" : \"$inputRoot.income\", \"userId\" : \"$context.authorizer.principalId\", } Then we can go back to the POST lambda file and change the hardcoded user id from: S: \"user_\" + Math.random() to: S: event.userId","title":"Retrieving users from custom authorizers"},{"location":"AWS/AWS%20Serverless%20APIs/05-authenticating-users-with-cognito-and-api-gateway/04-creating-a-cognito-user-pool/","text":"Creating a cognito user pool \u00b6 We can go to AWS Console -> Cognito -> Manage User Pools and click on Create a user pool . Then you can enter a name and choose wether you want to choose all the default settings or tune them yourself.","title":"Creating a cognito user pool"},{"location":"AWS/AWS%20Serverless%20APIs/05-authenticating-users-with-cognito-and-api-gateway/04-creating-a-cognito-user-pool/#creating-a-cognito-user-pool","text":"We can go to AWS Console -> Cognito -> Manage User Pools and click on Create a user pool . Then you can enter a name and choose wether you want to choose all the default settings or tune them yourself.","title":"Creating a cognito user pool"},{"location":"AWS/AWS%20Serverless%20APIs/05-authenticating-users-with-cognito-and-api-gateway/05-using-cognito-in-apps/","text":"Using Cognito in Apps \u00b6 You can use Cognito in iOS, Android and Web iOS SDK Documentation: http://docs.aws.amazon.com/cognito/latest/developerguide/walkthrough-using-the-ios-sdk.html Android SDK Documentation: http://docs.aws.amazon.com/cognito/latest/developerguide/setting-up-android-sdk.html JS SDK: https://github.com/aws/aws-amplify/tree/master/packages/amazon-cognito-identity-js","title":"Using Cognito in Apps"},{"location":"AWS/AWS%20Serverless%20APIs/05-authenticating-users-with-cognito-and-api-gateway/05-using-cognito-in-apps/#using-cognito-in-apps","text":"You can use Cognito in iOS, Android and Web iOS SDK Documentation: http://docs.aws.amazon.com/cognito/latest/developerguide/walkthrough-using-the-ios-sdk.html Android SDK Documentation: http://docs.aws.amazon.com/cognito/latest/developerguide/setting-up-android-sdk.html JS SDK: https://github.com/aws/aws-amplify/tree/master/packages/amazon-cognito-identity-js","title":"Using Cognito in Apps"},{"location":"AWS/AWS%20Serverless%20APIs/05-authenticating-users-with-cognito-and-api-gateway/06-use-cognito-to-authorize-endpoints/","text":"Use Cognito to authorize endpoints \u00b6 We can go to API Gateway and and create a new authorizer with Cognito. And now we can modify the endpoints to use cognito authorizer","title":"Use Cognito to authorize endpoints"},{"location":"AWS/AWS%20Serverless%20APIs/05-authenticating-users-with-cognito-and-api-gateway/06-use-cognito-to-authorize-endpoints/#use-cognito-to-authorize-endpoints","text":"We can go to API Gateway and and create a new authorizer with Cognito. And now we can modify the endpoints to use cognito authorizer","title":"Use Cognito to authorize endpoints"},{"location":"AWS/AWS%20Serverless%20APIs/05-authenticating-users-with-cognito-and-api-gateway/AWS%20Cognito/","text":"What is Cognito \u00b6 It is usefull when we want to add authentication to our web or mobile app. We can define how to authenticate users, store auth token on user devices. You can set up cognito user pools as well as add third-party providers like facebook, google auth. Overview: https://aws.amazon.com/cognito/ Documentation: http://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html Cognito user pools and federated identities \u00b6 User pools are solution for authentication process that is we don't want to add processes like auth with facebook or google. Federated identities will allow to connect with Facebook, Google to generate temporary IAM credentials. Cognito Auth Flow \u00b6","title":"What is Cognito"},{"location":"AWS/AWS%20Serverless%20APIs/05-authenticating-users-with-cognito-and-api-gateway/AWS%20Cognito/#what-is-cognito","text":"It is usefull when we want to add authentication to our web or mobile app. We can define how to authenticate users, store auth token on user devices. You can set up cognito user pools as well as add third-party providers like facebook, google auth. Overview: https://aws.amazon.com/cognito/ Documentation: http://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html","title":"What is Cognito"},{"location":"AWS/AWS%20Serverless%20APIs/05-authenticating-users-with-cognito-and-api-gateway/AWS%20Cognito/#cognito-user-pools-and-federated-identities","text":"User pools are solution for authentication process that is we don't want to add processes like auth with facebook or google. Federated identities will allow to connect with Facebook, Google to generate temporary IAM credentials.","title":"Cognito user pools and federated identities"},{"location":"AWS/AWS%20Serverless%20APIs/05-authenticating-users-with-cognito-and-api-gateway/AWS%20Cognito/#cognito-auth-flow","text":"","title":"Cognito Auth Flow"},{"location":"AWS/AWS%20Serverless%20APIs/06-hosting-a-serverless-single-page-application/01-what-is-s3/","text":"What is S3 \u00b6 S3 is a file storage service where you can store all kinds of files. You organize these files in buckets. There can be specific access controls. AWS S3 Overview: https://aws.amazon.com/s3/ AWS S3 Developer Guide: ^http://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html AWS S3 Permissions: http://docs.aws.amazon.com/AmazonS3/latest/dev/s3-access-control.html AWS S3 Static Website Hosting: http://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html AWS S3 Pricing: https://aws.amazon.com/s3/pricing/","title":"What is S3"},{"location":"AWS/AWS%20Serverless%20APIs/06-hosting-a-serverless-single-page-application/01-what-is-s3/#what-is-s3","text":"S3 is a file storage service where you can store all kinds of files. You organize these files in buckets. There can be specific access controls. AWS S3 Overview: https://aws.amazon.com/s3/ AWS S3 Developer Guide: ^http://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html AWS S3 Permissions: http://docs.aws.amazon.com/AmazonS3/latest/dev/s3-access-control.html AWS S3 Static Website Hosting: http://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html AWS S3 Pricing: https://aws.amazon.com/s3/pricing/","title":"What is S3"},{"location":"AWS/AWS%20Serverless%20APIs/06-hosting-a-serverless-single-page-application/02-creating-an-s3-bucket/","text":"Creating an S3 bucket \u00b6 We can go to AWS CONSOLE -> S3 -> Create bucket . We can configure the bucket in next steps like setting up: - Versioning - Access logging - Tags - Encryption - Permissions When done, we have created a bucket where we can upload files to:","title":"Creating an S3 bucket"},{"location":"AWS/AWS%20Serverless%20APIs/06-hosting-a-serverless-single-page-application/02-creating-an-s3-bucket/#creating-an-s3-bucket","text":"We can go to AWS CONSOLE -> S3 -> Create bucket . We can configure the bucket in next steps like setting up: - Versioning - Access logging - Tags - Encryption - Permissions When done, we have created a bucket where we can upload files to:","title":"Creating an S3 bucket"},{"location":"AWS/AWS%20Serverless%20APIs/06-hosting-a-serverless-single-page-application/03-uploading-web-app-to-s3/","text":"Uploading the web app to S3 \u00b6 First we are going to build the project cd project npm i npm run build When done you should have a dist folder with production built app. Now, in S3 we can click on upload, select all the files and drag them over.","title":"Uploading the web app to S3"},{"location":"AWS/AWS%20Serverless%20APIs/06-hosting-a-serverless-single-page-application/03-uploading-web-app-to-s3/#uploading-the-web-app-to-s3","text":"First we are going to build the project cd project npm i npm run build When done you should have a dist folder with production built app. Now, in S3 we can click on upload, select all the files and drag them over.","title":"Uploading the web app to S3"},{"location":"AWS/AWS%20Serverless%20APIs/06-hosting-a-serverless-single-page-application/04-tuning-s3-into-a-static-web-server/","text":"Turning s3 into a static web server \u00b6 You can go to permissions and clicking on bucket policy in s3 bucket and grant read-only permissions to an anonymous users. { \"Version\":\"2012-10-17\", \"Statement\":[ { \"Sid\":\"AddPerm\", \"Effect\":\"Allow\", \"Principal\": \"*\", \"Action\":[\"s3:GetObject\"], \"Resource\":[\"arn:aws:s3:::com.deiveris.compare-yourself/*\"] } ] } Now we can go to Properties and enable static website hosting. Now, if we open up the url provided after saving, we can see the web app","title":"Turning s3 into a static web server"},{"location":"AWS/AWS%20Serverless%20APIs/06-hosting-a-serverless-single-page-application/04-tuning-s3-into-a-static-web-server/#turning-s3-into-a-static-web-server","text":"You can go to permissions and clicking on bucket policy in s3 bucket and grant read-only permissions to an anonymous users. { \"Version\":\"2012-10-17\", \"Statement\":[ { \"Sid\":\"AddPerm\", \"Effect\":\"Allow\", \"Principal\": \"*\", \"Action\":[\"s3:GetObject\"], \"Resource\":[\"arn:aws:s3:::com.deiveris.compare-yourself/*\"] } ] } Now we can go to Properties and enable static website hosting. Now, if we open up the url provided after saving, we can see the web app","title":"Turning s3 into a static web server"},{"location":"AWS/AWS%20Serverless%20APIs/06-hosting-a-serverless-single-page-application/05-setting-up-logging/","text":"Setting up logging \u00b6 We can create a new bucket like com.deiveris.compare-yourself.logs and then going to com.deiveris.compare-yourself bucket, selecting Properties -> Server access logging . Now the static webserver logs will be written into the other bucket.","title":"Setting up logging"},{"location":"AWS/AWS%20Serverless%20APIs/06-hosting-a-serverless-single-page-application/05-setting-up-logging/#setting-up-logging","text":"We can create a new bucket like com.deiveris.compare-yourself.logs and then going to com.deiveris.compare-yourself bucket, selecting Properties -> Server access logging . Now the static webserver logs will be written into the other bucket.","title":"Setting up logging"},{"location":"AWS/AWS%20Serverless%20APIs/06-hosting-a-serverless-single-page-application/06-optimize-content-delivery-with-cloudfront/","text":"Optimizing content delivery with CloudFront \u00b6 Currently our static website files are located in eu-central-1, but if a user accesses from US, it might take a while while everything is loaded. We can use CloudFront to cache these files on CDN and deliver to the user from nearest location. AWS CloudFront Overview: https://aws.amazon.com/cloudfront/ AWS CloudFront Developer Guides: http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html AWS CloudFront Pricing: https://aws.amazon.com/cloudfront/pricing/ Creating a distribution on CloudFront \u00b6 We can go to AWS Console -> Cloudfront -> Create distribution , most likely we'll want to select web as the protocol. Then we need to set up things. In the Origin Domain Name we want to set up our s3 bucket that is used for static webserver. The other settings we might to look at - maximum TTL, forward cookies, compress objects automatically. In the Distribution settings setup correct price class, defailt root object as index.html and enable the cookie logging to the previously created log bucket. Now, when clicking on Create Distribution it will create it and will take some time to sync it across all the locations.","title":"Optimizing content delivery with CloudFront"},{"location":"AWS/AWS%20Serverless%20APIs/06-hosting-a-serverless-single-page-application/06-optimize-content-delivery-with-cloudfront/#optimizing-content-delivery-with-cloudfront","text":"Currently our static website files are located in eu-central-1, but if a user accesses from US, it might take a while while everything is loaded. We can use CloudFront to cache these files on CDN and deliver to the user from nearest location. AWS CloudFront Overview: https://aws.amazon.com/cloudfront/ AWS CloudFront Developer Guides: http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html AWS CloudFront Pricing: https://aws.amazon.com/cloudfront/pricing/","title":"Optimizing content delivery with CloudFront"},{"location":"AWS/AWS%20Serverless%20APIs/06-hosting-a-serverless-single-page-application/06-optimize-content-delivery-with-cloudfront/#creating-a-distribution-on-cloudfront","text":"We can go to AWS Console -> Cloudfront -> Create distribution , most likely we'll want to select web as the protocol. Then we need to set up things. In the Origin Domain Name we want to set up our s3 bucket that is used for static webserver. The other settings we might to look at - maximum TTL, forward cookies, compress objects automatically. In the Distribution settings setup correct price class, defailt root object as index.html and enable the cookie logging to the previously created log bucket. Now, when clicking on Create Distribution it will create it and will take some time to sync it across all the locations.","title":"Creating a distribution on CloudFront"},{"location":"AWS/AWS%20Serverless%20APIs/06-hosting-a-serverless-single-page-application/07-using-a-custom-domain-route-53/","text":"Using a custom domain with route 53 \u00b6 The Route 53 is a simple DNS server. AWS Route53 Overview: https://aws.amazon.com/route53/ AWS Route53 Developer Guide: http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/Welcome.html AWS Route53 Pricing: https://aws.amazon.com/route53/pricing/ AWS Route53 - Registering a Domain: http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/registrar.html Registering a domain \u00b6 We can go to AWS Console -> Route 53 -> Register a domain . Select a name, go to checking out process, after a while it will be processed. Connecting a domain to a cloudfront distribution \u00b6 When you have set up your domain, you can create a new recordset with an alias target to your generated cloudfront url.","title":"Using a custom domain with route 53"},{"location":"AWS/AWS%20Serverless%20APIs/06-hosting-a-serverless-single-page-application/07-using-a-custom-domain-route-53/#using-a-custom-domain-with-route-53","text":"The Route 53 is a simple DNS server. AWS Route53 Overview: https://aws.amazon.com/route53/ AWS Route53 Developer Guide: http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/Welcome.html AWS Route53 Pricing: https://aws.amazon.com/route53/pricing/ AWS Route53 - Registering a Domain: http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/registrar.html","title":"Using a custom domain with route 53"},{"location":"AWS/AWS%20Serverless%20APIs/06-hosting-a-serverless-single-page-application/07-using-a-custom-domain-route-53/#registering-a-domain","text":"We can go to AWS Console -> Route 53 -> Register a domain . Select a name, go to checking out process, after a while it will be processed.","title":"Registering a domain"},{"location":"AWS/AWS%20Serverless%20APIs/06-hosting-a-serverless-single-page-application/07-using-a-custom-domain-route-53/#connecting-a-domain-to-a-cloudfront-distribution","text":"When you have set up your domain, you can create a new recordset with an alias target to your generated cloudfront url.","title":"Connecting a domain to a cloudfront distribution"},{"location":"AWS/AWS%20Serverless%20APIs/07-beyond-the-basics/01-documenting-the-API/","text":"Documenting the API \u00b6 You can document your API in API Gateway -> APIs -> Documentation section. There you can create documentation parts, import and publish them.","title":"Documenting the API"},{"location":"AWS/AWS%20Serverless%20APIs/07-beyond-the-basics/01-documenting-the-API/#documenting-the-api","text":"You can document your API in API Gateway -> APIs -> Documentation section. There you can create documentation parts, import and publish them.","title":"Documenting the API"},{"location":"AWS/AWS%20Serverless%20APIs/07-beyond-the-basics/02-other-aws-triggers/","text":"Other AWS triggers \u00b6 When creating a lambda function, you can select various triggers that are not only limited to http requests. We can also create a trigger for s3 file uploads, cloudwatch schedules etc.","title":"Other AWS triggers"},{"location":"AWS/AWS%20Serverless%20APIs/07-beyond-the-basics/02-other-aws-triggers/#other-aws-triggers","text":"When creating a lambda function, you can select various triggers that are not only limited to http requests. We can also create a trigger for s3 file uploads, cloudwatch schedules etc.","title":"Other AWS triggers"},{"location":"AWS/AWS%20Serverless%20APIs/07-beyond-the-basics/03-non-api-serverless-app/","text":"non-API (MPA) serverless app \u00b6 In this course we had a single page app with a separate API. We can also run an express app on lambda and get back HTML pages. AWS Blog: Migrating your Express App to Lambda: https://aws.amazon.com/blogs/compute/going-serverless-migrating-an-express-application-to-amazon-api-gateway-and-aws-lambda/ Express Wrapper (Github): https://github.com/awslabs/aws-serverless-express Create a new lambda function and instead of using inline code, we can specify to upload a zip file. Then in API Gateway , create a new API, create a new metod with ANY and select Use Lambda integration . Things to consider \u00b6 The application is still stateless, your sessions and files should still be stored elsewhere. For apps that may not see traffic for several minutes, you could see cold starts","title":"non-API (MPA) serverless app"},{"location":"AWS/AWS%20Serverless%20APIs/07-beyond-the-basics/03-non-api-serverless-app/#non-api-mpa-serverless-app","text":"In this course we had a single page app with a separate API. We can also run an express app on lambda and get back HTML pages. AWS Blog: Migrating your Express App to Lambda: https://aws.amazon.com/blogs/compute/going-serverless-migrating-an-express-application-to-amazon-api-gateway-and-aws-lambda/ Express Wrapper (Github): https://github.com/awslabs/aws-serverless-express Create a new lambda function and instead of using inline code, we can specify to upload a zip file. Then in API Gateway , create a new API, create a new metod with ANY and select Use Lambda integration .","title":"non-API (MPA) serverless app"},{"location":"AWS/AWS%20Serverless%20APIs/07-beyond-the-basics/03-non-api-serverless-app/#things-to-consider","text":"The application is still stateless, your sessions and files should still be stored elsewhere. For apps that may not see traffic for several minutes, you could see cold starts","title":"Things to consider"},{"location":"AWS/AWS%20Serverless%20APIs/07-beyond-the-basics/04-serverless-and-security/","text":"Serverless and security \u00b6 While we don't really need to worry about our servers in serverless aproach, we do need to consider following security issues: Application related Unauthenticated Access Protect API endpoints with Cognito / Auth Restricting API usage with API Keys Compromised User Data Cognito uses SSL and ecrypts data Retrieve User data carefully Infrastructure related DDoS attacks Throttling and DDoS protection built-in NoSQL Injection Access through SDK, protection built-in Stolen AWS credentials","title":"Serverless and security"},{"location":"AWS/AWS%20Serverless%20APIs/07-beyond-the-basics/04-serverless-and-security/#serverless-and-security","text":"While we don't really need to worry about our servers in serverless aproach, we do need to consider following security issues: Application related Unauthenticated Access Protect API endpoints with Cognito / Auth Restricting API usage with API Keys Compromised User Data Cognito uses SSL and ecrypts data Retrieve User data carefully Infrastructure related DDoS attacks Throttling and DDoS protection built-in NoSQL Injection Access through SDK, protection built-in Stolen AWS credentials","title":"Serverless and security"},{"location":"AWS/AWS%20Serverless%20APIs/07-beyond-the-basics/05-better-development-and-deployment-process/","text":"Better development and deployment process \u00b6 In order to improve development and deployment process we can use (serverless framework)[https://serverless.com/]. This will create a project on your local machine as well as YAML configuration. Then you will be able to deploy it from terminal using serverless deploy . Serverless Framework Website: https://serverless.com/ AWS Getting Started Guide (with Serverless Framework): https://serverless.com/framework/docs/providers/aws/guide/quick-start/ Managing AWS Credentials (for using the Serverless Framework): https://serverless.com/framework/docs/providers/aws/guide/credentials/ Serverless Framework on Github: https://github.com/serverless/serverless","title":"Better development and deployment process"},{"location":"AWS/AWS%20Serverless%20APIs/07-beyond-the-basics/05-better-development-and-deployment-process/#better-development-and-deployment-process","text":"In order to improve development and deployment process we can use (serverless framework)[https://serverless.com/]. This will create a project on your local machine as well as YAML configuration. Then you will be able to deploy it from terminal using serverless deploy . Serverless Framework Website: https://serverless.com/ AWS Getting Started Guide (with Serverless Framework): https://serverless.com/framework/docs/providers/aws/guide/quick-start/ Managing AWS Credentials (for using the Serverless Framework): https://serverless.com/framework/docs/providers/aws/guide/credentials/ Serverless Framework on Github: https://github.com/serverless/serverless","title":"Better development and deployment process"},{"location":"AWS/AWS%20Serverless%20APIs/07-beyond-the-basics/06-serverless-application-model/","text":"Serverless application model \u00b6 This is a service that creates configuration files that are used with CloudFormation in order to generate multiple services on AWS with specific configurations. SAM Github Page: https://github.com/awslabs/serverless-application-model Using SAM: https://github.com/awslabs/serverless-application-model/blob/master/HOWTO.md Deploying Lambda Functions (with SAM and even automated!): http://docs.aws.amazon.com/lambda/latest/dg/deploying-lambda-apps.html","title":"Serverless application model"},{"location":"AWS/AWS%20Serverless%20APIs/07-beyond-the-basics/06-serverless-application-model/#serverless-application-model","text":"This is a service that creates configuration files that are used with CloudFormation in order to generate multiple services on AWS with specific configurations. SAM Github Page: https://github.com/awslabs/serverless-application-model Using SAM: https://github.com/awslabs/serverless-application-model/blob/master/HOWTO.md Deploying Lambda Functions (with SAM and even automated!): http://docs.aws.amazon.com/lambda/latest/dg/deploying-lambda-apps.html","title":"Serverless application model"},{"location":"AWS/AWS%20Serverless%20APIs/07-beyond-the-basics/07-testing-serverless-apps-with-localstack/","text":"Testing serverless apps with localstack \u00b6 localstack is a service to set up mocked aws services on your local machine to test your serverless apps. https://localstack.cloud/ https://github.com/localstack/localstack","title":"Testing serverless apps with localstack"},{"location":"AWS/AWS%20Serverless%20APIs/07-beyond-the-basics/07-testing-serverless-apps-with-localstack/#testing-serverless-apps-with-localstack","text":"localstack is a service to set up mocked aws services on your local machine to test your serverless apps. https://localstack.cloud/ https://github.com/localstack/localstack","title":"Testing serverless apps with localstack"},{"location":"AWS/AWS%20Serverless%20APIs/07-beyond-the-basics/08-other-useful-aws-services/","text":"Other useful AWS services \u00b6 SNS \u00b6 Send push notification, sms to mobile devices etc. You can also listen to sns notifcations to trigger lambdas. AWS SNS: https://aws.amazon.com/sns/ SES \u00b6 E-mail sending service. AWS SES: https://aws.amazon.com/ses/ SQS \u00b6 Message queues. Push certain jobs onto a queue, run a lambda function that runs, pulls an item from a queue and processes it. AWS SQS: https://aws.amazon.com/sqs/ AWS Step Functions \u00b6 Orchestrate lambda functions. Build bigger functions with multiple lambda functions - run multiple in parallel - branching - if one lambda fails, run another one - sequential - run one lambda after another AWS Step Functions: https://aws.amazon.com/step-functions/ Kinesis \u00b6 If you have something that constantly streams data. This bundles it and processes it into a structured data. AWS Kinesis: https://aws.amazon.com/kinesis/ IAM \u00b6 Handle access control CloudWatch \u00b6 View logs, set up scheduled triggers. AWS CloudWatch: https://aws.amazon.com/cloudwatch/ CodeBuild, CodePipeline \u00b6 When you have bigger projects, you might want to create a pipeline, where everything builds, deploys on commiting code into git. AWS CodeBuild: https://aws.amazon.com/codebuild/ AWS CodePipeline: https://aws.amazon.com/codepipeline","title":"Other useful AWS services"},{"location":"AWS/AWS%20Serverless%20APIs/07-beyond-the-basics/08-other-useful-aws-services/#other-useful-aws-services","text":"","title":"Other useful AWS services"},{"location":"AWS/AWS%20Serverless%20APIs/07-beyond-the-basics/08-other-useful-aws-services/#sns","text":"Send push notification, sms to mobile devices etc. You can also listen to sns notifcations to trigger lambdas. AWS SNS: https://aws.amazon.com/sns/","title":"SNS"},{"location":"AWS/AWS%20Serverless%20APIs/07-beyond-the-basics/08-other-useful-aws-services/#ses","text":"E-mail sending service. AWS SES: https://aws.amazon.com/ses/","title":"SES"},{"location":"AWS/AWS%20Serverless%20APIs/07-beyond-the-basics/08-other-useful-aws-services/#sqs","text":"Message queues. Push certain jobs onto a queue, run a lambda function that runs, pulls an item from a queue and processes it. AWS SQS: https://aws.amazon.com/sqs/","title":"SQS"},{"location":"AWS/AWS%20Serverless%20APIs/07-beyond-the-basics/08-other-useful-aws-services/#aws-step-functions","text":"Orchestrate lambda functions. Build bigger functions with multiple lambda functions - run multiple in parallel - branching - if one lambda fails, run another one - sequential - run one lambda after another AWS Step Functions: https://aws.amazon.com/step-functions/","title":"AWS Step Functions"},{"location":"AWS/AWS%20Serverless%20APIs/07-beyond-the-basics/08-other-useful-aws-services/#kinesis","text":"If you have something that constantly streams data. This bundles it and processes it into a structured data. AWS Kinesis: https://aws.amazon.com/kinesis/","title":"Kinesis"},{"location":"AWS/AWS%20Serverless%20APIs/07-beyond-the-basics/08-other-useful-aws-services/#iam","text":"Handle access control","title":"IAM"},{"location":"AWS/AWS%20Serverless%20APIs/07-beyond-the-basics/08-other-useful-aws-services/#cloudwatch","text":"View logs, set up scheduled triggers. AWS CloudWatch: https://aws.amazon.com/cloudwatch/","title":"CloudWatch"},{"location":"AWS/AWS%20Serverless%20APIs/07-beyond-the-basics/08-other-useful-aws-services/#codebuild-codepipeline","text":"When you have bigger projects, you might want to create a pipeline, where everything builds, deploys on commiting code into git. AWS CodeBuild: https://aws.amazon.com/codebuild/ AWS CodePipeline: https://aws.amazon.com/codepipeline","title":"CodeBuild, CodePipeline"},{"location":"AWS/Amazon%20MQ/Amazon%20MQ/","text":"Amazon MQ \u00b6 AWS SQS , AWS SNS are \"cloud-native\" services, and they're using proprietary protocols from AWS. Traditional applications running from on-premise may use open protocols such as [[MQTT]], [[AMQP]], [[STOMP]], [[Openwire]], [[WSS]] When migrating to [[cloud]], instead of re-engineering the application to use AWS SQS and AWS SNS , we can use Amazon MQ Amazon MQ = managed [[Apache ActiveMQ]] Amazon MQ doesn't scale as much as AWS SQS / AWS SNS Amazon MQ runs on a dedicated machine, can run in [[High Availability failover]] Amazon MQ has both queue feature (~ AWS SQS ) and topic features (~ AWS SNS )","title":"Amazon MQ"},{"location":"AWS/Amazon%20MQ/Amazon%20MQ/#amazon-mq","text":"AWS SQS , AWS SNS are \"cloud-native\" services, and they're using proprietary protocols from AWS. Traditional applications running from on-premise may use open protocols such as [[MQTT]], [[AMQP]], [[STOMP]], [[Openwire]], [[WSS]] When migrating to [[cloud]], instead of re-engineering the application to use AWS SQS and AWS SNS , we can use Amazon MQ Amazon MQ = managed [[Apache ActiveMQ]] Amazon MQ doesn't scale as much as AWS SQS / AWS SNS Amazon MQ runs on a dedicated machine, can run in [[High Availability failover]] Amazon MQ has both queue feature (~ AWS SQS ) and topic features (~ AWS SNS )","title":"Amazon MQ"},{"location":"AWS/AppSync/AWS%20AppSync/","text":"AWS AppSync \u00b6 Store and [[sync]] data across mobile and web apps in real-time Makes use of [[GraphQL]] (mobile technology from Facebook) [[Client Code]] can be generated automatically Integrations with DynamoDB / AWS Lambda Real-time subscriptions [[Offline data synchronization]] (replaces Cognito Sync ) [[Fine Grained Security]]","title":"AWS AppSync"},{"location":"AWS/AppSync/AWS%20AppSync/#aws-appsync","text":"Store and [[sync]] data across mobile and web apps in real-time Makes use of [[GraphQL]] (mobile technology from Facebook) [[Client Code]] can be generated automatically Integrations with DynamoDB / AWS Lambda Real-time subscriptions [[Offline data synchronization]] (replaces Cognito Sync ) [[Fine Grained Security]]","title":"AWS AppSync"},{"location":"AWS/Athena/AWS%20Athena/","text":"Athena overview \u00b6 Serverless service to perform analytics directly against AWS S3 files Uses [[SQL]] language to query the files Has a [[JDBC]] / [[ODBC]] driver Charged per query and amount of data scanned Supports [[CSV]], [[JSON]], [[ORC]], [[Avro]] and [[Parquet]] (built on [[Presto]]) Use cases: [[Business intelligence]] / analytics / reporting, analyse & query VPC Flow Logs + Athena , [[ Load Balancer Logs ]], CloudTrail trails etc Fully Serverless database with [[SQL]] capabilities Used to query data in AWS S3 Pay per query Output results back to AWS S3 Secured though IAM Use case: one time SQL queries, Serverless queries on AWS S3 , [[log analytics]]","title":"Athena overview"},{"location":"AWS/Athena/AWS%20Athena/#athena-overview","text":"Serverless service to perform analytics directly against AWS S3 files Uses [[SQL]] language to query the files Has a [[JDBC]] / [[ODBC]] driver Charged per query and amount of data scanned Supports [[CSV]], [[JSON]], [[ORC]], [[Avro]] and [[Parquet]] (built on [[Presto]]) Use cases: [[Business intelligence]] / analytics / reporting, analyse & query VPC Flow Logs + Athena , [[ Load Balancer Logs ]], CloudTrail trails etc Fully Serverless database with [[SQL]] capabilities Used to query data in AWS S3 Pay per query Output results back to AWS S3 Secured though IAM Use case: one time SQL queries, Serverless queries on AWS S3 , [[log analytics]]","title":"Athena overview"},{"location":"AWS/Athena/Athena%20Hands%20on/","text":"Athena Hands on \u00b6 We can access AWS Athena from [[AWS Console]]. When clicking on Get Started it redirects us to a [[Query Editor]]. Since it asked me to setup results path, I created a new AWS S3 Bucket . Then ran following queries: CREATE database s3_access_logs_db; Select the created database and run this query: CREATE EXTERNAL TABLE IF NOT EXISTS s3_access_logs_db.mybucket_logs( BucketOwner STRING, Bucket STRING, RequestDateTime STRING, RemoteIP STRING, Requester STRING, RequestID STRING, Operation STRING, Key STRING, RequestURI_operation STRING, RequestURI_key STRING, RequestURI_httpProtoversion STRING, HTTPstatus STRING, ErrorCode STRING, BytesSent BIGINT, ObjectSize BIGINT, TotalTime STRING, TurnAroundTime STRING, Referrer STRING, UserAgent STRING, VersionId STRING) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe' WITH SERDEPROPERTIES ( 'serialization.format' = '1', 'intput.regex' = '([^ ]*) ([^ ]*) \\\\[(.*?)\\\\] ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) \\\\\\\"([^ ]*) ([^ ]*) (- |[^ ]*)\\\\\\\" (-|[0-9]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) (\\\"[^\\\"]*\\\") (^ ]*)$' ) LOCATION 's3://cf-access-logs-be/'; Then we can run queries against our log files like: SELECT * FROM \"s3_access_logs_db\".\"mybucket_logs\" WHERE httpStatus='403'; SELECT requesturi_operation, httpstatus, count(*) FROM \"s3_access_logs_db\".\"mybucket_logs\" GROUP BY requesturi_operation, httpstatus; Cleanup DROP DATABASE s3_access_logs_db;","title":"Athena Hands on"},{"location":"AWS/Athena/Athena%20Hands%20on/#athena-hands-on","text":"We can access AWS Athena from [[AWS Console]]. When clicking on Get Started it redirects us to a [[Query Editor]]. Since it asked me to setup results path, I created a new AWS S3 Bucket . Then ran following queries: CREATE database s3_access_logs_db; Select the created database and run this query: CREATE EXTERNAL TABLE IF NOT EXISTS s3_access_logs_db.mybucket_logs( BucketOwner STRING, Bucket STRING, RequestDateTime STRING, RemoteIP STRING, Requester STRING, RequestID STRING, Operation STRING, Key STRING, RequestURI_operation STRING, RequestURI_key STRING, RequestURI_httpProtoversion STRING, HTTPstatus STRING, ErrorCode STRING, BytesSent BIGINT, ObjectSize BIGINT, TotalTime STRING, TurnAroundTime STRING, Referrer STRING, UserAgent STRING, VersionId STRING) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe' WITH SERDEPROPERTIES ( 'serialization.format' = '1', 'intput.regex' = '([^ ]*) ([^ ]*) \\\\[(.*?)\\\\] ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) \\\\\\\"([^ ]*) ([^ ]*) (- |[^ ]*)\\\\\\\" (-|[0-9]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) (\\\"[^\\\"]*\\\") (^ ]*)$' ) LOCATION 's3://cf-access-logs-be/'; Then we can run queries against our log files like: SELECT * FROM \"s3_access_logs_db\".\"mybucket_logs\" WHERE httpStatus='403'; SELECT requesturi_operation, httpstatus, count(*) FROM \"s3_access_logs_db\".\"mybucket_logs\" GROUP BY requesturi_operation, httpstatus; Cleanup DROP DATABASE s3_access_logs_db;","title":"Athena Hands on"},{"location":"AWS/Athena/Athena%20for%20Solutions%20Architect/","text":"Athena for Solutions Architect \u00b6 Operations : no operations needed, Serverless Security : IAM + S3 Security Reliability : managed service, uses [[Presto]] engine, High Availability Performance : queries scale based on data size Cost : pay per query / per TB of data scanned, Serverless","title":"Athena for Solutions Architect"},{"location":"AWS/Athena/Athena%20for%20Solutions%20Architect/#athena-for-solutions-architect","text":"Operations : no operations needed, Serverless Security : IAM + S3 Security Reliability : managed service, uses [[Presto]] engine, High Availability Performance : queries scale based on data size Cost : pay per query / per TB of data scanned, Serverless","title":"Athena for Solutions Architect"},{"location":"AWS/CICD/CICD/","text":"Continuous Integration Technology Stack for CICD \u00b6","title":"CICD"},{"location":"AWS/CICD/CICD/#technology-stack-for-cicd","text":"","title":"Technology Stack for CICD"},{"location":"AWS/CICD/Continuous%20Integration/","text":"Continuous Integration \u00b6 Developers push the code to a [[code repository]] often ([[Github]] / AWS CodeCommit / [[Bitbucket]] / etc) A testing / build server checks the code as soon as it's pushed ( AWS CodeBuild / [[Jenkins CI]] / etc) The developer gets feedback about the tests and checks that have passed / failed [[Find bugs]] early, [[fix bugs]] [[Deliver]] faster as the code is tested [[Deploy]] often Happier developers as they're unblocked Ensure that the software can be released reliably whenever needed Ensures deployments happen often and are quick Shift away from \"on release every 3 months\" to \"5 releases a day\" That usually means automated deployment AWS CodeDeploy [[Jekins CD]] [[Spinnaker]] Etc","title":"Continuous Integration"},{"location":"AWS/CICD/Continuous%20Integration/#continuous-integration","text":"Developers push the code to a [[code repository]] often ([[Github]] / AWS CodeCommit / [[Bitbucket]] / etc) A testing / build server checks the code as soon as it's pushed ( AWS CodeBuild / [[Jenkins CI]] / etc) The developer gets feedback about the tests and checks that have passed / failed [[Find bugs]] early, [[fix bugs]] [[Deliver]] faster as the code is tested [[Deploy]] often Happier developers as they're unblocked Ensure that the software can be released reliably whenever needed Ensures deployments happen often and are quick Shift away from \"on release every 3 months\" to \"5 releases a day\" That usually means automated deployment AWS CodeDeploy [[Jekins CD]] [[Spinnaker]] Etc","title":"Continuous Integration"},{"location":"AWS/CLI/AWS%20CLI%20Configuration/","text":"[[AWS CLI]] Configuration \u00b6 In order to configure [[AWS CLI]] we need to get access credentials from IAM . It will be located under Users -> {user} -> Security Credentials -> Access Keys . Then we are going to run \u279c ~ aws --version aws-cli/1.16.303 Python/3.8.0 Linux/5.4.2-arch1-1 botocore/1.13.39 \u279c ~ aws configure AWS Access Key ID [None]: XXX AWS Secret Access Key [None]: XXXX Default region name [None]: eu-west-1 Default output format [None]: \u279c ~ aws configure AWS Access Key ID [****************XXXX]: AWS Secret Access Key [****************XXXX]: Default region name [eu-west-1]: Default output format [None]: \u279c ~ ls ~/.aws config credentials \u279c ~ cat ~/.aws/config [default] region = eu-west-1 For configuring CLI on EC2 - there's a good way and a bad way. Look into AWS CLI on EC2 .","title":"[[AWS CLI]] Configuration"},{"location":"AWS/CLI/AWS%20CLI%20Configuration/#aws-cli-configuration","text":"In order to configure [[AWS CLI]] we need to get access credentials from IAM . It will be located under Users -> {user} -> Security Credentials -> Access Keys . Then we are going to run \u279c ~ aws --version aws-cli/1.16.303 Python/3.8.0 Linux/5.4.2-arch1-1 botocore/1.13.39 \u279c ~ aws configure AWS Access Key ID [None]: XXX AWS Secret Access Key [None]: XXXX Default region name [None]: eu-west-1 Default output format [None]: \u279c ~ aws configure AWS Access Key ID [****************XXXX]: AWS Secret Access Key [****************XXXX]: Default region name [eu-west-1]: Default output format [None]: \u279c ~ ls ~/.aws config credentials \u279c ~ cat ~/.aws/config [default] region = eu-west-1 For configuring CLI on EC2 - there's a good way and a bad way. Look into AWS CLI on EC2 .","title":"[[AWS CLI]] Configuration"},{"location":"AWS/CLI/AWS%20CLI%20Setup%20on%20Linux/","text":"[[AWS CLI ]]Setup on [[Linux]] \u00b6 The [[Linux]] version of [[AWS CLI]] can be downloaded from https://docs.aws.amazon.com/cli/latest/userguide/install-linux.html and will list all of the steps to install it. Once it's installed we can open terminal and check aws --version .","title":"[[AWS CLI ]]Setup on [[Linux]]"},{"location":"AWS/CLI/AWS%20CLI%20Setup%20on%20Linux/#aws-cli-setup-on-linux","text":"The [[Linux]] version of [[AWS CLI]] can be downloaded from https://docs.aws.amazon.com/cli/latest/userguide/install-linux.html and will list all of the steps to install it. Once it's installed we can open terminal and check aws --version .","title":"[[AWS CLI ]]Setup on [[Linux]]"},{"location":"AWS/CLI/AWS%20CLI%20Setup%20on%20MacOS/","text":"[[AWS CLI]] Setup on [[MacOS]] \u00b6 The [[MacOS]] version of [[AWS CLI]] can be downloaded from https://docs.aws.amazon.com/cli/latest/userguide/install-macos.html and will list all of the steps to install it. Once it's installed we can open terminal and check aws --version .","title":"[[AWS CLI]] Setup on [[MacOS]]"},{"location":"AWS/CLI/AWS%20CLI%20Setup%20on%20MacOS/#aws-cli-setup-on-macos","text":"The [[MacOS]] version of [[AWS CLI]] can be downloaded from https://docs.aws.amazon.com/cli/latest/userguide/install-macos.html and will list all of the steps to install it. Once it's installed we can open terminal and check aws --version .","title":"[[AWS CLI]] Setup on [[MacOS]]"},{"location":"AWS/CLI/AWS%20CLI%20Setup%20on%20Windows/","text":"[[AWS CLI]] Setup on [[Windows]] \u00b6 The [[Windows]] version of [[AWS CLI]] can be downloaded from https://docs.aws.amazon.com/cli/latest/userguide/install-windows.html and will list all of the steps to install it. Once it's installed we can open cmd and check aws --version .","title":"[[AWS CLI]] Setup on [[Windows]]"},{"location":"AWS/CLI/AWS%20CLI%20Setup%20on%20Windows/#aws-cli-setup-on-windows","text":"The [[Windows]] version of [[AWS CLI]] can be downloaded from https://docs.aws.amazon.com/cli/latest/userguide/install-windows.html and will list all of the steps to install it. Once it's installed we can open cmd and check aws --version .","title":"[[AWS CLI]] Setup on [[Windows]]"},{"location":"AWS/CLI/AWS%20CLI%20on%20EC2/","text":"AWS CLI on AWS EC2 \u00b6 AWS CLI ON AWS EC2 - The bad way \u00b6 We could run aws configure on EC2 just like we did (and it will work) But it's insecure never put your personal credentials on an EC2 Your personal credentials only belong on your personal computer If the EC2 is compromised, so is your personal account If the EC2 is shared, other people may perform AWS actions while impersonating you For EC2, there's a better way. It's called [[IAM Role]] The right way \u00b6 [[IAM Role]] can be attached to AWS EC2 Instances IAM Roles can come with a policy authorising exactly what the AWS EC2 instance should be able to do AWS EC2 Instances can then use these profiles automatically without any additional configuration This is the best practice on AWS and you should 100% do this. In order to do this, we go to IAM service and create a new role, attach it to an EC2 service, attach policies that we need.","title":"AWS CLI on [[AWS EC2]]"},{"location":"AWS/CLI/AWS%20CLI%20on%20EC2/#aws-cli-on-aws-ec2","text":"","title":"AWS CLI on AWS EC2"},{"location":"AWS/CLI/AWS%20CLI%20on%20EC2/#aws-cli-on-aws-ec2-the-bad-way","text":"We could run aws configure on EC2 just like we did (and it will work) But it's insecure never put your personal credentials on an EC2 Your personal credentials only belong on your personal computer If the EC2 is compromised, so is your personal account If the EC2 is shared, other people may perform AWS actions while impersonating you For EC2, there's a better way. It's called [[IAM Role]]","title":"AWS CLI ON AWS EC2 - The bad way"},{"location":"AWS/CLI/AWS%20CLI%20on%20EC2/#the-right-way","text":"[[IAM Role]] can be attached to AWS EC2 Instances IAM Roles can come with a policy authorising exactly what the AWS EC2 instance should be able to do AWS EC2 Instances can then use these profiles automatically without any additional configuration This is the best practice on AWS and you should 100% do this. In order to do this, we go to IAM service and create a new role, attach it to an EC2 service, attach policies that we need.","title":"The right way"},{"location":"AWS/CloudFormation/CloudFormation/","text":"CloudFormation \u00b6 What is CloudFormation \u00b6 CloudFormation is a [[declarative]] way of outlining your AWS [[Infrastructure]], for any resources (most of them are supported). For example, within a CloudFormation template, you say: I want a security group I want two AWS EC2 machines using this security group I want two Elastic IP s for these AWS EC2 machines I want an AWS S3 Bucket I want a [[Elastic Load Balancer]] in front of these machines Then CloudFormation creates those for you, in the right order, with the exact configuration you specify. Benefits of AWS CloudFormation \u00b6 Infrastructure as code No resources are manually created, which is excellent for control The code can be [[version controlled]] for example, using [[git]] Changes to the [[infrastructure]] are [[reviewed]] through code Cost Each resources within the stack are tagged with an identifier so you can easily see how much stack costs you You can estimate the costs of your resources using the [[CloudFormation template]] [[Savings strategy]]: in dev, you could make automation that deletes templates at 5pm and recreates them at 8am safely. Productivity Ability to destroy and re-create an infrastructure on the cloud on the fly Automated generation of [[Diagram]] for your templates [[Declarative programming]] (no need to figure out ordering and orchestration) Separation of concern - create many stacks for many apps, and many layers [[VPC stack]]s [[Network stack]]s [[App stack]]s Don't re-invent the wheel Leverage existing templates on the web! Leverage [[documentation]] How CloudFormation works \u00b6 Templates have to be uploaded in AWS S3 and then referenced in CloudFormation To update a template, we can't edit previous ones. We have to re-upload a new version of the template to AWS Stacks are identified by a name Deleting a stack deletes every single [[artifact]] was created by CloudFormation Deploying CloudFormation templates \u00b6 Manual way: Editing templates in the [[CloudFormation Designer]] Using the console to [[input parameter]]s, etc Automated way Editing templates in a [[YAML]] file Using the [[AWS CLI]] to [[deploy]] the templates Templates components \u00b6 Resources components Resources: your AWS resources declared in the template (mandatory) Parameters: the [[dynamic input]]s for your template Mappings: the [[static variable]]s for your template Outputs: [[Reference]]s to what has been created Conditionals: List of [[condition]]s to perform resource creation [[Metadata]] Templates helpers [[Reference]]s [[Function]]s Hands On \u00b6 When creating a new stack, you can choose between 3 options: - Upload your template - Use a sample template - Design your template in a [[GUI]] way","title":"CloudFormation"},{"location":"AWS/CloudFormation/CloudFormation/#cloudformation","text":"","title":"CloudFormation"},{"location":"AWS/CloudFormation/CloudFormation/#what-is-cloudformation","text":"CloudFormation is a [[declarative]] way of outlining your AWS [[Infrastructure]], for any resources (most of them are supported). For example, within a CloudFormation template, you say: I want a security group I want two AWS EC2 machines using this security group I want two Elastic IP s for these AWS EC2 machines I want an AWS S3 Bucket I want a [[Elastic Load Balancer]] in front of these machines Then CloudFormation creates those for you, in the right order, with the exact configuration you specify.","title":"What is CloudFormation"},{"location":"AWS/CloudFormation/CloudFormation/#benefits-of-aws-cloudformation","text":"Infrastructure as code No resources are manually created, which is excellent for control The code can be [[version controlled]] for example, using [[git]] Changes to the [[infrastructure]] are [[reviewed]] through code Cost Each resources within the stack are tagged with an identifier so you can easily see how much stack costs you You can estimate the costs of your resources using the [[CloudFormation template]] [[Savings strategy]]: in dev, you could make automation that deletes templates at 5pm and recreates them at 8am safely. Productivity Ability to destroy and re-create an infrastructure on the cloud on the fly Automated generation of [[Diagram]] for your templates [[Declarative programming]] (no need to figure out ordering and orchestration) Separation of concern - create many stacks for many apps, and many layers [[VPC stack]]s [[Network stack]]s [[App stack]]s Don't re-invent the wheel Leverage existing templates on the web! Leverage [[documentation]]","title":"Benefits of AWS CloudFormation"},{"location":"AWS/CloudFormation/CloudFormation/#how-cloudformation-works","text":"Templates have to be uploaded in AWS S3 and then referenced in CloudFormation To update a template, we can't edit previous ones. We have to re-upload a new version of the template to AWS Stacks are identified by a name Deleting a stack deletes every single [[artifact]] was created by CloudFormation","title":"How CloudFormation works"},{"location":"AWS/CloudFormation/CloudFormation/#deploying-cloudformation-templates","text":"Manual way: Editing templates in the [[CloudFormation Designer]] Using the console to [[input parameter]]s, etc Automated way Editing templates in a [[YAML]] file Using the [[AWS CLI]] to [[deploy]] the templates","title":"Deploying CloudFormation templates"},{"location":"AWS/CloudFormation/CloudFormation/#templates-components","text":"Resources components Resources: your AWS resources declared in the template (mandatory) Parameters: the [[dynamic input]]s for your template Mappings: the [[static variable]]s for your template Outputs: [[Reference]]s to what has been created Conditionals: List of [[condition]]s to perform resource creation [[Metadata]] Templates helpers [[Reference]]s [[Function]]s","title":"Templates components"},{"location":"AWS/CloudFormation/CloudFormation/#hands-on","text":"When creating a new stack, you can choose between 3 options: - Upload your template - Use a sample template - Design your template in a [[GUI]] way","title":"Hands On"},{"location":"AWS/CloudFront/AWS%20CloudFront%20Hands%20On/","text":"AWS CloudFront Hands On \u00b6 We'll create an AWS S3 Bucket \u00b6 Everything currently will be with default settings. Found a free [[HTML]] template to upload for testing. We'll create a Programming/AWS/CloudFront/AWS CloudFront distribution and an [[Origin Access identity]], limit the [[S3 bucket]] to be accessed only using this identity. \u00b6 Now we are going to Programming/AWS/CloudFront/AWS CloudFront service in AWS Console and create a distribution. In the Origin Domain Name we select our AWS S3 Bucket . Check Restrict bucket Access , create a new identity access-identity-demo , check Grant Read Permissions on Bucket . Viewer Protocol Policy - Redirect HTTP to HTTPS . Now the distribution is being created, it can take several minutes to complete. We should also see an Origin Access Identity created. Also, we can see that the public access has been enabled for the S3 bucket. Now, when visiting the generated CloudFront URL, we might see a redirect to the bucket itself and throwing an error. This is due to the fact that everything is set up but the [[AWS internal DNS]] has not been updated yet, it might take a couple of hours to update and then it should work.","title":"AWS CloudFront Hands On"},{"location":"AWS/CloudFront/AWS%20CloudFront%20Hands%20On/#aws-cloudfront-hands-on","text":"","title":"AWS CloudFront Hands On"},{"location":"AWS/CloudFront/AWS%20CloudFront%20Hands%20On/#well-create-an-aws-s3-bucket","text":"Everything currently will be with default settings. Found a free [[HTML]] template to upload for testing.","title":"We'll create an AWS S3 Bucket"},{"location":"AWS/CloudFront/AWS%20CloudFront%20Hands%20On/#well-create-a-programmingawscloudfrontaws-cloudfront-distribution-and-an-origin-access-identity-limit-the-s3-bucket-to-be-accessed-only-using-this-identity","text":"Now we are going to Programming/AWS/CloudFront/AWS CloudFront service in AWS Console and create a distribution. In the Origin Domain Name we select our AWS S3 Bucket . Check Restrict bucket Access , create a new identity access-identity-demo , check Grant Read Permissions on Bucket . Viewer Protocol Policy - Redirect HTTP to HTTPS . Now the distribution is being created, it can take several minutes to complete. We should also see an Origin Access Identity created. Also, we can see that the public access has been enabled for the S3 bucket. Now, when visiting the generated CloudFront URL, we might see a redirect to the bucket itself and throwing an error. This is due to the fact that everything is set up but the [[AWS internal DNS]] has not been updated yet, it might take a couple of hours to update and then it should work.","title":"We'll create a Programming/AWS/CloudFront/AWS CloudFront distribution and an [[Origin Access identity]], limit the [[S3 bucket]] to be accessed only using this identity."},{"location":"AWS/CloudFront/AWS%20CloudFront/","text":"AWS CloudFront \u00b6 [[Content Delivery Network (CDN)]] Improves read performance, content is cached at [[edge]] 136 points of presence globally ([[edge locations]]) Popular with AWS S3 but works with AWS EC2 , Programming/AWS/EC2/Elastic LoadBalancer 2/Load Balancing Can help to protect against [[network attacks]] Can provide [[SSL encryption]] ([[HTTPS]]) at the edge using ACM Programming/AWS/CloudFront/AWS CloudFront can use SSL encryption (HTTPS) to talk to your applications Support [[RTMP Protocol]] (videos/media) CloudFront Signed URL / Signed Cookies \u00b6 Say you wanted to distribute paid shared content to premium users over the world, the content lives int AWS S3 . If AWS S3 can only be accessed through CloudFront, we cannot use self-signed S3 URLS We can use CloudFront signed URLs. We attach a policy with: Included URL expiration Include IP ranges to access data from Trusted signers (which AWS accounts can create signed URLs) CloudFront signed URL can only be created using the AWS SDK, so you have to code an application to verify users and generate these URLs How long should the url be valid for? Shared content (movie, music): make it short (a few minutes) Private content (private to the user): you can make it last for years CloudFront vs S3 Cross Region Replication \u00b6 CloudFront Global [[Edge Network]] Files are cached for a TTL (maybe a day) Great for static content that must be available everywhere Geo Restriction - you can restrict who can access your distribution Whitelist - allow your users to access your content only if they're in one of the countries on a list of approved countries Blacklist - prevent your users from accessing your content if they're in one of the countries on a blacklist of banned countries. The country is determined by using a 3rd party Geo-IP database Use case - copyright laws to control access to content S3 Cross Region replication Must be setup for each region you want replication to happen Files are updated in near real-time Read only Great for dynamic content that needs to be available at low latency in few regions","title":"AWS CloudFront"},{"location":"AWS/CloudFront/AWS%20CloudFront/#aws-cloudfront","text":"[[Content Delivery Network (CDN)]] Improves read performance, content is cached at [[edge]] 136 points of presence globally ([[edge locations]]) Popular with AWS S3 but works with AWS EC2 , Programming/AWS/EC2/Elastic LoadBalancer 2/Load Balancing Can help to protect against [[network attacks]] Can provide [[SSL encryption]] ([[HTTPS]]) at the edge using ACM Programming/AWS/CloudFront/AWS CloudFront can use SSL encryption (HTTPS) to talk to your applications Support [[RTMP Protocol]] (videos/media)","title":"AWS CloudFront"},{"location":"AWS/CloudFront/AWS%20CloudFront/#cloudfront-signed-url-signed-cookies","text":"Say you wanted to distribute paid shared content to premium users over the world, the content lives int AWS S3 . If AWS S3 can only be accessed through CloudFront, we cannot use self-signed S3 URLS We can use CloudFront signed URLs. We attach a policy with: Included URL expiration Include IP ranges to access data from Trusted signers (which AWS accounts can create signed URLs) CloudFront signed URL can only be created using the AWS SDK, so you have to code an application to verify users and generate these URLs How long should the url be valid for? Shared content (movie, music): make it short (a few minutes) Private content (private to the user): you can make it last for years","title":"CloudFront Signed URL / Signed Cookies"},{"location":"AWS/CloudFront/AWS%20CloudFront/#cloudfront-vs-s3-cross-region-replication","text":"CloudFront Global [[Edge Network]] Files are cached for a TTL (maybe a day) Great for static content that must be available everywhere Geo Restriction - you can restrict who can access your distribution Whitelist - allow your users to access your content only if they're in one of the countries on a list of approved countries Blacklist - prevent your users from accessing your content if they're in one of the countries on a blacklist of banned countries. The country is determined by using a 3rd party Geo-IP database Use case - copyright laws to control access to content S3 Cross Region replication Must be setup for each region you want replication to happen Files are updated in near real-time Read only Great for dynamic content that needs to be available at low latency in few regions","title":"CloudFront vs S3 Cross Region Replication"},{"location":"AWS/CloudTrail/AWS%20CloudTrail/","text":"AWS CloudTrail \u00b6 Provides [[governance]], [[compliance]] and [[audit]] for your [[AWS account]] CloudTrail is enabled by default Get a history of events / API calls made within your AWS account by: [[AWS Console]] AWS SDK [[AWS CLI]] AWS Services Can put logs from CloudTrail into CloudWatch Logs If a resource is deleted in AWS, look into CloudTrail first","title":"AWS CloudTrail"},{"location":"AWS/CloudTrail/AWS%20CloudTrail/#aws-cloudtrail","text":"Provides [[governance]], [[compliance]] and [[audit]] for your [[AWS account]] CloudTrail is enabled by default Get a history of events / API calls made within your AWS account by: [[AWS Console]] AWS SDK [[AWS CLI]] AWS Services Can put logs from CloudTrail into CloudWatch Logs If a resource is deleted in AWS, look into CloudTrail first","title":"AWS CloudTrail"},{"location":"AWS/CloudWatch/AWS%20CLoudWatch%20EC2%20Detailed%20monitoring/","text":"AWS CloudWatch EC2 Detailed monitoring \u00b6 AWS EC2 instance metrics have metrics \"every 5 minutes\" With detailed monitoring (for a cost), you get data \"every 1 minute\" Use detailed monitoring if you want to more prompt scale your Auto Scaling Group (ASG) The AWS Free Tier allows us to have 10 detailed monitoring metrics Note: AWS EC2 Memory usage is by default not pushed (must be pushed from inside the instance as a custom metric)","title":"AWS CLoudWatch EC2 Detailed monitoring"},{"location":"AWS/CloudWatch/AWS%20CLoudWatch%20EC2%20Detailed%20monitoring/#aws-cloudwatch-ec2-detailed-monitoring","text":"AWS EC2 instance metrics have metrics \"every 5 minutes\" With detailed monitoring (for a cost), you get data \"every 1 minute\" Use detailed monitoring if you want to more prompt scale your Auto Scaling Group (ASG) The AWS Free Tier allows us to have 10 detailed monitoring metrics Note: AWS EC2 Memory usage is by default not pushed (must be pushed from inside the instance as a custom metric)","title":"AWS CloudWatch EC2 Detailed monitoring"},{"location":"AWS/CloudWatch/CloudWatch%20Alarm/","text":"CloudWatch Alarms \u00b6 Alarms are used to trigger notifications for any [[metric]] Alarms can go to Auto Scaling Group (ASG) , [[EC2 actions]], [[SNS notifications]] Various options (sampling, %, max, min, etc) Alarm States: OK INSUFFICIENT_DATA ALARM Period: Length of time in seconds to evaluate the metric High resolution custom metrics: can only choose 10 sec or 30 sec","title":"CloudWatch Alarms"},{"location":"AWS/CloudWatch/CloudWatch%20Alarm/#cloudwatch-alarms","text":"Alarms are used to trigger notifications for any [[metric]] Alarms can go to Auto Scaling Group (ASG) , [[EC2 actions]], [[SNS notifications]] Various options (sampling, %, max, min, etc) Alarm States: OK INSUFFICIENT_DATA ALARM Period: Length of time in seconds to evaluate the metric High resolution custom metrics: can only choose 10 sec or 30 sec","title":"CloudWatch Alarms"},{"location":"AWS/CloudWatch/CloudWatch%20Custom%20Metrics/","text":"AWS CloudWatch Custom metrics \u00b6 Possibility to define and send your own custom metrics to CloudWatch Ability to use dimensions (attributes) to segment metrics Instance.id Environment.name Metric resolution Standard: 1 minute High Resolution: up to 1 second (storageResolution API parameter) - higher cost Use API call PutMetricData Use exponential back off in case of throttle errors","title":"CloudWatch Custom Metrics"},{"location":"AWS/CloudWatch/CloudWatch%20Custom%20Metrics/#aws-cloudwatch-custom-metrics","text":"Possibility to define and send your own custom metrics to CloudWatch Ability to use dimensions (attributes) to segment metrics Instance.id Environment.name Metric resolution Standard: 1 minute High Resolution: up to 1 second (storageResolution API parameter) - higher cost Use API call PutMetricData Use exponential back off in case of throttle errors","title":"AWS CloudWatch Custom metrics"},{"location":"AWS/CloudWatch/CloudWatch%20Dashboards/","text":"CloudWatch Dashboards \u00b6 Great way to setup dashboards for quick access to key metrics Dashboards are global Dashboards can include graphs from different regions You can change the time zone & time range of the dashboards You can setup automatic refresh (10s, 1m, 2m, 5m, 15m) Pricing: 3 dashboards (up to 50 metrics) for free $3/dashboard/month afterwards","title":"CloudWatch Dashboards"},{"location":"AWS/CloudWatch/CloudWatch%20Dashboards/#cloudwatch-dashboards","text":"Great way to setup dashboards for quick access to key metrics Dashboards are global Dashboards can include graphs from different regions You can change the time zone & time range of the dashboards You can setup automatic refresh (10s, 1m, 2m, 5m, 15m) Pricing: 3 dashboards (up to 50 metrics) for free $3/dashboard/month afterwards","title":"CloudWatch Dashboards"},{"location":"AWS/CloudWatch/CloudWatch%20Events/","text":"CloudWatch Events \u00b6 Schedule: [[Cron Jobs]] [[Event Pattern]]: Event rules to react to if a service is doing something Ex: [[CodePipeline]] state changes Triggers to AWS Lambda functions, AWS SQS / AWS SNS / Kinesis Messages CloudWatch Event creates a small [[JSON]] document to give information about the change","title":"CloudWatch Events"},{"location":"AWS/CloudWatch/CloudWatch%20Events/#cloudwatch-events","text":"Schedule: [[Cron Jobs]] [[Event Pattern]]: Event rules to react to if a service is doing something Ex: [[CodePipeline]] state changes Triggers to AWS Lambda functions, AWS SQS / AWS SNS / Kinesis Messages CloudWatch Event creates a small [[JSON]] document to give information about the change","title":"CloudWatch Events"},{"location":"AWS/CloudWatch/CloudWatch%20Logs/","text":"CloudWatch Logs \u00b6 Applications can send logs to CloudWatch using the AWS SDK CloudWatch can collect log from: Elastic Beanstalk : collection of logs from application [[ECS]]: collection from containers AWS Lambda : collection from function logs VPC Flow Logs : VPC specific logs AWS API Gateway AWS CloudTrail based on filter [[CloudWatch log agent]]s: for example on EC2 machines AWS Route 53 : Log DNS queries CloudWatch Logs can go to: Batch exported to AWS S3 archival Stream to ElasticSearch cluster for further analytics AWS CloudWatch Logs \u00b6 Logs storage architecture: [[Log groups]]: arbitrary name, usually representing an application [[Log stream]]: instances within application / log files / containers Can define log expiration policies (never expire, 30 days, etc) Using the [[AWS CLI]] we can tail CloudWatch logs To send logs to CloudWatch , make sure IAM permissions are correct. Security: [[Encrpyion]] of logs using AWS KMS (Key Management Service) at the [[Group Level]] CloudWatch Logs Metric Filter & Insights \u00b6 CloudWatch Logs can use [[filter expressions]] For example, find a specific IP inside a log Metric filters can be used to trigger alarms [[CloudWatch Logs Insights]] (new - Nov 2018) can be used to query logs and add queries to CloudWatch Dashboards .","title":"CloudWatch Logs"},{"location":"AWS/CloudWatch/CloudWatch%20Logs/#cloudwatch-logs","text":"Applications can send logs to CloudWatch using the AWS SDK CloudWatch can collect log from: Elastic Beanstalk : collection of logs from application [[ECS]]: collection from containers AWS Lambda : collection from function logs VPC Flow Logs : VPC specific logs AWS API Gateway AWS CloudTrail based on filter [[CloudWatch log agent]]s: for example on EC2 machines AWS Route 53 : Log DNS queries CloudWatch Logs can go to: Batch exported to AWS S3 archival Stream to ElasticSearch cluster for further analytics","title":"CloudWatch Logs"},{"location":"AWS/CloudWatch/CloudWatch%20Logs/#aws-cloudwatch-logs","text":"Logs storage architecture: [[Log groups]]: arbitrary name, usually representing an application [[Log stream]]: instances within application / log files / containers Can define log expiration policies (never expire, 30 days, etc) Using the [[AWS CLI]] we can tail CloudWatch logs To send logs to CloudWatch , make sure IAM permissions are correct. Security: [[Encrpyion]] of logs using AWS KMS (Key Management Service) at the [[Group Level]]","title":"AWS CloudWatch Logs"},{"location":"AWS/CloudWatch/CloudWatch%20Logs/#cloudwatch-logs-metric-filter-insights","text":"CloudWatch Logs can use [[filter expressions]] For example, find a specific IP inside a log Metric filters can be used to trigger alarms [[CloudWatch Logs Insights]] (new - Nov 2018) can be used to query logs and add queries to CloudWatch Dashboards .","title":"CloudWatch Logs Metric Filter &amp; Insights"},{"location":"AWS/CloudWatch/CloudWatch%20Metrics/","text":"AWS CloudWatch Metrics \u00b6 CloudWatch provides [[metrics]] for every service in AWS Metric is a variable to monitor (CPUUtilization, NetworkIn...) Metrics belong to [[namespaces]] Dimension is an attribute of a metric (instance id, environment, etc) Up to 10 dimensions per metric Metrics have timestamps Can create CloudWatch Dashboards for metrics","title":"AWS CloudWatch Metrics"},{"location":"AWS/CloudWatch/CloudWatch%20Metrics/#aws-cloudwatch-metrics","text":"CloudWatch provides [[metrics]] for every service in AWS Metric is a variable to monitor (CPUUtilization, NetworkIn...) Metrics belong to [[namespaces]] Dimension is an attribute of a metric (instance id, environment, etc) Up to 10 dimensions per metric Metrics have timestamps Can create CloudWatch Dashboards for metrics","title":"AWS CloudWatch Metrics"},{"location":"AWS/CloudWatch/CloudWatch/","text":"CloudWatch Logs","title":"CloudWatch"},{"location":"AWS/Cognito/AWS%20Cognito/","text":"AWS Cognito \u00b6 We want to give our users an identity so that they can interact with our application","title":"AWS Cognito"},{"location":"AWS/Cognito/AWS%20Cognito/#aws-cognito","text":"We want to give our users an identity so that they can interact with our application","title":"AWS Cognito"},{"location":"AWS/Cognito/Cognito%20Federated%20Identity%20Pool/","text":"Cognito Identity Pools (Federated Identity) \u00b6 Provide AWS credentials to users so they can access AWS resources directly Integrate with Cognito User Pool as an [[identity provider]] Goal Provide direct access to AWS Resources from the Client Side How Log in to federated identity provider - or remain anonymous Get temporary AWS credentials back from the Federated Identity Pool These credentials come with a pre-defined IAM Policy stating their permissions Example provide (temporary) access to write to AWS S3 Bucket using Facebook Login For Public Applications \u00b6 Goal: Provide direct access to AWS Resources from the Client Side How: Log in to federated [[identity provider]] - or remain anonymous Get temporary AWS credentials back from the Federated Identity Pool These credentials come with a pre-defined IAM Policy stating their permissions Example provide (temporary) access to write to S3 bucket using Facebook Login Note Web Identity Federation is an alternative to using Programming/AWS/Cognito/AWS Cognito but AWS recommends against it.","title":"Cognito Federated Identity Pool"},{"location":"AWS/Cognito/Cognito%20Federated%20Identity%20Pool/#cognito-identity-pools-federated-identity","text":"Provide AWS credentials to users so they can access AWS resources directly Integrate with Cognito User Pool as an [[identity provider]] Goal Provide direct access to AWS Resources from the Client Side How Log in to federated identity provider - or remain anonymous Get temporary AWS credentials back from the Federated Identity Pool These credentials come with a pre-defined IAM Policy stating their permissions Example provide (temporary) access to write to AWS S3 Bucket using Facebook Login","title":"Cognito Identity Pools (Federated Identity)"},{"location":"AWS/Cognito/Cognito%20Federated%20Identity%20Pool/#for-public-applications","text":"Goal: Provide direct access to AWS Resources from the Client Side How: Log in to federated [[identity provider]] - or remain anonymous Get temporary AWS credentials back from the Federated Identity Pool These credentials come with a pre-defined IAM Policy stating their permissions Example provide (temporary) access to write to S3 bucket using Facebook Login Note Web Identity Federation is an alternative to using Programming/AWS/Cognito/AWS Cognito but AWS recommends against it.","title":"For Public Applications"},{"location":"AWS/Cognito/Cognito%20Sync/","text":"Cognito Sync \u00b6 Deprecated - use AWS AppSync now Store preferences, configuration, state of app [[Cross device synchronisation]] (any platform - iOS, Android, etc) Offline capability (synchronisation when back online) Requires Cognito Federated Identity Pool in Programming/AWS/Cognito/AWS Cognito (not Cognito User Pool ) Store data in datasets (up to 1MB) Up to 20 datasets to synchronise","title":"Cognito Sync"},{"location":"AWS/Cognito/Cognito%20Sync/#cognito-sync","text":"Deprecated - use AWS AppSync now Store preferences, configuration, state of app [[Cross device synchronisation]] (any platform - iOS, Android, etc) Offline capability (synchronisation when back online) Requires Cognito Federated Identity Pool in Programming/AWS/Cognito/AWS Cognito (not Cognito User Pool ) Store data in datasets (up to 1MB) Up to 20 datasets to synchronise","title":"Cognito Sync"},{"location":"AWS/Cognito/Cognito%20User%20Pool/","text":"Cognito User Pools \u00b6 Sign in functionality for app users Integrate with AWS API Gateway Create a Serverless database of users for your mobile apps Simple login: username (or email) and password combination Can enable Federated Identities (facebook, Google, [[SAML]]) Sends back [[JSON Web Tokens (JWT)]] Can be integrated with AWS API Gateway for [[authentication]]","title":"Cognito User Pool"},{"location":"AWS/Cognito/Cognito%20User%20Pool/#cognito-user-pools","text":"Sign in functionality for app users Integrate with AWS API Gateway Create a Serverless database of users for your mobile apps Simple login: username (or email) and password combination Can enable Federated Identities (facebook, Google, [[SAML]]) Sends back [[JSON Web Tokens (JWT)]] Can be integrated with AWS API Gateway for [[authentication]]","title":"Cognito User Pools"},{"location":"AWS/Cognito/Custom%20Identity%20Broker%20Application/","text":"Custom Identity Broker Application (For Enterprises) \u00b6 Use only if [[identity provider]] is not compatible with [[SAML 2.0]] The [[identity broker]] must determine the appropriate IAM Policy","title":"Custom Identity Broker Application"},{"location":"AWS/Cognito/Custom%20Identity%20Broker%20Application/#custom-identity-broker-application-for-enterprises","text":"Use only if [[identity provider]] is not compatible with [[SAML 2.0]] The [[identity broker]] must determine the appropriate IAM Policy","title":"Custom Identity Broker Application (For Enterprises)"},{"location":"AWS/Cognito/Identity%20Federation%20With%20SAML%20and%20Cognito/","text":"Identity Federation With SAML and Cognito \u00b6","title":"Identity Federation With SAML and Cognito"},{"location":"AWS/Cognito/Identity%20Federation%20With%20SAML%20and%20Cognito/#identity-federation-with-saml-and-cognito","text":"","title":"Identity Federation With SAML and Cognito"},{"location":"AWS/Cognito/SAML%20Federation/","text":"SAML Federation (For enterprises) \u00b6 To integrate [[Active Directory]] / [[ADFS]] with AWS (or any [[SAML]] 2.0) Provides access to [[AWS Console]] or [[AWS CLI]] (though [[temporary credentials]]) No need to create an IAM user for each of your employees","title":"SAML Federation"},{"location":"AWS/Cognito/SAML%20Federation/#saml-federation-for-enterprises","text":"To integrate [[Active Directory]] / [[ADFS]] with AWS (or any [[SAML]] 2.0) Provides access to [[AWS Console]] or [[AWS CLI]] (though [[temporary credentials]]) No need to create an IAM user for each of your employees","title":"SAML Federation (For enterprises)"},{"location":"AWS/Cognito/What%27s%20Identity%20Federation/","text":"What's Identity Federation? \u00b6 Federation lets users outside of AWS to assume temporary role for accessing AWS resources. These users assume identity provided access role. Federation assumes a form of 3rd part authentication: [[LDAP]] Microsoft [[Active Directory]] (~=[[SAML]]) AWS Single Sign ON (SSO) [[Open ID]] Programming/AWS/Cognito/AWS Cognito Using federation, you don't need to create IAM users (user management is outside of AWS)","title":"What's Identity Federation"},{"location":"AWS/Cognito/What%27s%20Identity%20Federation/#whats-identity-federation","text":"Federation lets users outside of AWS to assume temporary role for accessing AWS resources. These users assume identity provided access role. Federation assumes a form of 3rd part authentication: [[LDAP]] Microsoft [[Active Directory]] (~=[[SAML]]) AWS Single Sign ON (SSO) [[Open ID]] Programming/AWS/Cognito/AWS Cognito Using federation, you don't need to create IAM users (user management is outside of AWS)","title":"What's Identity Federation?"},{"location":"AWS/DynamoDB/DynamoDB%20Accelerator%20%28DAX%29/","text":"Seamless cache for DynamoDB , no application re-write Writes go though DynamoDB Accelerator (DAX) to DynamoDB Micro second latency for cached reads & queries Solves the [[Hot Key problem]] (too many reads) 5 minutes TTL for cache by default Up to 10 nodes in the cluster [[Multi AZ]] (3 nodes minimum recommended for production) Secure (Encryption at rest with AWS KMS (Key Management Service) , VPC Summary , IAM , CloudTrail )","title":"DynamoDB Accelerator (DAX)"},{"location":"AWS/DynamoDB/DynamoDB%20Advanced%20Features/","text":"Advanced Features \u00b6 - DynamoDB Streams - Changes in DynamoDB (Create, Update, Delete) can end up in a DynamoDB stream - This stream can be read by AWS Lambda and we can then do: - React to changes in real time (welcome email to new users) - Analytics - Create derivative tables / views - Insert into ElasticSearch - Clould implement cross region replication using Streams - Stream has 24 of data retention - [[Transactions]] - All or nothing type of operations - Coordinated insert, update, delete across multiple tables - Include up to 10 unique items or up to 4MB of data - On demand - No capacity planning needed ([[Write Capacity Unit]]/[[Read Capacity Unit]]) - scales automatically - 2.5x more expensive than provisioned capacity (use with care) - Helpful when spikes are un-predictable or the application has very low throughput - Security - VPC endpoint available to access DynamoDB without internet - Access fully controlled by IAM - Encryption at rest using AWS KMS (Key Management Service) - Encryption in transit using [[SSL]] / [[TLS]] - Backup and Restore feature available - Point in time restore like AWS RDS - No performance impact - [[Global Tables]] - [[Multi region]], fully replication, high performance - [[Amazon DMS]] can be used to migrate to DynamoDB (from [[MongoDB]], [[Oracle]], [[MySQL]], AWS S3 etc ...) - You can launch a [[local DynamoDB]] on your computer for development purposes","title":"DynamoDB Advanced Features"},{"location":"AWS/DynamoDB/DynamoDB%20Advanced%20Features/#advanced-features","text":"- DynamoDB Streams - Changes in DynamoDB (Create, Update, Delete) can end up in a DynamoDB stream - This stream can be read by AWS Lambda and we can then do: - React to changes in real time (welcome email to new users) - Analytics - Create derivative tables / views - Insert into ElasticSearch - Clould implement cross region replication using Streams - Stream has 24 of data retention - [[Transactions]] - All or nothing type of operations - Coordinated insert, update, delete across multiple tables - Include up to 10 unique items or up to 4MB of data - On demand - No capacity planning needed ([[Write Capacity Unit]]/[[Read Capacity Unit]]) - scales automatically - 2.5x more expensive than provisioned capacity (use with care) - Helpful when spikes are un-predictable or the application has very low throughput - Security - VPC endpoint available to access DynamoDB without internet - Access fully controlled by IAM - Encryption at rest using AWS KMS (Key Management Service) - Encryption in transit using [[SSL]] / [[TLS]] - Backup and Restore feature available - Point in time restore like AWS RDS - No performance impact - [[Global Tables]] - [[Multi region]], fully replication, high performance - [[Amazon DMS]] can be used to migrate to DynamoDB (from [[MongoDB]], [[Oracle]], [[MySQL]], AWS S3 etc ...) - You can launch a [[local DynamoDB]] on your computer for development purposes","title":"Advanced Features"},{"location":"AWS/DynamoDB/DynamoDB%20Basics/","text":"Basics \u00b6 Programming/AWS/DynamoDB/DynamoDB is made of tables Each table has a [[primary key]] (must be decided at creation time) Each table can have an infinite number of items (=rows) Each item has attributes (can be added over time - can be null) Maximum size of an item is 400KB Data types supported are: Scalar types: [[String]], [[Number]], [[Binary]], [[Boolean]], [[Null]] Document types: [[List]], [[Map]] Set types: [[String Set]], [[Number Set]], [[Binary Set]]","title":"DynamoDB Basics"},{"location":"AWS/DynamoDB/DynamoDB%20Basics/#basics","text":"Programming/AWS/DynamoDB/DynamoDB is made of tables Each table has a [[primary key]] (must be decided at creation time) Each table can have an infinite number of items (=rows) Each item has attributes (can be added over time - can be null) Maximum size of an item is 400KB Data types supported are: Scalar types: [[String]], [[Number]], [[Binary]], [[Boolean]], [[Null]] Document types: [[List]], [[Map]] Set types: [[String Set]], [[Number Set]], [[Binary Set]]","title":"Basics"},{"location":"AWS/DynamoDB/DynamoDB%20Provisioned%20Throughput/","text":"Provisioned Throughput \u00b6 Table must have provisioned read and write capacity units [[Read Capacity Unit]]s (RCU): throughput for reads ($0.00013 per RCU) 1 RCU = 1 strongly consistent read of 4 KB per second 1 RCU = 2 eventually consistent read of 4 KB per second [[Write Capacity Unit]]s (WCU): throughput for writes ($0.00065 per WCU) 1 WCU = 1 write of 1 KB per second Option to setup auto-scaling of throughput to meet demand Throughput can be exceeded temporarily using \"[[burst credit]]\" If burst credits are empty, you'll get a \"[[ProvisionedThoughputException]]\" It's then advised to do an [[exponential back-off retry]]","title":"DynamoDB Provisioned Throughput"},{"location":"AWS/DynamoDB/DynamoDB%20Provisioned%20Throughput/#provisioned-throughput","text":"Table must have provisioned read and write capacity units [[Read Capacity Unit]]s (RCU): throughput for reads ($0.00013 per RCU) 1 RCU = 1 strongly consistent read of 4 KB per second 1 RCU = 2 eventually consistent read of 4 KB per second [[Write Capacity Unit]]s (WCU): throughput for writes ($0.00065 per WCU) 1 WCU = 1 write of 1 KB per second Option to setup auto-scaling of throughput to meet demand Throughput can be exceeded temporarily using \"[[burst credit]]\" If burst credits are empty, you'll get a \"[[ProvisionedThoughputException]]\" It's then advised to do an [[exponential back-off retry]]","title":"Provisioned Throughput"},{"location":"AWS/DynamoDB/DynamoDB%20for%20Solutions%20Architect/","text":"DynamoDB for Solutions Architect \u00b6 Operations : no operations needed, [[AWS Auto Scaling]] capability, Serverless Security : full security though IAM Policy , AWS KMS (Key Management Service) encryption, [[SSL]] in flight Reliability : [[Multi AZ]], [[Backups]] Performance : Single digit millisecond performance, DynamoDB Accelerator (DAX) for [[caching]] reads, performance doesn't degrade if your application scales Cost : Pay per provisioned capacity and storage usage (no need to guess in advance any capacity - can use [[auto scaling]])","title":"DynamoDB for Solutions Architect"},{"location":"AWS/DynamoDB/DynamoDB%20for%20Solutions%20Architect/#dynamodb-for-solutions-architect","text":"Operations : no operations needed, [[AWS Auto Scaling]] capability, Serverless Security : full security though IAM Policy , AWS KMS (Key Management Service) encryption, [[SSL]] in flight Reliability : [[Multi AZ]], [[Backups]] Performance : Single digit millisecond performance, DynamoDB Accelerator (DAX) for [[caching]] reads, performance doesn't degrade if your application scales Cost : Pay per provisioned capacity and storage usage (no need to guess in advance any capacity - can use [[auto scaling]])","title":"DynamoDB for Solutions Architect"},{"location":"AWS/DynamoDB/DynamoDB/","text":"DynamoDB \u00b6 [[Fully Managed]], [[Highly Available]] [[replication]] across 3 Availability Zone s [[NoSQL database]] - not a [[relation database]] Scales to massive workloads, [[distributed database]] Millions of requests per seconds, trillions of rows, 100s of TB of storage Fast and consistent in performance (low latency of retrieval) Integrated with IAM for [[security]], [[authorisation]] and [[administration]] Enables event driven programming with DynamoDB Streams Low cost and autoscaling capabilities AWS proprietary technology, managed [[NoSQL]] database Serverless , [[provisioned capacity]], [[AWS Auto Scaling]], [[on demand capacity]] (Nov 2018) Can replace ElastiCache as a [[key-value store]] (storing session data for example) [[Highly Available]], [[Multi AZ]] by default, [[Read and Writes are decoupled]], DynamoDB Accelerator (DAX) for read [[cache]] Reads can be [[eventually consistent]] or [[strongly consistent]] Security, authentication and authorisation is done though IAM DynamoDB Streams to integrate with AWS Lambda Backup / Restore feature, Global Table feature [[Monitoring]] though CloudWatch Can only query on [[primary key]], [[sort key]], or [[indexes]] Use case: Serverless applications development (small documents 100s KB), distributed serverless cache, doesn't have [[SQL]] query language available, has [[transactions]] capability from Nov 2018.","title":"DynamoDB"},{"location":"AWS/DynamoDB/DynamoDB/#dynamodb","text":"[[Fully Managed]], [[Highly Available]] [[replication]] across 3 Availability Zone s [[NoSQL database]] - not a [[relation database]] Scales to massive workloads, [[distributed database]] Millions of requests per seconds, trillions of rows, 100s of TB of storage Fast and consistent in performance (low latency of retrieval) Integrated with IAM for [[security]], [[authorisation]] and [[administration]] Enables event driven programming with DynamoDB Streams Low cost and autoscaling capabilities AWS proprietary technology, managed [[NoSQL]] database Serverless , [[provisioned capacity]], [[AWS Auto Scaling]], [[on demand capacity]] (Nov 2018) Can replace ElastiCache as a [[key-value store]] (storing session data for example) [[Highly Available]], [[Multi AZ]] by default, [[Read and Writes are decoupled]], DynamoDB Accelerator (DAX) for read [[cache]] Reads can be [[eventually consistent]] or [[strongly consistent]] Security, authentication and authorisation is done though IAM DynamoDB Streams to integrate with AWS Lambda Backup / Restore feature, Global Table feature [[Monitoring]] though CloudWatch Can only query on [[primary key]], [[sort key]], or [[indexes]] Use case: Serverless applications development (small documents 100s KB), distributed serverless cache, doesn't have [[SQL]] query language available, has [[transactions]] capability from Nov 2018.","title":"DynamoDB"},{"location":"AWS/EC2/AWS%20EC2%20Instance%20Metadata/","text":"AWS EC2 Instance Metadata \u00b6 AWS EC2 Instance Metadata is powerful but one of the least known features to developers It allows AWS EC2 instances to \"learn about themselves\" without using an [[IAM Role]] for that purpose The URL is http://169.254.169.254/latest/meta-data You can retrieve the [[IAM Role]] name from the metadata, but you cannot retrieve the IAM Policy Metadata - info about the EC2 instance EC2 User Data - launch script of the EC2 This meta data will only work when running from an EC2 instance, will not work on your local computer. [ec2-user@ip-172-31-44-74 ~]$ curl http://169.254.169.254/latest/meta-data ami-id ami-launch-index ami-manifest-path block-device-mapping/ events/ hostname identity-credentials/ instance-action instance-id instance-type local-hostname local-ipv4 mac metrics/ network/ placement/ profile public-hostname public-ipv4 public-keys/ reservation-id security-groups services/[ec2-user@ip-172-31-44-74 ~]$ curl http://169.254.169.254/latest/meta-data/hostname ip-172-31-44-74.eu-west-1.compute.internal[ec2-user@ip-172-31-44-74 ~]$ curl http://169.254.169.254/latest/meta-data/instance-id i-078d9f318a100da22[ec2-user@ip-172-31-44-74 ~]$ curl http://169.254.169.254/latest/meta-data/events/ maintenance/[ec2-user@ip-172-31-44-74 ~]$","title":"AWS EC2 Instance Metadata"},{"location":"AWS/EC2/AWS%20EC2%20Instance%20Metadata/#aws-ec2-instance-metadata","text":"AWS EC2 Instance Metadata is powerful but one of the least known features to developers It allows AWS EC2 instances to \"learn about themselves\" without using an [[IAM Role]] for that purpose The URL is http://169.254.169.254/latest/meta-data You can retrieve the [[IAM Role]] name from the metadata, but you cannot retrieve the IAM Policy Metadata - info about the EC2 instance EC2 User Data - launch script of the EC2 This meta data will only work when running from an EC2 instance, will not work on your local computer. [ec2-user@ip-172-31-44-74 ~]$ curl http://169.254.169.254/latest/meta-data ami-id ami-launch-index ami-manifest-path block-device-mapping/ events/ hostname identity-credentials/ instance-action instance-id instance-type local-hostname local-ipv4 mac metrics/ network/ placement/ profile public-hostname public-ipv4 public-keys/ reservation-id security-groups services/[ec2-user@ip-172-31-44-74 ~]$ curl http://169.254.169.254/latest/meta-data/hostname ip-172-31-44-74.eu-west-1.compute.internal[ec2-user@ip-172-31-44-74 ~]$ curl http://169.254.169.254/latest/meta-data/instance-id i-078d9f318a100da22[ec2-user@ip-172-31-44-74 ~]$ curl http://169.254.169.254/latest/meta-data/events/ maintenance/[ec2-user@ip-172-31-44-74 ~]$","title":"AWS EC2 Instance Metadata"},{"location":"AWS/EC2/AWS%20EC2/","text":"EC2 Introduction \u00b6 EC2 is one of the most popular of AWS offerings It mainly consists of capability of: Renting [[virtual machine]]s (EC2) Storing data on [[virtual drive]]s (EBS) Distributing load across machines (ELB) Scaling the services using an Auto Scaling Group (ASG) Knowing EC2 is fundamental to understand how the [[Cloud]] works Launching an EC2 Instance running Linux","title":"EC2 Introduction"},{"location":"AWS/EC2/AWS%20EC2/#ec2-introduction","text":"EC2 is one of the most popular of AWS offerings It mainly consists of capability of: Renting [[virtual machine]]s (EC2) Storing data on [[virtual drive]]s (EBS) Distributing load across machines (ELB) Scaling the services using an Auto Scaling Group (ASG) Knowing EC2 is fundamental to understand how the [[Cloud]] works Launching an EC2 Instance running Linux","title":"EC2 Introduction"},{"location":"AWS/EC2/EC2%20User%20Data/","text":"EC2 User Data \u00b6 It is possible to bootstrap our instances using an EC2 User data script Bootstrapping means launching commands when a machine starts That script is only run once at the instance first start EC2 user data is used to automate boot tasks such as: Installing updates Installing software Downloading common files from the internet Anything you can think of The EC2 User Data Script runs with the root user We want to make sure that this EC2 instance has an Apache HTTP server installed on it - to display a simple web page For it we are going to write a user-data script. This script will be executed at the first boot of the instance. Hands on \u00b6 First we are going to terminate the instance we made previously. Then launch a new instance. We are going to follow the same steps as for the previous instance creation process , but when we are in the Configure Instance section, there is a Advanced Details section with user data settings. We will input it as text. #!/bin/bash yum update -y yum install -y httpd.x86_64 systemctl start httpd.service systemctl enable httpd.service echo \"Hello World from $(hostname -f)\" > /var/www/html/index.html For security group, this time we are going to use An existing security group . For keypair, we can reuse the previously created one. Once it has launched, we should be able to grab the Public IP and see the created web server publicly available.","title":"EC2 User Data"},{"location":"AWS/EC2/EC2%20User%20Data/#ec2-user-data","text":"It is possible to bootstrap our instances using an EC2 User data script Bootstrapping means launching commands when a machine starts That script is only run once at the instance first start EC2 user data is used to automate boot tasks such as: Installing updates Installing software Downloading common files from the internet Anything you can think of The EC2 User Data Script runs with the root user We want to make sure that this EC2 instance has an Apache HTTP server installed on it - to display a simple web page For it we are going to write a user-data script. This script will be executed at the first boot of the instance.","title":"EC2 User Data"},{"location":"AWS/EC2/EC2%20User%20Data/#hands-on","text":"First we are going to terminate the instance we made previously. Then launch a new instance. We are going to follow the same steps as for the previous instance creation process , but when we are in the Configure Instance section, there is a Advanced Details section with user data settings. We will input it as text. #!/bin/bash yum update -y yum install -y httpd.x86_64 systemctl start httpd.service systemctl enable httpd.service echo \"Hello World from $(hostname -f)\" > /var/www/html/index.html For security group, this time we are going to use An existing security group . For keypair, we can reuse the previously created one. Once it has launched, we should be able to grab the Public IP and see the created web server publicly available.","title":"Hands on"},{"location":"AWS/EC2/EC2%20for%20Solutions%20Architects/","text":"EC2 for Solutions Architects \u00b6 AWS EC2 instances are billed by the second, t2.micro is free tier On [[Linux]] / [[Mac]] we use [[SSH]], on [[Windows]] we use [[Putty]] [[SSH]] is on [[port]] 22, lock down the security group to your [[IP]] [[Timeout]] issues => Security Group issues Permission issues on the [[SSH key]] => run \"chmod 0400\" Security Group s can reference other Security Group s instead of [[IP range]]s (very popular exam question) Know the difference between Private IP , Public IP and Elastic IP You can customise an AWS EC2 instance at boot time using EC2 User Data Know the 4 EC2 Instance Launch Types : Know the basic instance types: R, C, M, I, G, T2/T3 You can create EC2 AMIs to pre-install software on your EC2 => faster boot EC2 AMIs can be copied across regions and accounts EC2 Placement Groups","title":"EC2 for Solutions Architects"},{"location":"AWS/EC2/EC2%20for%20Solutions%20Architects/#ec2-for-solutions-architects","text":"AWS EC2 instances are billed by the second, t2.micro is free tier On [[Linux]] / [[Mac]] we use [[SSH]], on [[Windows]] we use [[Putty]] [[SSH]] is on [[port]] 22, lock down the security group to your [[IP]] [[Timeout]] issues => Security Group issues Permission issues on the [[SSH key]] => run \"chmod 0400\" Security Group s can reference other Security Group s instead of [[IP range]]s (very popular exam question) Know the difference between Private IP , Public IP and Elastic IP You can customise an AWS EC2 instance at boot time using EC2 User Data Know the 4 EC2 Instance Launch Types : Know the basic instance types: R, C, M, I, G, T2/T3 You can create EC2 AMIs to pre-install software on your EC2 => faster boot EC2 AMIs can be copied across regions and accounts EC2 Placement Groups","title":"EC2 for Solutions Architects"},{"location":"AWS/EC2/Health%20Checks/","text":"Health Checks \u00b6 Health Checks are cruical for Load Balancers They enable the load balancer to know if instances it forwards traffic to are available to reply to requests The health check is done on a port and a route (/health is common) If the response is not 200 (OK), then the instance is unhealthy","title":"Health Checks"},{"location":"AWS/EC2/Health%20Checks/#health-checks","text":"Health Checks are cruical for Load Balancers They enable the load balancer to know if instances it forwards traffic to are available to reply to requests The health check is done on a port and a route (/health is common) If the response is not 200 (OK), then the instance is unhealthy","title":"Health Checks"},{"location":"AWS/EC2/Install%20Apache%20on%20EC2/","text":"Install Apache on EC2 \u00b6 The first step is - we ssh into our ec2 machine . \u279c notes git:(master) \u2717 ssh -i ~/Downloads/EC2Tutorial.pem ec2-user@52.211.212.92 Last login: Fri Nov 22 09:29:22 2019 from ec2-18-202-216-48.eu-west-1.compute.amazonaws.com __| __|_ ) _| ( / Amazon Linux 2 AMI ___|\\___|___| https://aws.amazon.com/amazon-linux-2/ 5 package(s) needed for security, out of 13 available Run \"sudo yum update\" to apply all updates. [ec2-user@ip-172-31-41-154 ~]$ sudo su [root@ip-172-31-41-154 ec2-user]# yum update -y [root@ip-172-31-41-154 ec2-user]# yum install -y httpd.x86_64 [root@ip-172-31-41-154 ec2-user]# systemctl start httpd.service [root@ip-172-31-41-154 ec2-user]# systemctl enable httpd.service Created symlink from /etc/systemd/system/multi-user.target.wants/httpd.service to /usr/lib/systemd/system/httpd.service. [root@ip-172-31-41-154 ec2-user]# curl localhost:80 <!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.1//EN\" \"http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd\"> <html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\"> <head> <title>Test Page for the Apache HTTP Server</title> ... If we grab the Public IP and visit it from browser, it will time out, since we haven't configured the security group . We are listening only to [[ssh connection]]s. We can go to the security group and edit it, add an [[HTTP]] [[inbound rule]]. Now we can visit the public IP and it will be accessible.","title":"Install Apache on EC2"},{"location":"AWS/EC2/Install%20Apache%20on%20EC2/#install-apache-on-ec2","text":"The first step is - we ssh into our ec2 machine . \u279c notes git:(master) \u2717 ssh -i ~/Downloads/EC2Tutorial.pem ec2-user@52.211.212.92 Last login: Fri Nov 22 09:29:22 2019 from ec2-18-202-216-48.eu-west-1.compute.amazonaws.com __| __|_ ) _| ( / Amazon Linux 2 AMI ___|\\___|___| https://aws.amazon.com/amazon-linux-2/ 5 package(s) needed for security, out of 13 available Run \"sudo yum update\" to apply all updates. [ec2-user@ip-172-31-41-154 ~]$ sudo su [root@ip-172-31-41-154 ec2-user]# yum update -y [root@ip-172-31-41-154 ec2-user]# yum install -y httpd.x86_64 [root@ip-172-31-41-154 ec2-user]# systemctl start httpd.service [root@ip-172-31-41-154 ec2-user]# systemctl enable httpd.service Created symlink from /etc/systemd/system/multi-user.target.wants/httpd.service to /usr/lib/systemd/system/httpd.service. [root@ip-172-31-41-154 ec2-user]# curl localhost:80 <!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.1//EN\" \"http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd\"> <html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\"> <head> <title>Test Page for the Apache HTTP Server</title> ... If we grab the Public IP and visit it from browser, it will time out, since we haven't configured the security group . We are listening only to [[ssh connection]]s. We can go to the security group and edit it, add an [[HTTP]] [[inbound rule]]. Now we can visit the public IP and it will be accessible.","title":"Install Apache on EC2"},{"location":"AWS/EC2/Launching%20an%20EC2%20Instance%20running%20Linux/","text":"Launching an EC2 Instance running Linux \u00b6 We'll be launching our first [[virtual server]] using the [[AWS Console]] We'll get a first high level approach to the various parameters We'll learn on how to start / stop / terminate our instances In order to do this, we go to the [[AWS console]] and launch EC2 . Make sure that you are in the region you want to set everything up in. The first thing we want to do is to launch an instance. We will given a list of [[distributions]] available, but it would be recommended to use the Amazon Linux 2 since it is optimized for use with AWS and it has free tier available. Then we can choose an instance type, which basically describes on how much [[vCPU]]s, [[RAM]], [[storage]] we need. We'll choose the t2.micro since it is free tier eligible and click on the Next: Configure Instance Details . Here will be various parameters that we can set up - instance count, [[networking]], [[IAM Role]]s etc. When clicking on next, we will be asked about the storage. The next section is for tags, which are key-value pairs to classify instances. The name tag is quite important because it will show in the UI. The next step is Security Groups, it is basically a firewall around our instance. Now we can review the instance creation task The last step, when we click on launch, is to create a key pair which is used to log into the instance. Click on Download Key Pair and Launch Instances . Then we should see the instance. Once it's created, it will show a state of Running . You can stop or terminate it by right clicking onto it.","title":"Launching an EC2 Instance running Linux"},{"location":"AWS/EC2/Launching%20an%20EC2%20Instance%20running%20Linux/#launching-an-ec2-instance-running-linux","text":"We'll be launching our first [[virtual server]] using the [[AWS Console]] We'll get a first high level approach to the various parameters We'll learn on how to start / stop / terminate our instances In order to do this, we go to the [[AWS console]] and launch EC2 . Make sure that you are in the region you want to set everything up in. The first thing we want to do is to launch an instance. We will given a list of [[distributions]] available, but it would be recommended to use the Amazon Linux 2 since it is optimized for use with AWS and it has free tier available. Then we can choose an instance type, which basically describes on how much [[vCPU]]s, [[RAM]], [[storage]] we need. We'll choose the t2.micro since it is free tier eligible and click on the Next: Configure Instance Details . Here will be various parameters that we can set up - instance count, [[networking]], [[IAM Role]]s etc. When clicking on next, we will be asked about the storage. The next section is for tags, which are key-value pairs to classify instances. The name tag is quite important because it will show in the UI. The next step is Security Groups, it is basically a firewall around our instance. Now we can review the instance creation task The last step, when we click on launch, is to create a key pair which is used to log into the instance. Click on Download Key Pair and Launch Instances . Then we should see the instance. Once it's created, it will show a state of Running . You can stop or terminate it by right clicking onto it.","title":"Launching an EC2 Instance running Linux"},{"location":"AWS/EC2/AMI/AMI%20Pricing/","text":"AMI Pricing \u00b6 EC2 AMIs live in AWS S3 , so you get charged for the actual space it takes in AWS S3 Overall, it is quite inexpensive to store private AMIs Make sure to remove the AMIs you don't use","title":"AMI Pricing"},{"location":"AWS/EC2/AMI/AMI%20Pricing/#ami-pricing","text":"EC2 AMIs live in AWS S3 , so you get charged for the actual space it takes in AWS S3 Overall, it is quite inexpensive to store private AMIs Make sure to remove the AMIs you don't use","title":"AMI Pricing"},{"location":"AWS/EC2/AMI/AMI%20Storage/","text":"AMI Storage \u00b6 Your EC2 AMIs take space and they live in AWS S3 AWS S3 is a durable, cheap and resilient storage where most of your backups will live (but you won't see them in the AWS S3 console) By default your EC2 AMIs are private and locked for your [[AWS Account]] / AWS Region You can also make your AMIs public and share them with other AWS accounts or sell them on the [[AMI Marketplace]]","title":"AMI Storage"},{"location":"AWS/EC2/AMI/AMI%20Storage/#ami-storage","text":"Your EC2 AMIs take space and they live in AWS S3 AWS S3 is a durable, cheap and resilient storage where most of your backups will live (but you won't see them in the AWS S3 console) By default your EC2 AMIs are private and locked for your [[AWS Account]] / AWS Region You can also make your AMIs public and share them with other AWS accounts or sell them on the [[AMI Marketplace]]","title":"AMI Storage"},{"location":"AWS/EC2/AMI/Cross%20Account%20AMI%20Copy/","text":"Cross Account AMI Copy \u00b6 https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html You can share an EC2 AMIs with another [[AWS Account]] Sharing an EC2 AMIs does not affect the ownership of the AMI If you copy an AMI that has been shared with your account, you are the owner of the target AMI in your account. You copy an AMI that was shared with you from another account, the owner of the source AMI must grant you read permissions for the storage that backs the AMI, either associated EBS Snapshot (for an [[Amazon EBS-backed AMI]]) or an associated AWS S3 Bucket (for an [[instance store-backed AMI]]). Limits: You can't copy an [[encrypted AMI]] that was shared with you from another account. Instead, if the underlying snapshot and encryption key were shared with you, you can copy the snapshot while re-encrypting it with a key of your own. You own the copied snapshot and can register it as a new AMI. You can't copy an AMI with an associated billingProduct code that was shared with you from another account. This includes Windows AMIs and AMIs from the AWS marketplace. To copy a shared AMI with a billingProduct code, launch an EC2 instance in your account using the shared AMI and then create an AMI from the instance. In order to share an AMI, you right click on it and select Modify Image Permissions .","title":"Cross Account AMI Copy"},{"location":"AWS/EC2/AMI/Cross%20Account%20AMI%20Copy/#cross-account-ami-copy","text":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html You can share an EC2 AMIs with another [[AWS Account]] Sharing an EC2 AMIs does not affect the ownership of the AMI If you copy an AMI that has been shared with your account, you are the owner of the target AMI in your account. You copy an AMI that was shared with you from another account, the owner of the source AMI must grant you read permissions for the storage that backs the AMI, either associated EBS Snapshot (for an [[Amazon EBS-backed AMI]]) or an associated AWS S3 Bucket (for an [[instance store-backed AMI]]). Limits: You can't copy an [[encrypted AMI]] that was shared with you from another account. Instead, if the underlying snapshot and encryption key were shared with you, you can copy the snapshot while re-encrypting it with a key of your own. You own the copied snapshot and can register it as a new AMI. You can't copy an AMI with an associated billingProduct code that was shared with you from another account. This includes Windows AMIs and AMIs from the AWS marketplace. To copy a shared AMI with a billingProduct code, launch an EC2 instance in your account using the shared AMI and then create an AMI from the instance. In order to share an AMI, you right click on it and select Modify Image Permissions .","title":"Cross Account AMI Copy"},{"location":"AWS/EC2/AMI/EC2%20AMI%20Hands%20On/","text":"Hands on \u00b6 We are going to use a fresh EC2 instance and ssh into it. \u279c notes git:(master) \u2717 ssh -i ~/Downloads/EC2Tutorial.pem ec2-user@63.35.203.134 The authenticity of host '63.35.203.134 (63.35.203.134)' can't be established. ECDSA key fingerprint is SHA256:IRLAu1mhR88VLPx7js5V1kxaFHlg6TtNNVDSWpZKz5c. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added '63.35.203.134' (ECDSA) to the list of known hosts. __| __|_ ) _| ( / Amazon Linux 2 AMI ___|\\___|___| https://aws.amazon.com/amazon-linux-2/ 5 package(s) needed for security, out of 13 available Run \"sudo yum update\" to apply all updates. [ec2-user@ip-172-31-34-217 ~]$ [ec2-user@ip-172-31-34-217 ~]$ sudo su [root@ip-172-31-34-217 ec2-user]# yum update -y [root@ip-172-31-34-217 ec2-user]# yum install -y httpd [root@ip-172-31-34-217 ec2-user]# systemctl enable httpd Created symlink from /etc/systemd/system/multi-user.target.wants/httpd.service to /usr/lib/systemd/system/httpd.service. [root@ip-172-31-34-217 ec2-user]# systemctl start httpd [root@ip-172-31-34-217 ec2-user]# echo \"Hello World $(hostname -f)\" > /var/www/html/index.html [root@ip-172-31-34-217 ec2-user]# curl localhost:80 Hello World ip-172-31-34-217.eu-west-1.compute.internal Visit the public IP from a browser: Now we can create an image from the instance by doing the following: Now we can go to AMIs section and see the image. When it's complete, we can do multiple things with it: We'll click on launch and it will take us through a new instance creation process. Once it's online, we can visit it in browser:","title":"EC2 AMI Hands On"},{"location":"AWS/EC2/AMI/EC2%20AMI%20Hands%20On/#hands-on","text":"We are going to use a fresh EC2 instance and ssh into it. \u279c notes git:(master) \u2717 ssh -i ~/Downloads/EC2Tutorial.pem ec2-user@63.35.203.134 The authenticity of host '63.35.203.134 (63.35.203.134)' can't be established. ECDSA key fingerprint is SHA256:IRLAu1mhR88VLPx7js5V1kxaFHlg6TtNNVDSWpZKz5c. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added '63.35.203.134' (ECDSA) to the list of known hosts. __| __|_ ) _| ( / Amazon Linux 2 AMI ___|\\___|___| https://aws.amazon.com/amazon-linux-2/ 5 package(s) needed for security, out of 13 available Run \"sudo yum update\" to apply all updates. [ec2-user@ip-172-31-34-217 ~]$ [ec2-user@ip-172-31-34-217 ~]$ sudo su [root@ip-172-31-34-217 ec2-user]# yum update -y [root@ip-172-31-34-217 ec2-user]# yum install -y httpd [root@ip-172-31-34-217 ec2-user]# systemctl enable httpd Created symlink from /etc/systemd/system/multi-user.target.wants/httpd.service to /usr/lib/systemd/system/httpd.service. [root@ip-172-31-34-217 ec2-user]# systemctl start httpd [root@ip-172-31-34-217 ec2-user]# echo \"Hello World $(hostname -f)\" > /var/www/html/index.html [root@ip-172-31-34-217 ec2-user]# curl localhost:80 Hello World ip-172-31-34-217.eu-west-1.compute.internal Visit the public IP from a browser: Now we can create an image from the instance by doing the following: Now we can go to AMIs section and see the image. When it's complete, we can do multiple things with it: We'll click on launch and it will take us through a new instance creation process. Once it's online, we can visit it in browser:","title":"Hands on"},{"location":"AWS/EC2/AMI/EC2%20AMIs/","text":"EC2 AMIs \u00b6 As we previously saw, AWS comes with base images such as: - [[Ubuntu]] - [[Fedora]] - [[RedHat]] - [[Windows]] - etc These images can be customised at runtime using EC2 User Data But, you can create your own image, which can be done with an AMI . Using custom built AMI can provide with following advantages: - Pre-installed packages needed - Faster boot time (no need for EC2 User Data at boot time) - Machine comes configured with monitoring / enterprise software - Security concerns - control over the machines in the network - Control of maintenance and updates of AMIs over time - [[Active Directory Integration]] out of the box - Installing your app ahead of time (for faster deploys when auto-scaling) - Using someone else's EC2 AMIs that is optimised for running an app, DB, etc.. AMI are built for a specific AWS region! Using public AMIs - You can leverage AMIs from other people - You can also pay for other people's AMI by the hour - These people have optimised the software - The machine is easy to run and configure - You basically rent \"expertise\"'from the AMI creator - AMI can be found and published on the Amazon Marketplace Warning - Do not use an AMI you don't trust - Some AMIs might come with malware or may not be secure for your enterprise EC2 AMI Hands On Cross Account AMI Copy","title":"EC2 AMIs"},{"location":"AWS/EC2/AMI/EC2%20AMIs/#ec2-amis","text":"As we previously saw, AWS comes with base images such as: - [[Ubuntu]] - [[Fedora]] - [[RedHat]] - [[Windows]] - etc These images can be customised at runtime using EC2 User Data But, you can create your own image, which can be done with an AMI . Using custom built AMI can provide with following advantages: - Pre-installed packages needed - Faster boot time (no need for EC2 User Data at boot time) - Machine comes configured with monitoring / enterprise software - Security concerns - control over the machines in the network - Control of maintenance and updates of AMIs over time - [[Active Directory Integration]] out of the box - Installing your app ahead of time (for faster deploys when auto-scaling) - Using someone else's EC2 AMIs that is optimised for running an app, DB, etc.. AMI are built for a specific AWS region! Using public AMIs - You can leverage AMIs from other people - You can also pay for other people's AMI by the hour - These people have optimised the software - The machine is easy to run and configure - You basically rent \"expertise\"'from the AMI creator - AMI can be found and published on the Amazon Marketplace Warning - Do not use an AMI you don't trust - Some AMIs might come with malware or may not be secure for your enterprise EC2 AMI Hands On Cross Account AMI Copy","title":"EC2 AMIs"},{"location":"AWS/EC2/Elastic%20LoadBalancer/Application%20Load%20Balancer%20%28v2%29/","text":"Application Load Balancer (v2) \u00b6 Application [[Load Balancer]]s ([[Layer 7]]) allow to do: Load balancing to multiple [[HTTP]] applications across machines ([[target group]]s) Load balancing to multiple applications on the same machine (ex: [[container]]s) Load balancing based on route in URL Load Balancing based on [[hostname]] in URL Basically, they're awesome for micro services & container-based applications (example: Docker & [[Amazon ECS]]) Has a port mapping feature to redirect to a dynamic port In comparison, we would need to create one Classic Load Balancer per application before. That was very expensive and inefficient. Stickiness can be enabled at the target group level Same request goes to the same instance [[Stickiness]] is directly generated by the ALB (not the application) ALB support [[HTTP]]/[[HTTPS]] & [[Websockets]] protocols The application servers don't see the IP of the client directly The true [[IP]] of the client is inserted in the [[header]] [[X-Forwarded-For]]. We can also get [[Port]] ([[X-Forwarded-Port]]) and [[proto]] ([[X-Forwarded-Proto]])","title":"Application Load Balancer (v2)"},{"location":"AWS/EC2/Elastic%20LoadBalancer/Application%20Load%20Balancer%20%28v2%29/#application-load-balancer-v2","text":"Application [[Load Balancer]]s ([[Layer 7]]) allow to do: Load balancing to multiple [[HTTP]] applications across machines ([[target group]]s) Load balancing to multiple applications on the same machine (ex: [[container]]s) Load balancing based on route in URL Load Balancing based on [[hostname]] in URL Basically, they're awesome for micro services & container-based applications (example: Docker & [[Amazon ECS]]) Has a port mapping feature to redirect to a dynamic port In comparison, we would need to create one Classic Load Balancer per application before. That was very expensive and inefficient. Stickiness can be enabled at the target group level Same request goes to the same instance [[Stickiness]] is directly generated by the ALB (not the application) ALB support [[HTTP]]/[[HTTPS]] & [[Websockets]] protocols The application servers don't see the IP of the client directly The true [[IP]] of the client is inserted in the [[header]] [[X-Forwarded-For]]. We can also get [[Port]] ([[X-Forwarded-Port]]) and [[proto]] ([[X-Forwarded-Proto]])","title":"Application Load Balancer (v2)"},{"location":"AWS/EC2/Elastic%20LoadBalancer/Auto%20Scaling%20Group%20%20%28ASG%29%20For%20Solution%20Architects/","text":"Auto Scaling Groups For Solutions Architects \u00b6 [[ASG Default Termination Policy]] (simplified version) Find the AZ which has the most number of instances If there are multiple instances in the Availability Zone s to choose from, delete the one with the oldest configuration ASG tries the balance the number of instances across Availability Zone s by default The [[cool-down period]] helps to ensure that your Auto Scaling group doesn't launch or terminate additional instances before the previous scaling activity takes effect. In addition to default cool-down for Auto Scaling Group (ASG) , we can create cool-downs that apply to a specific [[simple scaling policy]] A scaling-specific cool-down period overrides the [[default cool-down period]] One common use for scaling-specific cool-downs is with a [[scale-in policy]] - a policy that terminates instances based on a specific criteria or metric. Because this policy terminates instances, [[Amazon EC2 Auto Scaling]] needs less time to determine whether to terminate additional instances. If the default cool-down period of 300 seconds is too long - you can reduce costs by applying a scaling-specific cool-down period of 180 seconds to the [[scale-in policy]]. If your application is scaling up and down multiple times each hour, modify the auto scaling groups cool-down timers and the cloud watch alarm period that triggers the [[Scale in]]","title":"Auto Scaling Group  (ASG) For Solution Architects"},{"location":"AWS/EC2/Elastic%20LoadBalancer/Auto%20Scaling%20Group%20%20%28ASG%29%20For%20Solution%20Architects/#auto-scaling-groups-for-solutions-architects","text":"[[ASG Default Termination Policy]] (simplified version) Find the AZ which has the most number of instances If there are multiple instances in the Availability Zone s to choose from, delete the one with the oldest configuration ASG tries the balance the number of instances across Availability Zone s by default The [[cool-down period]] helps to ensure that your Auto Scaling group doesn't launch or terminate additional instances before the previous scaling activity takes effect. In addition to default cool-down for Auto Scaling Group (ASG) , we can create cool-downs that apply to a specific [[simple scaling policy]] A scaling-specific cool-down period overrides the [[default cool-down period]] One common use for scaling-specific cool-downs is with a [[scale-in policy]] - a policy that terminates instances based on a specific criteria or metric. Because this policy terminates instances, [[Amazon EC2 Auto Scaling]] needs less time to determine whether to terminate additional instances. If the default cool-down period of 300 seconds is too long - you can reduce costs by applying a scaling-specific cool-down period of 180 seconds to the [[scale-in policy]]. If your application is scaling up and down multiple times each hour, modify the auto scaling groups cool-down timers and the cloud watch alarm period that triggers the [[Scale in]]","title":"Auto Scaling Groups For Solutions Architects"},{"location":"AWS/EC2/Elastic%20LoadBalancer/Auto%20Scaling%20Group%20%28ASG%29/","text":"Auto Scaling Groups \u00b6 In real-life, the load on your websites and application can change In the cloud, you can create and get rid of servers very quickly The goal of an Auto Scaling Group (ASG) is to: [[Scale out]] (add AWS EC2 instances) to match an increased load [[Scale in]] (remove AWS EC2 instances) to match a decreased load Ensure we have a minimum and maximum number of machines running Automatically register new instances to a [[Load Balancer]] ASGs have the following attributes - A launch configuration - EC2 AMIs + EC2 Instance Types - EC2 User Data - EBS Volume - Security Group - [[SSH Key Pair]] - Min Size / Max Size / Initial Capacity - [[Network]] + Subnet Information - [[Load Balancer]] Information - [[Scaling Policies]] Auto Scaling Alarms \u00b6 It is possible to scale an ASG based on CloudWatch Alarm An alarm monitors a metric (such as Average [[CPU]]) Metrics are computed for the overall ASG instances Based on the alarm: We can create [[scale-out policies]] (increase the number of instances) We can create [[scale-in policy]] (decrease the number of instances) It is now possible to define \"better\" auto scaling rules that are directly managed by AWS EC2 Target average [[CPU usage]] Number of requests on the [[Elastic Load Balancer]] per instance [[Average Network In]] [[Average Network Out]] These rules are easier to set up and can make more sense We can also create custom metrics (ex: number of connected users) This can be done by sending a custom metric from application via PutMetric API, then creating a CloudWatch Alarm reacting to low / high values. [[Scaling policies]] can be on CPU, Network... and can even be on custom metrics or based on a schedule. ASGs use Launch configurations and you update an ASG by providing a new launch configuration [[IAM Role]] attached to an ASG will get assigned to AWS EC2 instances ASG are free. You pay for the underlying resources being launched Having instances under an ASG means that if they get terminated for whatever reason, the ASG will restart them. Extra safety! ASG can terminate instances marked as unhealthy by an [[Load Balancer]] (and hence replace them) Hands On \u00b6 We can go to the Auto Scaling Group section in EC2 to set up this up. When clicking on the Create Auto Scaling Group we are given the steps - we'll need to create and auto scaling template and the and then the group itself. Then we basically follow the steps as it would be for any EC2 instance. After finishing this, we are redirected to Auto Scaling Group Creation . After the initialization, we can see that it has added 1 instance to it.","title":"Auto Scaling Groups"},{"location":"AWS/EC2/Elastic%20LoadBalancer/Auto%20Scaling%20Group%20%28ASG%29/#auto-scaling-groups","text":"In real-life, the load on your websites and application can change In the cloud, you can create and get rid of servers very quickly The goal of an Auto Scaling Group (ASG) is to: [[Scale out]] (add AWS EC2 instances) to match an increased load [[Scale in]] (remove AWS EC2 instances) to match a decreased load Ensure we have a minimum and maximum number of machines running Automatically register new instances to a [[Load Balancer]] ASGs have the following attributes - A launch configuration - EC2 AMIs + EC2 Instance Types - EC2 User Data - EBS Volume - Security Group - [[SSH Key Pair]] - Min Size / Max Size / Initial Capacity - [[Network]] + Subnet Information - [[Load Balancer]] Information - [[Scaling Policies]]","title":"Auto Scaling Groups"},{"location":"AWS/EC2/Elastic%20LoadBalancer/Auto%20Scaling%20Group%20%28ASG%29/#auto-scaling-alarms","text":"It is possible to scale an ASG based on CloudWatch Alarm An alarm monitors a metric (such as Average [[CPU]]) Metrics are computed for the overall ASG instances Based on the alarm: We can create [[scale-out policies]] (increase the number of instances) We can create [[scale-in policy]] (decrease the number of instances) It is now possible to define \"better\" auto scaling rules that are directly managed by AWS EC2 Target average [[CPU usage]] Number of requests on the [[Elastic Load Balancer]] per instance [[Average Network In]] [[Average Network Out]] These rules are easier to set up and can make more sense We can also create custom metrics (ex: number of connected users) This can be done by sending a custom metric from application via PutMetric API, then creating a CloudWatch Alarm reacting to low / high values. [[Scaling policies]] can be on CPU, Network... and can even be on custom metrics or based on a schedule. ASGs use Launch configurations and you update an ASG by providing a new launch configuration [[IAM Role]] attached to an ASG will get assigned to AWS EC2 instances ASG are free. You pay for the underlying resources being launched Having instances under an ASG means that if they get terminated for whatever reason, the ASG will restart them. Extra safety! ASG can terminate instances marked as unhealthy by an [[Load Balancer]] (and hence replace them)","title":"Auto Scaling Alarms"},{"location":"AWS/EC2/Elastic%20LoadBalancer/Auto%20Scaling%20Group%20%28ASG%29/#hands-on","text":"We can go to the Auto Scaling Group section in EC2 to set up this up. When clicking on the Create Auto Scaling Group we are given the steps - we'll need to create and auto scaling template and the and then the group itself. Then we basically follow the steps as it would be for any EC2 instance. After finishing this, we are redirected to Auto Scaling Group Creation . After the initialization, we can see that it has added 1 instance to it.","title":"Hands On"},{"location":"AWS/EC2/Elastic%20LoadBalancer/High%20Availability%20and%20Scalability/","text":"High Availability and Scalability \u00b6 [[Scalability]] means that application / system can handle greater loads by adapting. There are two kinds of scalability: Vertical scalability Horizontal scalability Scalability is linked but different to Programming/AWS/Fundamentals/High Availability","title":"High Availability and Scalability"},{"location":"AWS/EC2/Elastic%20LoadBalancer/High%20Availability%20and%20Scalability/#high-availability-and-scalability","text":"[[Scalability]] means that application / system can handle greater loads by adapting. There are two kinds of scalability: Vertical scalability Horizontal scalability Scalability is linked but different to Programming/AWS/Fundamentals/High Availability","title":"High Availability and Scalability"},{"location":"AWS/EC2/Elastic%20LoadBalancer/Load%20Balancer%20stickiness/","text":"Load Balancer Stickiness \u00b6 Each instance saves the [[session]] In order to keep it, we need to route user to the same instance Can create [[load disbalance]]","title":"Load Balancer stickiness"},{"location":"AWS/EC2/Elastic%20LoadBalancer/Load%20Balancer%20stickiness/#load-balancer-stickiness","text":"Each instance saves the [[session]] In order to keep it, we need to route user to the same instance Can create [[load disbalance]]","title":"Load Balancer Stickiness"},{"location":"AWS/EC2/Elastic%20LoadBalancer/Load%20Balancing/","text":"Load Balancing \u00b6 Load balancers are servers that forward internet traffic to multiple servers (EC2 instances) downstream. Why use a [[load balancer]]? \u00b6 Spread load across multiple downstream instances Expose a single point of access (DNS) to your application Seamlessly handle failures of downstream instances Do regular health checks to your instances Provide [[SSL termination]] ([[HTTPS]]) for your websites Enforce [[stickiness]] with [[cookies]] High Availability across zones Separate [[public traffic]] from [[private traffic]] Why use an EC2 Load Balancer? \u00b6 An ELB (EC2 Load Balancer) is a managed load balancer AWS guarantees that it will be working AWS takes care of upgrades, maintenance, high availability AWS provides only a few configuration knobs It costs less to setup your own load balancer but it will be a lot more effort on your end. It is integrated with many AWS offerings / services Types of load balancers on AWS \u00b6 [[Classic Load Balancer]] (v1 - old generation) - 2009 Application Load Balancer (v2) (v2 - new generation) - 2016 Network Load Balancer (v2) (v2 - new generation) - 2017 Overall, it is recommended to use the newer / v2 generation load balancers as they provide more features. You can setup internal (private) or external (public) ELBs. Good to know \u00b6 [[Classic Load Balancer]] are Deprecated Application Load Balancer (v2) for [[HTTP]] / [[HTTPS]] & [[Websockets]] Network Load Balancer (v2) for [[TCP]] CLB and ALB support [[SSL certificates]] and provide [[SSL termination]] All Load Balancers have Health Checks capability ALB can route based on hostname / path ALB is a great fit with ECS - Elastic Container Service ( docker ) Any Load Balancer (CLB, ALB, NLB) has a [[static host name]]. Do not resolve and use underlying IP. LBs can scale but not instantaneously - contact AWS for a \"warm-up\" NLB directly see the client IP 4xx errors are client induced errors 5xx errors are application induced errors Load Balancer Errors 503 means at capacity or no registed target If the LB can't connect to your application, check your security groups. [[HTTP STATUS CODES]] Hands On \u00b6 We can create a Load Balancer in Load Balancers section. There we can choose between the 3 types of load balancers: Once the Load Balancer has been provisioned, we can get it's DNS name. And when opening it via browser, it should be working: In listeners section we can see that the port 80 is forwarded to the my-apache-target-group , if we want we can add more target groups. Or even add custom rules When visiting the target group, we can add more targets to the group. Lastly, we can go to the Security Groups and modify the my-first-security-group that the web app uses to accept traffic only from the load balancer. So now, when we visit the load balancer URL, everything should be working, but we visit the ec2 directly, it has a timeout. Adding instances to Load Balancer \u00b6 Currently the Load Balancer isn't really doing any load balancing, since there is only one instance. We might want to add more intances. Now, we'll create 3 instances with user data like in the previous sections. Now we can go to the Target Groups and add these targets. Now, when refreshing the load balancer url, we should see different hostnames. [[Stickiness]] \u00b6 It is possible to implement stickiness so that the same client is always redirected to the same instance behind a load balancer This works for Classic Load Balancers & Application Load Balancers The \"[[cookie]]\" used for stickiness has an expiration date you control Use case: make sure the user doesn't lose his session data Enabling stickiness may bring imbalance to the load over the backend EC2 instances The stickiness can be enabled while modifying target groups. Load Balancers for Solutions Architect \u00b6 Classic Load Balancers: questions on security groups, stickiness Application Load Balancer ([[Layer 7]] of [[OSI]]) Support routing based on [[hostname]] Support routing based on path Support redirects (e.g. [[HTTP]] to [[HTTPS]]) Support [[dynamic host port mapping]] with ECS NLB ([[Layer 4]] of [[OSI]]) gets a [[static IP]] per Availability Zone : Public facing: must attach Elastic IP - can help whitelist by clients Private facing: will get random Private IP based on free ones at the time of creation Has [[cross zone balancing]] Has [[SSL termination]] (Jan 2019)","title":"Load Balancing"},{"location":"AWS/EC2/Elastic%20LoadBalancer/Load%20Balancing/#load-balancing","text":"Load balancers are servers that forward internet traffic to multiple servers (EC2 instances) downstream.","title":"Load Balancing"},{"location":"AWS/EC2/Elastic%20LoadBalancer/Load%20Balancing/#why-use-a-load-balancer","text":"Spread load across multiple downstream instances Expose a single point of access (DNS) to your application Seamlessly handle failures of downstream instances Do regular health checks to your instances Provide [[SSL termination]] ([[HTTPS]]) for your websites Enforce [[stickiness]] with [[cookies]] High Availability across zones Separate [[public traffic]] from [[private traffic]]","title":"Why use a [[load balancer]]?"},{"location":"AWS/EC2/Elastic%20LoadBalancer/Load%20Balancing/#why-use-an-ec2-load-balancer","text":"An ELB (EC2 Load Balancer) is a managed load balancer AWS guarantees that it will be working AWS takes care of upgrades, maintenance, high availability AWS provides only a few configuration knobs It costs less to setup your own load balancer but it will be a lot more effort on your end. It is integrated with many AWS offerings / services","title":"Why use an EC2 Load Balancer?"},{"location":"AWS/EC2/Elastic%20LoadBalancer/Load%20Balancing/#types-of-load-balancers-on-aws","text":"[[Classic Load Balancer]] (v1 - old generation) - 2009 Application Load Balancer (v2) (v2 - new generation) - 2016 Network Load Balancer (v2) (v2 - new generation) - 2017 Overall, it is recommended to use the newer / v2 generation load balancers as they provide more features. You can setup internal (private) or external (public) ELBs.","title":"Types of load balancers on AWS"},{"location":"AWS/EC2/Elastic%20LoadBalancer/Load%20Balancing/#good-to-know","text":"[[Classic Load Balancer]] are Deprecated Application Load Balancer (v2) for [[HTTP]] / [[HTTPS]] & [[Websockets]] Network Load Balancer (v2) for [[TCP]] CLB and ALB support [[SSL certificates]] and provide [[SSL termination]] All Load Balancers have Health Checks capability ALB can route based on hostname / path ALB is a great fit with ECS - Elastic Container Service ( docker ) Any Load Balancer (CLB, ALB, NLB) has a [[static host name]]. Do not resolve and use underlying IP. LBs can scale but not instantaneously - contact AWS for a \"warm-up\" NLB directly see the client IP 4xx errors are client induced errors 5xx errors are application induced errors Load Balancer Errors 503 means at capacity or no registed target If the LB can't connect to your application, check your security groups. [[HTTP STATUS CODES]]","title":"Good to know"},{"location":"AWS/EC2/Elastic%20LoadBalancer/Load%20Balancing/#hands-on","text":"We can create a Load Balancer in Load Balancers section. There we can choose between the 3 types of load balancers: Once the Load Balancer has been provisioned, we can get it's DNS name. And when opening it via browser, it should be working: In listeners section we can see that the port 80 is forwarded to the my-apache-target-group , if we want we can add more target groups. Or even add custom rules When visiting the target group, we can add more targets to the group. Lastly, we can go to the Security Groups and modify the my-first-security-group that the web app uses to accept traffic only from the load balancer. So now, when we visit the load balancer URL, everything should be working, but we visit the ec2 directly, it has a timeout.","title":"Hands On"},{"location":"AWS/EC2/Elastic%20LoadBalancer/Load%20Balancing/#adding-instances-to-load-balancer","text":"Currently the Load Balancer isn't really doing any load balancing, since there is only one instance. We might want to add more intances. Now, we'll create 3 instances with user data like in the previous sections. Now we can go to the Target Groups and add these targets. Now, when refreshing the load balancer url, we should see different hostnames.","title":"Adding instances to Load Balancer"},{"location":"AWS/EC2/Elastic%20LoadBalancer/Load%20Balancing/#stickiness","text":"It is possible to implement stickiness so that the same client is always redirected to the same instance behind a load balancer This works for Classic Load Balancers & Application Load Balancers The \"[[cookie]]\" used for stickiness has an expiration date you control Use case: make sure the user doesn't lose his session data Enabling stickiness may bring imbalance to the load over the backend EC2 instances The stickiness can be enabled while modifying target groups.","title":"[[Stickiness]]"},{"location":"AWS/EC2/Elastic%20LoadBalancer/Load%20Balancing/#load-balancers-for-solutions-architect","text":"Classic Load Balancers: questions on security groups, stickiness Application Load Balancer ([[Layer 7]] of [[OSI]]) Support routing based on [[hostname]] Support routing based on path Support redirects (e.g. [[HTTP]] to [[HTTPS]]) Support [[dynamic host port mapping]] with ECS NLB ([[Layer 4]] of [[OSI]]) gets a [[static IP]] per Availability Zone : Public facing: must attach Elastic IP - can help whitelist by clients Private facing: will get random Private IP based on free ones at the time of creation Has [[cross zone balancing]] Has [[SSL termination]] (Jan 2019)","title":"Load Balancers for Solutions Architect"},{"location":"AWS/EC2/Elastic%20LoadBalancer/Network%20Load%20Balancer%20%28v2%29/","text":"Network Load Balancer (v2) \u00b6 Network [[Load Balancer]]s ([[layer 4]]) allow to do: Forward [[TCP]] traffic to your instances Handle millions of requests per seconds Support for [[static IP]] or elastic IP Less latency ~100ms (vs 400ms for Application Load Balancer (v2) ) Network Load Balancers are mostly used for extreme performance and should not be the the default load balancer you choose Overall, the creation process is the same as Application Load Balancer (v2)","title":"Network Load Balancer (v2)"},{"location":"AWS/EC2/Elastic%20LoadBalancer/Network%20Load%20Balancer%20%28v2%29/#network-load-balancer-v2","text":"Network [[Load Balancer]]s ([[layer 4]]) allow to do: Forward [[TCP]] traffic to your instances Handle millions of requests per seconds Support for [[static IP]] or elastic IP Less latency ~100ms (vs 400ms for Application Load Balancer (v2) ) Network Load Balancers are mostly used for extreme performance and should not be the the default load balancer you choose Overall, the creation process is the same as Application Load Balancer (v2)","title":"Network Load Balancer (v2)"},{"location":"AWS/EC2/IP/Elastic%20IP/","text":"Elastic IP \u00b6 When you stop and then start an AWS EC2 instance, it can change it's Public IP If you need to have a fixed public IP for your instance, you need an Elastic IP An elastic IP is a public [[IPv4]] IP you own as long as you don't delete it You can attach to it one instance at a time With an Elastic IP address, you can mask the failure of an instance or software by rapidly remapping the address to another instance on your account You can only have 5 Elastic IP in your account (you can ask AWS to increase that) Overall, try to avoid using Elastic IP They often reflect poor [[architectural decisions]] Instead, use a random public IP and register a DNS name to it Or use a Load Balancer and don't use a public IP at all","title":"Elastic IP"},{"location":"AWS/EC2/IP/Elastic%20IP/#elastic-ip","text":"When you stop and then start an AWS EC2 instance, it can change it's Public IP If you need to have a fixed public IP for your instance, you need an Elastic IP An elastic IP is a public [[IPv4]] IP you own as long as you don't delete it You can attach to it one instance at a time With an Elastic IP address, you can mask the failure of an instance or software by rapidly remapping the address to another instance on your account You can only have 5 Elastic IP in your account (you can ask AWS to increase that) Overall, try to avoid using Elastic IP They often reflect poor [[architectural decisions]] Instead, use a random public IP and register a DNS name to it Or use a Load Balancer and don't use a public IP at all","title":"Elastic IP"},{"location":"AWS/EC2/IP/Private%20IP/","text":"Private IP \u00b6 Private IP means the machine only be identified on a private network The IP must be unique across the private network But two different private networks (two companies) can have the same IPs Machines connect to internet through a gateway Only a specified range of IPs can be used as a private IP 10.0.0.0 - 10.255.255.255 (10.0.0.0/8) - in big networks 172.16.0.0 - 172.31.255.255 (172.16.0.0/12) - default AWS one 192.168.0.0-192.168.255.255 (192.168.0.0/16) - example: home networks","title":"Private IP"},{"location":"AWS/EC2/IP/Private%20IP/#private-ip","text":"Private IP means the machine only be identified on a private network The IP must be unique across the private network But two different private networks (two companies) can have the same IPs Machines connect to internet through a gateway Only a specified range of IPs can be used as a private IP 10.0.0.0 - 10.255.255.255 (10.0.0.0/8) - in big networks 172.16.0.0 - 172.31.255.255 (172.16.0.0/12) - default AWS one 192.168.0.0-192.168.255.255 (192.168.0.0/16) - example: home networks","title":"Private IP"},{"location":"AWS/EC2/IP/Private%20vs%20Public%20vs%20Elastic%20IP/","text":"Private vs Public vs Elastic IP \u00b6 By default, your AWS EC2 machine comes with: - A private IP for internal AWS network - A public IP for the [[internet]] When we are doing ssh into our AWS EC2 machines: - We can't use a private IP, because we are not in the same network - We can only use the public IP If your machine is stopped and then started, the public IP can change When viewing the EC2 instance, we can see these IP addresses: You can also visit the Elastic IP s section and allocate new Elastic IP addresses. Once it's created, we can associate it with an instance.","title":"Private vs Public vs Elastic IP"},{"location":"AWS/EC2/IP/Private%20vs%20Public%20vs%20Elastic%20IP/#private-vs-public-vs-elastic-ip","text":"By default, your AWS EC2 machine comes with: - A private IP for internal AWS network - A public IP for the [[internet]] When we are doing ssh into our AWS EC2 machines: - We can't use a private IP, because we are not in the same network - We can only use the public IP If your machine is stopped and then started, the public IP can change When viewing the EC2 instance, we can see these IP addresses: You can also visit the Elastic IP s section and allocate new Elastic IP addresses. Once it's created, we can associate it with an instance.","title":"Private vs Public vs Elastic IP"},{"location":"AWS/EC2/IP/Public%20IP/","text":"Public IP \u00b6 The machine can be identified on the internet Must be unique across the whole web Can be geo-located easily","title":"Public IP"},{"location":"AWS/EC2/IP/Public%20IP/#public-ip","text":"The machine can be identified on the internet Must be unique across the whole web Can be geo-located easily","title":"Public IP"},{"location":"AWS/EC2/Placement/EC2%20Cluster%20Placement%20Strategy/","text":"EC2 Cluster Placement Strategy \u00b6 Pros: - Great network (10gbps bandwidth between instances) Cons: - If the rack fails, all instances fails at the same time Use case: - Big Data job that needs to complete fast - Application that needs extremely low latency and high network throughput","title":"EC2 Cluster Placement Strategy"},{"location":"AWS/EC2/Placement/EC2%20Cluster%20Placement%20Strategy/#ec2-cluster-placement-strategy","text":"Pros: - Great network (10gbps bandwidth between instances) Cons: - If the rack fails, all instances fails at the same time Use case: - Big Data job that needs to complete fast - Application that needs extremely low latency and high network throughput","title":"EC2 Cluster Placement Strategy"},{"location":"AWS/EC2/Placement/EC2%20Partition%20Placement%20Strategy/","text":"Partition Placement Strategy \u00b6 Up to 7 partitions per Availability Zone Up to 100s of AWS EC2 instances The instances in a partition do not share racks with the instances in other partitions A [[partition failure]] can affect many AWS EC2 but won't affect other partitions AWS EC2 instances get access to the partition information as metadata Use cases: - [[HDFS]] - [[HBase]] - [[Cassandra]] - [[Kafka]]","title":"EC2 Partition Placement Strategy"},{"location":"AWS/EC2/Placement/EC2%20Partition%20Placement%20Strategy/#partition-placement-strategy","text":"Up to 7 partitions per Availability Zone Up to 100s of AWS EC2 instances The instances in a partition do not share racks with the instances in other partitions A [[partition failure]] can affect many AWS EC2 but won't affect other partitions AWS EC2 instances get access to the partition information as metadata Use cases: - [[HDFS]] - [[HBase]] - [[Cassandra]] - [[Kafka]]","title":"Partition Placement Strategy"},{"location":"AWS/EC2/Placement/EC2%20Placement%20Groups/","text":"EC2 Placement Groups \u00b6 Sometimes you want control over the AWS EC2 Instance placement strategy That strategy can be defined using placement groups When you create a placement group, you specify one of the following strategies for the group Cluster - clusters instances into a low-latency group in a single Availability Zone Spread - spreads instances across underlying hardware (max 7 instances per group per Availability Zone ) - critical applications Partition - spreads instances across many different partitions (which rely on sets of racks) within an Availability Zone . Scales to 100s of AWS EC2 instances per group ([[Hadoop]], [[Cassandra]], [[Kafka]]) Usage \u00b6 You can find these options under Network & Security -> Placement Groups . Now, when creating a new instance, we can specify a placement group.","title":"EC2 Placement Groups"},{"location":"AWS/EC2/Placement/EC2%20Placement%20Groups/#ec2-placement-groups","text":"Sometimes you want control over the AWS EC2 Instance placement strategy That strategy can be defined using placement groups When you create a placement group, you specify one of the following strategies for the group Cluster - clusters instances into a low-latency group in a single Availability Zone Spread - spreads instances across underlying hardware (max 7 instances per group per Availability Zone ) - critical applications Partition - spreads instances across many different partitions (which rely on sets of racks) within an Availability Zone . Scales to 100s of AWS EC2 instances per group ([[Hadoop]], [[Cassandra]], [[Kafka]])","title":"EC2 Placement Groups"},{"location":"AWS/EC2/Placement/EC2%20Placement%20Groups/#usage","text":"You can find these options under Network & Security -> Placement Groups . Now, when creating a new instance, we can specify a placement group.","title":"Usage"},{"location":"AWS/EC2/Placement/EC2%20Spread%20Placement%20Strategy/","text":"Spread Placement Strategy \u00b6 Pros: - Can span across Availability Zone s - Reduced risk of simultaneous failure - EC2 Instances are on different physical hardware Cons: - Limited to 7 instances per Availability Zone per EC2 Placement Groups Use case: - Application that needs to maximise High Availability - [[Critical Applications]] where each instance must be isolated from failure from each other","title":"EC2 Spread Placement Strategy"},{"location":"AWS/EC2/Placement/EC2%20Spread%20Placement%20Strategy/#spread-placement-strategy","text":"Pros: - Can span across Availability Zone s - Reduced risk of simultaneous failure - EC2 Instances are on different physical hardware Cons: - Limited to 7 instances per Availability Zone per EC2 Placement Groups Use case: - Application that needs to maximise High Availability - [[Critical Applications]] where each instance must be isolated from failure from each other","title":"Spread Placement Strategy"},{"location":"AWS/EC2/SSH/EC2%20SSH%20Instance%20Connect/","text":"EC2 Instance Connect \u00b6 While viewing the instances in our AWS EC2 dashboard, we can select an instance and click on Connect button.","title":"EC2 SSH Instance Connect"},{"location":"AWS/EC2/SSH/EC2%20SSH%20Instance%20Connect/#ec2-instance-connect","text":"While viewing the instances in our AWS EC2 dashboard, we can select an instance and click on Connect button.","title":"EC2 Instance Connect"},{"location":"AWS/EC2/SSH/EC2%20SSH%20Troubleshooting/","text":"Troubleshooting \u00b6 There's a connection timeout. This is a Security Group issue. Any timeout (not just for [[SSH]]) is related to Security Group s or a [[Firewall]]. Ensure your security group looks like this and correctly assigned to your AWS EC2 instance. There's still a connection timeout issue If your security group is properly configured as above, and you still have connection timeout issues, then that means a corporate [[firewall]] or a personal firewall is blocking the connection. Please use EC2 SSH Instance Connect as described in the next lecture. SSH does not work on Windows If it says: ssh command not found, that means you have to use Putty Follow again the video. If things don't work, please use EC2 SSH Instance Connect as described in the next lecture There's a connection refused This means the instance is reachable, but no [[SSH]] utility is running on the instance Try to restart the instance If it doesn't work, terminate the instance and create a new one. Make sure you're using [[Amazon Linux 2]] Permission denied (publickey,gssapi-keyex,gssapi-with-mic) This means either two things: You are using the wrong security key or not using a security key. Please look at your AWS EC2 instance configuration to make sure you have assigned the correct key to it. You are using the wrong user. Make sure you have started an [[Amazon Linux 2]] AWS EC2 instance, and make sure you're using the user ec2-user. This is something you specify when doing ec2-user@ (ex: ec2-user@35.180.242.162) in your SSH command or your [[Putty]] configuration","title":"EC2 SSH Troubleshooting"},{"location":"AWS/EC2/SSH/EC2%20SSH%20Troubleshooting/#troubleshooting","text":"There's a connection timeout. This is a Security Group issue. Any timeout (not just for [[SSH]]) is related to Security Group s or a [[Firewall]]. Ensure your security group looks like this and correctly assigned to your AWS EC2 instance. There's still a connection timeout issue If your security group is properly configured as above, and you still have connection timeout issues, then that means a corporate [[firewall]] or a personal firewall is blocking the connection. Please use EC2 SSH Instance Connect as described in the next lecture. SSH does not work on Windows If it says: ssh command not found, that means you have to use Putty Follow again the video. If things don't work, please use EC2 SSH Instance Connect as described in the next lecture There's a connection refused This means the instance is reachable, but no [[SSH]] utility is running on the instance Try to restart the instance If it doesn't work, terminate the instance and create a new one. Make sure you're using [[Amazon Linux 2]] Permission denied (publickey,gssapi-keyex,gssapi-with-mic) This means either two things: You are using the wrong security key or not using a security key. Please look at your AWS EC2 instance configuration to make sure you have assigned the correct key to it. You are using the wrong user. Make sure you have started an [[Amazon Linux 2]] AWS EC2 instance, and make sure you're using the user ec2-user. This is something you specify when doing ec2-user@ (ex: ec2-user@35.180.242.162) in your SSH command or your [[Putty]] configuration","title":"Troubleshooting"},{"location":"AWS/EC2/SSH/EC2%20SSH%20Using%20Windows%2010/","text":"[[SSH]] using [[Windows]] 10 \u00b6 You can use [[PowerShell]] or [[cmd]]. Type in ssh and it should be available. If it's not, you will need to use the [[PuTTY]] route. If it throws an error regarding [[keypair permissions]], open up the properties of the file, go to the Security tab -> Advanced , change the owner to yourself, remove any other users. If you cannot do that, disable the Inheritance.","title":"EC2 SSH Using Windows 10"},{"location":"AWS/EC2/SSH/EC2%20SSH%20Using%20Windows%2010/#ssh-using-windows-10","text":"You can use [[PowerShell]] or [[cmd]]. Type in ssh and it should be available. If it's not, you will need to use the [[PuTTY]] route. If it throws an error regarding [[keypair permissions]], open up the properties of the file, go to the Security tab -> Advanced , change the owner to yourself, remove any other users. If you cannot do that, disable the Inheritance.","title":"[[SSH]] using [[Windows]] 10"},{"location":"AWS/EC2/SSH/EC2%20SSH%20Using%20Windows/","text":"[[SSH]] Using [[Windows]] \u00b6 Download Putty and install it. We can open up Putty Key Generator to convert the [[keypair]] we downloaded previously from .pem to .ppk . Click on Load private key , open up the keypair. Load it and click on Save private key . Then we can open up PuTTY and enter the username and IP address of our EC2 machine, save it. Then go to Connection -> SSH -> Auth , where we can specify the private key.","title":"EC2 SSH Using Windows"},{"location":"AWS/EC2/SSH/EC2%20SSH%20Using%20Windows/#ssh-using-windows","text":"Download Putty and install it. We can open up Putty Key Generator to convert the [[keypair]] we downloaded previously from .pem to .ppk . Click on Load private key , open up the keypair. Load it and click on Save private key . Then we can open up PuTTY and enter the username and IP address of our EC2 machine, save it. Then go to Connection -> SSH -> Auth , where we can specify the private key.","title":"[[SSH]] Using [[Windows]]"},{"location":"AWS/EC2/SSH/EC2%20SSH%20from%20Linux%20or%20Mac/","text":"[[SSH]] from [[Linux]] or [[Mac]] \u00b6 Get the Public IP of the AWS EC2 instance you want to connect to: Open up a [[terminal]] on your OS. Move your key pair file you downloaded when creating the instance. \u279c notes git:(master) \u2717 sudo chmod 0400 ~/Downloads/EC2Tutorial.pem [sudo] password for davis: er@52.211.212.92 The authenticity of host '52.211.212.92 (52.211.212.92)' can't be established. ECDSA key fingerprint is SHA256:8os5YxNJ1iInCADO07oMntsT/8g+kSsK4z2R1xQvzZU. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added '52.211.212.92' (ECDSA) to the list of known hosts. __| __|_ ) _| ( / Amazon Linux 2 AMI ___|\\___|___| https://aws.amazon.com/amazon-linux-2/ 5 package(s) needed for security, out of 13 available Run \"sudo yum update\" to apply all updates. [ec2-user@ip-172-31-41-154 ~]$ pwd /home/ec2-user [ec2-user@ip-172-31-41-154 ~]$","title":"EC2 SSH from Linux or Mac"},{"location":"AWS/EC2/SSH/EC2%20SSH%20from%20Linux%20or%20Mac/#ssh-from-linux-or-mac","text":"Get the Public IP of the AWS EC2 instance you want to connect to: Open up a [[terminal]] on your OS. Move your key pair file you downloaded when creating the instance. \u279c notes git:(master) \u2717 sudo chmod 0400 ~/Downloads/EC2Tutorial.pem [sudo] password for davis: er@52.211.212.92 The authenticity of host '52.211.212.92 (52.211.212.92)' can't be established. ECDSA key fingerprint is SHA256:8os5YxNJ1iInCADO07oMntsT/8g+kSsK4z2R1xQvzZU. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added '52.211.212.92' (ECDSA) to the list of known hosts. __| __|_ ) _| ( / Amazon Linux 2 AMI ___|\\___|___| https://aws.amazon.com/amazon-linux-2/ 5 package(s) needed for security, out of 13 available Run \"sudo yum update\" to apply all updates. [ec2-user@ip-172-31-41-154 ~]$ pwd /home/ec2-user [ec2-user@ip-172-31-41-154 ~]$","title":"[[SSH]] from [[Linux]] or [[Mac]]"},{"location":"AWS/EC2/SSH/EC2%20SSH/","text":"SSH Overview \u00b6 We can use SSH to connect to the newly created instance. It works for Mac, Linux and Windows 10. If you're on windows below version 10, you can use Putty to ssh into the instance. Alternatively, you can use EC2 Instance Connect to do the same thing via the AWS Console . EC2 SSH from Linux or Mac EC2 SSH Using Windows EC2 SSH Using Windows 10 EC2 SSH Instance Connect EC2 SSH Troubleshooting","title":"SSH Overview"},{"location":"AWS/EC2/SSH/EC2%20SSH/#ssh-overview","text":"We can use SSH to connect to the newly created instance. It works for Mac, Linux and Windows 10. If you're on windows below version 10, you can use Putty to ssh into the instance. Alternatively, you can use EC2 Instance Connect to do the same thing via the AWS Console . EC2 SSH from Linux or Mac EC2 SSH Using Windows EC2 SSH Using Windows 10 EC2 SSH Instance Connect EC2 SSH Troubleshooting","title":"SSH Overview"},{"location":"AWS/EC2/Security%20Group/Security%20Group/","text":"Introduction to Security Groups \u00b6 Security Groups are the fundamental of [[network security]] in AWS They control how traffic is allowed into or out of our AWS EC2 machines. It is the most fundamental skill to learn to [[troubleshoot networking issues]] We can view our instance security groups and rules by clicking on them and viewing Security Groups . By clicking on a security group, we can see and edit the inbound, outbound rules of those security groups as well as tags for them. Security Groups Deep Dive","title":"Introduction to Security Groups"},{"location":"AWS/EC2/Security%20Group/Security%20Group/#introduction-to-security-groups","text":"Security Groups are the fundamental of [[network security]] in AWS They control how traffic is allowed into or out of our AWS EC2 machines. It is the most fundamental skill to learn to [[troubleshoot networking issues]] We can view our instance security groups and rules by clicking on them and viewing Security Groups . By clicking on a security group, we can see and edit the inbound, outbound rules of those security groups as well as tags for them. Security Groups Deep Dive","title":"Introduction to Security Groups"},{"location":"AWS/EC2/Security%20Group/Security%20Groups%20Deep%20Dive/","text":"Security Groups Deep Dive \u00b6 Security Group s are acting as a firewall on EC2 instances. They regulate Access to Ports Authorised IP ranges - IPv4 and IPv6 Control of inbound network (from other to the instance) Control of outbound network (from the instance to other) Can be attached to multiple instances Locked down to a AWS Region / VPC Summary combination Does live \"outside\" the AWS EC2 - if traffic is blocked the EC2 instance won't see it. It's a good to maintain one separate security group for ssh access If your application is not accessible (timeout) then it's a security group issue If your application gives a \"[[connection refused]]\"'error, then it's an application error or it's not launched All [[inbound traffic]] is blocked by default All [[outbound traffic]] is authorised by default Referencing other security groups \u00b6 When we have multiple instances on the same security groups, they can communicate.","title":"Security Groups Deep Dive"},{"location":"AWS/EC2/Security%20Group/Security%20Groups%20Deep%20Dive/#security-groups-deep-dive","text":"Security Group s are acting as a firewall on EC2 instances. They regulate Access to Ports Authorised IP ranges - IPv4 and IPv6 Control of inbound network (from other to the instance) Control of outbound network (from the instance to other) Can be attached to multiple instances Locked down to a AWS Region / VPC Summary combination Does live \"outside\" the AWS EC2 - if traffic is blocked the EC2 instance won't see it. It's a good to maintain one separate security group for ssh access If your application is not accessible (timeout) then it's a security group issue If your application gives a \"[[connection refused]]\"'error, then it's an application error or it's not launched All [[inbound traffic]] is blocked by default All [[outbound traffic]] is authorised by default","title":"Security Groups Deep Dive"},{"location":"AWS/EC2/Security%20Group/Security%20Groups%20Deep%20Dive/#referencing-other-security-groups","text":"When we have multiple instances on the same security groups, they can communicate.","title":"Referencing other security groups"},{"location":"AWS/EC2/Storage/EFS%20-%20Elastic%20File%20System/","text":"EFS - Elastic File System \u00b6 Managed [[NFS (network file system)]] that can be mounted on many AWS EC2 EFS works with EC2 instances in multi-AZ Highly available, scalable, expensive (3x GP2 (SSD) ), pay per use Use cases: Content management Web serving Data sharing Wordpress Uses [[NFSv4.1]] protocol Uses Security Group to control access to EFS Compatible with Linux based AMI (not Windows) Performance mode: General purpose (default) MAX I/O - used when thousands of AWS EC2 are using the Elastic File System (EFS) EFS file sync to sync from on-premise file system to EFS Backup EFS-to-EFS (incremental - can choose frequency) Encryption at rest using AWS KMS (Key Management Service)","title":"EFS - Elastic File System"},{"location":"AWS/EC2/Storage/EFS%20-%20Elastic%20File%20System/#efs-elastic-file-system","text":"Managed [[NFS (network file system)]] that can be mounted on many AWS EC2 EFS works with EC2 instances in multi-AZ Highly available, scalable, expensive (3x GP2 (SSD) ), pay per use Use cases: Content management Web serving Data sharing Wordpress Uses [[NFSv4.1]] protocol Uses Security Group to control access to EFS Compatible with Linux based AMI (not Windows) Performance mode: General purpose (default) MAX I/O - used when thousands of AWS EC2 are using the Elastic File System (EFS) EFS file sync to sync from on-premise file system to EFS Backup EFS-to-EFS (incremental - can choose frequency) Encryption at rest using AWS KMS (Key Management Service)","title":"EFS - Elastic File System"},{"location":"AWS/EC2/Storage/EFS%20Hands%20on/","text":"EFS Hands on \u00b6 Before we start using EFS - Elastic File System , we have to go to Security Group s and create a new security group . All the [[inbound]]/[[outbound]] rules can be left at default. Next, we can go to EFS Service . And click on Create file system Change the default security group to the previously created one. When going through, we should see that the EFS - Elastic File System is creating, it has a File system ID and it has 3 [[IP]]s on each specified Availability Zone s. When the [[EFS instance]]s are done setup, we can go to AWS EC2 and launch a new instance. When creating it, select an Availability Zone . When the instance has been created, right-click on it and select Launch more like this . Then click on Edit Instance details and select a different Availability Zone . Once that's done, we can [[SSH]] into the instances and setup the EFS - Elastic File System . The instructions for that are available at the EFS page. sudo yum install -y amazon-efs-utils sudo mkdir /efs sudo mount -t efs -o tls fs-aff48164:/ /efs When adding these rules, you will experience a timeout. mount.nfs4: Connection reset by peer Failed to initialize TLS tunnel for fs-aff48164 In order to fix this, we will need to change the settings to the Security Group by adding inbound NFS for the security group that the ec2 instances has. Now we can mount it and test it between the instances on different Availability Zone s:","title":"EFS Hands on"},{"location":"AWS/EC2/Storage/EFS%20Hands%20on/#efs-hands-on","text":"Before we start using EFS - Elastic File System , we have to go to Security Group s and create a new security group . All the [[inbound]]/[[outbound]] rules can be left at default. Next, we can go to EFS Service . And click on Create file system Change the default security group to the previously created one. When going through, we should see that the EFS - Elastic File System is creating, it has a File system ID and it has 3 [[IP]]s on each specified Availability Zone s. When the [[EFS instance]]s are done setup, we can go to AWS EC2 and launch a new instance. When creating it, select an Availability Zone . When the instance has been created, right-click on it and select Launch more like this . Then click on Edit Instance details and select a different Availability Zone . Once that's done, we can [[SSH]] into the instances and setup the EFS - Elastic File System . The instructions for that are available at the EFS page. sudo yum install -y amazon-efs-utils sudo mkdir /efs sudo mount -t efs -o tls fs-aff48164:/ /efs When adding these rules, you will experience a timeout. mount.nfs4: Connection reset by peer Failed to initialize TLS tunnel for fs-aff48164 In order to fix this, we will need to change the settings to the Security Group by adding inbound NFS for the security group that the ec2 instances has. Now we can mount it and test it between the instances on different Availability Zone s:","title":"EFS Hands on"},{"location":"AWS/EC2/Storage/GP2%20%28SSD%29/","text":"GP2 (SSD): General purpose SSD volume that balances price and performance for a wide variety of workloads Recommended for most workloads System boot volumes Virtual desktops Low latency interactive apps Development and test environments 1 GiB - 16 TiB capacity Small gp2 volumes can burst [[IOPS]] to 3000 Max [[IOPS]] is 16000 3 [[IOPS]] per GB, means at 5334GB we are at the max [[IOPS]]","title":"GP2 (SSD)"},{"location":"AWS/EC2/Storage/Hybrid%20Cloud%20Storage/","text":"Hybrid Cloud for Storage \u00b6 AWS is pushing for \"[[hybrid cloud]]\" Part of your [[infrastructure]] is on the [[cloud]] Part of your [[infrastructure]] is [[on-premise]] This can be due to Long cloud [[migration]]s [[Security requirements]] [[Compliance requirements]] [[IT strategy]] AWS S3 is a proprietary storage technology (unlike EFS - Elastic File System / [[NFS]]), so how do you expose the AWS S3 data on premise? AWS Storage Gateway .","title":"Hybrid Cloud for Storage"},{"location":"AWS/EC2/Storage/Hybrid%20Cloud%20Storage/#hybrid-cloud-for-storage","text":"AWS is pushing for \"[[hybrid cloud]]\" Part of your [[infrastructure]] is on the [[cloud]] Part of your [[infrastructure]] is [[on-premise]] This can be due to Long cloud [[migration]]s [[Security requirements]] [[Compliance requirements]] [[IT strategy]] AWS S3 is a proprietary storage technology (unlike EFS - Elastic File System / [[NFS]]), so how do you expose the AWS S3 data on premise? AWS Storage Gateway .","title":"Hybrid Cloud for Storage"},{"location":"AWS/EC2/Storage/IOI%20%28SSD%29/","text":"IOI (SSD): Highest performance SSD volume for mission-critical low-latency or high throughput workloads [[Critical business applications]] that require sustained [[IOPS]] performance, or more than 16000 IOPS per volume ( GP2 (SSD) limit) Large database workloads such as [[MongoDB]], [[Cassandra]], [[Microsoft SQL Server]], [[MySQL]], [[PostgreSQL]], [[Oracle]] 4 GiB - 16TiB [[IOPS]] is provisioned ([[PIOPS]]) - Min 100 - Max 64'000 ([[Nitro instances]]) else MAX 32'000 (other instances) The maximum ratio of provisioned IOPS to requested volume size (in GiB) is 50:1. (For each GiB we can request 50 [[IOPS]]).","title":"IOI (SSD)"},{"location":"AWS/EC2/Storage/SCI%20%28HDD%29/","text":"SCI (HDD): Lowest cost HDD volume designed for less frequently accessed workloads - [[Throughput-oriented storage]] for large volumes of data that is infrequently accessed - Scenarios where the lowest storage cost is important - Cannot be a [[boot volume]] - 500 GiB - 16 TiB - Max [[IOPS]] is 250 - Max [[throughput]] of 250 MiB/s - can [[burst]]","title":"SCI (HDD)"},{"location":"AWS/EC2/Storage/STI%20%28HDD%29/","text":"STI (HDD): Low cost HDD volume designed for frequently accessed, throughput intensive workloads Streaming workloads requiring consistent, fast throughput at low price. [[Big data]], [[Data warehouse]], [[Log processing]] [[Apache Kafka]] Cannot be a [[boot volume]] 500GiB - 16TiB Max [[IOPS]] is 500 Max [[Throughput]] of 500MiB/s - can burst","title":"STI (HDD)"},{"location":"AWS/EC2/Storage/EBS/EBS%20Hands%20on/","text":"EBS Hands on \u00b6 We are going to [[AWS console]], AWS EC2 panel and launch a new instance, every step will be the same as in the previous sections, but there is a step for storage, that we're interested in. Here we can change the size of the [[root partition]] as well as the volume type and [[IOPS]]. We are also going to add an extra volume on /dev/sdb. When choosing this option, we can also create the drive from a [[snapshot]], but since we don't have one, we'll leave it empty. We can also choose to use encryption , but it will not interest us this time. After instance creation, when we go to the [[Volume]] section, we can see that there are 2 volumes created: If we ssh into the AWS EC2 instance, we can see that the volume has been attached. \u279c ~ ssssh ec2-user@52.16.251.221 -i ~/Downloads/EC2Tutorial.pem The authenticity of host '52.16.251.221 (52.16.251.221)' can't be established. ECDSA key fingerprint is SHA256:C/H5J7MVMWQCW0A6ono1JmijSvo07HqEbrjRCG8FcFw. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added '52.16.251.221' (ECDSA) to the list of known hosts. __| __|_ ) _| ( / Amazon Linux 2 AMI ___|\\___|___| https://aws.amazon.com/amazon-linux-2/ 8 package(s) needed for security, out of 17 available Run \"sudo yum update\" to apply all updates. [ec2-user@ip-172-31-20-13 ~]$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT xvda 202:0 0 8G 0 disk \u2514\u2500xvda1 202:1 0 8G 0 part / xvdb 202:16 0 8G 0 disk If we use following command and get data , that means that we have to format the drive and mount it: [ec2-user@ip-172-31-20-13 ~]$ sudo file -s /dev/xvd /dev/xvd: cannot open (No such file or directory) [ec2-user@ip-172-31-20-13 ~]$ sudo file -s /dev/xvdb /dev/xvdb: data [ec2-user@ip-172-31-20-13 ~]$ sudo mkfs -t ext4 /dev/xvdb mke2fs 1.42.9 (28-Dec-2013) Filesystem label= OS type: Linux Block size=4096 (log=2) Fragment size=4096 (log=2) Stride=0 blocks, Stripe width=0 blocks 524288 inodes, 2097152 blocks 104857 blocks (5.00%) reserved for the super user First data block=0 Maximum filesystem blocks=2147483648 64 block groups 32768 blocks per group, 32768 fragments per group 8192 inodes per group Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632 Allocating group tables: done Writing inode tables: done Creating journal (32768 blocks): done Writing superblocks and filesystem accounting information: done [ec2-user@ip-172-31-20-13 ~]$ sudo mkdir /data [ec2-user@ip-172-31-20-13 ~]$ sudo mount /dev/xvdb /data [ec2-user@ip-172-31-20-13 ~]$ sudo touch /data/hello-drive [ec2-user@ip-172-31-20-13 ~]$ sudo ls /data hello-drive lost+found Then, we can backup and modify the /etc/fstab file so the volume automatically mounts on reboot. [ec2-user@ip-172-31-20-13 ~]$ sudo cp /etc/fstab /etc/fstab.orig [ec2-user@ip-172-31-20-13 ~]$ sudo vim /etc/fstab # UUID=e8f49d85-e739-436f-82ed-d474016253fe / xfs defaults,noatime 1 1 # Add new EBS Volume /dev/xvdb /data ext4 defaults,nofail 0 2 To test that it works: [ec2-user@ip-172-31-20-13 ~]$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT xvda 202:0 0 8G 0 disk \u2514\u2500xvda1 202:1 0 8G 0 part / xvdb 202:16 0 8G 0 disk /data [ec2-user@ip-172-31-20-13 ~]$ sudo umount /data [ec2-user@ip-172-31-20-13 ~]$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT xvda 202:0 0 8G 0 disk \u2514\u2500xvda1 202:1 0 8G 0 part / xvdb 202:16 0 8G 0 disk [ec2-user@ip-172-31-20-13 ~]$ sudo mount -a [ec2-user@ip-172-31-20-13 ~]$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT xvda 202:0 0 8G 0 disk \u2514\u2500xvda1 202:1 0 8G 0 part / xvdb 202:16 0 8G 0 disk /data","title":"EBS Hands on"},{"location":"AWS/EC2/Storage/EBS/EBS%20Hands%20on/#ebs-hands-on","text":"We are going to [[AWS console]], AWS EC2 panel and launch a new instance, every step will be the same as in the previous sections, but there is a step for storage, that we're interested in. Here we can change the size of the [[root partition]] as well as the volume type and [[IOPS]]. We are also going to add an extra volume on /dev/sdb. When choosing this option, we can also create the drive from a [[snapshot]], but since we don't have one, we'll leave it empty. We can also choose to use encryption , but it will not interest us this time. After instance creation, when we go to the [[Volume]] section, we can see that there are 2 volumes created: If we ssh into the AWS EC2 instance, we can see that the volume has been attached. \u279c ~ ssssh ec2-user@52.16.251.221 -i ~/Downloads/EC2Tutorial.pem The authenticity of host '52.16.251.221 (52.16.251.221)' can't be established. ECDSA key fingerprint is SHA256:C/H5J7MVMWQCW0A6ono1JmijSvo07HqEbrjRCG8FcFw. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added '52.16.251.221' (ECDSA) to the list of known hosts. __| __|_ ) _| ( / Amazon Linux 2 AMI ___|\\___|___| https://aws.amazon.com/amazon-linux-2/ 8 package(s) needed for security, out of 17 available Run \"sudo yum update\" to apply all updates. [ec2-user@ip-172-31-20-13 ~]$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT xvda 202:0 0 8G 0 disk \u2514\u2500xvda1 202:1 0 8G 0 part / xvdb 202:16 0 8G 0 disk If we use following command and get data , that means that we have to format the drive and mount it: [ec2-user@ip-172-31-20-13 ~]$ sudo file -s /dev/xvd /dev/xvd: cannot open (No such file or directory) [ec2-user@ip-172-31-20-13 ~]$ sudo file -s /dev/xvdb /dev/xvdb: data [ec2-user@ip-172-31-20-13 ~]$ sudo mkfs -t ext4 /dev/xvdb mke2fs 1.42.9 (28-Dec-2013) Filesystem label= OS type: Linux Block size=4096 (log=2) Fragment size=4096 (log=2) Stride=0 blocks, Stripe width=0 blocks 524288 inodes, 2097152 blocks 104857 blocks (5.00%) reserved for the super user First data block=0 Maximum filesystem blocks=2147483648 64 block groups 32768 blocks per group, 32768 fragments per group 8192 inodes per group Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632 Allocating group tables: done Writing inode tables: done Creating journal (32768 blocks): done Writing superblocks and filesystem accounting information: done [ec2-user@ip-172-31-20-13 ~]$ sudo mkdir /data [ec2-user@ip-172-31-20-13 ~]$ sudo mount /dev/xvdb /data [ec2-user@ip-172-31-20-13 ~]$ sudo touch /data/hello-drive [ec2-user@ip-172-31-20-13 ~]$ sudo ls /data hello-drive lost+found Then, we can backup and modify the /etc/fstab file so the volume automatically mounts on reboot. [ec2-user@ip-172-31-20-13 ~]$ sudo cp /etc/fstab /etc/fstab.orig [ec2-user@ip-172-31-20-13 ~]$ sudo vim /etc/fstab # UUID=e8f49d85-e739-436f-82ed-d474016253fe / xfs defaults,noatime 1 1 # Add new EBS Volume /dev/xvdb /data ext4 defaults,nofail 0 2 To test that it works: [ec2-user@ip-172-31-20-13 ~]$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT xvda 202:0 0 8G 0 disk \u2514\u2500xvda1 202:1 0 8G 0 part / xvdb 202:16 0 8G 0 disk /data [ec2-user@ip-172-31-20-13 ~]$ sudo umount /data [ec2-user@ip-172-31-20-13 ~]$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT xvda 202:0 0 8G 0 disk \u2514\u2500xvda1 202:1 0 8G 0 part / xvdb 202:16 0 8G 0 disk [ec2-user@ip-172-31-20-13 ~]$ sudo mount -a [ec2-user@ip-172-31-20-13 ~]$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT xvda 202:0 0 8G 0 disk \u2514\u2500xvda1 202:1 0 8G 0 part / xvdb 202:16 0 8G 0 disk /data","title":"EBS Hands on"},{"location":"AWS/EC2/Storage/EBS/EBS%20Migration/","text":"EBS Migration \u00b6 EBS Volume s are only locked to a specific Availability Zone To migrate it to a different Availability Zone (or AWS Region ): Snapshot the volume (optional) Copy the volume to a different region Create a volume from the snapshot in the AZ of your choice","title":"EBS Migration"},{"location":"AWS/EC2/Storage/EBS/EBS%20Migration/#ebs-migration","text":"EBS Volume s are only locked to a specific Availability Zone To migrate it to a different Availability Zone (or AWS Region ): Snapshot the volume (optional) Copy the volume to a different region Create a volume from the snapshot in the AZ of your choice","title":"EBS Migration"},{"location":"AWS/EC2/Storage/EBS/EBS%20Operations/","text":"EBS Operations \u00b6 EBS Snapshot EBS Migration EBS Volume Encryption","title":"EBS Operations"},{"location":"AWS/EC2/Storage/EBS/EBS%20Operations/#ebs-operations","text":"EBS Snapshot EBS Migration EBS Volume Encryption","title":"EBS Operations"},{"location":"AWS/EC2/Storage/EBS/EBS%20RAID%20Options/","text":"EBS RAID Options \u00b6 EBS is already [[redundant storage]] (replicated within AZ) But what if you want to increase [[IOPS]] to say 100 000 [[IOPS]]? What if you want to mirror your EBS volume s? You would mount volumes in parallel in [[RAID settings]]. RAID is possible as long as your OS supports it Some RAID options are: [[RAID 0]] [[RAID 1]] [[RAID 5]] (not recommended for EBS - see documentation) [[RAID 6]] (not recommended for EBS - see documentation) [[RAID 0]] (increase performance) \u00b6 Combining 2 or more volumes and getting the total disk space and I/O But one disk fails, all the data fails Use cases would be: An application that needs a lot of IOPS and doesn't need fault-tolerance A database that has replication already built-in Using this, we can have a very big disk wih a lot of [[IOPS]] For example: two 500 GiB Amazon EBS IO volumes with 4,000 priovisioned IOPS each will create a 1000 GiB RAID 0 array with an available bandwith of 8,000 IOPS and 1,000 MB/s of throughput [[RAID 1]] (increase fault tolerance) \u00b6 RAID 1 = Mirroring a volume to another If one disk fails, our logical volume is still working We have to send the data to two EBS volumes at the same time (2x network) Use case: Application that needs increased fault tolerance Application where you need to service disks For example: Two 500 GiB Amazon EBS IOI volumes with 4,000 provisioned IOPS each will create a 500 GiB RAID 1 array with an available bandwidth of 4,000 IOPS and 500 MB/s of throughput.","title":"EBS RAID Options"},{"location":"AWS/EC2/Storage/EBS/EBS%20RAID%20Options/#ebs-raid-options","text":"EBS is already [[redundant storage]] (replicated within AZ) But what if you want to increase [[IOPS]] to say 100 000 [[IOPS]]? What if you want to mirror your EBS volume s? You would mount volumes in parallel in [[RAID settings]]. RAID is possible as long as your OS supports it Some RAID options are: [[RAID 0]] [[RAID 1]] [[RAID 5]] (not recommended for EBS - see documentation) [[RAID 6]] (not recommended for EBS - see documentation)","title":"EBS RAID Options"},{"location":"AWS/EC2/Storage/EBS/EBS%20RAID%20Options/#raid-0-increase-performance","text":"Combining 2 or more volumes and getting the total disk space and I/O But one disk fails, all the data fails Use cases would be: An application that needs a lot of IOPS and doesn't need fault-tolerance A database that has replication already built-in Using this, we can have a very big disk wih a lot of [[IOPS]] For example: two 500 GiB Amazon EBS IO volumes with 4,000 priovisioned IOPS each will create a 1000 GiB RAID 0 array with an available bandwith of 8,000 IOPS and 1,000 MB/s of throughput","title":"[[RAID 0]] (increase performance)"},{"location":"AWS/EC2/Storage/EBS/EBS%20RAID%20Options/#raid-1-increase-fault-tolerance","text":"RAID 1 = Mirroring a volume to another If one disk fails, our logical volume is still working We have to send the data to two EBS volumes at the same time (2x network) Use case: Application that needs increased fault tolerance Application where you need to service disks For example: Two 500 GiB Amazon EBS IOI volumes with 4,000 provisioned IOPS each will create a 500 GiB RAID 1 array with an available bandwidth of 4,000 IOPS and 500 MB/s of throughput.","title":"[[RAID 1]] (increase fault tolerance)"},{"location":"AWS/EC2/Storage/EBS/EBS%20Snapshot/","text":"EBS Snapshots \u00b6 Incremental - only backups changed blocks EBS Backups use [[IO]] and you shouldn't run them while your application is handling a lot of traffic [[Snapshots]] will be stored in AWS S3 (but you won't directly see them) Not necessary to detach volume to do snapshot, but recommended Max 100,000 snapshots Can copy snapshots across Availability Zone or AWS Region Can make Image ( EC2 AMIs ) from snapshot EBS Volume restored by snapshots need to be pre-warmed (using [[fio]] or [[dd]] command to read the entire volume) Snapshots can be automated using [[Amazon Data Lifecycle Manager]] The snapshots can be created by right-clicking on the volume. When the snapshot is created we can do several operations with it: - Create a new volume from the snapshot - Create Image ( EC2 AMIs ) from the snapshot We can also go to the Lifecycle Manger section and add a new snapshot policy, like we are going to make snapshots for all the volumes that have a tag EBS Demo every 12 hours and keep the last 7 copies.","title":"EBS Snapshot"},{"location":"AWS/EC2/Storage/EBS/EBS%20Snapshot/#ebs-snapshots","text":"Incremental - only backups changed blocks EBS Backups use [[IO]] and you shouldn't run them while your application is handling a lot of traffic [[Snapshots]] will be stored in AWS S3 (but you won't directly see them) Not necessary to detach volume to do snapshot, but recommended Max 100,000 snapshots Can copy snapshots across Availability Zone or AWS Region Can make Image ( EC2 AMIs ) from snapshot EBS Volume restored by snapshots need to be pre-warmed (using [[fio]] or [[dd]] command to read the entire volume) Snapshots can be automated using [[Amazon Data Lifecycle Manager]] The snapshots can be created by right-clicking on the volume. When the snapshot is created we can do several operations with it: - Create a new volume from the snapshot - Create Image ( EC2 AMIs ) from the snapshot We can also go to the Lifecycle Manger section and add a new snapshot policy, like we are going to make snapshots for all the volumes that have a tag EBS Demo every 12 hours and keep the last 7 copies.","title":"EBS Snapshots"},{"location":"AWS/EC2/Storage/EBS/EBS%20Volume%20Encryption/","text":"Volume Encryption \u00b6 When you create an encrypted EBS Volume , you get the following: Data at rest is encrypted inside the volume All the data in flight moving between the instance and volume is encrypted All snapshots are encrypted Encryption and decryption are handled transparently (you have nothing to do) Encryption has a minimal impact on latency EBS Encryption leverages keys from AWS KMS (Key Management Service) ([[AES-256]]) Copying an unencrypted snapshot allows encryption Snapshots of encrypted volumes are encrypted Encrypt an unencrypted EBS volume \u00b6 Create an EBS Snapshot of the volume Encrypt the EBS snapshot (using copy) Create a new EBS volume from the snapshot (the volume will also be encrypted) Now you can attach the encrypted volume to the original instance.","title":"EBS Volume Encryption"},{"location":"AWS/EC2/Storage/EBS/EBS%20Volume%20Encryption/#volume-encryption","text":"When you create an encrypted EBS Volume , you get the following: Data at rest is encrypted inside the volume All the data in flight moving between the instance and volume is encrypted All snapshots are encrypted Encryption and decryption are handled transparently (you have nothing to do) Encryption has a minimal impact on latency EBS Encryption leverages keys from AWS KMS (Key Management Service) ([[AES-256]]) Copying an unencrypted snapshot allows encryption Snapshots of encrypted volumes are encrypted","title":"Volume Encryption"},{"location":"AWS/EC2/Storage/EBS/EBS%20Volume%20Encryption/#encrypt-an-unencrypted-ebs-volume","text":"Create an EBS Snapshot of the volume Encrypt the EBS snapshot (using copy) Create a new EBS volume from the snapshot (the volume will also be encrypted) Now you can attach the encrypted volume to the original instance.","title":"Encrypt an unencrypted EBS volume"},{"location":"AWS/EC2/Storage/EBS/EBS%20Volume%20Types/","text":"EBS Volume Types \u00b6 EBS Volumes come in 4 Types: EBS Volumes are characterised in Size | [[Throughput]] | [[IOPS]] (I/O Ops per sec) When in doubt always consult with the AWS documentations. Only GP2 (SSD) and IOI (SSD) can be used as boot volumes.","title":"EBS Volume Types"},{"location":"AWS/EC2/Storage/EBS/EBS%20Volume%20Types/#ebs-volume-types","text":"EBS Volumes come in 4 Types: EBS Volumes are characterised in Size | [[Throughput]] | [[IOPS]] (I/O Ops per sec) When in doubt always consult with the AWS documentations. Only GP2 (SSD) and IOI (SSD) can be used as boot volumes.","title":"EBS Volume Types"},{"location":"AWS/EC2/Storage/EBS/EBS%20Volume/","text":"What's an EBS volume? \u00b6 An EC2 machine loses it's root volume (main drive) when it's manually terminated. Unexpected terminations might happen from time to time (aws would email you) Sometimes, you need a way to store your instance data somewhere An EBS (Elastic Block Store) volume is a network drive you can attach to your instances while they run It allows your instances to persist data EBS Volume \u00b6 It's a network drive (i.e. not a physical drive) It uses the network to communicate the instance, which means there might be a bit of latency It can be detached from an AWS EC2 instance and attached to another one quickly as long as their in the same AZ It's locked to an Availability Zone An EBS Volume in us-east-1a cannot be attached to us-east-1b To move a volume across, you first need to snapshot it Have a provisioned capacity (size in GBs and IOPS) You get billed for all the provisioned capacity You can increase the capacity of the drive over time","title":"What's an EBS volume?"},{"location":"AWS/EC2/Storage/EBS/EBS%20Volume/#whats-an-ebs-volume","text":"An EC2 machine loses it's root volume (main drive) when it's manually terminated. Unexpected terminations might happen from time to time (aws would email you) Sometimes, you need a way to store your instance data somewhere An EBS (Elastic Block Store) volume is a network drive you can attach to your instances while they run It allows your instances to persist data","title":"What's an EBS volume?"},{"location":"AWS/EC2/Storage/EBS/EBS%20Volume/#ebs-volume","text":"It's a network drive (i.e. not a physical drive) It uses the network to communicate the instance, which means there might be a bit of latency It can be detached from an AWS EC2 instance and attached to another one quickly as long as their in the same AZ It's locked to an Availability Zone An EBS Volume in us-east-1a cannot be attached to us-east-1b To move a volume across, you first need to snapshot it Have a provisioned capacity (size in GBs and IOPS) You get billed for all the provisioned capacity You can increase the capacity of the drive over time","title":"EBS Volume"},{"location":"AWS/EC2/Storage/EBS/EBS%20vs%20Instance%20Store/","text":"EBS vs EC2 Instance Store \u00b6 Some instances do not come with Root EBS Volume s Instead, they come with \" EC2 Instance Store \" (= [[ephemeral storage]]) Instance store is physically attached to the machine ( EBS Volume is a [[network drive]]) Pros: Better I/O performance Good for [[buffer]] / [[cache]] / [[scratch data]] / [[temporary content]] Data survives reboots Cons: On stop or termination, the EC2 Instance Store is lost You can't resize the instance store Backups must be operated by the user We can see this if we were to launch an instance like m5ad.2xlarge that has a 300 GB SSD.","title":"EBS vs Instance Store"},{"location":"AWS/EC2/Storage/EBS/EBS%20vs%20Instance%20Store/#ebs-vs-ec2-instance-store","text":"Some instances do not come with Root EBS Volume s Instead, they come with \" EC2 Instance Store \" (= [[ephemeral storage]]) Instance store is physically attached to the machine ( EBS Volume is a [[network drive]]) Pros: Better I/O performance Good for [[buffer]] / [[cache]] / [[scratch data]] / [[temporary content]] Data survives reboots Cons: On stop or termination, the EC2 Instance Store is lost You can't resize the instance store Backups must be operated by the user We can see this if we were to launch an instance like m5ad.2xlarge that has a 300 GB SSD.","title":"EBS vs EC2 Instance Store"},{"location":"AWS/EC2/Types/EC2%20Dedicated%20Hosts/","text":"Dedicated Hosts \u00b6 Physical dedicated AWS EC2 server for your use Full control of AWS EC2 instance placement Visibility into the underlying [[sockets]] / [[physical cores]] of the [[hardware]] Allocated for you account for a 3 year period reservation More expensive Useful for software that have complicated licensing model (BYOL - [[Bring Your Own License]]) Or for companies that have strong regulatory or compliance needs","title":"EC2 Dedicated Hosts"},{"location":"AWS/EC2/Types/EC2%20Dedicated%20Hosts/#dedicated-hosts","text":"Physical dedicated AWS EC2 server for your use Full control of AWS EC2 instance placement Visibility into the underlying [[sockets]] / [[physical cores]] of the [[hardware]] Allocated for you account for a 3 year period reservation More expensive Useful for software that have complicated licensing model (BYOL - [[Bring Your Own License]]) Or for companies that have strong regulatory or compliance needs","title":"Dedicated Hosts"},{"location":"AWS/EC2/Types/EC2%20Dedicated%20Instances/","text":"Dedicated instances \u00b6 - Instances running on hardware that's dedicated to you - May share hardware with other instances in same account - No control over instance placement (can move hardware after Stop / Start)","title":"EC2 Dedicated Instances"},{"location":"AWS/EC2/Types/EC2%20Dedicated%20Instances/#dedicated-instances","text":"- Instances running on hardware that's dedicated to you - May share hardware with other instances in same account - No control over instance placement (can move hardware after Stop / Start)","title":"Dedicated instances"},{"location":"AWS/EC2/Types/EC2%20Instance%20Launch%20Types%20Hands%20On/","text":"Hands on \u00b6 When viewing AWS EC2 instances on the left sidebar, we have a list of these types available. Then, we can search for a reserved instance with a term of 12 months and payment upfront. Similar process goes for all the other types. For spot instances, we can click on launch an instance and in the configuration, set settings for it. Same goes with dedicated hardware, we can select that in the same form.","title":"EC2 Instance Launch Types Hands On"},{"location":"AWS/EC2/Types/EC2%20Instance%20Launch%20Types%20Hands%20On/#hands-on","text":"When viewing AWS EC2 instances on the left sidebar, we have a list of these types available. Then, we can search for a reserved instance with a term of 12 months and payment upfront. Similar process goes for all the other types. For spot instances, we can click on launch an instance and in the configuration, set settings for it. Same goes with dedicated hardware, we can select that in the same form.","title":"Hands on"},{"location":"AWS/EC2/Types/EC2%20Instance%20Launch%20Types/","text":"EC2 Instance Launch Types \u00b6 EC2 Instance Launch Types Hands On Deep Dive \u00b6 R - applications that need a lot of RAM - in-memory caches C - applications that needs good CPU - compute / databases M - applications that are balanced - general / web app I - applications that needs good local I/O - storage G - applications that need a GPU - video rendering / machine learning T2 / T3 - burstable instaces AWS has the concept of burstable instances Burst means that overall, the instance has OK CPU performance When the machine needs to process something unexpected (a spike in load for example), it can burst, and CPU can be VERY good. If the machine bursts, it utilizes \"burst credits\" If all the credits are gone, the CPU becomes bad If the machine stops bursting, credits are accumulated over time Amazin to handle unexpected traffic and getting the instance that it will handle it correctly If your instance consistently runs low on credit, you need to move to a different kind of non-burstable instance T2 / T3 - unlimited It is possible to have an unlimited burst credit balance You pay extra money if you go over your credit balance, but you don't lose performance Overall, it is a new offering, so be careful, costs could go high if you're not monitoring the health of your instances We can see all the instance types available at https://ec2instances.info/ .","title":"EC2 Instance Launch Types"},{"location":"AWS/EC2/Types/EC2%20Instance%20Launch%20Types/#ec2-instance-launch-types","text":"EC2 Instance Launch Types Hands On","title":"EC2 Instance Launch Types"},{"location":"AWS/EC2/Types/EC2%20Instance%20Launch%20Types/#deep-dive","text":"R - applications that need a lot of RAM - in-memory caches C - applications that needs good CPU - compute / databases M - applications that are balanced - general / web app I - applications that needs good local I/O - storage G - applications that need a GPU - video rendering / machine learning T2 / T3 - burstable instaces AWS has the concept of burstable instances Burst means that overall, the instance has OK CPU performance When the machine needs to process something unexpected (a spike in load for example), it can burst, and CPU can be VERY good. If the machine bursts, it utilizes \"burst credits\" If all the credits are gone, the CPU becomes bad If the machine stops bursting, credits are accumulated over time Amazin to handle unexpected traffic and getting the instance that it will handle it correctly If your instance consistently runs low on credit, you need to move to a different kind of non-burstable instance T2 / T3 - unlimited It is possible to have an unlimited burst credit balance You pay extra money if you go over your credit balance, but you don't lose performance Overall, it is a new offering, so be careful, costs could go high if you're not monitoring the health of your instances We can see all the instance types available at https://ec2instances.info/ .","title":"Deep Dive"},{"location":"AWS/EC2/Types/EC2%20On%20Demand%20Instances/","text":"On Demand Instances \u00b6 - Pay for what you use (billing per second, after the first minute) - Has higher cost but no upfront payment - No long term commitment - Recommended for short-term and un-interrupted workloads, where you can't predict how the application will behave.","title":"EC2 On Demand Instances"},{"location":"AWS/EC2/Types/EC2%20On%20Demand%20Instances/#on-demand-instances","text":"- Pay for what you use (billing per second, after the first minute) - Has higher cost but no upfront payment - No long term commitment - Recommended for short-term and un-interrupted workloads, where you can't predict how the application will behave.","title":"On Demand Instances"},{"location":"AWS/EC2/Types/EC2%20Reserved%20Instances/","text":"Reserved instances \u00b6 - Up to 75% discount compared to On-demand - Pay upfront for what you use with long term commitment - Reservation period can be 1 or 3 years - Reserve a specific instance type - Recommended for steady state usage applications (like databases) Convertible Reserved Instances can change the [[EC2 instance type]] Up to 54% discount Scheduled Reserved Instances launch within time window you reserve","title":"EC2 Reserved Instances"},{"location":"AWS/EC2/Types/EC2%20Reserved%20Instances/#reserved-instances","text":"- Up to 75% discount compared to On-demand - Pay upfront for what you use with long term commitment - Reservation period can be 1 or 3 years - Reserve a specific instance type - Recommended for steady state usage applications (like databases) Convertible Reserved Instances can change the [[EC2 instance type]] Up to 54% discount Scheduled Reserved Instances launch within time window you reserve","title":"Reserved instances"},{"location":"AWS/EC2/Types/EC2%20Spot%20Instances/","text":"Spot instances \u00b6 short workloads, for cheap, can lose instances Can get a discount of up to 90% compared to EC2 On Demand Instances You bid a price and get the instance as long as it's price is under the price Price varies on offer and demand Spot instances are reclaimed with a 2 minute notification warning when the spot price goes above your bid Used for [[batch jobs]], [[Big Data analysis]], or workloads that are resilient to failures. Not great for [[critical jobs]] or [[databases]].","title":"EC2 Spot Instances"},{"location":"AWS/EC2/Types/EC2%20Spot%20Instances/#spot-instances","text":"short workloads, for cheap, can lose instances Can get a discount of up to 90% compared to EC2 On Demand Instances You bid a price and get the instance as long as it's price is under the price Price varies on offer and demand Spot instances are reclaimed with a 2 minute notification warning when the spot price goes above your bid Used for [[batch jobs]], [[Big Data analysis]], or workloads that are resilient to failures. Not great for [[critical jobs]] or [[databases]].","title":"Spot instances"},{"location":"AWS/ECS/AWS%20ECS%20-%20ALB%20Integration/","text":"AWS ECS - ALB Integration \u00b6 Application Load Balancer (v2) has a direct integration feature with ECS - Elastic Container Service called \"[[port mapping]]\" This allows you to run multiple instances of the same application on the same AWS EC2 machine Use cases Increased [[resiliency]] even if running on one AWS EC2 instance Maximise [[utilisation]] of [[CPU]] / cores Ability to perform [[rolling upgrade]]s without impacting [[application uptime]]","title":"AWS ECS   ALB Integration"},{"location":"AWS/ECS/AWS%20ECS%20-%20ALB%20Integration/#aws-ecs-alb-integration","text":"Application Load Balancer (v2) has a direct integration feature with ECS - Elastic Container Service called \"[[port mapping]]\" This allows you to run multiple instances of the same application on the same AWS EC2 machine Use cases Increased [[resiliency]] even if running on one AWS EC2 instance Maximise [[utilisation]] of [[CPU]] / cores Ability to perform [[rolling upgrade]]s without impacting [[application uptime]]","title":"AWS ECS - ALB Integration"},{"location":"AWS/ECS/AWS%20ECS%20-%20ECR%20Integration/","text":"AWS ECR - Elastic Container Registry \u00b6 Store, managed and deploy your [[containers]] on AWS [[Fully integrated]] with IAM & ECS - Elastic Container Service Sent over [[HTTPS]] ( encryption in flight ) and Encryption at rest","title":"AWS ECS   ECR Integration"},{"location":"AWS/ECS/AWS%20ECS%20-%20ECR%20Integration/#aws-ecr-elastic-container-registry","text":"Store, managed and deploy your [[containers]] on AWS [[Fully integrated]] with IAM & ECS - Elastic Container Service Sent over [[HTTPS]] ( encryption in flight ) and Encryption at rest","title":"AWS ECR - Elastic Container Registry"},{"location":"AWS/ECS/AWS%20ECS%20Setup%20and%20Config%20File/","text":"AWS ECS - ECS Setup & Config file \u00b6 Run an AWS EC2 instance, install the [[ECS agent]] with ECS Config file Or use the [[ECS-ready Linux AMI]] (still need to modify the config file)","title":"AWS ECS Setup and Config File"},{"location":"AWS/ECS/AWS%20ECS%20Setup%20and%20Config%20File/#aws-ecs-ecs-setup-config-file","text":"Run an AWS EC2 instance, install the [[ECS agent]] with ECS Config file Or use the [[ECS-ready Linux AMI]] (still need to modify the config file)","title":"AWS ECS - ECS Setup &amp; Config file"},{"location":"AWS/ECS/ECS%20-%20Elastic%20Container%20Service/","text":"ECS - Elastic Container Service \u00b6 ECS is a container orchestration service ECS helps you run Docker container s on AWS EC2 machines ECS is complicated and made of: \"[[ECS Core]]\" : Running ECS on user-provisioned AWS EC2 instances [[Fargate]]: Running ECS task s on AWS-provisioned compute ( serverless ) AWS EKS : Running ECS on AWS-powered Kubernetes (running on AWS EC2 ) AWS ECR (Elastic Container Registry) : Docker [[Container Registry]] hosted by AWS ECS - Elastic Container Service & Docker are very popular for [[micro-service]]s IAM security and [[IAM Role]]s at the ECS task level ECS - Use cases \u00b6 Run [[micro-service]]s Ability to run multiple docker container s on the same machine Ease Service Discovery features to enhance communication Direct integration with Application Load Balancer (v2) [[Auto scaling]] capability Run [[batch processing]] / [[scheduled task]]s Schedule AWS EC2 containers to run EC2 On Demand Instances / EC2 Reserved Instances / EC2 Spot Instances [[Migrate]] applications to the [[cloud]] Dockerize [[legacy application]]s running on premise Move docker container s to run on ECS - Elastic Container Service AWS ECS - Concepts \u00b6 ECS cluster : set of EC2 instances ECS service : applications definitions running on ECS cluster ECS task s + definition: containers running to create the application ECS IAM role s: roles assigned to tasks to interact with AWS AWS ECS - ALB Integration AWS ECS Setup and Config File AWS ECS - ECR Integration","title":"ECS - Elastic Container Service"},{"location":"AWS/ECS/ECS%20-%20Elastic%20Container%20Service/#ecs-elastic-container-service","text":"ECS is a container orchestration service ECS helps you run Docker container s on AWS EC2 machines ECS is complicated and made of: \"[[ECS Core]]\" : Running ECS on user-provisioned AWS EC2 instances [[Fargate]]: Running ECS task s on AWS-provisioned compute ( serverless ) AWS EKS : Running ECS on AWS-powered Kubernetes (running on AWS EC2 ) AWS ECR (Elastic Container Registry) : Docker [[Container Registry]] hosted by AWS ECS - Elastic Container Service & Docker are very popular for [[micro-service]]s IAM security and [[IAM Role]]s at the ECS task level","title":"ECS - Elastic Container Service"},{"location":"AWS/ECS/ECS%20-%20Elastic%20Container%20Service/#ecs-use-cases","text":"Run [[micro-service]]s Ability to run multiple docker container s on the same machine Ease Service Discovery features to enhance communication Direct integration with Application Load Balancer (v2) [[Auto scaling]] capability Run [[batch processing]] / [[scheduled task]]s Schedule AWS EC2 containers to run EC2 On Demand Instances / EC2 Reserved Instances / EC2 Spot Instances [[Migrate]] applications to the [[cloud]] Dockerize [[legacy application]]s running on premise Move docker container s to run on ECS - Elastic Container Service","title":"ECS - Use cases"},{"location":"AWS/ECS/ECS%20-%20Elastic%20Container%20Service/#aws-ecs-concepts","text":"ECS cluster : set of EC2 instances ECS service : applications definitions running on ECS cluster ECS task s + definition: containers running to create the application ECS IAM role s: roles assigned to tasks to interact with AWS AWS ECS - ALB Integration AWS ECS Setup and Config File AWS ECS - ECR Integration","title":"AWS ECS - Concepts"},{"location":"AWS/ECS/ECS%20Config%20file/","text":"ECS Config file is at /etc/ecs/ecs.config ECS_CLUSTER=MyCluster # Assign EC2 instance to an ECS cluster ECS_ENGINE_AUTH_DATA={...} # to pull images from private registries ECS_AVAILABLE_LOGGING_DRIVERS=[...] # CloudWatch container logging ECS_ENABLE_TASK_IAM_ROLE=true # Enable IAM roles for ECS tasks","title":"ECS Config file"},{"location":"AWS/ECS/ECS%20IAM%20role/","text":"IAM assigned to tasks to interact with AWS","title":"ECS IAM role"},{"location":"AWS/ECS/ECS%20cluster/","text":"A set of AWS EC2 instances","title":"ECS cluster"},{"location":"AWS/ECS/ECS%20service/","text":"Applications definitions running on ECS cluster","title":"ECS service"},{"location":"AWS/ECS/ECS%20task/","text":"[[containers]] running to create the application","title":"ECS task"},{"location":"AWS/EMR/AWS%20EMR%20%28Elastic%20Map%20Reduce%29/","text":"Amazon Elastic MapReduce \u00b6 EMR stands for \"Elastic MapReduce\" EMR helps creating [[Hadoop]] clusters ([[Big Data]]) to analyse and process vast amounts of data The clusters can be made of hundreds of AWS EC2 instances Also supports [[Apache Spark]], [[HBase]], [[Presto]], [[Flink]] EMR takes care of all the provisioning and configuration [[Auto-scaling]] and integrated with EC2 Spot Instances s Use cases: [[data processing]], [[machine learning]], [[web indexing]], [[big data]]","title":"Amazon Elastic MapReduce"},{"location":"AWS/EMR/AWS%20EMR%20%28Elastic%20Map%20Reduce%29/#amazon-elastic-mapreduce","text":"EMR stands for \"Elastic MapReduce\" EMR helps creating [[Hadoop]] clusters ([[Big Data]]) to analyse and process vast amounts of data The clusters can be made of hundreds of AWS EC2 instances Also supports [[Apache Spark]], [[HBase]], [[Presto]], [[Flink]] EMR takes care of all the provisioning and configuration [[Auto-scaling]] and integrated with EC2 Spot Instances s Use cases: [[data processing]], [[machine learning]], [[web indexing]], [[big data]]","title":"Amazon Elastic MapReduce"},{"location":"AWS/ElastiCache/ElastiCache%20for%20Solutions%20Architect/","text":"ElastiCache for Solutions Architect \u00b6 Operations : same as RDS for Solutions Architect Security : AWS responsible for [[OS security]], we are responsible for setting up AWS KMS (Key Management Service) , Security Group s, IAM Policy , users ([[Redis Auth]]), using [[SSL]] Reliability : [[Clustering]], [[Multi AZ]] Performance : Sub-millisecond performance, in memory, [[read replica]]s for [[sharding]], very popular [[cache]] option Cost : Pay per hour based on AWS EC2 and storage use","title":"ElastiCache for Solutions Architect"},{"location":"AWS/ElastiCache/ElastiCache%20for%20Solutions%20Architect/#elasticache-for-solutions-architect","text":"Operations : same as RDS for Solutions Architect Security : AWS responsible for [[OS security]], we are responsible for setting up AWS KMS (Key Management Service) , Security Group s, IAM Policy , users ([[Redis Auth]]), using [[SSL]] Reliability : [[Clustering]], [[Multi AZ]] Performance : Sub-millisecond performance, in memory, [[read replica]]s for [[sharding]], very popular [[cache]] option Cost : Pay per hour based on AWS EC2 and storage use","title":"ElastiCache for Solutions Architect"},{"location":"AWS/ElastiCache/ElastiCache%20hands%20on/","text":"ElastiCache hands on \u00b6 We can navigate to ElastiCache from the AWS console and click on Get Started Now . There we can choose either to use Redis or Memcached . Then we can set configuration, Availability Zone s, [[High Availability failover]], Encryption , [[Backup]]s and [[maintenance window]]s.","title":"ElastiCache hands on"},{"location":"AWS/ElastiCache/ElastiCache%20hands%20on/#elasticache-hands-on","text":"We can navigate to ElastiCache from the AWS console and click on Get Started Now . There we can choose either to use Redis or Memcached . Then we can set configuration, Availability Zone s, [[High Availability failover]], Encryption , [[Backup]]s and [[maintenance window]]s.","title":"ElastiCache hands on"},{"location":"AWS/ElastiCache/ElastiCache/","text":"ElastiCache overview \u00b6 The same way AWS RDS is to get managed Relational Databases ElastiCache is to get managed Redis or Memcached Caches are in-memory databases with really high performance, low latency Helps reduce load off of databases for read intensive workloads Helps make your application stateless Write Scaling using [[sharding]] Read Scaling using [[Read Replica]]s Multi AZ with Failover Capability AWS takes care of [[OS maintenance]] / [[patching]], optimisations, setup, configuration, monitoring, [[failure recovery]] and [[backups]] In-memory data store, sub-millisecond latency Must provision an AWS EC2 instance type Support for [[Clustering]] ( Redis ) and [[Multi AZ]], [[Read Replica]]s ([[sharding]]) Security though IAM , Security Group s, AWS KMS (Key Management Service) , [[Redis Auth]] [[Backup]] / [[Snapshot]] / [[Point in time restore]] feature Managed and Scheduled maintenance Monitoring though CloudWatch Use case: key/value store, frequent reads, less writes, [[cache]] results for DB queries, store session data for websites, cannot use SQL. ElastiCache for Solutions Architects \u00b6 Security Redis support [[Redis AUTH]] (username / password) SSL [[in-flight encryption]] must be enabled and used Memcached supports [[SASL authentication]] (advanced) None of the caches support IAM authentication IAM policies on ElastiCache are only used for AWS API-level security Patterns for ElastiCache","title":"ElastiCache overview"},{"location":"AWS/ElastiCache/ElastiCache/#elasticache-overview","text":"The same way AWS RDS is to get managed Relational Databases ElastiCache is to get managed Redis or Memcached Caches are in-memory databases with really high performance, low latency Helps reduce load off of databases for read intensive workloads Helps make your application stateless Write Scaling using [[sharding]] Read Scaling using [[Read Replica]]s Multi AZ with Failover Capability AWS takes care of [[OS maintenance]] / [[patching]], optimisations, setup, configuration, monitoring, [[failure recovery]] and [[backups]] In-memory data store, sub-millisecond latency Must provision an AWS EC2 instance type Support for [[Clustering]] ( Redis ) and [[Multi AZ]], [[Read Replica]]s ([[sharding]]) Security though IAM , Security Group s, AWS KMS (Key Management Service) , [[Redis Auth]] [[Backup]] / [[Snapshot]] / [[Point in time restore]] feature Managed and Scheduled maintenance Monitoring though CloudWatch Use case: key/value store, frequent reads, less writes, [[cache]] results for DB queries, store session data for websites, cannot use SQL.","title":"ElastiCache overview"},{"location":"AWS/ElastiCache/ElastiCache/#elasticache-for-solutions-architects","text":"Security Redis support [[Redis AUTH]] (username / password) SSL [[in-flight encryption]] must be enabled and used Memcached supports [[SASL authentication]] (advanced) None of the caches support IAM authentication IAM policies on ElastiCache are only used for AWS API-level security Patterns for ElastiCache","title":"ElastiCache for Solutions Architects"},{"location":"AWS/ElastiCache/Memcached/","text":"Memcached \u00b6 Memcached is an [[in-memory object store]] [[Cache]] doesn't survive reboots Use cases Quick retrieval of objects from memory Cache often accessed objects Overall, Redis has largely grown in popularity and has better feature sets than Memcached","title":"Memcached"},{"location":"AWS/ElastiCache/Memcached/#memcached","text":"Memcached is an [[in-memory object store]] [[Cache]] doesn't survive reboots Use cases Quick retrieval of objects from memory Cache often accessed objects Overall, Redis has largely grown in popularity and has better feature sets than Memcached","title":"Memcached"},{"location":"AWS/ElastiCache/Redis/","text":"Redis \u00b6 Redis is an [[in-memory]] [[key-value store]] Super low latency (sub ms) [[Cache]] survive reboots by default (it's called [[persitance]]) Great to host User sessions Leaderboard (for gaming) Distributed states Relieve pressure on databases (such as RDS) Pub / Sub capability for messaging Multi AZ with Automatic Failover for disaster recovery if you don't want to lose your cache data Support for Read Replicas","title":"Redis"},{"location":"AWS/ElastiCache/Redis/#redis","text":"Redis is an [[in-memory]] [[key-value store]] Super low latency (sub ms) [[Cache]] survive reboots by default (it's called [[persitance]]) Great to host User sessions Leaderboard (for gaming) Distributed states Relieve pressure on databases (such as RDS) Pub / Sub capability for messaging Multi AZ with Automatic Failover for disaster recovery if you don't want to lose your cache data Support for Read Replicas","title":"Redis"},{"location":"AWS/ElastiCache/Pattern/Lazy%20Loading%20Pattern/","text":"Lazy Loading Pattern \u00b6 All the read data is cached, data can become Stale data in cache","title":"Lazy Loading Pattern"},{"location":"AWS/ElastiCache/Pattern/Lazy%20Loading%20Pattern/#lazy-loading-pattern","text":"All the read data is cached, data can become Stale data in cache","title":"Lazy Loading Pattern"},{"location":"AWS/ElastiCache/Pattern/Session%20Store%20Pattern/","text":"Session Store Pattern \u00b6 Store temporary session data in a cache (using TTL features)","title":"Session Store Pattern"},{"location":"AWS/ElastiCache/Pattern/Session%20Store%20Pattern/#session-store-pattern","text":"Store temporary session data in a cache (using TTL features)","title":"Session Store Pattern"},{"location":"AWS/ElastiCache/Pattern/Write%20through%20Pattern/","text":"Write through Pattern \u00b6 Adds or updates data in the cache when written to a DB (no Stale data )","title":"Write through Pattern"},{"location":"AWS/ElastiCache/Pattern/Write%20through%20Pattern/#write-through-pattern","text":"Adds or updates data in the cache when written to a DB (no Stale data )","title":"Write through Pattern"},{"location":"AWS/Elastic%20Beanstalk/Beanstalk%20hands%20on/","text":"Beanstalk hands on \u00b6 We can find Elastic Beanstalk in our AWS Console. In it, we can create a new application and upload the application code. This process will automatically create ([[Load Balancer]]+ Auto Scaling Group (ASG) ) instances for our application. We can easily configure it as well. It also has [[monitoring]], [[alarms]] etc. And we can still see these instances in the AWS EC2 section as well.","title":"Beanstalk hands on"},{"location":"AWS/Elastic%20Beanstalk/Beanstalk%20hands%20on/#beanstalk-hands-on","text":"We can find Elastic Beanstalk in our AWS Console. In it, we can create a new application and upload the application code. This process will automatically create ([[Load Balancer]]+ Auto Scaling Group (ASG) ) instances for our application. We can easily configure it as well. It also has [[monitoring]], [[alarms]] etc. And we can still see these instances in the AWS EC2 section as well.","title":"Beanstalk hands on"},{"location":"AWS/Elastic%20Beanstalk/Elastic%20Beanstalk/","text":"Beanstalk overview \u00b6 Developer problems on AWS \u00b6 [[Managing infrastructure]] [[Deploying Code]] Configuring all the [[database]]s, [[load balancer]]s etc [[Scaling]] concerns Most web apps have the same [[architecture]] ( Application Load Balancer (v2) + Auto Scaling Group (ASG) ) All the developers want their code to run Possibly, consistently across different applications and environments AWS ElasticBeanStalk Overview \u00b6 ElasticBeanstalk is a developer centric view of deploying an application on AWS It uses all the components we've seen before: AWS EC2 , Auto Scaling Group (ASG) , [[Elastic Load Balancer]], AWS RDS etc. But it's all in one view that's easy to make sense of. We still have full control over configuration BeanStalk is free but you pay for the underlying instances. It is a [[Managed Service]] Instance Configuration / OS is handled by beanstalk Deployment strategy is configurable but performed by ElasticBeanStalk Just the application code is the responsibility of the developer Three architecture models: Single Instance deployment: good for dev LB + Application Load Balancer (v2) : great for production or pre-production web-applications Auto Scaling Group (ASG) only: great for non-web apps in production (workers, etc) Elastic BeanStalk has three components: Application Application version: each deployment gets assigned a version Environment name (dev, test, prod): free naming You deploy application versions to environments and can promote application versions to the next environment [[Rollback]] feature to previous application versions Full control over [[lifecycle]] of environments Support for many platforms: [[Go]] [[Java SE]] [[Java with Tomcat]] .NET on Windows Server with IIS [[Node.js]] [[PHP]] [[Python]] [[Ruby]] [[Packer Builder]] [[Single Container Docker]] [[Multicontainer Docker]] [[Preconfigured Docker]] If not supported, you can write your [[beanstalk custom platform]]","title":"Beanstalk overview"},{"location":"AWS/Elastic%20Beanstalk/Elastic%20Beanstalk/#beanstalk-overview","text":"","title":"Beanstalk overview"},{"location":"AWS/Elastic%20Beanstalk/Elastic%20Beanstalk/#developer-problems-on-aws","text":"[[Managing infrastructure]] [[Deploying Code]] Configuring all the [[database]]s, [[load balancer]]s etc [[Scaling]] concerns Most web apps have the same [[architecture]] ( Application Load Balancer (v2) + Auto Scaling Group (ASG) ) All the developers want their code to run Possibly, consistently across different applications and environments","title":"Developer problems on AWS"},{"location":"AWS/Elastic%20Beanstalk/Elastic%20Beanstalk/#aws-elasticbeanstalk-overview","text":"ElasticBeanstalk is a developer centric view of deploying an application on AWS It uses all the components we've seen before: AWS EC2 , Auto Scaling Group (ASG) , [[Elastic Load Balancer]], AWS RDS etc. But it's all in one view that's easy to make sense of. We still have full control over configuration BeanStalk is free but you pay for the underlying instances. It is a [[Managed Service]] Instance Configuration / OS is handled by beanstalk Deployment strategy is configurable but performed by ElasticBeanStalk Just the application code is the responsibility of the developer Three architecture models: Single Instance deployment: good for dev LB + Application Load Balancer (v2) : great for production or pre-production web-applications Auto Scaling Group (ASG) only: great for non-web apps in production (workers, etc) Elastic BeanStalk has three components: Application Application version: each deployment gets assigned a version Environment name (dev, test, prod): free naming You deploy application versions to environments and can promote application versions to the next environment [[Rollback]] feature to previous application versions Full control over [[lifecycle]] of environments Support for many platforms: [[Go]] [[Java SE]] [[Java with Tomcat]] .NET on Windows Server with IIS [[Node.js]] [[PHP]] [[Python]] [[Ruby]] [[Packer Builder]] [[Single Container Docker]] [[Multicontainer Docker]] [[Preconfigured Docker]] If not supported, you can write your [[beanstalk custom platform]]","title":"AWS ElasticBeanStalk Overview"},{"location":"AWS/Elastic%20Beanstalk/Install%20Elastic%20Beanstalk%20CLI/","text":"Install Elastic Beanstalk CLI \u00b6 Install pip install awscli pip install awsebcli --upgrade --user Verify aws help eb --version Configure aws configure","title":"Install Elastic Beanstalk CLI"},{"location":"AWS/Elastic%20Beanstalk/Install%20Elastic%20Beanstalk%20CLI/#install-elastic-beanstalk-cli","text":"Install pip install awscli pip install awsebcli --upgrade --user Verify aws help eb --version Configure aws configure","title":"Install Elastic Beanstalk CLI"},{"location":"AWS/Elastic%20Beanstalk/Time%20based%20scaling/","text":"Time based scaling \u00b6 It is available to add time events to change the default scaling configuration in the ELB Configuration -> Capacity . Things worth to mention: The events work only one way - once the event has ended, it won't revert the changes made. If you need to change scaling at a specific time and then revert it - you will need to create 2 separate events . Make sure to provide the dates in UTC+0000 time not local time zone. The Recurrence parameter - when the event will be triggered. Start time and End time shows from what time to what time the event will be considered active (will trigger cron).","title":"Time based scaling"},{"location":"AWS/Elastic%20Beanstalk/Time%20based%20scaling/#time-based-scaling","text":"It is available to add time events to change the default scaling configuration in the ELB Configuration -> Capacity . Things worth to mention: The events work only one way - once the event has ended, it won't revert the changes made. If you need to change scaling at a specific time and then revert it - you will need to create 2 separate events . Make sure to provide the dates in UTC+0000 time not local time zone. The Recurrence parameter - when the event will be triggered. Start time and End time shows from what time to what time the event will be considered active (will trigger cron).","title":"Time based scaling"},{"location":"AWS/Elastic%20Transcoder/AWS%20Elastic%20Transcoder/","text":"AWS Elastic Transcoder \u00b6 [[Convert]] [[media files]] ([[video]] + [[music]]) stored in AWS S3 into various formats for tablets, PC, Smartphone, TV etc Features: [[bit rate optimisation]], [[thumbnail]], [[watermarks]], [[captions]], [[DRM]], [[progressive download]], encryption 4 components: [[Jobs]]: what does the work of the transcoder [[Pipeline]]: [[Queue]] that manages the transcoding job [[Presets]]: Template for converting media from on format to another [[Notifications]]: AWS SNS for example Pay for what you use, scales automatically, [[fully managed]]","title":"AWS Elastic Transcoder"},{"location":"AWS/Elastic%20Transcoder/AWS%20Elastic%20Transcoder/#aws-elastic-transcoder","text":"[[Convert]] [[media files]] ([[video]] + [[music]]) stored in AWS S3 into various formats for tablets, PC, Smartphone, TV etc Features: [[bit rate optimisation]], [[thumbnail]], [[watermarks]], [[captions]], [[DRM]], [[progressive download]], encryption 4 components: [[Jobs]]: what does the work of the transcoder [[Pipeline]]: [[Queue]] that manages the transcoding job [[Presets]]: Template for converting media from on format to another [[Notifications]]: AWS SNS for example Pay for what you use, scales automatically, [[fully managed]]","title":"AWS Elastic Transcoder"},{"location":"AWS/ElasticSearch/ElasticSearch%20for%20Solutions%20Architect/","text":"ElasticSearch for Solutions Architect \u00b6 Operations : Similar to RDS for Solutions Architect Security : Programming/AWS/Cognito/AWS Cognito , IAM , VPC Summary , AWS KMS (Key Management Service) , [[SSL]] Reliability : [[Multi AZ]], [[Clustering]] Performance : based on ElasticSearch project (open source), petabyte scale Cost : pay per node provisioned (similar to RDS for Solutions Architect ) Remember: ElasticSearch = [[Search]] / [[Indexing]]","title":"ElasticSearch for Solutions Architect"},{"location":"AWS/ElasticSearch/ElasticSearch%20for%20Solutions%20Architect/#elasticsearch-for-solutions-architect","text":"Operations : Similar to RDS for Solutions Architect Security : Programming/AWS/Cognito/AWS Cognito , IAM , VPC Summary , AWS KMS (Key Management Service) , [[SSL]] Reliability : [[Multi AZ]], [[Clustering]] Performance : based on ElasticSearch project (open source), petabyte scale Cost : pay per node provisioned (similar to RDS for Solutions Architect ) Remember: ElasticSearch = [[Search]] / [[Indexing]]","title":"ElasticSearch for Solutions Architect"},{"location":"AWS/ElasticSearch/ElasticSearch/","text":"ElasticSearch \u00b6 Example: In DynamoDB , you can only find by [[primary key]] or [[indexes]] With ElasticSearch you can search any field, even partial matches It's common to use ElasticSearch as a complement to another [[database]] ElasticSearch also has some usage for [[Big Data]] applications You can provision a [[cluster]] of instances Built-in integrations: Kinesis Firehose , [[AWS IoT]], and CloudWatch Logs for data ingestion Security though Programming/AWS/Cognito/AWS Cognito & IAM , AWS KMS (Key Management Service) encryption, [[SSL]] & VPC Summary Comes with [[Kibana]] (visualization) & [[Logstash]] (log ingestion)- [[ELK stack]]","title":"ElasticSearch"},{"location":"AWS/ElasticSearch/ElasticSearch/#elasticsearch","text":"Example: In DynamoDB , you can only find by [[primary key]] or [[indexes]] With ElasticSearch you can search any field, even partial matches It's common to use ElasticSearch as a complement to another [[database]] ElasticSearch also has some usage for [[Big Data]] applications You can provision a [[cluster]] of instances Built-in integrations: Kinesis Firehose , [[AWS IoT]], and CloudWatch Logs for data ingestion Security though Programming/AWS/Cognito/AWS Cognito & IAM , AWS KMS (Key Management Service) encryption, [[SSL]] & VPC Summary Comes with [[Kibana]] (visualization) & [[Logstash]] (log ingestion)- [[ELK stack]]","title":"ElasticSearch"},{"location":"AWS/Exam/Solutions%20Architect%20Associate/Exam%20Walkthrough%20and%20Signup/","text":"Exam Walkthrough and Signup \u00b6 You'll have to register online at https://www.aws.training/ Fee for the exam is 150 USD Provide two identity documents (ID, Credit Card, details are in emails sent to you) No notes are allowed, no pen is allowed, no speaking 65 questions will be asked in 130 minutes At the end you can optionally review all the questions / answers You will know right away if you passed / failed the exams You will not know which answers were right / wrong You will know the overall score a few days later (email notification) Yo pass you need a score of at least 720 out of 1000 points If you fail, you can retake the exam again 14 days later","title":"Exam Walkthrough and Signup"},{"location":"AWS/Exam/Solutions%20Architect%20Associate/Exam%20Walkthrough%20and%20Signup/#exam-walkthrough-and-signup","text":"You'll have to register online at https://www.aws.training/ Fee for the exam is 150 USD Provide two identity documents (ID, Credit Card, details are in emails sent to you) No notes are allowed, no pen is allowed, no speaking 65 questions will be asked in 130 minutes At the end you can optionally review all the questions / answers You will know right away if you passed / failed the exams You will not know which answers were right / wrong You will know the overall score a few days later (email notification) Yo pass you need a score of at least 720 out of 1000 points If you fail, you can retake the exam again 14 days later","title":"Exam Walkthrough and Signup"},{"location":"AWS/Exam/Solutions%20Architect%20Associate/Exam%20tips/","text":"Exam tips \u00b6 Practice makes perfect \u00b6 If you're new to AWS, take a bit of AWS practice thanks to this course before rushing to the exam The exam recommends you to have one or more years of hands-on experience on AWS Practice makes perfect! If you feel overwhelmed by the amount of knowledge you just learned, just go though it one more time Proceed by elimination \u00b6 Most questions are going to be scenario based For all the questions, rule out answers that you know for sure are wrong For the remaining answers, understand which one makes the most sense There are very few trick questions Don't over think it If a solution seems feasible but highly complicated, it's probably wrong Skim the AWS Whitepapers \u00b6 AWS Whitepapers Read each service's FAQ \u00b6 Example: https://aws.amazon.com/vpc/faqs FAQ Cover a lot of the questions asked in exam They help confirm your understanding of a service Get into the AWS Community \u00b6 Read forums online Read online blogs Attend local meetups and discuss with other AWS engineers Watch re-invent videos on YouTube (AWS Converence)","title":"Exam tips"},{"location":"AWS/Exam/Solutions%20Architect%20Associate/Exam%20tips/#exam-tips","text":"","title":"Exam tips"},{"location":"AWS/Exam/Solutions%20Architect%20Associate/Exam%20tips/#practice-makes-perfect","text":"If you're new to AWS, take a bit of AWS practice thanks to this course before rushing to the exam The exam recommends you to have one or more years of hands-on experience on AWS Practice makes perfect! If you feel overwhelmed by the amount of knowledge you just learned, just go though it one more time","title":"Practice makes perfect"},{"location":"AWS/Exam/Solutions%20Architect%20Associate/Exam%20tips/#proceed-by-elimination","text":"Most questions are going to be scenario based For all the questions, rule out answers that you know for sure are wrong For the remaining answers, understand which one makes the most sense There are very few trick questions Don't over think it If a solution seems feasible but highly complicated, it's probably wrong","title":"Proceed by elimination"},{"location":"AWS/Exam/Solutions%20Architect%20Associate/Exam%20tips/#skim-the-aws-whitepapers","text":"AWS Whitepapers","title":"Skim the AWS Whitepapers"},{"location":"AWS/Exam/Solutions%20Architect%20Associate/Exam%20tips/#read-each-services-faq","text":"Example: https://aws.amazon.com/vpc/faqs FAQ Cover a lot of the questions asked in exam They help confirm your understanding of a service","title":"Read each service's FAQ"},{"location":"AWS/Exam/Solutions%20Architect%20Associate/Exam%20tips/#get-into-the-aws-community","text":"Read forums online Read online blogs Attend local meetups and discuss with other AWS engineers Watch re-invent videos on YouTube (AWS Converence)","title":"Get into the AWS Community"},{"location":"AWS/Exam/Solutions%20Architect%20Associate/Get%20an%20extra%2030%20minutes%20on%20your%20AWS%20Exam%20-%20non%20native%20english%20speakers%20only/","text":"Get an extra 30 minutes on your AWS Exam - non native english speakers only \u00b6 You can request special exam accommodations and select ESL +30 minutes . You only need to apply once for this and all future exam registrations will have this.","title":"Get an extra 30 minutes on your AWS Exam - non native english speakers only"},{"location":"AWS/Exam/Solutions%20Architect%20Associate/Get%20an%20extra%2030%20minutes%20on%20your%20AWS%20Exam%20-%20non%20native%20english%20speakers%20only/#get-an-extra-30-minutes-on-your-aws-exam-non-native-english-speakers-only","text":"You can request special exam accommodations and select ESL +30 minutes . You only need to apply once for this and all future exam registrations will have this.","title":"Get an extra 30 minutes on your AWS Exam - non native english speakers only"},{"location":"AWS/Exam/Solutions%20Architect%20Associate/Save%2050%25%20on%20your%20AWS%20Exam%20cost/","text":"Save 50% on your AWS Exam cost \u00b6 In the site where you are registering for the exam, you can visit the Benefits page and claim benefits for up 50% off your exams.","title":"Save 50% on your AWS Exam cost"},{"location":"AWS/Exam/Solutions%20Architect%20Associate/Save%2050%25%20on%20your%20AWS%20Exam%20cost/#save-50-on-your-aws-exam-cost","text":"In the site where you are registering for the exam, you can visit the Benefits page and claim benefits for up 50% off your exams.","title":"Save 50% on your AWS Exam cost"},{"location":"AWS/Exam/Solutions%20Architect%20Associate/State%20of%20learning%20checkpoint/","text":"State of learning checkpoint \u00b6 Let's look how far we've gone on our learning journey https://aws.amazon.com/certification/certified-solutions-architect-associate/ Check out Exam Guide","title":"State of learning checkpoint"},{"location":"AWS/Exam/Solutions%20Architect%20Associate/State%20of%20learning%20checkpoint/#state-of-learning-checkpoint","text":"Let's look how far we've gone on our learning journey https://aws.amazon.com/certification/certified-solutions-architect-associate/ Check out Exam Guide","title":"State of learning checkpoint"},{"location":"AWS/Fundamentals/AWS%20Cheat%20sheet/","text":"Cheat sheet \u00b6 AWS CodeCommit : service where you can store your code. Similar service is [[GitHub]] AWS CodeBuild : build and testing service in your [[CICD pipeline]]s AWS CodeDeploy : deploy the [[packaged code]] onto AWS EC2 and AWS Lambda AWS CodePipeline : orchestrate the actions of your [[CICD pipeline]]s ([[build stage]]s, [[manual approval]]s, many deploys, etc) CloudFormation : Infrastructure as Code for AWS. [[Declarative]] way to manage, create and update resources. ECS - Elastic Container Service : Docker [[container management system]] on AWS. Helps with creating [[micro-service]]s. AWS ECR (Elastic Container Registry) : Docker [[image repository]] on AWS. [[Docker Image]]s can be pushed and pulled from there AWS Step Functions : [[Orchestrate]] / [[Coordinate]] AWS Lambda functions and [[AWS ECS]] containers into a [[workflow]] Simple Workflow Service : Old way of [[orchestrating]] a big [[workflow]]. AWS EMR (Elastic Map Reduce) : [[Big Data]] / [[Hadoop]] / [[Spark]] clusters on AWS, deployed on AWS EC2 for you AWS Glue : [[ETL (Extract, Transform & Load)]] service on AWS AWS OpsWorks : managed [[Chef]] & [[Puppet]] on AWS AWS Elastic Transcoder : managed [[media converter]] ([[video]], [[music]]) service into various [[optimised format]]s AWS Organizations : [[hierarchy]] and [[centralised management]] of multiple [[AWS Account]]s AWS WorkSpaces : [[Virtual Desktop]] on Demand in the [[Cloud]]. Replaces traditional on-premise [[VDI infrastructure]] AWS AppSync : [[GraphQL]] as a service on AWS AWS Single Sign ON (SSO) : One login managed by AWS to log in to various business [[SAML 2.0]]-compatible applications ([[office 365]] etc)","title":"AWS Cheat sheet"},{"location":"AWS/Fundamentals/AWS%20Cheat%20sheet/#cheat-sheet","text":"AWS CodeCommit : service where you can store your code. Similar service is [[GitHub]] AWS CodeBuild : build and testing service in your [[CICD pipeline]]s AWS CodeDeploy : deploy the [[packaged code]] onto AWS EC2 and AWS Lambda AWS CodePipeline : orchestrate the actions of your [[CICD pipeline]]s ([[build stage]]s, [[manual approval]]s, many deploys, etc) CloudFormation : Infrastructure as Code for AWS. [[Declarative]] way to manage, create and update resources. ECS - Elastic Container Service : Docker [[container management system]] on AWS. Helps with creating [[micro-service]]s. AWS ECR (Elastic Container Registry) : Docker [[image repository]] on AWS. [[Docker Image]]s can be pushed and pulled from there AWS Step Functions : [[Orchestrate]] / [[Coordinate]] AWS Lambda functions and [[AWS ECS]] containers into a [[workflow]] Simple Workflow Service : Old way of [[orchestrating]] a big [[workflow]]. AWS EMR (Elastic Map Reduce) : [[Big Data]] / [[Hadoop]] / [[Spark]] clusters on AWS, deployed on AWS EC2 for you AWS Glue : [[ETL (Extract, Transform & Load)]] service on AWS AWS OpsWorks : managed [[Chef]] & [[Puppet]] on AWS AWS Elastic Transcoder : managed [[media converter]] ([[video]], [[music]]) service into various [[optimised format]]s AWS Organizations : [[hierarchy]] and [[centralised management]] of multiple [[AWS Account]]s AWS WorkSpaces : [[Virtual Desktop]] on Demand in the [[Cloud]]. Replaces traditional on-premise [[VDI infrastructure]] AWS AppSync : [[GraphQL]] as a service on AWS AWS Single Sign ON (SSO) : One login managed by AWS to log in to various business [[SAML 2.0]]-compatible applications ([[office 365]] etc)","title":"Cheat sheet"},{"location":"AWS/Fundamentals/AWS%20Region/","text":"AWS has AWS Region all around the world (us-east-1, eu-west-1) All the regions can be viewed at https://aws.amazon.com/about-aws/global-infrastructure/","title":"AWS Region"},{"location":"AWS/Fundamentals/AWS%20Regions%20and%20AZs/","text":"AWS Regions and AZs \u00b6","title":"AWS Regions and AZs"},{"location":"AWS/Fundamentals/AWS%20Regions%20and%20AZs/#aws-regions-and-azs","text":"","title":"AWS Regions and AZs"},{"location":"AWS/Fundamentals/AWS%20Shared%20Responsibility%20Model/","text":"AWS Shared Responsibility Model \u00b6 AWS responsibility - Security of the [[Cloud]] Protecting infrastructure ([[hardware]], [[software]], facilities, and [[networking]]) that runs all of the AWS services Managed services like AWS S3 , DynamoDB , AWS RDS etc Customer responsibility - Security in the Cloud For AWS EC2 instance, customer is responsible for management of the guest OS (including security patches and updates), [[firewall]] & [[network configuration]], IAM etc Example for AWS RDS \u00b6 AWS responsibility Manage the underlying AWS EC2 instance, disable [[SSH]] access Automated [[DB patching]] Automated [[OS patching]] Audit the underlying instance and disks & guarantee it functions Your responsibility Check the [[ports]] / [[IP]] / Security Group inbound rules in DB's Security Group s In-database user creation and permissions Creating a [[database]] with or without [[public access]] Ensure [[parameter groups]] or DB is configured to only allow [[SSL connection]]s Database [[encryption testing]] Example for S3 \u00b6 AWS responsibility Guarantee you get unlimited storage Guarantee you get encryption Ensure separation of the data between different customers Ensure AWS employees can't access your data Your responsibility AWS S3 Bucket configuration [[AWS Bucket Policy]] / public settings IAM user and roles Enabling encryption","title":"AWS Shared Responsibility Model"},{"location":"AWS/Fundamentals/AWS%20Shared%20Responsibility%20Model/#aws-shared-responsibility-model","text":"AWS responsibility - Security of the [[Cloud]] Protecting infrastructure ([[hardware]], [[software]], facilities, and [[networking]]) that runs all of the AWS services Managed services like AWS S3 , DynamoDB , AWS RDS etc Customer responsibility - Security in the Cloud For AWS EC2 instance, customer is responsible for management of the guest OS (including security patches and updates), [[firewall]] & [[network configuration]], IAM etc","title":"AWS Shared Responsibility Model"},{"location":"AWS/Fundamentals/AWS%20Shared%20Responsibility%20Model/#example-for-aws-rds","text":"AWS responsibility Manage the underlying AWS EC2 instance, disable [[SSH]] access Automated [[DB patching]] Automated [[OS patching]] Audit the underlying instance and disks & guarantee it functions Your responsibility Check the [[ports]] / [[IP]] / Security Group inbound rules in DB's Security Group s In-database user creation and permissions Creating a [[database]] with or without [[public access]] Ensure [[parameter groups]] or DB is configured to only allow [[SSL connection]]s Database [[encryption testing]]","title":"Example for AWS RDS"},{"location":"AWS/Fundamentals/AWS%20Shared%20Responsibility%20Model/#example-for-s3","text":"AWS responsibility Guarantee you get unlimited storage Guarantee you get encryption Ensure separation of the data between different customers Ensure AWS employees can't access your data Your responsibility AWS S3 Bucket configuration [[AWS Bucket Policy]] / public settings IAM user and roles Enabling encryption","title":"Example for S3"},{"location":"AWS/Fundamentals/AWS%20Whitepapers/","text":"You can rad about some AWS White papers here: Architecting for the Cloud: AWS Best Practices (https://media.amazonwebservices.com/AWS_Cloud_Best_Practices.pdf) Well Architected Framework (https://aws.amazon.com/architecture/well-architected/) Disaster Recovery (https://aws.amazon.com/disaster-recovery) Overall we've explored all the most important concepts in the course It's never bad to have a look at the whitepapers you think are interesting!","title":"AWS Whitepapers"},{"location":"AWS/Fundamentals/Availability%20Zone/","text":"Each AWS Region has an Availability Zones (us-east-1a, us-east-1b,...). Each availability zone is a physical datacenter in the region, but separate from the other ones (so that they're isolated from disasters).","title":"Availability Zone"},{"location":"AWS/Fundamentals/Choosing%20the%20right%20database/","text":"Choosing the right database \u00b6 We have a lot of managed databases on AWS to choose from Questions to choose the right [[database]] based on your [[architecture]] [[Read-heavy]], [[write-heavy]], or [[balanced workload]]? [[Throughput]] needs? Will it change, does it need to scale or fluctuate during the day? How much data to store and for how long? Will it grow? Average object size? How are they accessed? [[Data durability]]? [[Source of truth]] for the data? [[Latency]] requirements? [[Concurrent users]]? [[Data model]]? How will you query the data? Joins? Structured? Semi-Structured? [[Strong schema]]? More flexibility? Reporting? Search? [[RBDMS]] / [[NoSQL]]? License costs? Switch to Cloud Native DB such as Aurora? Database types \u00b6 [[RDBMS]] (= [[SQL]]/[[OLTP]]): AWS RDS , AWS Aurora - great for joins [[NoSQL]] database: Programming/AWS/DynamoDB/DynamoDB (~[[JSON]]), ElastiCache (key/value pairs), ~ Inbox/Neptune ([[graphs]]) - no joins, no SQL [[Object Store]]: AWS S3 (for big objects) / S3 Glacier (for backups / archives) [[Data Warehouse]] (= [[SQL Analytics]] / [[BI]]): Redshift ([[OLAP]]), AWS Athena [[Search]]: ElasticSearch ([[JSON]]) - [[free text]], [[unstructured searches]] [[Graphs]]: Neptune - display relationships between data","title":"Choosing the right database"},{"location":"AWS/Fundamentals/Choosing%20the%20right%20database/#choosing-the-right-database","text":"We have a lot of managed databases on AWS to choose from Questions to choose the right [[database]] based on your [[architecture]] [[Read-heavy]], [[write-heavy]], or [[balanced workload]]? [[Throughput]] needs? Will it change, does it need to scale or fluctuate during the day? How much data to store and for how long? Will it grow? Average object size? How are they accessed? [[Data durability]]? [[Source of truth]] for the data? [[Latency]] requirements? [[Concurrent users]]? [[Data model]]? How will you query the data? Joins? Structured? Semi-Structured? [[Strong schema]]? More flexibility? Reporting? Search? [[RBDMS]] / [[NoSQL]]? License costs? Switch to Cloud Native DB such as Aurora?","title":"Choosing the right database"},{"location":"AWS/Fundamentals/Choosing%20the%20right%20database/#database-types","text":"[[RDBMS]] (= [[SQL]]/[[OLTP]]): AWS RDS , AWS Aurora - great for joins [[NoSQL]] database: Programming/AWS/DynamoDB/DynamoDB (~[[JSON]]), ElastiCache (key/value pairs), ~ Inbox/Neptune ([[graphs]]) - no joins, no SQL [[Object Store]]: AWS S3 (for big objects) / S3 Glacier (for backups / archives) [[Data Warehouse]] (= [[SQL Analytics]] / [[BI]]): Redshift ([[OLAP]]), AWS Athena [[Search]]: ElasticSearch ([[JSON]]) - [[free text]], [[unstructured searches]] [[Graphs]]: Neptune - display relationships between data","title":"Database types"},{"location":"AWS/Fundamentals/Creating%20an%20AWS%20Account/","text":"Creating an AWS Account \u00b6 You can create an [[AWS Free tier]] account by visiting https://portal.aws.amazon.com/billing/signup?refid=em_127222&redirect_url=https%3A%2F%2Faws.amazon.com%2Fregistration-confirmation#/start","title":"Creating an AWS Account"},{"location":"AWS/Fundamentals/Creating%20an%20AWS%20Account/#creating-an-aws-account","text":"You can create an [[AWS Free tier]] account by visiting https://portal.aws.amazon.com/billing/signup?refid=em_127222&redirect_url=https%3A%2F%2Faws.amazon.com%2Fregistration-confirmation#/start","title":"Creating an AWS Account"},{"location":"AWS/Fundamentals/High%20Availability/","text":"High Availability \u00b6 High Availability usually goes hand in hand with Horizontal scalability High availability means running your application / system in at least 2 data centers ( Availability Zone ) The goal of high availability is to survive a datacenter loss The high availability can be passive (for AWS RDS Multi AZ for example) The high availability can be active (for Horizontal scalability )","title":"High Availability"},{"location":"AWS/Fundamentals/High%20Availability/#high-availability","text":"High Availability usually goes hand in hand with Horizontal scalability High availability means running your application / system in at least 2 data centers ( Availability Zone ) The goal of high availability is to survive a datacenter loss The high availability can be passive (for AWS RDS Multi AZ for example) The high availability can be active (for Horizontal scalability )","title":"High Availability"},{"location":"AWS/Fundamentals/Horizontal%20scalability/","text":"Horizontal Scalability \u00b6 Horizontal Scalability means increasing the number of instances / systems for your application Horizontal scaling implies distributed systems This is very common for web applications / modern applications It's easy to horizontally scale thanks to the cloud offerings such as AWS EC2","title":"Horizontal scalability"},{"location":"AWS/Fundamentals/Horizontal%20scalability/#horizontal-scalability","text":"Horizontal Scalability means increasing the number of instances / systems for your application Horizontal scaling implies distributed systems This is very common for web applications / modern applications It's easy to horizontally scale thanks to the cloud offerings such as AWS EC2","title":"Horizontal Scalability"},{"location":"AWS/Fundamentals/Infrastructure%20as%20Code/","text":"Infrastructure as Code \u00b6 Current we have been doing a lot of [[manual work]] All this manual work will be very tough to reproduce In a another region In another [[AWS account]] Within the same region if everything was deleted Wouldn't it be great if all our infrastructure was code? That code would be deployed and create/update/delete our infrastructure","title":"Infrastructure as Code"},{"location":"AWS/Fundamentals/Infrastructure%20as%20Code/#infrastructure-as-code","text":"Current we have been doing a lot of [[manual work]] All this manual work will be very tough to reproduce In a another region In another [[AWS account]] Within the same region if everything was deleted Wouldn't it be great if all our infrastructure was code? That code would be deployed and create/update/delete our infrastructure","title":"Infrastructure as Code"},{"location":"AWS/Fundamentals/Instantiating%20applications%20quickly/","text":"Instantiating applications quickly \u00b6 When launching a full-stack ( AWS EC2 , EBS Volume , AWS RDS ), it can take time to: Install applications Insert initial (or recovery) data Configure everything Launch application We can take advantage of the cloud to speed that up. EC2 Instances Use a [[Golden AMI]]: Install your applications, OS dependencies etc before hand and launch your EC2 instance from the Golden AMI. Bootstrap using User Data: For dynamic configuration, use user data scripts. Hybrid: mix Golden AMI and User Data (Elastic Beanstalk). AWS RDS : Restore from snapshot: the database will have schemas and data ready. EBS Volume : Restore from a snapshot: the disk will already be formatted and have data.","title":"Instantiating applications quickly"},{"location":"AWS/Fundamentals/Instantiating%20applications%20quickly/#instantiating-applications-quickly","text":"When launching a full-stack ( AWS EC2 , EBS Volume , AWS RDS ), it can take time to: Install applications Insert initial (or recovery) data Configure everything Launch application We can take advantage of the cloud to speed that up. EC2 Instances Use a [[Golden AMI]]: Install your applications, OS dependencies etc before hand and launch your EC2 instance from the Golden AMI. Bootstrap using User Data: For dynamic configuration, use user data scripts. Hybrid: mix Golden AMI and User Data (Elastic Beanstalk). AWS RDS : Restore from snapshot: the database will have schemas and data ready. EBS Volume : Restore from a snapshot: the disk will already be formatted and have data.","title":"Instantiating applications quickly"},{"location":"AWS/Fundamentals/Introduction%20to%20Messaging/","text":"Introduction to Messaging \u00b6 When we start to deploying multiple applications, they will inevitably need to communicate with one another There are two patterns of application communication [[Synchronous communications]] (application to application) [[Asynchronous]] / [[Event based]] (application to queue to application) Synchronous between applications can be problematic if there are sudden spikes of traffic What if you need to suddenly encode 1000 videos but usually it's 10? in that case it's better to decouple your applications using SQS : queue model using SNS : pub/sub model using Kinesis : real-time streaming model These services can scale independently from our application","title":"Introduction to Messaging"},{"location":"AWS/Fundamentals/Introduction%20to%20Messaging/#introduction-to-messaging","text":"When we start to deploying multiple applications, they will inevitably need to communicate with one another There are two patterns of application communication [[Synchronous communications]] (application to application) [[Asynchronous]] / [[Event based]] (application to queue to application) Synchronous between applications can be problematic if there are sudden spikes of traffic What if you need to suddenly encode 1000 videos but usually it's 10? in that case it's better to decouple your applications using SQS : queue model using SNS : pub/sub model using Kinesis : real-time streaming model These services can scale independently from our application","title":"Introduction to Messaging"},{"location":"AWS/Fundamentals/SQS%20vs%20SNS%20vs%20Kinesis/","text":"SQS vs SNS vs Kinesis \u00b6 AWS SQS \u00b6 Consumer \"pulls data\" Data is deleted after being consumed Can have as many workers (consumers) as we want No need to provision throughput No ordering guarantee (except FIFO queues) Individual message delay capability AWS SNS \u00b6 Push data to many subscribers Up to 10,000,000 subscribers Data is not persisted (list if not delivered) [[Pub-Sub]] Up to 100,000 topics No need to provision throughput Integrates with AWS SQS for fanout architecture pattern Kinesis \u00b6 Consumers pull data As many consumers as we want Possibility to replay data Meant for real-time big data analytics and [[ETL]] Ordering at the shard level Data expires after X days Must provision throughput","title":"SQS vs SNS vs Kinesis"},{"location":"AWS/Fundamentals/SQS%20vs%20SNS%20vs%20Kinesis/#sqs-vs-sns-vs-kinesis","text":"","title":"SQS vs SNS vs Kinesis"},{"location":"AWS/Fundamentals/SQS%20vs%20SNS%20vs%20Kinesis/#aws-sqs","text":"Consumer \"pulls data\" Data is deleted after being consumed Can have as many workers (consumers) as we want No need to provision throughput No ordering guarantee (except FIFO queues) Individual message delay capability","title":"AWS SQS"},{"location":"AWS/Fundamentals/SQS%20vs%20SNS%20vs%20Kinesis/#aws-sns","text":"Push data to many subscribers Up to 10,000,000 subscribers Data is not persisted (list if not delivered) [[Pub-Sub]] Up to 100,000 topics No need to provision throughput Integrates with AWS SQS for fanout architecture pattern","title":"AWS SNS"},{"location":"AWS/Fundamentals/SQS%20vs%20SNS%20vs%20Kinesis/#kinesis","text":"Consumers pull data As many consumers as we want Possibility to replay data Meant for real-time big data analytics and [[ETL]] Ordering at the shard level Data expires after X days Must provision throughput","title":"Kinesis"},{"location":"AWS/Fundamentals/Serverless%20in%20AWS/","text":"Serverless in AWS \u00b6 AWS Lambda & Step functions & Simple Workflow Service Programming/AWS/DynamoDB/DynamoDB Programming/AWS/Cognito/AWS Cognito AWS API Gateway AWS S3 AWS SNS & AWS SQS Kinesis AWS Aurora Serverless","title":"Serverless in AWS"},{"location":"AWS/Fundamentals/Serverless%20in%20AWS/#serverless-in-aws","text":"AWS Lambda & Step functions & Simple Workflow Service Programming/AWS/DynamoDB/DynamoDB Programming/AWS/Cognito/AWS Cognito AWS API Gateway AWS S3 AWS SNS & AWS SQS Kinesis AWS Aurora Serverless","title":"Serverless in AWS"},{"location":"AWS/Fundamentals/Serverless/","text":"Serverless is a new paradigm in which the developers don't have to manage servers anymore. They just [[deploy]] the code, functions Initially serverless = [[FaaS (Function as a Service)]] Serverless was pioneered by AWS Lambda but now also includes anything that's managed \"[[Database]], [[Messaging]], [[Storage]], etc\". Serverless does not mean that there are no servers, it means you just don't manage / [[provision]] / see them. Serverless in AWS","title":"Serverless"},{"location":"AWS/Fundamentals/Setting%20up%20AWS%20Budget/","text":"AWS Budget Setup \u00b6 As mentioned in previous section, most of the course will utilise free tier, but in case our account does go over it, we will setup budget so we receive a notification when exceeding the free tier and not spending too much. In order to do this, we go click on My Account and going to My Billing Dashboard , Then on the left side there will be an option called Budgets . Then click on a Create Budget -> Cost Budget . When configuring alerts, we'll set an alert on $0.01, so we'll know when we have exceeded the free tier.","title":"AWS Budget Setup"},{"location":"AWS/Fundamentals/Setting%20up%20AWS%20Budget/#aws-budget-setup","text":"As mentioned in previous section, most of the course will utilise free tier, but in case our account does go over it, we will setup budget so we receive a notification when exceeding the free tier and not spending too much. In order to do this, we go click on My Account and going to My Billing Dashboard , Then on the left side there will be an option called Budgets . Then click on a Create Budget -> Cost Budget . When configuring alerts, we'll set an alert on $0.01, so we'll know when we have exceeded the free tier.","title":"AWS Budget Setup"},{"location":"AWS/Fundamentals/Vertical%20scalability/","text":"Vertical Scalability \u00b6 Vertical scalability means increasing the size of the instance For example, your application runs on a t2.micro Scaling that application vertically means running it on a t2.large Vertical scalability is very common for non distributed systems, such as databases. AWS RDS , ElastiCache are services that can scale vertically. There's usually a limit to how much you can vertically scale ([[hardware limit]])","title":"Vertical scalability"},{"location":"AWS/Fundamentals/Vertical%20scalability/#vertical-scalability","text":"Vertical scalability means increasing the size of the instance For example, your application runs on a t2.micro Scaling that application vertically means running it on a t2.large Vertical scalability is very common for non distributed systems, such as databases. AWS RDS , ElastiCache are services that can scale vertically. There's usually a limit to how much you can vertically scale ([[hardware limit]])","title":"Vertical Scalability"},{"location":"AWS/Fundamentals/user%20cookies/","text":"User cookies \u00b6 Stateless HTTP requests are heavier Security risk (cookies can be altered) Cookies must be validated Cookies must be less than 4KB","title":"User cookies"},{"location":"AWS/Fundamentals/user%20cookies/#user-cookies","text":"Stateless HTTP requests are heavier Security risk (cookies can be altered) Cookies must be validated Cookies must be less than 4KB","title":"User cookies"},{"location":"AWS/Fundamentals/Disaster%20Recovery/Disaster%20Recovery%20-%20All%20AWS%20Multi%20Region/","text":"All AWS Multi Region \u00b6","title":"Disaster Recovery   All AWS Multi Region"},{"location":"AWS/Fundamentals/Disaster%20Recovery/Disaster%20Recovery%20-%20All%20AWS%20Multi%20Region/#all-aws-multi-region","text":"","title":"All AWS Multi Region"},{"location":"AWS/Fundamentals/Disaster%20Recovery/Disaster%20Recovery%20-%20Backup%20and%20Restore/","text":"Backup and Restore \u00b6 High [[Recovery Point Objective]]","title":"Disaster Recovery   Backup and Restore"},{"location":"AWS/Fundamentals/Disaster%20Recovery/Disaster%20Recovery%20-%20Backup%20and%20Restore/#backup-and-restore","text":"High [[Recovery Point Objective]]","title":"Backup and Restore"},{"location":"AWS/Fundamentals/Disaster%20Recovery/Disaster%20Recovery%20-%20Hot%20Site%20-%20Multi%20Site%20Approach/","text":"Multi Site / Hot Site Approach \u00b6 Very low [[Recovery Time Objective]] (minutes or seconds) - very expensive Full [[Production Scale]] is running AWS and On Premise","title":"Disaster Recovery   Hot Site   Multi Site Approach"},{"location":"AWS/Fundamentals/Disaster%20Recovery/Disaster%20Recovery%20-%20Hot%20Site%20-%20Multi%20Site%20Approach/#multi-site-hot-site-approach","text":"Very low [[Recovery Time Objective]] (minutes or seconds) - very expensive Full [[Production Scale]] is running AWS and On Premise","title":"Multi Site / Hot Site Approach"},{"location":"AWS/Fundamentals/Disaster%20Recovery/Disaster%20Recovery%20-%20Pilot%20Light/","text":"Disaster Recovery - Pilot Light \u00b6 A small version of the app is always running in the cloud Useful for the critical core (pilot light) Very similar to Disaster Recovery - Backup and Restore Faster than Disaster Recovery - Backup and Restore as critical systems are already up","title":"Disaster Recovery   Pilot Light"},{"location":"AWS/Fundamentals/Disaster%20Recovery/Disaster%20Recovery%20-%20Pilot%20Light/#disaster-recovery-pilot-light","text":"A small version of the app is always running in the cloud Useful for the critical core (pilot light) Very similar to Disaster Recovery - Backup and Restore Faster than Disaster Recovery - Backup and Restore as critical systems are already up","title":"Disaster Recovery - Pilot Light"},{"location":"AWS/Fundamentals/Disaster%20Recovery/Disaster%20Recovery%20-%20Warm%20Standby/","text":"Warm Standby \u00b6 Full system is up and running, but at minimum size Upon disaster, we can scale to [[production load]]","title":"Disaster Recovery   Warm Standby"},{"location":"AWS/Fundamentals/Disaster%20Recovery/Disaster%20Recovery%20-%20Warm%20Standby/#warm-standby","text":"Full system is up and running, but at minimum size Upon disaster, we can scale to [[production load]]","title":"Warm Standby"},{"location":"AWS/Fundamentals/Disaster%20Recovery/Disaster%20Recovery%20Tips/","text":"Disaster Recovery Tips \u00b6 [[Backup]] EBS Snapshot s, RDS Backups / Snapshots etc Regular pushes to AWS S3 / S3 Standard-Infrequent Access (IA) Tier / S3 Glacier , S3 Lifecycle rules , S3 Cross region replication From on-premise: AWS Snowball or AWS Storage Gateway High Availability Use AWS Route 53 to migrate [[DNS]] over from Region to Region RDS Multi AZ (Disaster Recovery) , [[ElastiCache Multi-AZ]], EFS - Elastic File System , AWS S3 Site to Site VPN as a recovery from Direct Connect Replication [[RDS Replication]] (Cross Region), AWS Aurora + [[Global Databases]] Database replication from on-premise to AWS RDS AWS Storage Gateway Automation CloudFormation / Elastic Beanstalk to re-create a whole new environment Recover / Reboot AWS EC2 isntances with CloudWatch if CloudWatch Alarm s fail AWS Lambda functions for customised automations Chaos Netflix has a \"[[simian-army]]\" randomly terminating AWS EC2","title":"Disaster Recovery Tips"},{"location":"AWS/Fundamentals/Disaster%20Recovery/Disaster%20Recovery%20Tips/#disaster-recovery-tips","text":"[[Backup]] EBS Snapshot s, RDS Backups / Snapshots etc Regular pushes to AWS S3 / S3 Standard-Infrequent Access (IA) Tier / S3 Glacier , S3 Lifecycle rules , S3 Cross region replication From on-premise: AWS Snowball or AWS Storage Gateway High Availability Use AWS Route 53 to migrate [[DNS]] over from Region to Region RDS Multi AZ (Disaster Recovery) , [[ElastiCache Multi-AZ]], EFS - Elastic File System , AWS S3 Site to Site VPN as a recovery from Direct Connect Replication [[RDS Replication]] (Cross Region), AWS Aurora + [[Global Databases]] Database replication from on-premise to AWS RDS AWS Storage Gateway Automation CloudFormation / Elastic Beanstalk to re-create a whole new environment Recover / Reboot AWS EC2 isntances with CloudWatch if CloudWatch Alarm s fail AWS Lambda functions for customised automations Chaos Netflix has a \"[[simian-army]]\" randomly terminating AWS EC2","title":"Disaster Recovery Tips"},{"location":"AWS/Fundamentals/Disaster%20Recovery/Disaster%20Recovery/","text":"Disaster Recovery \u00b6 Any event that has a negative impact on a company's business continuity or finances is a disaster Disaster recovery (DR) is about preparing for and recovering from a disaster What kind of disaster recovery? On-premise => On-premise: traditional DR, and very expensive On-premise => AWS Cloud: [[hybrid recovery]] AWS Cloud Region A => AWS Cloud Region B Need to define two terms: RPO : [[Recovery Point Objective]] RTO : [[Recovery Time Objective]] RPO and RTO \u00b6 Disaster Recovery Strategies \u00b6","title":"Disaster Recovery"},{"location":"AWS/Fundamentals/Disaster%20Recovery/Disaster%20Recovery/#disaster-recovery","text":"Any event that has a negative impact on a company's business continuity or finances is a disaster Disaster recovery (DR) is about preparing for and recovering from a disaster What kind of disaster recovery? On-premise => On-premise: traditional DR, and very expensive On-premise => AWS Cloud: [[hybrid recovery]] AWS Cloud Region A => AWS Cloud Region B Need to define two terms: RPO : [[Recovery Point Objective]] RTO : [[Recovery Time Objective]]","title":"Disaster Recovery"},{"location":"AWS/Fundamentals/Disaster%20Recovery/Disaster%20Recovery/#rpo-and-rto","text":"","title":"RPO and RTO"},{"location":"AWS/Fundamentals/Disaster%20Recovery/Disaster%20Recovery/#disaster-recovery-strategies","text":"","title":"Disaster Recovery Strategies"},{"location":"AWS/Fundamentals/Encryption/Client%20Side%20Encryption/","text":"Client side encryption \u00b6 Data is [[encrypted]] by the client and never decrypted by the server Data will be [[decrypted]] by a receiving client The server should not be able to decrypt the data Could leverage [[Envelope Encryption]]","title":"Client Side Encryption"},{"location":"AWS/Fundamentals/Encryption/Client%20Side%20Encryption/#client-side-encryption","text":"Data is [[encrypted]] by the client and never decrypted by the server Data will be [[decrypted]] by a receiving client The server should not be able to decrypt the data Could leverage [[Envelope Encryption]]","title":"Client side encryption"},{"location":"AWS/Fundamentals/Encryption/Encryption%20at%20rest/","text":"Server side encryption at rest \u00b6 Data is [[encrypted]] after being received by the server Data is [[decrypted]] before being sent It is stored into an encrypted form thanks to a key (usually a [[data key]]) The encryption / decryption keys must be managed somewhere and the server must have access to it.","title":"Encryption at rest"},{"location":"AWS/Fundamentals/Encryption/Encryption%20at%20rest/#server-side-encryption-at-rest","text":"Data is [[encrypted]] after being received by the server Data is [[decrypted]] before being sent It is stored into an encrypted form thanks to a key (usually a [[data key]]) The encryption / decryption keys must be managed somewhere and the server must have access to it.","title":"Server side encryption at rest"},{"location":"AWS/Fundamentals/Encryption/Encryption%20in%20flight/","text":"Encryption in flight ([[SSL]]) \u00b6 Data is [[encrypted]] before sending and decrypted after receiving [[SSL]] certificates help with encryption ([[HTTPS]]) Encryption in flight ensures no MITM ([[man in the middle attack]]) can happen","title":"Encryption in flight"},{"location":"AWS/Fundamentals/Encryption/Encryption%20in%20flight/#encryption-in-flight-ssl","text":"Data is [[encrypted]] before sending and decrypted after receiving [[SSL]] certificates help with encryption ([[HTTPS]]) Encryption in flight ensures no MITM ([[man in the middle attack]]) can happen","title":"Encryption in flight ([[SSL]])"},{"location":"AWS/Fundamentals/Encryption/Encryption/","text":"Encryption \u00b6","title":"Encryption"},{"location":"AWS/Fundamentals/Encryption/Encryption/#encryption","text":"","title":"Encryption"},{"location":"AWS/Fundamentals/Well%20Architected%20Framework/Well%20Architected%20Examples/","text":"More Architecture Examples \u00b6 We've explored the most important architectural patterns: Classic: AWS EC2 , Elastic Beanstalk , AWS RDS , ElastiCache , etc.. Serverless : AWS S3 , AWS Lambda , DynamoDB , Programming/AWS/CloudFront/AWS CloudFront , AWS API Gateway etc If you want to see more AWS architectures: https://aws.amazon.com/architecture https://aws.amazon.com/solutions https://github.com/aws-samples","title":"More Architecture Examples"},{"location":"AWS/Fundamentals/Well%20Architected%20Framework/Well%20Architected%20Examples/#more-architecture-examples","text":"We've explored the most important architectural patterns: Classic: AWS EC2 , Elastic Beanstalk , AWS RDS , ElastiCache , etc.. Serverless : AWS S3 , AWS Lambda , DynamoDB , Programming/AWS/CloudFront/AWS CloudFront , AWS API Gateway etc If you want to see more AWS architectures: https://aws.amazon.com/architecture https://aws.amazon.com/solutions https://github.com/aws-samples","title":"More Architecture Examples"},{"location":"AWS/Fundamentals/Well%20Architected%20Framework/Well%20Architected%20Framework%20-%20Cost%20Optimization/","text":"Fifth Pillar: Cost Optimization \u00b6 Includes the ability to run systems to deliver business value at the lowest price point Design principles Adopt a consumption mode - Pay only for what you use Measure overall efficiency - Use CloudWatch Stop spending money on data center operations - AWS does the infrastructure part and enables customer to focus on organization projects Analyze and attribute expenditure - Accurate identification of system usage and costs, helps measure return of investment (ROI) - make sure to use [[tags]] Use managed and application level services to reduce cost of ownership - As managed services operate at cloud scale, they can offer a lower cost per transaction or service AWS services Expenditure Awareness [[AWS Budgets]] [[AWS Cost and Usage Report]] [[AWS Cost Explorer]] [[Reserved Instance Reporting]] Cost Effective Resources EC2 Spot Instances EC2 Reserved Instances S3 Glacier Matching supply and demand [[Auto Scaling]] AWS Lambda Optimising over time AWS Trusted Advisor [[AWS Cost and Usage Report]]","title":"Fifth Pillar: Cost Optimization"},{"location":"AWS/Fundamentals/Well%20Architected%20Framework/Well%20Architected%20Framework%20-%20Cost%20Optimization/#fifth-pillar-cost-optimization","text":"Includes the ability to run systems to deliver business value at the lowest price point Design principles Adopt a consumption mode - Pay only for what you use Measure overall efficiency - Use CloudWatch Stop spending money on data center operations - AWS does the infrastructure part and enables customer to focus on organization projects Analyze and attribute expenditure - Accurate identification of system usage and costs, helps measure return of investment (ROI) - make sure to use [[tags]] Use managed and application level services to reduce cost of ownership - As managed services operate at cloud scale, they can offer a lower cost per transaction or service AWS services Expenditure Awareness [[AWS Budgets]] [[AWS Cost and Usage Report]] [[AWS Cost Explorer]] [[Reserved Instance Reporting]] Cost Effective Resources EC2 Spot Instances EC2 Reserved Instances S3 Glacier Matching supply and demand [[Auto Scaling]] AWS Lambda Optimising over time AWS Trusted Advisor [[AWS Cost and Usage Report]]","title":"Fifth Pillar: Cost Optimization"},{"location":"AWS/Fundamentals/Well%20Architected%20Framework/Well%20Architected%20Framework%20-%20Operational%20Excellence/","text":"Well Architected Framework - Operational Excellence \u00b6 Includes the ability to run and [[monitor]] systems to deliver [[business value]] and to continually improve supporting processes and procedures [[Design Principles]] Perform operations as code ( infrastructure as code ) [[Annotate documentation]] - Automate the creation of annotated documentation after every build Make frequent, small, [[reversible change]]s - so that in case of any failure you can reverse it Refine operations procedures frequently - and ensure that team members are familiar with it Anticipate [[failure]] Learn from all [[operational failure]]s AWS Service \u00b6 Prepare CloudFormation [[AWS Config]] Operate CloudFormation [[AWS Config]] CloudTrail CloudWatch AWS X-Ray Evolve CloudFormation AWS CodeBuild AWS CodeCommit AWS CodeDeploy AWS CodePipeline","title":"Well Architected Framework - Operational Excellence"},{"location":"AWS/Fundamentals/Well%20Architected%20Framework/Well%20Architected%20Framework%20-%20Operational%20Excellence/#well-architected-framework-operational-excellence","text":"Includes the ability to run and [[monitor]] systems to deliver [[business value]] and to continually improve supporting processes and procedures [[Design Principles]] Perform operations as code ( infrastructure as code ) [[Annotate documentation]] - Automate the creation of annotated documentation after every build Make frequent, small, [[reversible change]]s - so that in case of any failure you can reverse it Refine operations procedures frequently - and ensure that team members are familiar with it Anticipate [[failure]] Learn from all [[operational failure]]s","title":"Well Architected Framework - Operational Excellence"},{"location":"AWS/Fundamentals/Well%20Architected%20Framework/Well%20Architected%20Framework%20-%20Operational%20Excellence/#aws-service","text":"Prepare CloudFormation [[AWS Config]] Operate CloudFormation [[AWS Config]] CloudTrail CloudWatch AWS X-Ray Evolve CloudFormation AWS CodeBuild AWS CodeCommit AWS CodeDeploy AWS CodePipeline","title":"AWS Service"},{"location":"AWS/Fundamentals/Well%20Architected%20Framework/Well%20Architected%20Framework%20-%20Performance%20Effciency/","text":"Fourth Pillar - Performance Efficiency \u00b6 Includes the ability to use computing resources efficiently to meet [[system requirements]], and to maintain that [[efficiency]] as demand changes and technologies evolve. Design principles Democratize advanced technologies - Advance technologies become services and hence you can focus more on [[product development]] Go global in minutes - Easy deployment in multiple AWS Region Use serverless architectures - Avoid burden of managing servers Experiment more often - Easy to carry our [[comparative testing]] Mechanical sympathy - Be aware of all AWS services AWS Services Selection [[Auto Scaling]] AWS Lambda Amazon Elastic Block Store (EBS) AWS S3 AWS RDS Review CloudFormation Monitoring CloudWatch AWS Lambda Tradeoffs AWS RDS ElastiCache AWS Snowball Programming/AWS/CloudFront/AWS CloudFront","title":"Fourth Pillar - Performance Efficiency"},{"location":"AWS/Fundamentals/Well%20Architected%20Framework/Well%20Architected%20Framework%20-%20Performance%20Effciency/#fourth-pillar-performance-efficiency","text":"Includes the ability to use computing resources efficiently to meet [[system requirements]], and to maintain that [[efficiency]] as demand changes and technologies evolve. Design principles Democratize advanced technologies - Advance technologies become services and hence you can focus more on [[product development]] Go global in minutes - Easy deployment in multiple AWS Region Use serverless architectures - Avoid burden of managing servers Experiment more often - Easy to carry our [[comparative testing]] Mechanical sympathy - Be aware of all AWS services AWS Services Selection [[Auto Scaling]] AWS Lambda Amazon Elastic Block Store (EBS) AWS S3 AWS RDS Review CloudFormation Monitoring CloudWatch AWS Lambda Tradeoffs AWS RDS ElastiCache AWS Snowball Programming/AWS/CloudFront/AWS CloudFront","title":"Fourth Pillar - Performance Efficiency"},{"location":"AWS/Fundamentals/Well%20Architected%20Framework/Well%20Architected%20Framework%20-%20Reliability/","text":"Third Pillar - Reliability \u00b6 Ability of a system to recover from [[infrastructure]] or Service [[disruption]]s, dynamically acquire computing resources to meet demand, and mitigate disruptions such as misconfigurations or transient network issues Design principles Test [[recovery procedure]]s - Use automation to simulate different failures or to recreate scenarios that led to failures before Automatically recover from failure - Anticipate and remediate failures before they occur Scale horizontally to increase aggregate system availability - Distribute requests across multiple, smaller resources to ensure that they don't share a common point of failure Stop guessing capacity - Maintain the optimal level to satisfy demand without over under provisioning - Use [[Auto Scaling]] Manage change in automation - Use automation to make changes to infrastructure AWS Services Foundations IAM VPC [[Service Limits]] AWS Trusted Advisor Change Management [[Auto Scaling]] CloudWatch CloudTrail [[AWS Config]] Failure Management [[Backups]] CloudFormation AWS S3 S3 Glacier AWS Route 53","title":"Third Pillar - Reliability"},{"location":"AWS/Fundamentals/Well%20Architected%20Framework/Well%20Architected%20Framework%20-%20Reliability/#third-pillar-reliability","text":"Ability of a system to recover from [[infrastructure]] or Service [[disruption]]s, dynamically acquire computing resources to meet demand, and mitigate disruptions such as misconfigurations or transient network issues Design principles Test [[recovery procedure]]s - Use automation to simulate different failures or to recreate scenarios that led to failures before Automatically recover from failure - Anticipate and remediate failures before they occur Scale horizontally to increase aggregate system availability - Distribute requests across multiple, smaller resources to ensure that they don't share a common point of failure Stop guessing capacity - Maintain the optimal level to satisfy demand without over under provisioning - Use [[Auto Scaling]] Manage change in automation - Use automation to make changes to infrastructure AWS Services Foundations IAM VPC [[Service Limits]] AWS Trusted Advisor Change Management [[Auto Scaling]] CloudWatch CloudTrail [[AWS Config]] Failure Management [[Backups]] CloudFormation AWS S3 S3 Glacier AWS Route 53","title":"Third Pillar - Reliability"},{"location":"AWS/Fundamentals/Well%20Architected%20Framework/Well%20Architected%20Framework%20-%20Security/","text":"Second Pillar - Security \u00b6 Includes the ability to protect information, systems and assets while delivering business value though [[risk assessment]]s and [[mitigation strategies]] Design principles Implement a string identity foundation - [[centralise privilege management]] and reduce (or even eliminate) reliance on long-term credentials - [[principe of least privilege]] IAM Enable [[traceability]] - Integrate logs and metrics with systems to automatically respond and take action Apply security at all layers - Like [[edge network]], VPC , subnet , [[Load Balancer]], every instance, [[operating system]] and [[application]] Automate [[security best practices]] Protect data in transit and at rest - encryption , [[tokenization]] and [[access control]] Keep people away from data - Reduce or eliminate the need for [[direct access]] or [[manual processing]] of data Prepare for security events - run [[incident response simulation]]s and use tools with automation to increase your speed for [[detection]], [[investigation]] and [[recovery]] Services [[Identity and Access Management]] IAM AWS STS - Security Token Service [[Multi factor authentication]] AWS Organizations Detective Controls [[AWS Config]] CloudTrail CloudWatch Infrastructure Protection Programming/AWS/CloudFront/AWS CloudFront VPC [[AWS Shield]] [[AWS WAF]] [[Amazon Inspector]] Data Protection AWS KMS (Key Management Service) AWS S3 [[Elastic Load Balancer]] EBS Volume RDS Security Incident Response IAM CloudFormation CloudWatch Events","title":"Second Pillar - Security"},{"location":"AWS/Fundamentals/Well%20Architected%20Framework/Well%20Architected%20Framework%20-%20Security/#second-pillar-security","text":"Includes the ability to protect information, systems and assets while delivering business value though [[risk assessment]]s and [[mitigation strategies]] Design principles Implement a string identity foundation - [[centralise privilege management]] and reduce (or even eliminate) reliance on long-term credentials - [[principe of least privilege]] IAM Enable [[traceability]] - Integrate logs and metrics with systems to automatically respond and take action Apply security at all layers - Like [[edge network]], VPC , subnet , [[Load Balancer]], every instance, [[operating system]] and [[application]] Automate [[security best practices]] Protect data in transit and at rest - encryption , [[tokenization]] and [[access control]] Keep people away from data - Reduce or eliminate the need for [[direct access]] or [[manual processing]] of data Prepare for security events - run [[incident response simulation]]s and use tools with automation to increase your speed for [[detection]], [[investigation]] and [[recovery]] Services [[Identity and Access Management]] IAM AWS STS - Security Token Service [[Multi factor authentication]] AWS Organizations Detective Controls [[AWS Config]] CloudTrail CloudWatch Infrastructure Protection Programming/AWS/CloudFront/AWS CloudFront VPC [[AWS Shield]] [[AWS WAF]] [[Amazon Inspector]] Data Protection AWS KMS (Key Management Service) AWS S3 [[Elastic Load Balancer]] EBS Volume RDS Security Incident Response IAM CloudFormation CloudWatch Events","title":"Second Pillar - Security"},{"location":"AWS/Fundamentals/Well%20Architected%20Framework/Well%20Architected%20Framework/","text":"Well Architected Framework \u00b6 General Guiding Principles \u00b6 Stop guessing your capacity needs [[Test]] systems at [[production scale]] Automate to make [[architectural expermentation]] easier Allow for [[evolutionary architecture]]s Design based on [[changing requirements]] Drive architectures using data Improve through [[game day]]s [[Simulate application]]s for flash sale days 5 Pillars \u00b6 Well Architected Framework - Operational Excellence Well Architected Framework - Security Well Architected Framework - Reliability Well Architected Framework - Performance Effciency Well Architected Framework - Cost Optimization Well Architected Examples They are not something to balance or trade-offs, they're a synergy. Well-Architected Tool","title":"Well Architected Framework"},{"location":"AWS/Fundamentals/Well%20Architected%20Framework/Well%20Architected%20Framework/#well-architected-framework","text":"","title":"Well Architected Framework"},{"location":"AWS/Fundamentals/Well%20Architected%20Framework/Well%20Architected%20Framework/#general-guiding-principles","text":"Stop guessing your capacity needs [[Test]] systems at [[production scale]] Automate to make [[architectural expermentation]] easier Allow for [[evolutionary architecture]]s Design based on [[changing requirements]] Drive architectures using data Improve through [[game day]]s [[Simulate application]]s for flash sale days","title":"General Guiding Principles"},{"location":"AWS/Fundamentals/Well%20Architected%20Framework/Well%20Architected%20Framework/#5-pillars","text":"Well Architected Framework - Operational Excellence Well Architected Framework - Security Well Architected Framework - Reliability Well Architected Framework - Performance Effciency Well Architected Framework - Cost Optimization Well Architected Examples They are not something to balance or trade-offs, they're a synergy. Well-Architected Tool","title":"5 Pillars"},{"location":"AWS/Fundamentals/Well%20Architected%20Framework/Well-Architected%20Tool/","text":"Well-Architected Tool \u00b6 https://console.aws.amazon.com/wellarchitected Here you can review your workloads against current [[AWS best practices]] and get guidance on how to improve your [[cloud architecture]].","title":"Well Architected Tool"},{"location":"AWS/Fundamentals/Well%20Architected%20Framework/Well-Architected%20Tool/#well-architected-tool","text":"https://console.aws.amazon.com/wellarchitected Here you can review your workloads against current [[AWS best practices]] and get guidance on how to improve your [[cloud architecture]].","title":"Well-Architected Tool"},{"location":"AWS/Glue/AWS%20Glue/","text":"AWS Glue \u00b6 [[Fully managed]] [[ETL (Extract, Transform & Load)]] service [[Automating]] time consuming steps of [[data preparation for analytics]] Serverless , pay as you go, fully managed, provisions [[Apache Spark]] [[Crawl]]s data sources and identifies data formats ([[schema inference]]) Automated [[Code Generation]] Sources: AWS Aurora , AWS RDS , Redshift & AWS S3 Sinks: AWS S3 , Redshift , etc. [[Glue Data Catalog]]: [[Metadata]] (definition & schema) of [[Source Table]]s","title":"AWS Glue"},{"location":"AWS/Glue/AWS%20Glue/#aws-glue","text":"[[Fully managed]] [[ETL (Extract, Transform & Load)]] service [[Automating]] time consuming steps of [[data preparation for analytics]] Serverless , pay as you go, fully managed, provisions [[Apache Spark]] [[Crawl]]s data sources and identifies data formats ([[schema inference]]) Automated [[Code Generation]] Sources: AWS Aurora , AWS RDS , Redshift & AWS S3 Sinks: AWS S3 , Redshift , etc. [[Glue Data Catalog]]: [[Metadata]] (definition & schema) of [[Source Table]]s","title":"AWS Glue"},{"location":"AWS/IAM/Access%20Advisor/","text":"Access Advisor \u00b6 IAM Access Advisor (user-level) - Access advisor shows the service permissions granted to a user and when those services were last accessed. - Can be used to enforce the principle of least privilege - revise the policies for each user. This tool can be accessed by opening up a user in the IAM and going to Access Advisor tab. Here we can see all the allowed services and when they were last accessed.","title":"Access Advisor"},{"location":"AWS/IAM/Access%20Advisor/#access-advisor","text":"IAM Access Advisor (user-level) - Access advisor shows the service permissions granted to a user and when those services were last accessed. - Can be used to enforce the principle of least privilege - revise the policies for each user. This tool can be accessed by opening up a user in the IAM and going to Access Advisor tab. Here we can see all the allowed services and when they were last accessed.","title":"Access Advisor"},{"location":"AWS/IAM/Credential%20Report/","text":"Credential Report \u00b6 IAM Credentials Report (account-level) A report that lists all your account's users and the status of their various credentials. In the IAM, we can access the Credential Report on the left side-menu. Then click on the Download Report . It will give us a CSV file with a information like: User created at Is password enabled Is MFA enabled Are access keys active","title":"Credential Report"},{"location":"AWS/IAM/Credential%20Report/#credential-report","text":"IAM Credentials Report (account-level) A report that lists all your account's users and the status of their various credentials. In the IAM, we can access the Credential Report on the left side-menu. Then click on the Download Report . It will give us a CSV file with a information like: User created at Is password enabled Is MFA enabled Are access keys active","title":"Credential Report"},{"location":"AWS/IAM/IAM%20Best%20Practices/","text":"Best practices One IAM User per physical person One [[IAM Role]] per application IAM credentials should never be shared Never write IAM credentials in code Never use the root account except for initial setup Never use the root IAM credentials Assign users to groups and assign permissions to groups Create a strong password policy Use and enforce the use of Multi Factor Authentication (MFA) Create and use Roles for giving permissions to AWS services Use Access Keys for Programmatic Access (CLI/SDK) Audit permissions of your account with the IAM Credentials Report Never share IAM users & access keys","title":"IAM Best Practices"},{"location":"AWS/IAM/IAM%20Federation/","text":"IAM Federation Big enterprises usually integrate their own repository of users with IAM This way, one can login into AWS using their company credentials Identity Federation uses the [[SAML standard]] ([[Active Directory]])","title":"IAM Federation"},{"location":"AWS/IAM/IAM%20Hands%20On/","text":"IAM Hands on \u00b6 We can go to the AWS Console and navigate to the IAM service. When we're starting, there is virtually nothing in our accounts: Below there is a security status listed that tells what we can do to improve security. So, we are going to: Delete the ROOT access keys Setup MFA for the root user Create a new user Manage groups Now we can go to the user davis and detach the permissions, since it's not directly manageable. Setup IAM password policy Now, the security should be set up. The last step is to create an account alias for editing the sign-in link into their accounts. Now we can log out of the root user and got to the admin account we created. And it will then ask us for changing the password because of the password policy we set up.","title":"IAM Hands on"},{"location":"AWS/IAM/IAM%20Hands%20On/#iam-hands-on","text":"We can go to the AWS Console and navigate to the IAM service. When we're starting, there is virtually nothing in our accounts: Below there is a security status listed that tells what we can do to improve security. So, we are going to: Delete the ROOT access keys Setup MFA for the root user Create a new user Manage groups Now we can go to the user davis and detach the permissions, since it's not directly manageable. Setup IAM password policy Now, the security should be set up. The last step is to create an account alias for editing the sign-in link into their accounts. Now we can log out of the root user and got to the admin account we created. And it will then ask us for changing the password because of the password policy we set up.","title":"IAM Hands on"},{"location":"AWS/IAM/IAM%20Policy/","text":"IAM Policy \u00b6 We can have IAM group that contain people. If we attach policies to those groups, all the users will receive these permissions. IAM user s can be in multiple groups. There is an option to attach an Inline policy as well. In this case the policy is attached directly to a user. IAM Policies Structure \u00b6 Consists of Version: policy language version, always include \"2012-10-17\" Id: an identifier for the policy (optional) Statement: one or more individual statements (required) Statements consist of Sid: an identifier for the statement (optional) Effect: whether the statement allows or denies access (Allow, Deny) Action: list of actions this policy allows or denies Resource: list of resources to which the actions applied to Condition: conditions for when this policy is in effect (optional) { \"Version\": \"2012-10-17\", \"Id\": \"S3-Account-Permissions\", \"Statement\": [ { \"Sid\": \"1\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\" [\"arn:aws:iam:123456789012:root\"] }, \"Action\": ] \"s3:GetObject\", \"s3:PutObject\" ], \"Resource\": [\"arn:aws:s3:::mybucket/*\"] } ] }","title":"IAM Policy"},{"location":"AWS/IAM/IAM%20Policy/#iam-policy","text":"We can have IAM group that contain people. If we attach policies to those groups, all the users will receive these permissions. IAM user s can be in multiple groups. There is an option to attach an Inline policy as well. In this case the policy is attached directly to a user.","title":"IAM Policy"},{"location":"AWS/IAM/IAM%20Policy/#iam-policies-structure","text":"Consists of Version: policy language version, always include \"2012-10-17\" Id: an identifier for the policy (optional) Statement: one or more individual statements (required) Statements consist of Sid: an identifier for the statement (optional) Effect: whether the statement allows or denies access (Allow, Deny) Action: list of actions this policy allows or denies Resource: list of resources to which the actions applied to Condition: conditions for when this policy is in effect (optional) { \"Version\": \"2012-10-17\", \"Id\": \"S3-Account-Permissions\", \"Statement\": [ { \"Sid\": \"1\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\" [\"arn:aws:iam:123456789012:root\"] }, \"Action\": ] \"s3:GetObject\", \"s3:PutObject\" ], \"Resource\": [\"arn:aws:s3:::mybucket/*\"] } ] }","title":"IAM Policies Structure"},{"location":"AWS/IAM/IAM%20Security%20Tools/","text":"IAM Security Tools Hands On \u00b6 Credential Report Access Advisor","title":"IAM Security Tools Hands On"},{"location":"AWS/IAM/IAM%20Security%20Tools/#iam-security-tools-hands-on","text":"Credential Report Access Advisor","title":"IAM Security Tools Hands On"},{"location":"AWS/IAM/IAM%20group/","text":"IAM group s can only contain users not other groups. IAM user s can belong to multiple groups though.","title":"IAM group"},{"location":"AWS/IAM/IAM%20user/","text":"People within your organization and can be grouped.","title":"IAM user"},{"location":"AWS/IAM/IAM/","text":"IAM Introduction \u00b6 IAM stands for Identity and Access Management Your whole AWS security is there: Users Groups Roles Root account should never be used (and shared) Users must be created with proper permissions IAM is at the center of AWS Policies are written in [[JSON]] IAM has a global view [[Multi factor authentication]] can be setup IAM has predefined \"managed policies\" It's best to give users the minimal amount of permissions they need to perform their job ( least privilege principle ) IAM Hands On","title":"IAM Introduction"},{"location":"AWS/IAM/IAM/#iam-introduction","text":"IAM stands for Identity and Access Management Your whole AWS security is there: Users Groups Roles Root account should never be used (and shared) Users must be created with proper permissions IAM is at the center of AWS Policies are written in [[JSON]] IAM has a global view [[Multi factor authentication]] can be setup IAM has predefined \"managed policies\" It's best to give users the minimal amount of permissions they need to perform their job ( least privilege principle ) IAM Hands On","title":"IAM Introduction"},{"location":"AWS/IAM/Inline%20policy/","text":"Usually IAM Policy is attached to IAM group s, but you can attach them directly to IAM user - inlining a policy.","title":"Inline policy"},{"location":"AWS/IAM/Root%20account/","text":"Created by default when you register on AWS, shouldn't be used or shared.","title":"Root account"},{"location":"AWS/IAM/Users%2C%20Groups%2C%20Policies/","text":"IAM introduction: Users, Groups, Policies \u00b6 IAM stands for Identity And Access Management It is a global service Root account is created by default, shouldn't be used or shared. IAM user s are people within your organization and can be grouped IAM group s can only contain users not other groups. Users can belong to multiple groups though. Groups are being created because: You can create [[JSON]] documents called IAM Policy and assign them to IAM group s or IAM user s. These policies define the permissions of the users In AWS you apply the least privilege principle : don't give more permissions than a user needs.","title":"IAM introduction: Users, Groups, Policies"},{"location":"AWS/IAM/Users%2C%20Groups%2C%20Policies/#iam-introduction-users-groups-policies","text":"IAM stands for Identity And Access Management It is a global service Root account is created by default, shouldn't be used or shared. IAM user s are people within your organization and can be grouped IAM group s can only contain users not other groups. Users can belong to multiple groups though. Groups are being created because: You can create [[JSON]] documents called IAM Policy and assign them to IAM group s or IAM user s. These policies define the permissions of the users In AWS you apply the least privilege principle : don't give more permissions than a user needs.","title":"IAM introduction: Users, Groups, Policies"},{"location":"AWS/IAM/least%20privilege%20principle/","text":"don't give more permissions than a user needs","title":"Least privilege principle"},{"location":"AWS/KMS/AWS%20KMS%20%28Key%20Management%20Service%29/","text":"AWS KMS (Key Management Service) \u00b6 Anytime you hear \" encryption \" for an AWS service, it's most likely KMS Easy way to [[control access]] to your data, AWS manages keys for us Fully integrated with IAM for authorisation Seamlessly integrated into: EBS Volume : encrypt volumes AWS S3 : Server side encryption of objects Redshift : encryption of data AWS RDS : encryption of data SSM Parameter Store : Parameter store etc But you can also use the [[AWS CLI]] / AWS SDK Able to fully manage the keys & policies Create Rotation policies Disable Enable Able to audit key usage (using CloudTrail ) Three types of Customer Master Keys ([[CMK]]) AWS Managed Service Default CMK: free User Keys created in KMS: $1 / month User Keys imported (must be 256-bit [[symmetric key]]): $1/month pay for API call to KMS ($0.03/ 10000 calls)","title":"AWS KMS (Key Management Service)"},{"location":"AWS/KMS/AWS%20KMS%20%28Key%20Management%20Service%29/#aws-kms-key-management-service","text":"Anytime you hear \" encryption \" for an AWS service, it's most likely KMS Easy way to [[control access]] to your data, AWS manages keys for us Fully integrated with IAM for authorisation Seamlessly integrated into: EBS Volume : encrypt volumes AWS S3 : Server side encryption of objects Redshift : encryption of data AWS RDS : encryption of data SSM Parameter Store : Parameter store etc But you can also use the [[AWS CLI]] / AWS SDK Able to fully manage the keys & policies Create Rotation policies Disable Enable Able to audit key usage (using CloudTrail ) Three types of Customer Master Keys ([[CMK]]) AWS Managed Service Default CMK: free User Keys created in KMS: $1 / month User Keys imported (must be 256-bit [[symmetric key]]): $1/month pay for API call to KMS ($0.03/ 10000 calls)","title":"AWS KMS (Key Management Service)"},{"location":"AWS/KMS/Encryption%20in%20AWS%20Services/","text":"Encryption in AWS Services \u00b6 Requires migration (though [[Snapshot]] / [[Backup]]) EBS Volume s AWS RDS databases ElastiCache EFS - Elastic File System In-place encryption AWS S3","title":"Encryption in AWS Services"},{"location":"AWS/KMS/Encryption%20in%20AWS%20Services/#encryption-in-aws-services","text":"Requires migration (though [[Snapshot]] / [[Backup]]) EBS Volume s AWS RDS databases ElastiCache EFS - Elastic File System In-place encryption AWS S3","title":"Encryption in AWS Services"},{"location":"AWS/KMS/KMS%20101/","text":"AWS KMS 101 \u00b6 Anytime you need to share sensitive information - use AWS KMS (Key Management Service) Database passwords Credentials to external service Private key of [[SSL certificate]]s The value in AWS KMS (Key Management Service) is that the [[CMK]] used to [[encrypt]] data can never be retrieved by the user, and the [[CMK]] can be rotated for extra security Never ever store your secrets in [[plaintext]], especially in your code! Encrypted [[secrets]] can be stored in the code / [[environment variables]] KMS can only help in encrypting up to 4KB of data per call IF data > 4 KB, use [[envelope encryption]] To give access to AWS KMS (Key Management Service) to someone: Make sure [[Key Policy]] allows the user Make sure IAM Policy allows the API Calls","title":"KMS 101"},{"location":"AWS/KMS/KMS%20101/#aws-kms-101","text":"Anytime you need to share sensitive information - use AWS KMS (Key Management Service) Database passwords Credentials to external service Private key of [[SSL certificate]]s The value in AWS KMS (Key Management Service) is that the [[CMK]] used to [[encrypt]] data can never be retrieved by the user, and the [[CMK]] can be rotated for extra security Never ever store your secrets in [[plaintext]], especially in your code! Encrypted [[secrets]] can be stored in the code / [[environment variables]] KMS can only help in encrypting up to 4KB of data per call IF data > 4 KB, use [[envelope encryption]] To give access to AWS KMS (Key Management Service) to someone: Make sure [[Key Policy]] allows the user Make sure IAM Policy allows the API Calls","title":"AWS KMS 101"},{"location":"AWS/KMS/KMS%20API/","text":"API - Encrypt and Decrypt \u00b6","title":"KMS API"},{"location":"AWS/KMS/KMS%20API/#api-encrypt-and-decrypt","text":"","title":"API - Encrypt and Decrypt"},{"location":"AWS/Kinesis/AWS%20Kinesis%20-%20Consumers/","text":"AWS Kinesis API - Consumers \u00b6 Can use a normal consumer (CLI, AWS SDK , etc) Can use Kinesis Client library (in [[Java]], [[Node]], [[Python]], [[Ruby]], [[.Net]]) KCL uses DynamoDB to checkpoint offsets KCL uses DynamoDB to track other workers and share the work amongst shards","title":"AWS Kinesis   Consumers"},{"location":"AWS/Kinesis/AWS%20Kinesis%20-%20Consumers/#aws-kinesis-api-consumers","text":"Can use a normal consumer (CLI, AWS SDK , etc) Can use Kinesis Client library (in [[Java]], [[Node]], [[Python]], [[Ruby]], [[.Net]]) KCL uses DynamoDB to checkpoint offsets KCL uses DynamoDB to track other workers and share the work amongst shards","title":"AWS Kinesis API - Consumers"},{"location":"AWS/Kinesis/AWS%20Kinesis%20API%20-%20Exceptions/","text":"AWS Kinesis API - Exceptions \u00b6 ProvisionedThroughputExceeded Exceptions Happens when sending more data (exceeding MB/s or TPS for any shard) Make sure you don't have a [[hot shard]] (such as your partition key is bad and too much data goes to that partition) Solution: Retries with backoff Increase shards (scaling) Ensure your partition key is a good one","title":"AWS Kinesis API   Exceptions"},{"location":"AWS/Kinesis/AWS%20Kinesis%20API%20-%20Exceptions/#aws-kinesis-api-exceptions","text":"ProvisionedThroughputExceeded Exceptions Happens when sending more data (exceeding MB/s or TPS for any shard) Make sure you don't have a [[hot shard]] (such as your partition key is bad and too much data goes to that partition) Solution: Retries with backoff Increase shards (scaling) Ensure your partition key is a good one","title":"AWS Kinesis API - Exceptions"},{"location":"AWS/Kinesis/AWS%20Kinesis%20API%20-%20Put%20Records/","text":"AWS Kinesis API - Put records \u00b6 PutRecord API + [[Partition key]] that gets [[hashed]] Key is hashed to determine [[shard id]] The same key goes to the same [[partition]] (helps with ordering for a specific key) Messages sent get a \"[[sequence number]]\" Choose a partition key that is highly distributed (helps prevent \"hot partition\") user_id if many users Not country_id if 90% of the users are in one country Use [[batching]] with PutRecords to reduce costs and increase throughput ProvisionedThroughPutExceeded if we go over the limits Can use CLI, AWS SDK, or producer libraries from various frameworks","title":"AWS Kinesis API   Put Records"},{"location":"AWS/Kinesis/AWS%20Kinesis%20API%20-%20Put%20Records/#aws-kinesis-api-put-records","text":"PutRecord API + [[Partition key]] that gets [[hashed]] Key is hashed to determine [[shard id]] The same key goes to the same [[partition]] (helps with ordering for a specific key) Messages sent get a \"[[sequence number]]\" Choose a partition key that is highly distributed (helps prevent \"hot partition\") user_id if many users Not country_id if 90% of the users are in one country Use [[batching]] with PutRecords to reduce costs and increase throughput ProvisionedThroughPutExceeded if we go over the limits Can use CLI, AWS SDK, or producer libraries from various frameworks","title":"AWS Kinesis API - Put records"},{"location":"AWS/Kinesis/Kinesis%20Data%20Analytics/","text":"Kinesis Data Analytics \u00b6 Perform real-time analytics on [[Kinesis Streams]] using [[SQL]] Kinesis Data Analytics [[Auto Scaling]] Managed: no servers to provision Continuous: [[real time]] Pay for actual [[consumption rate]] Can create streams out of the real-time queries","title":"Kinesis Data Analytics"},{"location":"AWS/Kinesis/Kinesis%20Data%20Analytics/#kinesis-data-analytics","text":"Perform real-time analytics on [[Kinesis Streams]] using [[SQL]] Kinesis Data Analytics [[Auto Scaling]] Managed: no servers to provision Continuous: [[real time]] Pay for actual [[consumption rate]] Can create streams out of the real-time queries","title":"Kinesis Data Analytics"},{"location":"AWS/Kinesis/Kinesis%20Firehose/","text":"Kinesis Firehose \u00b6 Fully managed service, no administration Near Real Time (60 seconds latency) Load data into Redshift / AWS S3 / ElasticSearch / [[Splunk]] Automatic Scaling Support many data formats (pay for conversion) Pay for the amount of data going through Firehose","title":"Kinesis Firehose"},{"location":"AWS/Kinesis/Kinesis%20Firehose/#kinesis-firehose","text":"Fully managed service, no administration Near Real Time (60 seconds latency) Load data into Redshift / AWS S3 / ElasticSearch / [[Splunk]] Automatic Scaling Support many data formats (pay for conversion) Pay for the amount of data going through Firehose","title":"Kinesis Firehose"},{"location":"AWS/Kinesis/Kinesis%20Hands%20On/","text":"Hands on \u00b6 Visit Kinesis from [[AWS Console]]. Create a data stream. Then you can work with it using the CLI. $ aws kinesis help $ aws kinesis list-streams help $ aws kinesis list-streams { \"StreamNames\": [ \"my-first-stream\" ] } $ aws kinesis describe-stream help $ aws kinesis describe-stream --stream-name my-first-stream {...} $ aws kinesis put-record --stream-name my-first-stream --data \"user signup\" --partition-key user_123 { \"ShardId\": \"shardId-00000000000\", \"SequenceNumber\": \"4958877456415503463748463555080986096182857601881824546\" } $ aws kinesis get-shard-iterator --stream-name my-first-stream --shard-id shardId-00000000000 --shard-iterator-type TRIM_HORIZON { \"ShardIterator\": \"...\" } $ aws kinesis get-records help $ aws kinesis get-records --shard-iterator \"...\" { \"Records\": [ { \"SequenceNumber\": \"4955877456415503463...\", \"AppriximateArrivalTImestamp\": 1538394164.478, \"Data\": \"dXNlciBzaWdudXa=\", // base64 encoded \"PartitionKey\": \"user_123\" }, ... ], \"NextShardIterator\": \"AAA...\", \"MillisBehindLatest\": 0 }","title":"Kinesis Hands On"},{"location":"AWS/Kinesis/Kinesis%20Hands%20On/#hands-on","text":"Visit Kinesis from [[AWS Console]]. Create a data stream. Then you can work with it using the CLI. $ aws kinesis help $ aws kinesis list-streams help $ aws kinesis list-streams { \"StreamNames\": [ \"my-first-stream\" ] } $ aws kinesis describe-stream help $ aws kinesis describe-stream --stream-name my-first-stream {...} $ aws kinesis put-record --stream-name my-first-stream --data \"user signup\" --partition-key user_123 { \"ShardId\": \"shardId-00000000000\", \"SequenceNumber\": \"4958877456415503463748463555080986096182857601881824546\" } $ aws kinesis get-shard-iterator --stream-name my-first-stream --shard-id shardId-00000000000 --shard-iterator-type TRIM_HORIZON { \"ShardIterator\": \"...\" } $ aws kinesis get-records help $ aws kinesis get-records --shard-iterator \"...\" { \"Records\": [ { \"SequenceNumber\": \"4955877456415503463...\", \"AppriximateArrivalTImestamp\": 1538394164.478, \"Data\": \"dXNlciBzaWdudXa=\", // base64 encoded \"PartitionKey\": \"user_123\" }, ... ], \"NextShardIterator\": \"AAA...\", \"MillisBehindLatest\": 0 }","title":"Hands on"},{"location":"AWS/Kinesis/Kinesis%20Security/","text":"Security \u00b6 Control access / authorisation using IAM Policy Encryption in flight using [[HTTP]] endpoints Encryption at rest using AWS KMS (Key Management Service) Possibility to encrypt / decrypt data client side (harder) VPC endpoint available for kinesis to access within VPC Summary","title":"Kinesis Security"},{"location":"AWS/Kinesis/Kinesis%20Security/#security","text":"Control access / authorisation using IAM Policy Encryption in flight using [[HTTP]] endpoints Encryption at rest using AWS KMS (Key Management Service) Possibility to encrypt / decrypt data client side (harder) VPC endpoint available for kinesis to access within VPC Summary","title":"Security"},{"location":"AWS/Kinesis/Kinesis%20Streams%20Shards/","text":"Kinesis Streams Shards \u00b6 One steam is made of many different shards 1MB/s or 1000 messages/s at write PER [[SHARD]] 2MB/s at read PER SHARD Billing is per shard provisioned, can have as many shards as you want Batching available or per message calls The number of shards can evolve over time ([[reshard]] / [[merge]]) Records are ordered per shard","title":"Kinesis Streams Shards"},{"location":"AWS/Kinesis/Kinesis%20Streams%20Shards/#kinesis-streams-shards","text":"One steam is made of many different shards 1MB/s or 1000 messages/s at write PER [[SHARD]] 2MB/s at read PER SHARD Billing is per shard provisioned, can have as many shards as you want Batching available or per message calls The number of shards can evolve over time ([[reshard]] / [[merge]]) Records are ordered per shard","title":"Kinesis Streams Shards"},{"location":"AWS/Kinesis/Kinesis/","text":"Kinesis \u00b6 Kinesis is a managed alternative to [[Apache Kafka]] Great for allocation logs, metrics, IoT, clickstreams Great for \"real-time\" [[big data]] Great for streaming processing frameworks ([[Spark]], [[NiFi]], etc...) Data is automatically replicated to 3 Availability Zone s [[Kinesis Streams]]: low latency streaming ingest at scale [[Kinesis Analytics]]: perform real-time analytics on streams using SQL Kinesis Firehose : load streams into AWS S3 , Redshift , ElasticSearch ... Kinesis overview \u00b6 Streams are divided in ordered Shards / Partitions (like ordered queue) Data retention is 1 day by default, can go up to 7 days Ability to reprocess / replay data Multiple applications can consume the same stream Real-time processing with scale of throughput Once data inserted in Kinesis, it can't be deleted ([[immutability]]) Kinesis Hands On","title":"Kinesis"},{"location":"AWS/Kinesis/Kinesis/#kinesis","text":"Kinesis is a managed alternative to [[Apache Kafka]] Great for allocation logs, metrics, IoT, clickstreams Great for \"real-time\" [[big data]] Great for streaming processing frameworks ([[Spark]], [[NiFi]], etc...) Data is automatically replicated to 3 Availability Zone s [[Kinesis Streams]]: low latency streaming ingest at scale [[Kinesis Analytics]]: perform real-time analytics on streams using SQL Kinesis Firehose : load streams into AWS S3 , Redshift , ElasticSearch ...","title":"Kinesis"},{"location":"AWS/Kinesis/Kinesis/#kinesis-overview","text":"Streams are divided in ordered Shards / Partitions (like ordered queue) Data retention is 1 day by default, can go up to 7 days Ability to reprocess / replay data Multiple applications can consume the same stream Real-time processing with scale of throughput Once data inserted in Kinesis, it can't be deleted ([[immutability]]) Kinesis Hands On","title":"Kinesis overview"},{"location":"AWS/Lambda/AWS%20Lambda/","text":"AWS Lambda \u00b6","title":"AWS Lambda"},{"location":"AWS/Lambda/AWS%20Lambda/#aws-lambda","text":"","title":"AWS Lambda"},{"location":"AWS/Lambda/Benefits%20of%20AWS%20Lambda/","text":"Benefits of AWS Lamda \u00b6 Easy Pricing Pay per request and compute time Free tier of 1,000,000 AWS Lambda requests and 400,000 GBs of compute time Integrated with the whole AWS Stack Integrated with many programming languages Easy monitoring through CloudWatch Easy to get more resources per functions (up to 3GB of RAM) Increasing [[RAM]] will also improve [[CPU]] and network","title":"Benefits of AWS Lambda"},{"location":"AWS/Lambda/Benefits%20of%20AWS%20Lambda/#benefits-of-aws-lamda","text":"Easy Pricing Pay per request and compute time Free tier of 1,000,000 AWS Lambda requests and 400,000 GBs of compute time Integrated with the whole AWS Stack Integrated with many programming languages Easy monitoring through CloudWatch Easy to get more resources per functions (up to 3GB of RAM) Increasing [[RAM]] will also improve [[CPU]] and network","title":"Benefits of AWS Lamda"},{"location":"AWS/Lambda/KMS%20And%20Lambda%20practice/","text":"KMS And Lambda practice \u00b6 AWS KMS (Key Management Service) \u00b6 In AWS KMS (Key Management Service) we have a [[AWS Managed keys]] section available that stores keys for aws [[managed service]]s that we have had enabled encryption. You can also go to [[Customer managed keys]] and create your own keys. When created a key, we can also open it up and enable key rotation: AWS Lambda \u00b6 We are going to create a new lambda function. Now, if we wanted to use database password, this is bad: import json dbpassword = \"supersecret\" def lambda_handler(event, context): return dbpassword We can leverage [[environment variables]] though, but it is still not perfect, because if someone accesses the AWS Lambda UI, the password is still visible. We can leverage the encryption option with the previously created Customer managed key though. Now if we change the code to run this: import boto3 import os from base64 import b64decode ENCRYPTED = os.environ['DB_PASSWORD'] # Decrypt code should run once and variables stored outside of the function # handler so that these are decrypted once per container DECRYPTED = boto3.client('kms').decrypt(CiphertextBlob=b64decode(ENCRYPTED))['Plaintext'].decode('utf-8') def lambda_handler(event, context): print(ENCRYPTED) print(DECRYPTED) return DECRYPTED When testing, we will get an error: An error occurred (AccessDeniedException) when calling the Decrypt operation: The ciphertext refers to a customer master key that does not exist, does not exist in this region, or you are not allowed to access. This is because our AWS Lambda function does not have the permission to decrypt it. We are going to open up a new tab with the View the lamda-demo-kms-role-fo9pvnp8 role . Then create and attach a IAM Policy . Test it once more: If we view the logs, we can see the both prints of the [[encrypted]] and [[decrypted]] versions:","title":"KMS And Lambda practice"},{"location":"AWS/Lambda/KMS%20And%20Lambda%20practice/#kms-and-lambda-practice","text":"","title":"KMS And Lambda practice"},{"location":"AWS/Lambda/KMS%20And%20Lambda%20practice/#aws-kms-key-management-service","text":"In AWS KMS (Key Management Service) we have a [[AWS Managed keys]] section available that stores keys for aws [[managed service]]s that we have had enabled encryption. You can also go to [[Customer managed keys]] and create your own keys. When created a key, we can also open it up and enable key rotation:","title":"AWS KMS (Key Management Service)"},{"location":"AWS/Lambda/KMS%20And%20Lambda%20practice/#aws-lambda","text":"We are going to create a new lambda function. Now, if we wanted to use database password, this is bad: import json dbpassword = \"supersecret\" def lambda_handler(event, context): return dbpassword We can leverage [[environment variables]] though, but it is still not perfect, because if someone accesses the AWS Lambda UI, the password is still visible. We can leverage the encryption option with the previously created Customer managed key though. Now if we change the code to run this: import boto3 import os from base64 import b64decode ENCRYPTED = os.environ['DB_PASSWORD'] # Decrypt code should run once and variables stored outside of the function # handler so that these are decrypted once per container DECRYPTED = boto3.client('kms').decrypt(CiphertextBlob=b64decode(ENCRYPTED))['Plaintext'].decode('utf-8') def lambda_handler(event, context): print(ENCRYPTED) print(DECRYPTED) return DECRYPTED When testing, we will get an error: An error occurred (AccessDeniedException) when calling the Decrypt operation: The ciphertext refers to a customer master key that does not exist, does not exist in this region, or you are not allowed to access. This is because our AWS Lambda function does not have the permission to decrypt it. We are going to open up a new tab with the View the lamda-demo-kms-role-fo9pvnp8 role . Then create and attach a IAM Policy . Test it once more: If we view the logs, we can see the both prints of the [[encrypted]] and [[decrypted]] versions:","title":"AWS Lambda"},{"location":"AWS/Lambda/Lambda%20Configuration/","text":"Configuration \u00b6 [[Timeout]]: default 3 seconds, max 15 minutes [[Environment variables]] Allocated [[memory]] (127MB to 3GB) Ability to deploy within a VPC Summary + assign Security Group s [[IAM execution role]] must be attached to the AWS Lambda function","title":"Lambda Configuration"},{"location":"AWS/Lambda/Lambda%20Configuration/#configuration","text":"[[Timeout]]: default 3 seconds, max 15 minutes [[Environment variables]] Allocated [[memory]] (127MB to 3GB) Ability to deploy within a VPC Summary + assign Security Group s [[IAM execution role]] must be attached to the AWS Lambda function","title":"Configuration"},{"location":"AWS/Lambda/Lambda%20Example%20-%20Serverless%20Thumbnail%20creation/","text":"Example: Serverless Thumbnail creation \u00b6 New image in AWS S3 Trigger AWS Lambda & creates a thumbnail Push new thumbnail to AWS S3 Push metadata to DynamoDB","title":"Lambda Example   Serverless Thumbnail creation"},{"location":"AWS/Lambda/Lambda%20Example%20-%20Serverless%20Thumbnail%20creation/#example-serverless-thumbnail-creation","text":"New image in AWS S3 Trigger AWS Lambda & creates a thumbnail Push new thumbnail to AWS S3 Push metadata to DynamoDB","title":"Example: Serverless Thumbnail creation"},{"location":"AWS/Lambda/Lambda%20Integrations/","text":"Integrations \u00b6 AWS API Gateway Kinesis DynamoDB [[S3]] [[AWS IoT]] CloudWatch Events s CloudWatch Logs AWS SNS Programming/AWS/Cognito/AWS Cognito AWS SQS","title":"Lambda Integrations"},{"location":"AWS/Lambda/Lambda%20Integrations/#integrations","text":"AWS API Gateway Kinesis DynamoDB [[S3]] [[AWS IoT]] CloudWatch Events s CloudWatch Logs AWS SNS Programming/AWS/Cognito/AWS Cognito AWS SQS","title":"Integrations"},{"location":"AWS/Lambda/Lambda%20Language%20Support/","text":"Language support \u00b6 [[Node.js]] [[Python]] [[Java]] [[C Language#]] / [[.NET Core]] [[Golang]] [[C Language#]] / [[Powershell]]","title":"Lambda Language Support"},{"location":"AWS/Lambda/Lambda%20Language%20Support/#language-support","text":"[[Node.js]] [[Python]] [[Java]] [[C Language#]] / [[.NET Core]] [[Golang]] [[C Language#]] / [[Powershell]]","title":"Language support"},{"location":"AWS/Lambda/Lambda%20Limits/","text":"AWS Lamda Limits \u00b6 Execution [[Memory]] allocation: 128MB to 3008MB (64MB increments) Maximum [[execution time]]: 15 minutes Disk capacity in the \"function container\" (in /tmp): 512MB [[Learn concurrency]] limits: 1000 (no of functions that can concurrently execute) Can be increased by writing a support ticket. Deployment AWS Lambda function deployment size (compressed .zip): 50MB Size of uncompressed deployment (code + dependencies): 250 MB Can use the /tmp directory to load other files at startup Size of [[environment variables]]: 4KB","title":"Lambda Limits"},{"location":"AWS/Lambda/Lambda%20Limits/#aws-lamda-limits","text":"Execution [[Memory]] allocation: 128MB to 3008MB (64MB increments) Maximum [[execution time]]: 15 minutes Disk capacity in the \"function container\" (in /tmp): 512MB [[Learn concurrency]] limits: 1000 (no of functions that can concurrently execute) Can be increased by writing a support ticket. Deployment AWS Lambda function deployment size (compressed .zip): 50MB Size of uncompressed deployment (code + dependencies): 250 MB Can use the /tmp directory to load other files at startup Size of [[environment variables]]: 4KB","title":"AWS Lamda Limits"},{"location":"AWS/Lambda/Lambda%20use%20cases/","text":"Use cases: - [[Website Security and Privacy]] - [[Dynamic Web Application]] at the [[Edge]] - [[Search Engine Optimisation (SEO)]] - Intelligently Route Across Origins and Data Centers - [[Bot Mitigation]] at the Edge - Real-time [[Image Transformation]] - A/B Testing - [[User Authentication and Authorisation]] - [[User Prioritisation]] - [[User Tracking and Analytics]]","title":"Lambda use cases"},{"location":"AWS/Lambda/Lambda%40Edge/","text":"Lamda@Edge \u00b6 You have deployed a [[CDN]] using Programming/AWS/CloudFront/AWS CloudFront What if you wanted to run a global AWS Lambda alongside? Or how to implement request filtering before reaching your application? For this, you can use Lambda@Edge Deploy Lambda functions alongside your Programming/AWS/CloudFront/AWS CloudFront CDN Build more responsive applications You don't manage servers, Lambda is deployed globally Customise the [[CDN]] content Pay only for what you use You can use AWS Lambda to change CloudFront requests and responses After CloudFront receives a request from a viewer (viewer request) Before CloudFront forwards the request to the origin (origin request) After CloudFront receives the response from the origin (origin response) Before CloudFront forwards the response to the viewer (viewer response) You can also generate responses to viewers without ever sending the request to the origin.","title":"Lambda@Edge"},{"location":"AWS/Lambda/Lambda%40Edge/#lamdaedge","text":"You have deployed a [[CDN]] using Programming/AWS/CloudFront/AWS CloudFront What if you wanted to run a global AWS Lambda alongside? Or how to implement request filtering before reaching your application? For this, you can use Lambda@Edge Deploy Lambda functions alongside your Programming/AWS/CloudFront/AWS CloudFront CDN Build more responsive applications You don't manage servers, Lambda is deployed globally Customise the [[CDN]] content Pay only for what you use You can use AWS Lambda to change CloudFront requests and responses After CloudFront receives a request from a viewer (viewer request) Before CloudFront forwards the request to the origin (origin request) After CloudFront receives the response from the origin (origin response) Before CloudFront forwards the response to the viewer (viewer response) You can also generate responses to viewers without ever sending the request to the origin.","title":"Lamda@Edge"},{"location":"AWS/Lambda/Why%20AWS%20Lambda/","text":"Why AWS Lamda \u00b6 AWS EC2 Virtual Servers in the Cloud Limited by [[RAM]] and [[CPU]] Continuously running Scaling means intervention to add / remove servers AWS Lambda Virtual functions - no servers to manage Limited by time - short executions Run on-demand Scaling is automated","title":"Why AWS Lambda"},{"location":"AWS/Lambda/Why%20AWS%20Lambda/#why-aws-lamda","text":"AWS EC2 Virtual Servers in the Cloud Limited by [[RAM]] and [[CPU]] Continuously running Scaling means intervention to add / remove servers AWS Lambda Virtual functions - no servers to manage Limited by time - short executions Run on-demand Scaling is automated","title":"Why AWS Lamda"},{"location":"AWS/Neptune/Neptune%20for%20Solutions%20Architect/","text":"Neptune for Solutions Architect \u00b6 Operations : Similar to RDS for Solutions Architect Security : IAM , VPC Summary , AWS KMS (Key Management Service) , [[SSL]] (similar to RDS) + [[IAM Authentication]] Reliability : [[Multi AZ]], [[clustering]] Performance : best suited for [[graphs]], clustering to improve performance Cost : pay per node provisioned (similar to RDS) Remember - Neptune - graphs","title":"Neptune for Solutions Architect"},{"location":"AWS/Neptune/Neptune%20for%20Solutions%20Architect/#neptune-for-solutions-architect","text":"Operations : Similar to RDS for Solutions Architect Security : IAM , VPC Summary , AWS KMS (Key Management Service) , [[SSL]] (similar to RDS) + [[IAM Authentication]] Reliability : [[Multi AZ]], [[clustering]] Performance : best suited for [[graphs]], clustering to improve performance Cost : pay per node provisioned (similar to RDS) Remember - Neptune - graphs","title":"Neptune for Solutions Architect"},{"location":"AWS/Neptune/Neptune/","text":"Neptune \u00b6 [[Fully managed]] [[graph database]] When do we use [[graphs]]? [[High relationship data]] Social networking: users friends with users, replied to comment on post of user and likes other comments [[Knowledge graphs]] (wikipedia) [[Highly Available]] across 3 Availability Zone s, with up to 15 [[read replica]]s [[Point in Time Restore]], [[continuous backup]] to AWS S3 Support for AWS KMS (Key Management Service) encryption at rest + [[HTTPS]]","title":"Neptune"},{"location":"AWS/Neptune/Neptune/#neptune","text":"[[Fully managed]] [[graph database]] When do we use [[graphs]]? [[High relationship data]] Social networking: users friends with users, replied to comment on post of user and likes other comments [[Knowledge graphs]] (wikipedia) [[Highly Available]] across 3 Availability Zone s, with up to 15 [[read replica]]s [[Point in Time Restore]], [[continuous backup]] to AWS S3 Support for AWS KMS (Key Management Service) encryption at rest + [[HTTPS]]","title":"Neptune"},{"location":"AWS/OpsWorks/AWS%20OpsWorks/","text":"AWS OpsWorks \u00b6 [[Chef]] & [[Puppet]] help you perform [[server configuration]] automatically or repetitive actions They work great with AWS EC2 & [[On Premise VM]] AWS Opsworks = Managed [[Chef]] & [[Puppet]] It's an alternative to SSM Parameter Store Quick word on Chef / Puppet \u00b6 They help with managing configuration as code Helps in having [[consistent deployments]] Works with [[Linux]] / [[Windows]] Can automate: user accounts, [[CRON]], [[NTP]], packages, services... The leverage \"Recipes\" or \"Manifests\" [[Chef]] / [[Puppet]] have similarities with SSM Parameter Store / Elastic Beanstalk / CloudFormation but they're open-source tools that work cross-cloud","title":"AWS OpsWorks"},{"location":"AWS/OpsWorks/AWS%20OpsWorks/#aws-opsworks","text":"[[Chef]] & [[Puppet]] help you perform [[server configuration]] automatically or repetitive actions They work great with AWS EC2 & [[On Premise VM]] AWS Opsworks = Managed [[Chef]] & [[Puppet]] It's an alternative to SSM Parameter Store","title":"AWS OpsWorks"},{"location":"AWS/OpsWorks/AWS%20OpsWorks/#quick-word-on-chef-puppet","text":"They help with managing configuration as code Helps in having [[consistent deployments]] Works with [[Linux]] / [[Windows]] Can automate: user accounts, [[CRON]], [[NTP]], packages, services... The leverage \"Recipes\" or \"Manifests\" [[Chef]] / [[Puppet]] have similarities with SSM Parameter Store / Elastic Beanstalk / CloudFormation but they're open-source tools that work cross-cloud","title":"Quick word on Chef / Puppet"},{"location":"AWS/Organizations/AWS%20Organizations/","text":"AWS Organizations \u00b6 Global service Allows to manage multiple [[AWS account]]s The main account is the master account - you can't change it Other accounts are member accounts Member accounts can only be part of one organization Consolidated Billing across all accounts - single payment method Pricing benefits from aggregated usage (volume discount for AWS EC2 , AWS S3 ...) API is available to automate AWS account creation OU & Service Control Policies (SCPs) \u00b6 Organise accounts in Organizational Unit (OU) Can be anything: dev / test / prod or Finance / HR / IT Can nest OU within OU Apply Service Control Policies (SCPs) to OU Permit / Deny access to AWS services SCP has a similar syntax to IAM It's a filter to IAM Help sandbox account creation Help to separate dev and prod resources Helpful to only allow approved services","title":"AWS Organizations"},{"location":"AWS/Organizations/AWS%20Organizations/#aws-organizations","text":"Global service Allows to manage multiple [[AWS account]]s The main account is the master account - you can't change it Other accounts are member accounts Member accounts can only be part of one organization Consolidated Billing across all accounts - single payment method Pricing benefits from aggregated usage (volume discount for AWS EC2 , AWS S3 ...) API is available to automate AWS account creation","title":"AWS Organizations"},{"location":"AWS/Organizations/AWS%20Organizations/#ou-service-control-policies-scps","text":"Organise accounts in Organizational Unit (OU) Can be anything: dev / test / prod or Finance / HR / IT Can nest OU within OU Apply Service Control Policies (SCPs) to OU Permit / Deny access to AWS services SCP has a similar syntax to IAM It's a filter to IAM Help sandbox account creation Help to separate dev and prod resources Helpful to only allow approved services","title":"OU &amp; Service Control Policies (SCPs)"},{"location":"AWS/Organizations/Organizational%20Unit/","text":"AWS Organizations","title":"Organizational Unit"},{"location":"AWS/Organizations/Service%20Control%20Policies/","text":"AWS Organizations","title":"Service Control Policies"},{"location":"AWS/RDS/AWS%20RDS/","text":"AWS RDS Overview \u00b6 RDS stands for [[Relational Database]] Service It's a manager DB service for DB use SQL as as a query language It allows you to create databases in the cloud that are managed by AWS [[PostgreSQL]] [[Oracle]] [[MySQL]] [[MariaDB]] [[Microsoft SQL Server]] AWS Aurora (AWS Proprietary database) Must provision an AWS EC2 instance & EBS Volume type and size Support for [[Read Replica]]s and [[Multi AZ]] Security through IAM , Security Group s, AWS KMS (Key Management Service) , [[SSL]] in transit [[Backup]] / [[Snapshot]] / [[Point in time restore]] feature Managed and Scheduled maintenance Monitoring though CloudWatch Use case: store relational datasets ([[RDBMS]]/[[OLTP]]), perform queries, transactional inserts / update / delete is available.","title":"AWS RDS Overview"},{"location":"AWS/RDS/AWS%20RDS/#aws-rds-overview","text":"RDS stands for [[Relational Database]] Service It's a manager DB service for DB use SQL as as a query language It allows you to create databases in the cloud that are managed by AWS [[PostgreSQL]] [[Oracle]] [[MySQL]] [[MariaDB]] [[Microsoft SQL Server]] AWS Aurora (AWS Proprietary database) Must provision an AWS EC2 instance & EBS Volume type and size Support for [[Read Replica]]s and [[Multi AZ]] Security through IAM , Security Group s, AWS KMS (Key Management Service) , [[SSL]] in transit [[Backup]] / [[Snapshot]] / [[Point in time restore]] feature Managed and Scheduled maintenance Monitoring though CloudWatch Use case: store relational datasets ([[RDBMS]]/[[OLTP]]), perform queries, transactional inserts / update / delete is available.","title":"AWS RDS Overview"},{"location":"AWS/RDS/RDS%20Backups/","text":"RDS Backups \u00b6 Backups are automatically enabled in AWS RDS Automated backups: Daily full [[snapshot]] of the database Capture [[transaction logs]] in real time Ability to restore to any point in time 7 days retention (can be increased to 35 days) DB snapshots Manually triggered by the user Retention of backup for as long as you want","title":"RDS Backups"},{"location":"AWS/RDS/RDS%20Backups/#rds-backups","text":"Backups are automatically enabled in AWS RDS Automated backups: Daily full [[snapshot]] of the database Capture [[transaction logs]] in real time Ability to restore to any point in time 7 days retention (can be increased to 35 days) DB snapshots Manually triggered by the user Retention of backup for as long as you want","title":"RDS Backups"},{"location":"AWS/RDS/RDS%20Encryption/","text":"RDS Encryption \u00b6 Encryption at rest capability with AWS KMS (Key Management Service) - [[AES-256]] encryption [[SSL certificates]] to encrypt data to AWS RDS in flight To enforce SSL [[PostgreSQL]]: rds.force_ssl1 in the AWS RDS console ([[Parameter Groups]]) [[MySQL]]: GRANT USAGE ON *.* TO 'mysqluser'@'%' REQUIRE SSL; To connect using SSL: Provide the [[SSL Trust certificate]] (can be downloaded from AWS) Provide [[SSL options]] when connecting to database","title":"RDS Encryption"},{"location":"AWS/RDS/RDS%20Encryption/#rds-encryption","text":"Encryption at rest capability with AWS KMS (Key Management Service) - [[AES-256]] encryption [[SSL certificates]] to encrypt data to AWS RDS in flight To enforce SSL [[PostgreSQL]]: rds.force_ssl1 in the AWS RDS console ([[Parameter Groups]]) [[MySQL]]: GRANT USAGE ON *.* TO 'mysqluser'@'%' REQUIRE SSL; To connect using SSL: Provide the [[SSL Trust certificate]] (can be downloaded from AWS) Provide [[SSL options]] when connecting to database","title":"RDS Encryption"},{"location":"AWS/RDS/RDS%20Hands%20on/","text":"RDS Hands on \u00b6 We can go to the AWS RDS in AWS. And click on create a database . We are going to use Standard Create and MySQL because the aurora does not provide free tier. Then we need: - Set a template ([[production]], dev/test/free tier) - Specify identifier - Specify admin credentials - Choose instance type - Specify [[storage]] - Choose whether to use Multi-AZ deployment - Choose the VPC Summary to deploy to, security group s, [[publicly accessible]]? - Setup additional configuration - [[Backups]] - [[Monitoring]] - [[Logging]] - [[Maintenance]] - [[Deletion prevention]] Normally we wouldn't want for the Database to be publicly accessible, but we are going to select it for testing purposes. Once everything is set up, the instance will need a few minutes to deploy. Once that is done, we can grab the endpoint URL and connect to it: When opening up the database section, we can see all the configuration, monitoring dashboards etc. We can also take multiple actions like delete database, upgrade it, create replicas or snapshots, migrate them to other regions.","title":"RDS Hands on"},{"location":"AWS/RDS/RDS%20Hands%20on/#rds-hands-on","text":"We can go to the AWS RDS in AWS. And click on create a database . We are going to use Standard Create and MySQL because the aurora does not provide free tier. Then we need: - Set a template ([[production]], dev/test/free tier) - Specify identifier - Specify admin credentials - Choose instance type - Specify [[storage]] - Choose whether to use Multi-AZ deployment - Choose the VPC Summary to deploy to, security group s, [[publicly accessible]]? - Setup additional configuration - [[Backups]] - [[Monitoring]] - [[Logging]] - [[Maintenance]] - [[Deletion prevention]] Normally we wouldn't want for the Database to be publicly accessible, but we are going to select it for testing purposes. Once everything is set up, the instance will need a few minutes to deploy. Once that is done, we can grab the endpoint URL and connect to it: When opening up the database section, we can see all the configuration, monitoring dashboards etc. We can also take multiple actions like delete database, upgrade it, create replicas or snapshots, migrate them to other regions.","title":"RDS Hands on"},{"location":"AWS/RDS/RDS%20Multi%20AZ%20%28Disaster%20Recovery%29/","text":"RDS [[Multi AZ]] ( Disaster Recovery ) \u00b6 [[SYNC replication]] One [[DNS name]] - automatic app [[failover]] to standby Increase [[availability]] Failover in case of loss of Availability Zone , loss of network, instance or storage failure No manual intervention in apps Not used for scaling","title":"RDS Multi AZ (Disaster Recovery)"},{"location":"AWS/RDS/RDS%20Multi%20AZ%20%28Disaster%20Recovery%29/#rds-multi-az-disaster-recovery","text":"[[SYNC replication]] One [[DNS name]] - automatic app [[failover]] to standby Increase [[availability]] Failover in case of loss of Availability Zone , loss of network, instance or storage failure No manual intervention in apps Not used for scaling","title":"RDS [[Multi AZ]] (Disaster Recovery)"},{"location":"AWS/RDS/RDS%20Read%20replicas%20for%20read%20scalability/","text":"RDS Read replicas for read scalability \u00b6 Up to 5 [[Read Replica]]s Within Availability Zone , [[Cross Availability Zone]] or [[Cross Region]] [[Replication]] is [[ASYNC]], so reads are eventually consistent. Applications must update the connection string to leverage read replicas","title":"RDS Read replicas for read scalability"},{"location":"AWS/RDS/RDS%20Read%20replicas%20for%20read%20scalability/#rds-read-replicas-for-read-scalability","text":"Up to 5 [[Read Replica]]s Within Availability Zone , [[Cross Availability Zone]] or [[Cross Region]] [[Replication]] is [[ASYNC]], so reads are eventually consistent. Applications must update the connection string to leverage read replicas","title":"RDS Read replicas for read scalability"},{"location":"AWS/RDS/RDS%20Security%20for%20SysOps/","text":"RDS Security for SysOps \u00b6 Encryption at rest Is done only when you first create the DB Instance or create [[snapshot]] from the [[database]], copy snapshot as [[encrypted]], create DB from snapshot Your responsibility Check the [[ports]] / [[IP]] / Security group inbound rules for DB's Security Group In-database user creation and permissions Creating a database with or without public access Ensure parameter groups or DB is configured to only allow SSL connections AWS responsibility No [[SSH]] access No manual [[DB patching]] No [[OS patching]] No way to audit the underlying instance","title":"RDS Security for SysOps"},{"location":"AWS/RDS/RDS%20Security%20for%20SysOps/#rds-security-for-sysops","text":"Encryption at rest Is done only when you first create the DB Instance or create [[snapshot]] from the [[database]], copy snapshot as [[encrypted]], create DB from snapshot Your responsibility Check the [[ports]] / [[IP]] / Security group inbound rules for DB's Security Group In-database user creation and permissions Creating a database with or without public access Ensure parameter groups or DB is configured to only allow SSL connections AWS responsibility No [[SSH]] access No manual [[DB patching]] No [[OS patching]] No way to audit the underlying instance","title":"RDS Security for SysOps"},{"location":"AWS/RDS/RDS%20Security/","text":"RDS Security \u00b6 RDS databases are usually deployed within a [[private subnet]], not a [[public subnet]] RDS security works by leveraging Security Group s (the same concept as for AWS EC2 instances) - it controls who can communicate with AWS RDS . IAM Policy help control who can manage AWS RDS Traditional Username and Password can be used to login to the database IAM user s can now be used too (for [[MySQL]] / AWS Aurora )","title":"RDS Security"},{"location":"AWS/RDS/RDS%20Security/#rds-security","text":"RDS databases are usually deployed within a [[private subnet]], not a [[public subnet]] RDS security works by leveraging Security Group s (the same concept as for AWS EC2 instances) - it controls who can communicate with AWS RDS . IAM Policy help control who can manage AWS RDS Traditional Username and Password can be used to login to the database IAM user s can now be used too (for [[MySQL]] / AWS Aurora )","title":"RDS Security"},{"location":"AWS/RDS/RDS%20for%20Solutions%20Architect/","text":"RDS for Solutions Architect \u00b6 Read replicas are used for SELECT (=read) only kind of statements (not INSERT, UPDATE, DELETE). Amazon RDS supports [[Transparent Data Encryption]] for DB encryption: Oracle or SQL Server DB instance only TDE can be used on top of KMS - may affect performance [[IAM Authentication]] (versus traditional username / password): Works for [[MySQL]], [[PostgreSQL]] Lifespan of an authentication token is 15 minutes (short-lived) Tokens are generated by AWS credentials [[SSL]] must be used when connecting to the database Easy to use [[EC2 Instance Roles]] to connect to the RDS database Operations : small [[downtime]] when [[failover]] happens, when maintenance happens, scaling in [[Read Replica]]s / AWS EC2 instance / restore EBS implies manual intervention, application changes Security : AWS responsible for [[OS security]], we are responsible for setting up AWS KMS (Key Management Service) , Security Group s, IAM Policy , authorising users in DB, using [[SSL]] Reliability : [[Multi AZ]] feature, [[failover]] in case of failures Performance : depends on [[EC2 instance type]], EBS Volume , ability to add [[Read Replica]]s. Doesn't auto-scale Cost : Pay per hour based on provisioned AWS EC2 and EBS Volume","title":"RDS for Solutions Architect"},{"location":"AWS/RDS/RDS%20for%20Solutions%20Architect/#rds-for-solutions-architect","text":"Read replicas are used for SELECT (=read) only kind of statements (not INSERT, UPDATE, DELETE). Amazon RDS supports [[Transparent Data Encryption]] for DB encryption: Oracle or SQL Server DB instance only TDE can be used on top of KMS - may affect performance [[IAM Authentication]] (versus traditional username / password): Works for [[MySQL]], [[PostgreSQL]] Lifespan of an authentication token is 15 minutes (short-lived) Tokens are generated by AWS credentials [[SSL]] must be used when connecting to the database Easy to use [[EC2 Instance Roles]] to connect to the RDS database Operations : small [[downtime]] when [[failover]] happens, when maintenance happens, scaling in [[Read Replica]]s / AWS EC2 instance / restore EBS implies manual intervention, application changes Security : AWS responsible for [[OS security]], we are responsible for setting up AWS KMS (Key Management Service) , Security Group s, IAM Policy , authorising users in DB, using [[SSL]] Reliability : [[Multi AZ]] feature, [[failover]] in case of failures Performance : depends on [[EC2 instance type]], EBS Volume , ability to add [[Read Replica]]s. Doesn't auto-scale Cost : Pay per hour based on provisioned AWS EC2 and EBS Volume","title":"RDS for Solutions Architect"},{"location":"AWS/RDS/RDS%20vs%20Aurora/","text":"RDS vs Aurora \u00b6 AWS Aurora is a [[proprietary technology]] from AWS (not open sourced) [[PostgreSQL]] and [[MySQL]] are both supported as AWS Aurora DB (that means your drivers will work as if Aurora was a Postgres or MySQL database) AWS Aurora is AWS cloud optimized and claims 5x performance improvement over [[MySQL]] on AWS RDS , over 3x the performance of Postgres on RDS. Aurora storage automatically grows in increments of 10GB up to 64TB. Aurora can have 15 [[Read Replica]]s while [[MySQL]] has 5, and the replication process is faster (sub 10 ms replica lag). [[Failover]] in Aurora is instantaneous. It's High Availability native. Aurora costs more than RDS (20% more) - but it's more efficient.","title":"RDS vs Aurora"},{"location":"AWS/RDS/RDS%20vs%20Aurora/#rds-vs-aurora","text":"AWS Aurora is a [[proprietary technology]] from AWS (not open sourced) [[PostgreSQL]] and [[MySQL]] are both supported as AWS Aurora DB (that means your drivers will work as if Aurora was a Postgres or MySQL database) AWS Aurora is AWS cloud optimized and claims 5x performance improvement over [[MySQL]] on AWS RDS , over 3x the performance of Postgres on RDS. Aurora storage automatically grows in increments of 10GB up to 64TB. Aurora can have 15 [[Read Replica]]s while [[MySQL]] has 5, and the replication process is faster (sub 10 ms replica lag). [[Failover]] in Aurora is instantaneous. It's High Availability native. Aurora costs more than RDS (20% more) - but it's more efficient.","title":"RDS vs Aurora"},{"location":"AWS/RDS/RDS%20vs%20DB%20on%20EC2/","text":"Advantage over using RDS vs DB on EC2 \u00b6 Managed service OS patching level Continuous backups and restore to specific timestamp ([[Point in Time Restore]]) Monitoring dashboards Read replicas for improved read performance [[Multi AZ]] setup for Disaster Recovery Maintenance windows for for upgrades Scaling capability ( Vertical scalability and Horizontal scalability ) But you can't EC2 SSH into your instances","title":"RDS vs DB on EC2"},{"location":"AWS/RDS/RDS%20vs%20DB%20on%20EC2/#advantage-over-using-rds-vs-db-on-ec2","text":"Managed service OS patching level Continuous backups and restore to specific timestamp ([[Point in Time Restore]]) Monitoring dashboards Read replicas for improved read performance [[Multi AZ]] setup for Disaster Recovery Maintenance windows for for upgrades Scaling capability ( Vertical scalability and Horizontal scalability ) But you can't EC2 SSH into your instances","title":"Advantage over using RDS vs DB on EC2"},{"location":"AWS/RDS/Aurora/AWS%20Aurora/","text":"Aurora overview \u00b6 Aurora is a proprietary technology from AWS (not open sourced) [[PostgreSQL]] and [[MySQL]] are both supported as Aurora DB (that means your drivers will work as if Aurora was a PostgreSQL or MySQL database) Aurora is \"AWS cloud optimized\" and claims 5x performance improvement over MySQL on RDS, over 3x the performance of PostgreSQL on RDS. Aurora storage automatically grows in increments of 10GB, up to 64TB. Aurora can have 15 replicas while MySQL has 5, and the replication process is faster (sub 10 ms replica lag) Failover in Aurora is instantaneous. It's Programming/AWS/Fundamentals/High Availability native. Aurora costs more than RDS (20% more) - but is more efficient. Auto healing capability -[[ Multi AZ]], Auto Scaling [[Read Replica]]s [[Read Replica]]s can be Global Aurora database can be Global for Disaster Recovery or [[latency]] purposes Auto scaling of storage from 10GB to 64 TB Define AWS EC2 instance type for aurora instances Same security / monitoring / maintenance features as RDS Security Aurora Serverless option Use case: same as AWS RDS , but with less maintenance / more flexibility / more performance","title":"Aurora overview"},{"location":"AWS/RDS/Aurora/AWS%20Aurora/#aurora-overview","text":"Aurora is a proprietary technology from AWS (not open sourced) [[PostgreSQL]] and [[MySQL]] are both supported as Aurora DB (that means your drivers will work as if Aurora was a PostgreSQL or MySQL database) Aurora is \"AWS cloud optimized\" and claims 5x performance improvement over MySQL on RDS, over 3x the performance of PostgreSQL on RDS. Aurora storage automatically grows in increments of 10GB, up to 64TB. Aurora can have 15 replicas while MySQL has 5, and the replication process is faster (sub 10 ms replica lag) Failover in Aurora is instantaneous. It's Programming/AWS/Fundamentals/High Availability native. Aurora costs more than RDS (20% more) - but is more efficient. Auto healing capability -[[ Multi AZ]], Auto Scaling [[Read Replica]]s [[Read Replica]]s can be Global Aurora database can be Global for Disaster Recovery or [[latency]] purposes Auto scaling of storage from 10GB to 64 TB Define AWS EC2 instance type for aurora instances Same security / monitoring / maintenance features as RDS Security Aurora Serverless option Use case: same as AWS RDS , but with less maintenance / more flexibility / more performance","title":"Aurora overview"},{"location":"AWS/RDS/Aurora/Aurora%20DB%20Cluster/","text":"Aurora DB Cluster \u00b6","title":"Aurora DB Cluster"},{"location":"AWS/RDS/Aurora/Aurora%20DB%20Cluster/#aurora-db-cluster","text":"","title":"Aurora DB Cluster"},{"location":"AWS/RDS/Aurora/Aurora%20Features/","text":"Features of Aurora \u00b6 [[Automatic failover]] [[Backup and recovery]] [[Isolation]] and [[security]] -[[ Industry compliance]] Push-button scaling [[Automated Patching]] with [[Zero Downtime]] Advanced [[Monitoring]] Routine [[Maintenance]] Backtrack: restore data at any point of time without using backups.","title":"Aurora Features"},{"location":"AWS/RDS/Aurora/Aurora%20Features/#features-of-aurora","text":"[[Automatic failover]] [[Backup and recovery]] [[Isolation]] and [[security]] -[[ Industry compliance]] Push-button scaling [[Automated Patching]] with [[Zero Downtime]] Advanced [[Monitoring]] Routine [[Maintenance]] Backtrack: restore data at any point of time without using backups.","title":"Features of Aurora"},{"location":"AWS/RDS/Aurora/Aurora%20For%20Solutions%20Architects/","text":"Aurora for Solutions Architects \u00b6 Can use [[IAM authentication]] for AWS Aurora [[MySQL]] and [[PostgreSQL]] [[Aurora Global]] databases span multiple regions and enable Disaster Recovery One primary region One DR region The DR region can be used for lower latency reads < 1 second replica lag on average If not using Global Databases, you can create cross-region read replicas But the FAQ recommends you using [[Global Databases]] instead Operations : less operations, [[auto scaling storage]] Security : AWS responsible for [[OS security]], we are responsible for setting up AWS KMS (Key Management Service) , Security Group s, IAM Policy , authorising users in DB, using [[SSL]] Reliability : [[Multi AZ]], Programming/AWS/Fundamentals/High Availability , possibly more than AWS RDS , Aurora Serverless option. Performance : 5x performance (according to AWS) due to architectural optimisations. Up to 15 [[Read Replica]]s (only 5 for AWS RDS ) Cost : Pay per hour based on AWS EC2 and storage usage. Possibly lower costs compared to Enterprise grade databases such as [[Oracle]].","title":"Aurora For Solutions Architects"},{"location":"AWS/RDS/Aurora/Aurora%20For%20Solutions%20Architects/#aurora-for-solutions-architects","text":"Can use [[IAM authentication]] for AWS Aurora [[MySQL]] and [[PostgreSQL]] [[Aurora Global]] databases span multiple regions and enable Disaster Recovery One primary region One DR region The DR region can be used for lower latency reads < 1 second replica lag on average If not using Global Databases, you can create cross-region read replicas But the FAQ recommends you using [[Global Databases]] instead Operations : less operations, [[auto scaling storage]] Security : AWS responsible for [[OS security]], we are responsible for setting up AWS KMS (Key Management Service) , Security Group s, IAM Policy , authorising users in DB, using [[SSL]] Reliability : [[Multi AZ]], Programming/AWS/Fundamentals/High Availability , possibly more than AWS RDS , Aurora Serverless option. Performance : 5x performance (according to AWS) due to architectural optimisations. Up to 15 [[Read Replica]]s (only 5 for AWS RDS ) Cost : Pay per hour based on AWS EC2 and storage usage. Possibly lower costs compared to Enterprise grade databases such as [[Oracle]].","title":"Aurora for Solutions Architects"},{"location":"AWS/RDS/Aurora/Aurora%20High%20Availability%20and%20Read%20Scaling/","text":"Aurora High Availability and [[Read Scaling]] \u00b6 6 copies of your data across 3 Availability Zone 4 copies out of 6 needed for writes 3 copies out of 6 needed for reads [[Self healing]] with [[peer-to-peer replication]] Storage is [[striped]] across 100s of volumes One AWS Aurora instance takes writes (master) Automated [[failover]] for master in less than 30 seconds Master + up to 15 aurora read replica serve reads Support for [[Cross Region Replication]]","title":"Aurora High Availability and Read Scaling"},{"location":"AWS/RDS/Aurora/Aurora%20High%20Availability%20and%20Read%20Scaling/#aurora-high-availability-and-read-scaling","text":"6 copies of your data across 3 Availability Zone 4 copies out of 6 needed for writes 3 copies out of 6 needed for reads [[Self healing]] with [[peer-to-peer replication]] Storage is [[striped]] across 100s of volumes One AWS Aurora instance takes writes (master) Automated [[failover]] for master in less than 30 seconds Master + up to 15 aurora read replica serve reads Support for [[Cross Region Replication]]","title":"Aurora High Availability and [[Read Scaling]]"},{"location":"AWS/RDS/Aurora/Aurora%20Security/","text":"Aurora Security \u00b6 Encryption at rest using AWS KMS (Key Management Service) Automated backups, snapshots and replicas are also encrypted Encryption in flight using [[SSL]] (same process as [[MySQL]] or [[PostgreSQL]]) Authentication using IAM You are responsible for protecting the instance with Security Group s You can't [[SSH]]","title":"Aurora Security"},{"location":"AWS/RDS/Aurora/Aurora%20Security/#aurora-security","text":"Encryption at rest using AWS KMS (Key Management Service) Automated backups, snapshots and replicas are also encrypted Encryption in flight using [[SSL]] (same process as [[MySQL]] or [[PostgreSQL]]) Authentication using IAM You are responsible for protecting the instance with Security Group s You can't [[SSH]]","title":"Aurora Security"},{"location":"AWS/RDS/Aurora/Aurora%20Serverless/","text":"Aurora Serverless \u00b6 No need to choose an instance size Only supports [[MySQL]] 5.6 (as of Jan 2019) & [[MySQL]] (beta) Helpful when you can't predict workload DB cluster starts, shutdown and scales automatically based on CPU / connections Can migrate from Aurora DB Cluster to AWS Aurora serverless and vice versa Aurora serverless usage is measured in ACU ([[Aurora Capacity Units]]) Billed in 5 minutes increment of ACU Note: Some features of aurora aren't supported in Serverless mode, be sure to check the documentation if you plan on using it. NOTE: aurora serverless v2 supports MySQL 8 now","title":"Aurora Serverless"},{"location":"AWS/RDS/Aurora/Aurora%20Serverless/#aurora-serverless","text":"No need to choose an instance size Only supports [[MySQL]] 5.6 (as of Jan 2019) & [[MySQL]] (beta) Helpful when you can't predict workload DB cluster starts, shutdown and scales automatically based on CPU / connections Can migrate from Aurora DB Cluster to AWS Aurora serverless and vice versa Aurora serverless usage is measured in ACU ([[Aurora Capacity Units]]) Billed in 5 minutes increment of ACU Note: Some features of aurora aren't supported in Serverless mode, be sure to check the documentation if you plan on using it. NOTE: aurora serverless v2 supports MySQL 8 now","title":"Aurora Serverless"},{"location":"AWS/RDS/Aurora/Aurora%20hands%20on/","text":"Aurora hands on \u00b6 Once again, we are going to RDS service in [[AWS console]] and choosing create database . This time we are going to select AWS Aurora engine. Select version and location Choose a template Setup identifier and credentials Choose instance size Availability & durability Connectivity Database authentication When done, it will create a master and a read [[replica]]. If needed, you can manually add more readers or create an [[auto scaling policy]].","title":"Aurora hands on"},{"location":"AWS/RDS/Aurora/Aurora%20hands%20on/#aurora-hands-on","text":"Once again, we are going to RDS service in [[AWS console]] and choosing create database . This time we are going to select AWS Aurora engine. Select version and location Choose a template Setup identifier and credentials Choose instance size Availability & durability Connectivity Database authentication When done, it will create a master and a read [[replica]]. If needed, you can manually add more readers or create an [[auto scaling policy]].","title":"Aurora hands on"},{"location":"AWS/Redshift/Redshift%20for%20Solutions%20Architect/","text":"Redshift for Solutions Architect \u00b6 Operations : similar to RDS for Solutions Architect Security : IAM , VPC Summary , AWS KMS (Key Management Service) , [[SSL]] (similar to RDS for Solutions Architect ) Reliability : [[Highly Available]], [[auto healing]] features Performance : 10x performance vs other data warehousing, [[compression]] Cost : pay per node provisioned, 1/10th of the cost vs [[data warehouse]]s Remember: Redshift = [[analytics]] / [[BI]] / [[Data warehouse]]","title":"Redshift for Solutions Architect"},{"location":"AWS/Redshift/Redshift%20for%20Solutions%20Architect/#redshift-for-solutions-architect","text":"Operations : similar to RDS for Solutions Architect Security : IAM , VPC Summary , AWS KMS (Key Management Service) , [[SSL]] (similar to RDS for Solutions Architect ) Reliability : [[Highly Available]], [[auto healing]] features Performance : 10x performance vs other data warehousing, [[compression]] Cost : pay per node provisioned, 1/10th of the cost vs [[data warehouse]]s Remember: Redshift = [[analytics]] / [[BI]] / [[Data warehouse]]","title":"Redshift for Solutions Architect"},{"location":"AWS/Redshift/Redshift/","text":"Redshift \u00b6 Redshift is based on [[PostgreSQL]], but it's not used for [[OLTP]] It's [[OLAP]] - online analytical processing (analytics and data warehousing) 10x better performance than other data warehouses, scale to PBs of data [[Massively Parrallel Query Execution (MPP)]], [[Highly Available]] Pay as you go based on the instances provisioned Has a [[SQL]] interface for performing the queries BI tools such as [[AWS Quicksight]] or [[Tableau]] integrate with it Data is loaded from AWS S3 , Programming/AWS/DynamoDB/DynamoDB , [[Amazon DMS]], other DBs From 1 node to 128 nodes, up to 160GB of space for node Leader node: for [[query planning]], [[results aggregation]] Compute node: for performing the queries, send results to leader [[Redshift Spectrum]]: perform queries directly against AWS S3 (no need to load) Backup & Restore, Security VPC Summary / IAM / AWS KMS (Key Management Service) , [[Monitoring]] [[Redshift Enhanced VPC Routing]]: COPY / UNLOAD goes through VPC Summary","title":"Redshift"},{"location":"AWS/Redshift/Redshift/#redshift","text":"Redshift is based on [[PostgreSQL]], but it's not used for [[OLTP]] It's [[OLAP]] - online analytical processing (analytics and data warehousing) 10x better performance than other data warehouses, scale to PBs of data [[Massively Parrallel Query Execution (MPP)]], [[Highly Available]] Pay as you go based on the instances provisioned Has a [[SQL]] interface for performing the queries BI tools such as [[AWS Quicksight]] or [[Tableau]] integrate with it Data is loaded from AWS S3 , Programming/AWS/DynamoDB/DynamoDB , [[Amazon DMS]], other DBs From 1 node to 128 nodes, up to 160GB of space for node Leader node: for [[query planning]], [[results aggregation]] Compute node: for performing the queries, send results to leader [[Redshift Spectrum]]: perform queries directly against AWS S3 (no need to load) Backup & Restore, Security VPC Summary / IAM / AWS KMS (Key Management Service) , [[Monitoring]] [[Redshift Enhanced VPC Routing]]: COPY / UNLOAD goes through VPC Summary","title":"Redshift"},{"location":"AWS/Route%2053/AWS%20Route%2053/","text":"AWS Route 53 Overview \u00b6 Route53 is a Managed [[DNS (Domain Name System)]] DNS is a collection of rules and records which helps clients understand how to reach a server through URLs In AWS, the most common records are: [[A RECORD ]]: URL to IPv4 [[AAAA RECORD]]: URL to IPv6 [[CNAME RECORD]]: URL to URL [[ALIAS RECORD]]: URL to AWS resource Route53 can use: public domain names you own (or buy) private domain domain names that can be resolved by your instances in your VPCs Route53 has advanced features such as: Load Balancing (through DNS - also called client load balancing) Health Checks (although limited) [[Routing Policy]]: simple, failover, geolocation, latency, weighted, multi value You pay $0.50 per month per hosted zone. Route 53 Hands on","title":"AWS Route 53 Overview"},{"location":"AWS/Route%2053/AWS%20Route%2053/#aws-route-53-overview","text":"Route53 is a Managed [[DNS (Domain Name System)]] DNS is a collection of rules and records which helps clients understand how to reach a server through URLs In AWS, the most common records are: [[A RECORD ]]: URL to IPv4 [[AAAA RECORD]]: URL to IPv6 [[CNAME RECORD]]: URL to URL [[ALIAS RECORD]]: URL to AWS resource Route53 can use: public domain names you own (or buy) private domain domain names that can be resolved by your instances in your VPCs Route53 has advanced features such as: Load Balancing (through DNS - also called client load balancing) Health Checks (although limited) [[Routing Policy]]: simple, failover, geolocation, latency, weighted, multi value You pay $0.50 per month per hosted zone. Route 53 Hands on","title":"AWS Route 53 Overview"},{"location":"AWS/Route%2053/CNAME%20vs%20ALIAS/","text":"CNAME vs ALIAS \u00b6 AWS Resources ([[Load Balancer]], Programming/AWS/CloudFront/AWS CloudFront etc) expose an [[AWS URL]] (lb1-1234.us-east-2.elb.amazonaws.com) and you want it to be myapp.mydomain.com - CNAME : - Points a URL to any other URL. (app.mydomain.com => xyz.anything.com) - [[Alias]] - Points a URL to AWS resource (app.mydomain.com => xyz.amazonaws.com) - Works for root domain and non-root domain - Free of charge - native health checks So, for subdomains we can create both CNAME and [[ALIAS]] for our [[load balancer]], but alias would be the recommended way to go. For root domain, we can only use ALIAS.","title":"CNAME vs ALIAS"},{"location":"AWS/Route%2053/CNAME%20vs%20ALIAS/#cname-vs-alias","text":"AWS Resources ([[Load Balancer]], Programming/AWS/CloudFront/AWS CloudFront etc) expose an [[AWS URL]] (lb1-1234.us-east-2.elb.amazonaws.com) and you want it to be myapp.mydomain.com - CNAME : - Points a URL to any other URL. (app.mydomain.com => xyz.anything.com) - [[Alias]] - Points a URL to AWS resource (app.mydomain.com => xyz.amazonaws.com) - Works for root domain and non-root domain - Free of charge - native health checks So, for subdomains we can create both CNAME and [[ALIAS]] for our [[load balancer]], but alias would be the recommended way to go. For root domain, we can only use ALIAS.","title":"CNAME vs ALIAS"},{"location":"AWS/Route%2053/Route%2053%20Hands%20on/","text":"Route53 hands on \u00b6 You can navigate to AWS Route 53 via your [[AWS Console]]. There we can Register domain if you wish to buy a domain, we can also transfer already owned domains. When opening up hosted zone from a domain, we can setup its [[DNS records]]. EC2 Setup \u00b6 We can create an AWS EC2 instances in multiple Availability Zone , create an Application Load Balancer (v2) for them, then create an alias in the AWS Route 53 to the [[Load Balancer]]. Simple Routing Policy Weighted Routing Policy Latency based Routing Policy Failover Routing Policy Geolocation Routing Policy Multi-Value Routing Policy Route 53 Health Checks","title":"Route53 hands on"},{"location":"AWS/Route%2053/Route%2053%20Hands%20on/#route53-hands-on","text":"You can navigate to AWS Route 53 via your [[AWS Console]]. There we can Register domain if you wish to buy a domain, we can also transfer already owned domains. When opening up hosted zone from a domain, we can setup its [[DNS records]].","title":"Route53 hands on"},{"location":"AWS/Route%2053/Route%2053%20Hands%20on/#ec2-setup","text":"We can create an AWS EC2 instances in multiple Availability Zone , create an Application Load Balancer (v2) for them, then create an alias in the AWS Route 53 to the [[Load Balancer]]. Simple Routing Policy Weighted Routing Policy Latency based Routing Policy Failover Routing Policy Geolocation Routing Policy Multi-Value Routing Policy Route 53 Health Checks","title":"EC2 Setup"},{"location":"AWS/Route%2053/Route%2053%20Health%20Checks/","text":"Route 53 Health Checks \u00b6 If we have X health checks failed, we mark resource as unhealthy (default 3) After X health checks passed, mark as healthy (default 3) Default Health Check interval: 30 seconds (can set up to 10s - higher cost) About 15 health checkers will check the endpoint health One request every 2 seconds on average Can have [[HTTP]], [[TCP]] and [[HTTPS]] health checks (no [[SSL verification]]) Possibility of integrating the health check with CloudWatch Health checks can be linked to AWS Route 53 [[DNS queries]].","title":"Route 53 Health Checks"},{"location":"AWS/Route%2053/Route%2053%20Health%20Checks/#route-53-health-checks","text":"If we have X health checks failed, we mark resource as unhealthy (default 3) After X health checks passed, mark as healthy (default 3) Default Health Check interval: 30 seconds (can set up to 10s - higher cost) About 15 health checkers will check the endpoint health One request every 2 seconds on average Can have [[HTTP]], [[TCP]] and [[HTTPS]] health checks (no [[SSL verification]]) Possibility of integrating the health check with CloudWatch Health checks can be linked to AWS Route 53 [[DNS queries]].","title":"Route 53 Health Checks"},{"location":"AWS/Route%2053/Route%2053%20as%20Registrar/","text":"Route53 as a Registrar \u00b6 A [[domain name registrar]] is an organisation that manages the reservation of internet domain names like GoDaddy, Google Domains and also AWS Route 53 . But, if you buy a domain from a 3rd party, you can still use AWS Route 53 . Create A [[Hosted Zone]] in AWS Route 53 . Update [[NS Record]]s on 3rd party to use Route 53 [[name server]]s.","title":"Route 53 as Registrar"},{"location":"AWS/Route%2053/Route%2053%20as%20Registrar/#route53-as-a-registrar","text":"A [[domain name registrar]] is an organisation that manages the reservation of internet domain names like GoDaddy, Google Domains and also AWS Route 53 . But, if you buy a domain from a 3rd party, you can still use AWS Route 53 . Create A [[Hosted Zone]] in AWS Route 53 . Update [[NS Record]]s on 3rd party to use Route 53 [[name server]]s.","title":"Route53 as a Registrar"},{"location":"AWS/Route%2053/TTL/","text":"TTL \u00b6 In order not to overload the [[DNS]], we use TTL to cache the [[DNS]] response for the provided duration. If the [[IP address]] of the server would change not all of the clients would see the change before the TTL expires. The TTL can be set when creating records:","title":"TTL"},{"location":"AWS/Route%2053/TTL/#ttl","text":"In order not to overload the [[DNS]], we use TTL to cache the [[DNS]] response for the provided duration. If the [[IP address]] of the server would change not all of the clients would see the change before the TTL expires. The TTL can be set when creating records:","title":"TTL"},{"location":"AWS/Route%2053/Routing%20Policy/Failover%20Routing%20Policy/","text":"Route 53 - Routing Policies - Failover (Active - Passive) \u00b6 The [[failover]] routing policy uses health checks to determine a health for a resource. When it's marked as unhealthy, the [[DNS]] will be sent to the secondary ([[failover]]) instance.","title":"Route 53 - Routing Policies - Failover (Active - Passive)"},{"location":"AWS/Route%2053/Routing%20Policy/Failover%20Routing%20Policy/#route-53-routing-policies-failover-active-passive","text":"The [[failover]] routing policy uses health checks to determine a health for a resource. When it's marked as unhealthy, the [[DNS]] will be sent to the secondary ([[failover]]) instance.","title":"Route 53 - Routing Policies - Failover (Active - Passive)"},{"location":"AWS/Route%2053/Routing%20Policy/Geolocation%20Routing%20Policy/","text":"Route 53 - Routing Policies - Geolocation \u00b6 Different from Latency based Routing Policy ! This routing is based on user [[location]] Specify location by Continent, Country or by US State (if there's overlapping, most precise location is selected) Should create a [[Default record]] (in case there's no match on location) Use cases: [[website localization]], restrict content distribution, load balancing .. Can be associated with health checks","title":"Route 53 - Routing Policies - Geolocation"},{"location":"AWS/Route%2053/Routing%20Policy/Geolocation%20Routing%20Policy/#route-53-routing-policies-geolocation","text":"Different from Latency based Routing Policy ! This routing is based on user [[location]] Specify location by Continent, Country or by US State (if there's overlapping, most precise location is selected) Should create a [[Default record]] (in case there's no match on location) Use cases: [[website localization]], restrict content distribution, load balancing .. Can be associated with health checks","title":"Route 53 - Routing Policies - Geolocation"},{"location":"AWS/Route%2053/Routing%20Policy/Latency%20based%20Routing%20Policy/","text":"Route 53 - Routing Policies - Latency based \u00b6 Redirect to the resource that has the least [[latency]] close to us Super helpful when [[latency]] for users is a priority Latency is based on traffic between users and AWS Region s Germany users may be directed to the US (if that's the lowest latency) Can be associated with Health Checks (has a failover capability)","title":"Route 53 - Routing Policies - Latency based"},{"location":"AWS/Route%2053/Routing%20Policy/Latency%20based%20Routing%20Policy/#route-53-routing-policies-latency-based","text":"Redirect to the resource that has the least [[latency]] close to us Super helpful when [[latency]] for users is a priority Latency is based on traffic between users and AWS Region s Germany users may be directed to the US (if that's the lowest latency) Can be associated with Health Checks (has a failover capability)","title":"Route 53 - Routing Policies - Latency based"},{"location":"AWS/Route%2053/Routing%20Policy/Multi-Value%20Routing%20Policy/","text":"Route 53 - Routing Policies - Multi-Value \u00b6 Use when routing traffic to multiple resources AWS Route 53 return multiple values/resources Can be associated with Health Checks (return only values for healthy resources) Up to 8 healthy records are returned for each [[Multi-Value query]] Multi Value is not a substitute for having an ELB (it's kind of a [[client side load balancer]]) The difference between simple routing policy is that this supports health checks and will return records based on them.","title":"Route 53 - Routing Policies - Multi-Value"},{"location":"AWS/Route%2053/Routing%20Policy/Multi-Value%20Routing%20Policy/#route-53-routing-policies-multi-value","text":"Use when routing traffic to multiple resources AWS Route 53 return multiple values/resources Can be associated with Health Checks (return only values for healthy resources) Up to 8 healthy records are returned for each [[Multi-Value query]] Multi Value is not a substitute for having an ELB (it's kind of a [[client side load balancer]]) The difference between simple routing policy is that this supports health checks and will return records based on them.","title":"Route 53 - Routing Policies - Multi-Value"},{"location":"AWS/Route%2053/Routing%20Policy/Route%2053%20Routing%20Policies/","text":"AWS Route 53 - Routing Policies \u00b6 Define how AWS Route 53 responds to [[DNS queries]] Don't get confused by the word \"Routing\" It's not the same as [[Load Balancer]] routing which routes the traffic [[DNS]] does not route any traffic, it only responds to [[DNS queries]] Route 53 supports the following routing policies Simple Routing Policy Weighted Routing Policy Failover Routing Policy Latency based Routing Policy Geolocation Routing Policy Multi-Value Routing Policy Geolocation Routing Policy (using [[Route 53 Traffic Flow]] feature)","title":"[[AWS Route 53]] - Routing Policies"},{"location":"AWS/Route%2053/Routing%20Policy/Route%2053%20Routing%20Policies/#aws-route-53-routing-policies","text":"Define how AWS Route 53 responds to [[DNS queries]] Don't get confused by the word \"Routing\" It's not the same as [[Load Balancer]] routing which routes the traffic [[DNS]] does not route any traffic, it only responds to [[DNS queries]] Route 53 supports the following routing policies Simple Routing Policy Weighted Routing Policy Failover Routing Policy Latency based Routing Policy Geolocation Routing Policy Multi-Value Routing Policy Geolocation Routing Policy (using [[Route 53 Traffic Flow]] feature)","title":"AWS Route 53 - Routing Policies"},{"location":"AWS/Route%2053/Routing%20Policy/Simple%20Routing%20Policy/","text":"Simple Routing Policy \u00b6 Typically, route traffic to a single resource Can specify multiple values in the same record If multiple values are returned, a random one is chosen by the client When Alias is enabled, specify only one AWS resource Can't be associated with health checks","title":"Simple Routing Policy"},{"location":"AWS/Route%2053/Routing%20Policy/Simple%20Routing%20Policy/#simple-routing-policy","text":"Typically, route traffic to a single resource Can specify multiple values in the same record If multiple values are returned, a random one is chosen by the client When Alias is enabled, specify only one AWS resource Can't be associated with health checks","title":"Simple Routing Policy"},{"location":"AWS/Route%2053/Routing%20Policy/Weighted%20Routing%20Policy/","text":"AWS Route 53 - Routing Policies - Weighted \u00b6 Control the % of the requests that go to each specific resource Assign each record a relative weight: traffic (%) = (weight for a specific record) / (sum of all weights) Weights don't need to sum up to 100 [[DNS records]] must have the same name and type Can be associated with health checks Use cases: load balancing between regions, testing new application versions... Assign a weight of 0 to a record to stop sending traffic to that resource If all records have weight of 0 then all records will be returned equally","title":"[[AWS Route 53]] - Routing Policies - Weighted"},{"location":"AWS/Route%2053/Routing%20Policy/Weighted%20Routing%20Policy/#aws-route-53-routing-policies-weighted","text":"Control the % of the requests that go to each specific resource Assign each record a relative weight: traffic (%) = (weight for a specific record) / (sum of all weights) Weights don't need to sum up to 100 [[DNS records]] must have the same name and type Can be associated with health checks Use cases: load balancing between regions, testing new application versions... Assign a weight of 0 to a record to stop sending traffic to that resource If all records have weight of 0 then all records will be returned equally","title":"AWS Route 53 - Routing Policies - Weighted"},{"location":"AWS/S3/AWS%20S3%20-%20Consistency%20model/","text":"AWS S3 - Consistency model \u00b6 Read after write consistency for [[HTTP PUT]]S of new objects As soon as an object is written, we can retrieve it ([[HTTP PUT]] 200 -> [[HTTP GET]] 200) This is true, except if we did a GET before to see if the object existed ([[HTTP GET]] 404 -> [[HTTP PUT]] 200 -> [[HTTP GET]] 404) - eventually consistent [[Eventual Consistency]] for [[HTTP DELETE]]S and PUTS of existing objects If we read an object after updating, we might get the older version If we delete an object, we might still be able to retrieve it for a short time","title":"AWS S3 - Consistency model"},{"location":"AWS/S3/AWS%20S3%20-%20Consistency%20model/#aws-s3-consistency-model","text":"Read after write consistency for [[HTTP PUT]]S of new objects As soon as an object is written, we can retrieve it ([[HTTP PUT]] 200 -> [[HTTP GET]] 200) This is true, except if we did a GET before to see if the object existed ([[HTTP GET]] 404 -> [[HTTP PUT]] 200 -> [[HTTP GET]] 404) - eventually consistent [[Eventual Consistency]] for [[HTTP DELETE]]S and PUTS of existing objects If we read an object after updating, we might get the older version If we delete an object, we might still be able to retrieve it for a short time","title":"AWS S3 - Consistency model"},{"location":"AWS/S3/AWS%20S3%20Bucket/","text":"AWS S3 Overview - Buckets \u00b6 AWS S3 allows people to store AWS S3 Objects (files) in \" AWS S3 Bucket \" (directories) Buckets have a globally unique name Buckets are defined at the region level Naming convention No uppercase No underscore 3 to 64 characters long Not an IP Must start with lowercase letter or number AWS S3 Objects","title":"AWS S3 Overview - Buckets"},{"location":"AWS/S3/AWS%20S3%20Bucket/#aws-s3-overview-buckets","text":"AWS S3 allows people to store AWS S3 Objects (files) in \" AWS S3 Bucket \" (directories) Buckets have a globally unique name Buckets are defined at the region level Naming convention No uppercase No underscore 3 to 64 characters long Not an IP Must start with lowercase letter or number AWS S3 Objects","title":"AWS S3 Overview - Buckets"},{"location":"AWS/S3/AWS%20S3%20CORS/","text":"AWS S3 CORS \u00b6 If you request data from another AWS S3 Bucket , you need to enable [[CORS]] [[Cross Origin Resource Sharing]] allows you to limit the number of websites that can request your files in S3 (and limit your costs)","title":"AWS S3 CORS"},{"location":"AWS/S3/AWS%20S3%20CORS/#aws-s3-cors","text":"If you request data from another AWS S3 Bucket , you need to enable [[CORS]] [[Cross Origin Resource Sharing]] allows you to limit the number of websites that can request your files in S3 (and limit your costs)","title":"AWS S3 CORS"},{"location":"AWS/S3/AWS%20S3%20Objects/","text":"Objects \u00b6 AWS S3 Objects (files) have a Key. The key is the full path. /my_file.txt /my_folder1/another/my_file.txt There's no concept of \"directories\" within AWS S3 Bucket (although the UI will trick you to think otherwise). Just keys with very long names that contain slashes (\"/\") Object values are the content of the body: Max Size is 5TB If uploading more than 5GB, must use \"multi-part upload\" Metadata (list of text /key pairs - system or user metadata) Tags ([[unicode]] key/value pair - up to 10) - useful for [[security]] / [[lifecycle]]. Version ID - if versioning enabled The creation process is fairly easy: When opening the bucket, we can upload files to it:","title":"AWS S3 Objects"},{"location":"AWS/S3/AWS%20S3%20Objects/#objects","text":"AWS S3 Objects (files) have a Key. The key is the full path. /my_file.txt /my_folder1/another/my_file.txt There's no concept of \"directories\" within AWS S3 Bucket (although the UI will trick you to think otherwise). Just keys with very long names that contain slashes (\"/\") Object values are the content of the body: Max Size is 5TB If uploading more than 5GB, must use \"multi-part upload\" Metadata (list of text /key pairs - system or user metadata) Tags ([[unicode]] key/value pair - up to 10) - useful for [[security]] / [[lifecycle]]. Version ID - if versioning enabled The creation process is fairly easy: When opening the bucket, we can upload files to it:","title":"Objects"},{"location":"AWS/S3/AWS%20S3%20Websites/","text":"AWS S3 Websites \u00b6 AWS S3 can host static websites and have them accessible on the www The website URL will be <bucket-name>.s3-website-<aws-region>.amazonaws.com OR <bucket-name>.s3-website.<aws-region>.amazonaws.com If you get a [[403 (forbidden)]] error, make sure the [[bucket policy]] allows [[public reads]]. { \"Id\": \"Policy1537533827691\", \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Stmt1537533826523\", \"Action\": [ \"s3:GetOBject\" ], \"Effect\": \"Allow\", \"Resource\": \"arm:aws:s3::::thebucket/*\", \"Principal\": \"*\" } ] }","title":"AWS S3 Websites"},{"location":"AWS/S3/AWS%20S3%20Websites/#aws-s3-websites","text":"AWS S3 can host static websites and have them accessible on the www The website URL will be <bucket-name>.s3-website-<aws-region>.amazonaws.com OR <bucket-name>.s3-website.<aws-region>.amazonaws.com If you get a [[403 (forbidden)]] error, make sure the [[bucket policy]] allows [[public reads]]. { \"Id\": \"Policy1537533827691\", \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Stmt1537533826523\", \"Action\": [ \"s3:GetOBject\" ], \"Effect\": \"Allow\", \"Resource\": \"arm:aws:s3::::thebucket/*\", \"Principal\": \"*\" } ] }","title":"AWS S3 Websites"},{"location":"AWS/S3/AWS%20S3/","text":"Amazon S3 \u00b6 Amazon S3 is one of the main building blocks of AWS It's advertised as \"infinitely scaling\" storage It's widely popular Many websites use AWS S3 as a backbone Many AWS services uses AWS S3 as an integration as well S3 is a key/value store for objects Great for big objects, not so great for small objects Serverless , scales infinitely, max object size is 5TB [[Eventual consistency]] for overwrites and deletes Tiers: S3 Standard Tier (General Purpose) , S3 Standard-Infrequent Access (IA) Tier , S3 One Zone-Infrequent Access (IA) Tier , S3 Glacier for backups Security: IAM , [[bucket policy]], [[ACL]] Encryption: SSE-S3 , SSE-KMS , SSE-C , S3 Client Side Encryption , [[SSL in transit]] Use case: static files, key value store for big files, website hosting.","title":"Amazon S3"},{"location":"AWS/S3/AWS%20S3/#amazon-s3","text":"Amazon S3 is one of the main building blocks of AWS It's advertised as \"infinitely scaling\" storage It's widely popular Many websites use AWS S3 as a backbone Many AWS services uses AWS S3 as an integration as well S3 is a key/value store for objects Great for big objects, not so great for small objects Serverless , scales infinitely, max object size is 5TB [[Eventual consistency]] for overwrites and deletes Tiers: S3 Standard Tier (General Purpose) , S3 Standard-Infrequent Access (IA) Tier , S3 One Zone-Infrequent Access (IA) Tier , S3 Glacier for backups Security: IAM , [[bucket policy]], [[ACL]] Encryption: SSE-S3 , SSE-KMS , SSE-C , S3 Client Side Encryption , [[SSL in transit]] Use case: static files, key value store for big files, website hosting.","title":"Amazon S3"},{"location":"AWS/S3/S3%20Access%20logs/","text":"S3 Access logs \u00b6 For audit purposes, you may want to log all access to AWS S3 Bucket Any request made to S3, from any account, authorised or denied, will be logged into another AWS S3 Bucket That data can be analysed using data analysis tools Or AWS Athena The log format is at https://docs.aws.amazon.com/AmazonS3/latest/dev/LogFormat.html","title":"S3 Access logs"},{"location":"AWS/S3/S3%20Access%20logs/#s3-access-logs","text":"For audit purposes, you may want to log all access to AWS S3 Bucket Any request made to S3, from any account, authorised or denied, will be logged into another AWS S3 Bucket That data can be analysed using data analysis tools Or AWS Athena The log format is at https://docs.aws.amazon.com/AmazonS3/latest/dev/LogFormat.html","title":"S3 Access logs"},{"location":"AWS/S3/S3%20Cross%20region%20replication/","text":"S3 Cross region replication \u00b6 Must enable S3 Versioning (source and destination) Buckets must be in different AWS Region s Can be in different accounts Copying is asynchronous Must give proper [[IAM permission]] to AWS S3 Use cases [[Compliance]] Lower [[latency]] access [[Replication]] across accounts","title":"S3 Cross region replication"},{"location":"AWS/S3/S3%20Cross%20region%20replication/#s3-cross-region-replication","text":"Must enable S3 Versioning (source and destination) Buckets must be in different AWS Region s Can be in different accounts Copying is asynchronous Must give proper [[IAM permission]] to AWS S3 Use cases [[Compliance]] Lower [[latency]] access [[Replication]] across accounts","title":"S3 Cross region replication"},{"location":"AWS/S3/S3%20Default%20encryption/","text":"S3 Default encryption \u00b6 The old way to enable default Encryption was to use a [[bucket policy]] and refuse any [[HTTP]] command without the proper headers. The new way is to use the \" S3 default encryption \" option in AWS S3 Note: [[bucket policy| Bucket Policies]] are evaluated before \" Programming/AWS/S3/S3 Default encryption \"","title":"S3 Default encryption"},{"location":"AWS/S3/S3%20Default%20encryption/#s3-default-encryption","text":"The old way to enable default Encryption was to use a [[bucket policy]] and refuse any [[HTTP]] command without the proper headers. The new way is to use the \" S3 default encryption \" option in AWS S3 Note: [[bucket policy| Bucket Policies]] are evaluated before \" Programming/AWS/S3/S3 Default encryption \"","title":"S3 Default encryption"},{"location":"AWS/S3/S3%20For%20Solutions%20Architect/","text":"S3 for Soltions Architect \u00b6 Operations : no operations needed Security : IAM , [[bucket policy]], [[ACL]], Encryption (Server/Client), [[SSL]] Reliability : 99.999999999% durability/ 99.99% availability, [[Multi AZ]], [[CRR]] Performance : scales to thousands of read / writes per second, transfer acceleration / multi-part for big files Cost : pay per storage usage, network cost, requests number","title":"S3 For Solutions Architect"},{"location":"AWS/S3/S3%20For%20Solutions%20Architect/#s3-for-soltions-architect","text":"Operations : no operations needed Security : IAM , [[bucket policy]], [[ACL]], Encryption (Server/Client), [[SSL]] Reliability : 99.999999999% durability/ 99.99% availability, [[Multi AZ]], [[CRR]] Performance : scales to thousands of read / writes per second, transfer acceleration / multi-part for big files Cost : pay per storage usage, network cost, requests number","title":"S3 for Soltions Architect"},{"location":"AWS/S3/S3%20MFA%20Delete/","text":"S3 MFA Delete \u00b6 [[Multi factor authentication]] forces user to generate a code on a device (usually mobile phone or hardware) before doing important operations on S3 To use MFA-Delete, enable S3 Versioning on the AWS S3 Bucket You will need [[Multi factor authentication]] to Permanently delete an [[S3 Object Version]] Suspend version on the bucket You won't need [[Multi factor authentication]] for Enabling S3 Versioning Listing deleted versions Only the bucket owner ([[AWS Root Account]]) can enable/disable MFA-delete MFA-Delete currently can only be enabled using the CLI aws s3api put-bucket-versioning --bucket mfa-demo --versioning-configuration Status=Enabled,MFADelete=Enabled --mfa \"arn-of-mfa-device mfa-code\" --profile root-datacumulus The code can be gotten from IAM when [[Multi factor authentication]] is set up.","title":"S3 MFA Delete"},{"location":"AWS/S3/S3%20MFA%20Delete/#s3-mfa-delete","text":"[[Multi factor authentication]] forces user to generate a code on a device (usually mobile phone or hardware) before doing important operations on S3 To use MFA-Delete, enable S3 Versioning on the AWS S3 Bucket You will need [[Multi factor authentication]] to Permanently delete an [[S3 Object Version]] Suspend version on the bucket You won't need [[Multi factor authentication]] for Enabling S3 Versioning Listing deleted versions Only the bucket owner ([[AWS Root Account]]) can enable/disable MFA-delete MFA-Delete currently can only be enabled using the CLI aws s3api put-bucket-versioning --bucket mfa-demo --versioning-configuration Status=Enabled,MFADelete=Enabled --mfa \"arn-of-mfa-device mfa-code\" --profile root-datacumulus The code can be gotten from IAM when [[Multi factor authentication]] is set up.","title":"S3 MFA Delete"},{"location":"AWS/S3/S3%20Pre-signed%20URLs/","text":"S3 Pre-signed URLs \u00b6 Can generate pre-signed [[URL]]s using AWS SDK or [[AWS CLI]] For downloads (easy, can use the [[AWS CLI]]) For uploads (harder, must use the AWS SDK ) Valid for a default of 3600 seconds, can change timeout with --expires-in argument Users given a pre-signed URL inherit the permissions of the person who generated the [[URL]] for [[HTTP GET]]/[[HTTP PUT]] Examples Allow only logged-in users to download a premium video on your AWS S3 Bucket Allow an ever changing list of users to download files by generating [[URL]]s dynamically Allow temporarily a user to upload file to a precise location in your bucket aws s3 presign s3://my-sample-bucket-monitored/file.txt --expires-in 300 --region eu-west-1","title":"S3 Pre-signed URLs"},{"location":"AWS/S3/S3%20Pre-signed%20URLs/#s3-pre-signed-urls","text":"Can generate pre-signed [[URL]]s using AWS SDK or [[AWS CLI]] For downloads (easy, can use the [[AWS CLI]]) For uploads (harder, must use the AWS SDK ) Valid for a default of 3600 seconds, can change timeout with --expires-in argument Users given a pre-signed URL inherit the permissions of the person who generated the [[URL]] for [[HTTP GET]]/[[HTTP PUT]] Examples Allow only logged-in users to download a premium video on your AWS S3 Bucket Allow an ever changing list of users to download files by generating [[URL]]s dynamically Allow temporarily a user to upload file to a precise location in your bucket aws s3 presign s3://my-sample-bucket-monitored/file.txt --expires-in 300 --region eu-west-1","title":"S3 Pre-signed URLs"},{"location":"AWS/S3/S3%20Security/","text":"S3 Security \u00b6 User based IAM Policy - which API calls should be allowed for a specific user from IAM console Resource based [[bucket policy]] - bucket wide rules from the s3 console - allows cross account [[Object Access Control List (ACL)]] - finer grain [[Bucket Access Control List (ACL)]] - less common [[JSON]] based policies Resources - buckets and objects Actions - set of API to allow or deny Effect - Allow or deny Principal - the account or user to apply the policy to Use AWS S3 Bucket for policy to: Grant public access to the bucket Force objects to be encrypted at upload Grant access to another account (cross account) Other: [[Networking]] Supports VPC endpoint (For instances in VPC without www internet) [[Logging]] and [[Audit]] S3 access logs can be stored in other S3 buckets [[API]] calls can be logged in AWS CloudTrail [[User Security]] [[Multi factor authentication]] can be required in versioned AWS S3 Bucket to delete AWS S3 Objects S3 Pre-signed URLs : [[URL]]s that are valid only for a limited time (ex premium video service for logged in users) The permissions can be modified in the permissions tab of the AWS S3 Bucket . When using the Bucket Policy tag, we can use https://awspolicygen.s3.amazonaws.com/policygen.html to generate the [[JSON]] code.","title":"S3 Security"},{"location":"AWS/S3/S3%20Security/#s3-security","text":"User based IAM Policy - which API calls should be allowed for a specific user from IAM console Resource based [[bucket policy]] - bucket wide rules from the s3 console - allows cross account [[Object Access Control List (ACL)]] - finer grain [[Bucket Access Control List (ACL)]] - less common [[JSON]] based policies Resources - buckets and objects Actions - set of API to allow or deny Effect - Allow or deny Principal - the account or user to apply the policy to Use AWS S3 Bucket for policy to: Grant public access to the bucket Force objects to be encrypted at upload Grant access to another account (cross account) Other: [[Networking]] Supports VPC endpoint (For instances in VPC without www internet) [[Logging]] and [[Audit]] S3 access logs can be stored in other S3 buckets [[API]] calls can be logged in AWS CloudTrail [[User Security]] [[Multi factor authentication]] can be required in versioned AWS S3 Bucket to delete AWS S3 Objects S3 Pre-signed URLs : [[URL]]s that are valid only for a limited time (ex premium video service for logged in users) The permissions can be modified in the permissions tab of the AWS S3 Bucket . When using the Bucket Policy tag, we can use https://awspolicygen.s3.amazonaws.com/policygen.html to generate the [[JSON]] code.","title":"S3 Security"},{"location":"AWS/S3/S3%20Versioning/","text":"S3 Versioning \u00b6 You can version your files in AWS S3 It is enabled at the AWS S3 Bucket level Same key overwrite will increment it's version It is best practice your AWS S3 Bucket Protect against unintended deletes (ability to restore a version) Easy [[roll back]] to previous version Any file that is not versioned prior to enabling versioning will have version \"null\" If we upload the same file once more: Even if we hide the versions and delete the file, it will still be visible under the versions tab.","title":"S3 Versioning"},{"location":"AWS/S3/S3%20Versioning/#s3-versioning","text":"You can version your files in AWS S3 It is enabled at the AWS S3 Bucket level Same key overwrite will increment it's version It is best practice your AWS S3 Bucket Protect against unintended deletes (ability to restore a version) Easy [[roll back]] to previous version Any file that is not versioned prior to enabling versioning will have version \"null\" If we upload the same file once more: Even if we hide the versions and delete the file, it will still be visible under the versions tab.","title":"S3 Versioning"},{"location":"AWS/S3/S3%20versioning/","text":"S3 Versioning \u00b6 You can version your files in AWS S3 It is enabled at the AWS S3 Bucket level Same key overwrite will increment it's version It is best practice your AWS S3 Bucket Protect against unintended deletes (ability to restore a version) Easy [[roll back]] to previous version Any file that is not versioned prior to enabling versioning will have version \"null\" If we upload the same file once more: Even if we hide the versions and delete the file, it will still be visible under the versions tab.","title":"S3 Versioning"},{"location":"AWS/S3/S3%20versioning/#s3-versioning","text":"You can version your files in AWS S3 It is enabled at the AWS S3 Bucket level Same key overwrite will increment it's version It is best practice your AWS S3 Bucket Protect against unintended deletes (ability to restore a version) Easy [[roll back]] to previous version Any file that is not versioned prior to enabling versioning will have version \"null\" If we upload the same file once more: Even if we hide the versions and delete the file, it will still be visible under the versions tab.","title":"S3 Versioning"},{"location":"AWS/S3/Encryption/Encryption%20in%20transit/","text":"Encryption in transit (SSL) \u00b6 AWS S3 exposes [[HTTP endpoint]]: not encrypted [[HTTPS endpoint]]: encryption in flight You're free to use the [[endpoint]] you want, but [[HTTPS]] is recommended [[HTTPS]] is mandatory for SSE-C Encryption in flight is also called [[SSL - TLS]]","title":"Encryption in transit"},{"location":"AWS/S3/Encryption/Encryption%20in%20transit/#encryption-in-transit-ssl","text":"AWS S3 exposes [[HTTP endpoint]]: not encrypted [[HTTPS endpoint]]: encryption in flight You're free to use the [[endpoint]] you want, but [[HTTPS]] is recommended [[HTTPS]] is mandatory for SSE-C Encryption in flight is also called [[SSL - TLS]]","title":"Encryption in transit (SSL)"},{"location":"AWS/S3/Encryption/S3%20Client%20Side%20Encryption/","text":"Client Side Encryption \u00b6 Client library such as [[Amazon S3 Encryption Client]] Clients must [[encrypt]] data themselves before sending to AWS S3 Clients must [[decrypt]] data themselves when retrieving from AWS S3 Customer fully manages the [[encryption key]]s and [[encryption cycle]]","title":"S3 Client Side Encryption"},{"location":"AWS/S3/Encryption/S3%20Client%20Side%20Encryption/#client-side-encryption","text":"Client library such as [[Amazon S3 Encryption Client]] Clients must [[encrypt]] data themselves before sending to AWS S3 Clients must [[decrypt]] data themselves when retrieving from AWS S3 Customer fully manages the [[encryption key]]s and [[encryption cycle]]","title":"Client Side Encryption"},{"location":"AWS/S3/Encryption/S3%20Encryption%20for%20objects/","text":"S3 Encryption for objects \u00b6 There are 4 methods of encrypting objects in S3","title":"S3 Encryption for objects"},{"location":"AWS/S3/Encryption/S3%20Encryption%20for%20objects/#s3-encryption-for-objects","text":"There are 4 methods of encrypting objects in S3","title":"S3 Encryption for objects"},{"location":"AWS/S3/Encryption/SSE-C/","text":"SSE-C when you want to manage your own [[encryption key]]s AWS S3 does not store the [[encryption key]] you provide [[HTTPS]] must be used [[Encryption key]] must be provided in [[HTTP headers]], for every [[HTTP request]] made","title":"SSE C"},{"location":"AWS/S3/Encryption/SSE-KMS/","text":"SSE-KMS \u00b6 leverage AWS KMS (Key Management Service) to mange [[encryption key]]s KMS Advantages: user control + audit trail Object is encrypted [[server side]] Must set header \"x-amz-server-side-encryption\": \"aws:kms\"","title":"SSE KMS"},{"location":"AWS/S3/Encryption/SSE-KMS/#sse-kms","text":"leverage AWS KMS (Key Management Service) to mange [[encryption key]]s KMS Advantages: user control + audit trail Object is encrypted [[server side]] Must set header \"x-amz-server-side-encryption\": \"aws:kms\"","title":"SSE-KMS"},{"location":"AWS/S3/Encryption/SSE-S3/","text":"SSE-S3 \u00b6 encrypts AWS S3 Objects using keys handled & managed by AWS AWS S3 Objects are encrypted server side [[AES-256]] encryption type Must set header \"x-amz-server-side-encryption\": \"AES256\"","title":"SSE S3"},{"location":"AWS/S3/Encryption/SSE-S3/#sse-s3","text":"encrypts AWS S3 Objects using keys handled & managed by AWS AWS S3 Objects are encrypted server side [[AES-256]] encryption type Must set header \"x-amz-server-side-encryption\": \"AES256\"","title":"SSE-S3"},{"location":"AWS/S3/Snowball/AWS%20Snowball/","text":"Snowball Overview \u00b6 Physical data transport solution that helps moving TBs or PBs of data in or out of AWS Alternative to moving data over the network (and paying network fees) Secure, tamper resistant, uses AWS KMS (Key Management Service) 256 bit encryption Tracking using AWS SNS and text messages. E-ink shipping label. Pay per data transfer job Use cases: large data [[cloud migrations]], DC decommission, Disaster Recovery If it takes more than a week to transfer over the network, use Snowball devices! Snowball Process \u00b6 Request snowball devices from the [[AWS Console]] for delivery Install the snowball client on your [[server]]s Connect the snowball to your servers and copy files using the client Ship back the device when you're done (goes to the right AWS facility) Data will be loaded into an AWS S3 Bucket Snowball is completely wiped Tracking is done using AWS SNS , text messages and the [[AWS Console]] Snowball Edge \u00b6 Snowball Edges add computational capability to the devices 100 TB capacity with either: Storage optimised - 24 vCPU Compute optimised - 52 vCPU & optional GPU Supports a custom EC2 AMIs so you can perform processing on the go Supports custom AWS Lambda functions Very useful to [[pre-process]] the data while moving Use case: [[data migration]], [[image collation]], [[IoT capture]], [[machine learning]] AWS Snowmobile \u00b6 Transfer exabytes of data Each Snowmobile has 100 PB of capacity (use multiple in parallel) Better than Snowball if you transfer more than 10 PB Snowball Hands On","title":"Snowball Overview"},{"location":"AWS/S3/Snowball/AWS%20Snowball/#snowball-overview","text":"Physical data transport solution that helps moving TBs or PBs of data in or out of AWS Alternative to moving data over the network (and paying network fees) Secure, tamper resistant, uses AWS KMS (Key Management Service) 256 bit encryption Tracking using AWS SNS and text messages. E-ink shipping label. Pay per data transfer job Use cases: large data [[cloud migrations]], DC decommission, Disaster Recovery If it takes more than a week to transfer over the network, use Snowball devices!","title":"Snowball Overview"},{"location":"AWS/S3/Snowball/AWS%20Snowball/#snowball-process","text":"Request snowball devices from the [[AWS Console]] for delivery Install the snowball client on your [[server]]s Connect the snowball to your servers and copy files using the client Ship back the device when you're done (goes to the right AWS facility) Data will be loaded into an AWS S3 Bucket Snowball is completely wiped Tracking is done using AWS SNS , text messages and the [[AWS Console]]","title":"Snowball Process"},{"location":"AWS/S3/Snowball/AWS%20Snowball/#snowball-edge","text":"Snowball Edges add computational capability to the devices 100 TB capacity with either: Storage optimised - 24 vCPU Compute optimised - 52 vCPU & optional GPU Supports a custom EC2 AMIs so you can perform processing on the go Supports custom AWS Lambda functions Very useful to [[pre-process]] the data while moving Use case: [[data migration]], [[image collation]], [[IoT capture]], [[machine learning]]","title":"Snowball Edge"},{"location":"AWS/S3/Snowball/AWS%20Snowball/#aws-snowmobile","text":"Transfer exabytes of data Each Snowmobile has 100 PB of capacity (use multiple in parallel) Better than Snowball if you transfer more than 10 PB Snowball Hands On","title":"AWS Snowmobile"},{"location":"AWS/S3/Snowball/Snowball%20Hands%20On/","text":"Snowball Hands On \u00b6 You can visit Snowball page in your AWS Console and create a job. Later you will be asked to insert your shipping details, choose a device to ship as well as to setup AWS S3 Bucket , EC2 AMIs , permissions and notifications.","title":"Snowball Hands On"},{"location":"AWS/S3/Snowball/Snowball%20Hands%20On/#snowball-hands-on","text":"You can visit Snowball page in your AWS Console and create a job. Later you will be asked to insert your shipping details, choose a device to ship as well as to setup AWS S3 Bucket , EC2 AMIs , permissions and notifications.","title":"Snowball Hands On"},{"location":"AWS/S3/Storage%20Gateway/AWS%20Storage%20Gateway/","text":"AWS Storage Gateway \u00b6 Bridge between on-premise data and cloud data in AWS S3 Use cases: Disaster Recovery , backup & restore, [[Tiered storage]] 3 types of Storage Gateway File access / [[NFS]] => File Gateway (backed by AWS S3 ) Volumes / [[Block Storage]] / [[iSCSI]] => Volume gateway (backed by AWS S3 with EBS Snapshot s) [[VTL Tape]] solution / Backup with [[iSCSI]] => Tape Gateway (backed by AWS S3 and S3 Glacier )","title":"AWS Storage Gateway"},{"location":"AWS/S3/Storage%20Gateway/AWS%20Storage%20Gateway/#aws-storage-gateway","text":"Bridge between on-premise data and cloud data in AWS S3 Use cases: Disaster Recovery , backup & restore, [[Tiered storage]] 3 types of Storage Gateway File access / [[NFS]] => File Gateway (backed by AWS S3 ) Volumes / [[Block Storage]] / [[iSCSI]] => Volume gateway (backed by AWS S3 with EBS Snapshot s) [[VTL Tape]] solution / Backup with [[iSCSI]] => Tape Gateway (backed by AWS S3 and S3 Glacier )","title":"AWS Storage Gateway"},{"location":"AWS/S3/Storage%20Gateway/File%20Gateway/","text":"File Gateway \u00b6 Configured AWS S3 Bucket are accessible using the [[NFS]] and [[SMB]] protocol Supports S3 Standard Tier (General Purpose) , S3 Standard-Infrequent Access (IA) Tier , S3 One Zone-Infrequent Access (IA) Tier AWS S3 Bucket access using [[IAM Role]]s for each File Gateway Most recently used data is cached in the File Gateway Can be mounted on many servers","title":"File Gateway"},{"location":"AWS/S3/Storage%20Gateway/File%20Gateway/#file-gateway","text":"Configured AWS S3 Bucket are accessible using the [[NFS]] and [[SMB]] protocol Supports S3 Standard Tier (General Purpose) , S3 Standard-Infrequent Access (IA) Tier , S3 One Zone-Infrequent Access (IA) Tier AWS S3 Bucket access using [[IAM Role]]s for each File Gateway Most recently used data is cached in the File Gateway Can be mounted on many servers","title":"File Gateway"},{"location":"AWS/S3/Storage%20Gateway/Storage%20Gateway%20Hands%20On/","text":"Storage Gateway Hands On \u00b6 You can visit AWS Storage Gateway in AWS Console.","title":"Storage Gateway Hands On"},{"location":"AWS/S3/Storage%20Gateway/Storage%20Gateway%20Hands%20On/#storage-gateway-hands-on","text":"You can visit AWS Storage Gateway in AWS Console.","title":"Storage Gateway Hands On"},{"location":"AWS/S3/Storage%20Gateway/Tape%20Gateway/","text":"Tape Gateway \u00b6 Some companies have backup processes using physical tapes With Tape Gateway, companies use the same process but in the cloud [[Virtual Tape Library]] (VTL) backed by AWS S3 and Glacier Back up data using existing tape-based processes (and [[iSCSI]] interface) Works with leading backup software vendors","title":"Tape Gateway"},{"location":"AWS/S3/Storage%20Gateway/Tape%20Gateway/#tape-gateway","text":"Some companies have backup processes using physical tapes With Tape Gateway, companies use the same process but in the cloud [[Virtual Tape Library]] (VTL) backed by AWS S3 and Glacier Back up data using existing tape-based processes (and [[iSCSI]] interface) Works with leading backup software vendors","title":"Tape Gateway"},{"location":"AWS/S3/Storage%20Gateway/Volume%20Gateway/","text":"Volume Gateway \u00b6 Block storage using [[iSCSI]] protocol backed by AWS S3 Backed by EBS Snapshot s which can help restore [[on-premise volume]]s [[Cached volume]]s: low latency access to most recent data [[Stored volume]]s: entire dataset is on premise, scheduled backups to AWS S3","title":"Volume Gateway"},{"location":"AWS/S3/Storage%20Gateway/Volume%20Gateway/#volume-gateway","text":"Block storage using [[iSCSI]] protocol backed by AWS S3 Backed by EBS Snapshot s which can help restore [[on-premise volume]]s [[Cached volume]]s: low latency access to most recent data [[Stored volume]]s: entire dataset is on premise, scheduled backups to AWS S3","title":"Volume Gateway"},{"location":"AWS/S3/Tiers/Amazon%20S3%20Reduced%20Redundancy%20Storage%20%28deprecated%29/","text":"Amazon S3 Reduced Redundancy Storage (deprecated) \u00b6 Designed to provide 99.99% [[durability]] 99.99% [[availability]] of objects over a given year Designed to sustain the loss of data in a single facility Use cases: noncritical, [[reproducible data]] at lower levels of [[redundancy]] than S3 Standard Tier (General Purpose) (thumbnails, transcoded media, processed data that can be reproduced)","title":"Amazon S3 Reduced Redundancy Storage (deprecated)"},{"location":"AWS/S3/Tiers/Amazon%20S3%20Reduced%20Redundancy%20Storage%20%28deprecated%29/#amazon-s3-reduced-redundancy-storage-deprecated","text":"Designed to provide 99.99% [[durability]] 99.99% [[availability]] of objects over a given year Designed to sustain the loss of data in a single facility Use cases: noncritical, [[reproducible data]] at lower levels of [[redundancy]] than S3 Standard Tier (General Purpose) (thumbnails, transcoded media, processed data that can be reproduced)","title":"Amazon S3 Reduced Redundancy Storage (deprecated)"},{"location":"AWS/S3/Tiers/S3%20Glacier/","text":"Amazon Glacier \u00b6 Low cost object storage meant for [[archiving]] / [[backup]]s Data is retained for the longer term (10s of years) Alternative to on-premise [[magnetic tape storage]] Average to annual durability is 99.999999999% Cost per storage per month ($0.004 / GB) + retrieval cost Each item in Glacier is called \"Archive\" (up to 40TB) Archives are stored in \"Vaults\" 3 retrieval options: Expedited (1 to 5 minutes retrieval) - $0.04 per GB and $0.01 per request Standard (3 to 5 hours) - $0.01 per GB and 0.05 per 1000 requests Bulk (5 to 12 hours) - $0.0025 per GB and $0.025 per 1000 requests","title":"Amazon Glacier"},{"location":"AWS/S3/Tiers/S3%20Glacier/#amazon-glacier","text":"Low cost object storage meant for [[archiving]] / [[backup]]s Data is retained for the longer term (10s of years) Alternative to on-premise [[magnetic tape storage]] Average to annual durability is 99.999999999% Cost per storage per month ($0.004 / GB) + retrieval cost Each item in Glacier is called \"Archive\" (up to 40TB) Archives are stored in \"Vaults\" 3 retrieval options: Expedited (1 to 5 minutes retrieval) - $0.04 per GB and $0.01 per request Standard (3 to 5 hours) - $0.01 per GB and 0.05 per 1000 requests Bulk (5 to 12 hours) - $0.0025 per GB and $0.025 per 1000 requests","title":"Amazon Glacier"},{"location":"AWS/S3/Tiers/S3%20Intelligent%20Tiering%20%28new%29/","text":"Amazon S3 Intelligent Tiering (new) Same low [[latency]] and high [[throughput performance]] of S3 Standard Tier (General Purpose) Small monthly [[monitoring]] and [[auto-tiering]] fee Automatically moves objects between two access tiers based on changing access patterns Designed for durability across multiple Availability Zone s Resilient against events that impact an entire Availability Zone Designed for 99.99% [[availability]] over a given year","title":"S3 Intelligent Tiering (new)"},{"location":"AWS/S3/Tiers/S3%20Lifecycle%20rules/","text":"S3 Lifecycle Rules \u00b6 Set rules to move data between different tiers to save storage cost Example: S3 Standard Tier (General Purpose) => S3 Standard-Infrequent Access (IA) Tier => S3 Glacier Transition actions: It defines when objects are transitioned to another [[storage class]]. Eg: We can choose to move AWS S3 Objects to S3 Standard-Infrequent Access (IA) Tier class 60 days after you created them or can move to S3 Glacier for archiving after 6 months Expiration actions: Helps to configure objects to expire after a certain time period. S3 deletes expired AWS S3 Objects on our behalf Eg: S3 Access logs files can be set to delete after a specified period of time Can be used to delete incomplete [[multi-part uploads]]","title":"S3 Lifecycle rules"},{"location":"AWS/S3/Tiers/S3%20Lifecycle%20rules/#s3-lifecycle-rules","text":"Set rules to move data between different tiers to save storage cost Example: S3 Standard Tier (General Purpose) => S3 Standard-Infrequent Access (IA) Tier => S3 Glacier Transition actions: It defines when objects are transitioned to another [[storage class]]. Eg: We can choose to move AWS S3 Objects to S3 Standard-Infrequent Access (IA) Tier class 60 days after you created them or can move to S3 Glacier for archiving after 6 months Expiration actions: Helps to configure objects to expire after a certain time period. S3 deletes expired AWS S3 Objects on our behalf Eg: S3 Access logs files can be set to delete after a specified period of time Can be used to delete incomplete [[multi-part uploads]]","title":"S3 Lifecycle Rules"},{"location":"AWS/S3/Tiers/S3%20One%20Zone-Infrequent%20Access%20%28IA%29%20Tier/","text":"Amazon S3 One Zone-Infrequent Access \u00b6 Same as S3 Standard-Infrequent Access (IA) Tier but is stored in a single Availability Zone High [[durability]] (99.999999999%) of objects in a single Availability Zone ; data lose when Availability Zone is destroyed 99.95 Availability Low latency and high throughput performance Supports [[SSL]] for data at transit and encryption at rest Low cost compared to S3 Standard-Infrequent Access (IA) Tier (by 20%) Use cases: storing secondary backup copies of on-premise data, or storing data you can recreate","title":"S3 One Zone Infrequent Access (IA) Tier"},{"location":"AWS/S3/Tiers/S3%20One%20Zone-Infrequent%20Access%20%28IA%29%20Tier/#amazon-s3-one-zone-infrequent-access","text":"Same as S3 Standard-Infrequent Access (IA) Tier but is stored in a single Availability Zone High [[durability]] (99.999999999%) of objects in a single Availability Zone ; data lose when Availability Zone is destroyed 99.95 Availability Low latency and high throughput performance Supports [[SSL]] for data at transit and encryption at rest Low cost compared to S3 Standard-Infrequent Access (IA) Tier (by 20%) Use cases: storing secondary backup copies of on-premise data, or storing data you can recreate","title":"Amazon S3 One Zone-Infrequent Access"},{"location":"AWS/S3/Tiers/S3%20Standard%20Tier%20%28General%20Purpose%29/","text":"Amazon S3 Standard - General Purpose High [[durability]] (99.999999999%) of AWS S3 Objects across multiple Availability Zone s If you store 10,000,000 objects with AWS S3 , you can on average expect to incur a loss of a single object once every 10,000 years. 99.99% availability over a given year Sustain 2 concurrent facility failures Use Cases: Big Data analytics, mobile & gaming applications, content distribution.","title":"S3 Standard Tier (General Purpose)"},{"location":"AWS/S3/Tiers/S3%20Standard-Infrequent%20Access%20%28IA%29%20Tier/","text":"Amazon S3 Standard-Infrequent Access (IA) - Suitable for data that is less frequently accessed, but requires rapid access when needed - High durability (99.999999999%) of objects accross multiple Availability Zone s - 99.99% [[availability]] - Low cost compared to S3 Standard Tier (General Purpose) - Sustain 2 concurrent facility failures - Use cases: AS a data store for Disaster Recovery , backups","title":"S3 Standard Infrequent Access (IA) Tier"},{"location":"AWS/S3/Tiers/S3%20Storage%20Tiers/","text":"S3 Storage Tiers \u00b6","title":"S3 Storage Tiers"},{"location":"AWS/S3/Tiers/S3%20Storage%20Tiers/#s3-storage-tiers","text":"","title":"S3 Storage Tiers"},{"location":"AWS/SDK/AWS%20SDK/","text":"AWS SDK Overview \u00b6 What if you want to perform actions on AWS directly from your applications code? (Without using the [[AWS CLI]]). You can use [[SDK]] (Software development kit) Official SDKs are: [[Java]] [[.NET]] [[Node.js]] [[PHP]] [[Python]] (named boto3 / botocore) [[Go]] [[Ruby]] [[C++]] We have to use the AWS SDK when coding against AWS Services such as DynamoDB The [[AWS CLI]]I uses the Python SDK (boto3) Good to know: if you don't specify or configure a default AWS Region , then us-east-1 will be chosen by default. It's recommended to use the default credential provider chain The default credential provider chain works seamlessly with: AWS credentials at ~/.aws/credentials/ (only on our computers or on premise) Instance Profile Credentials using [[IAM Role]]s (fro AWS EC2 machines, etc) [[Environment variables]] (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY) Overall, never store aws credentials in your code. Best practice is for credetnials to be inherited from mechanisms above and 100% [[IAM Role]]s if working from within AWS services.","title":"AWS SDK Overview"},{"location":"AWS/SDK/AWS%20SDK/#aws-sdk-overview","text":"What if you want to perform actions on AWS directly from your applications code? (Without using the [[AWS CLI]]). You can use [[SDK]] (Software development kit) Official SDKs are: [[Java]] [[.NET]] [[Node.js]] [[PHP]] [[Python]] (named boto3 / botocore) [[Go]] [[Ruby]] [[C++]] We have to use the AWS SDK when coding against AWS Services such as DynamoDB The [[AWS CLI]]I uses the Python SDK (boto3) Good to know: if you don't specify or configure a default AWS Region , then us-east-1 will be chosen by default. It's recommended to use the default credential provider chain The default credential provider chain works seamlessly with: AWS credentials at ~/.aws/credentials/ (only on our computers or on premise) Instance Profile Credentials using [[IAM Role]]s (fro AWS EC2 machines, etc) [[Environment variables]] (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY) Overall, never store aws credentials in your code. Best practice is for credetnials to be inherited from mechanisms above and 100% [[IAM Role]]s if working from within AWS services.","title":"AWS SDK Overview"},{"location":"AWS/SNS/AWS%20SNS/","text":"AWS SNS \u00b6 The \"event producer\" only sends message to one [[SNS topic]] As many \"event receivers\" (subscriptions) as we want to listen to the SNS topic notifications Each subscriber to the topic will get all the messages (note: new feature to filter messages) Up to 10,000,000 subscriptions per topic 100,000 topics limit Subscribers can be: AWS SQS [[HTTP]] / [[HTTPS]] (with delivery retries - how many times) Lambda [[Email]]s [[SMS]] messages [[Mobile notification]]s SNS integrates with a lot of Amazon Products \u00b6 Some services can send data directly to SNS for notifications CloudWatch (for alarms) Auto Scaling Group (ASG) notifications AWS S3 (on bucket events) CloudFormation (upon state changes => failed to build etc) Etc SNS + AWS SQS : Fan Out \u00b6 Publish once in SNS, receive in many SQS Fully decoupled No data loss Ability to add receivers of data later SQS allows for delayed processing SQS allows for retries of work May have many workers on one queue and one worker on the othere queue Hands on \u00b6 You can type SNS or Simple Notification Service in AWS Console. Then create MyFirstTopic . Now create a subscription. Confirm subscription Now publish a message.","title":"AWS SNS"},{"location":"AWS/SNS/AWS%20SNS/#aws-sns","text":"The \"event producer\" only sends message to one [[SNS topic]] As many \"event receivers\" (subscriptions) as we want to listen to the SNS topic notifications Each subscriber to the topic will get all the messages (note: new feature to filter messages) Up to 10,000,000 subscriptions per topic 100,000 topics limit Subscribers can be: AWS SQS [[HTTP]] / [[HTTPS]] (with delivery retries - how many times) Lambda [[Email]]s [[SMS]] messages [[Mobile notification]]s","title":"AWS SNS"},{"location":"AWS/SNS/AWS%20SNS/#sns-integrates-with-a-lot-of-amazon-products","text":"Some services can send data directly to SNS for notifications CloudWatch (for alarms) Auto Scaling Group (ASG) notifications AWS S3 (on bucket events) CloudFormation (upon state changes => failed to build etc) Etc","title":"SNS integrates with a lot of Amazon Products"},{"location":"AWS/SNS/AWS%20SNS/#sns-aws-sqs-fan-out","text":"Publish once in SNS, receive in many SQS Fully decoupled No data loss Ability to add receivers of data later SQS allows for delayed processing SQS allows for retries of work May have many workers on one queue and one worker on the othere queue","title":"SNS + AWS SQS: Fan Out"},{"location":"AWS/SNS/AWS%20SNS/#hands-on","text":"You can type SNS or Simple Notification Service in AWS Console. Then create MyFirstTopic . Now create a subscription. Confirm subscription Now publish a message.","title":"Hands on"},{"location":"AWS/SNS/SNS%20-%20How%20To%20Publish/","text":"SNS - How to publish \u00b6","title":"SNS   How To Publish"},{"location":"AWS/SNS/SNS%20-%20How%20To%20Publish/#sns-how-to-publish","text":"","title":"SNS - How to publish"},{"location":"AWS/SNS/SNS%20Direct%20Publish/","text":"[[Direct Publish]] (for mobile apps [[SDK]]) Create a platform application Create a platform endpoint Publish to the platform endpoint Works with [[Google GCM]], [[Apple APNS]], [[Amazon ADM]]...","title":"SNS Direct Publish"},{"location":"AWS/SNS/SNS%20Topic%20Publish/","text":"[[Topic Publish]] (within you AWS server - using the AWS SDK ) Create a [[SNS topic]] Create a [[SNS Subscription]] (or many) Publish to the topic","title":"SNS Topic Publish"},{"location":"AWS/SQS/AWS%20SQS/","text":"AWS SQS \u00b6 SQS Hands On","title":"AWS SQS"},{"location":"AWS/SQS/AWS%20SQS/#aws-sqs","text":"SQS Hands On","title":"AWS SQS"},{"location":"AWS/SQS/SQS%20-%20FIFO%20Queue/","text":"AWS SQS - FIFO Queue \u00b6 Newer offering (First In - First Out - [[FIFO]]) - not available in all regions Name of the queue must end in .fifo Lower [[throughput]] (up to 3,000 per second with batching, 300/s without) Messages are processed in order by the consumer Messages are sent exactly once No per message delay (only per queue delay) Ability to do content based [[de-duplication]] 5 minute interval de-duplication using \"Duplication ID\" Message groups Possibility to group messages for [[FIFO]] ordering using \"Message GroupID\" Only one worker can be assigned per message group so that messages are processed in order Message group is just an extra tag on the message","title":"SQS   FIFO Queue"},{"location":"AWS/SQS/SQS%20-%20FIFO%20Queue/#aws-sqs-fifo-queue","text":"Newer offering (First In - First Out - [[FIFO]]) - not available in all regions Name of the queue must end in .fifo Lower [[throughput]] (up to 3,000 per second with batching, 300/s without) Messages are processed in order by the consumer Messages are sent exactly once No per message delay (only per queue delay) Ability to do content based [[de-duplication]] 5 minute interval de-duplication using \"Duplication ID\" Message groups Possibility to group messages for [[FIFO]] ordering using \"Message GroupID\" Only one worker can be assigned per message group so that messages are processed in order Message group is just an extra tag on the message","title":"AWS SQS - FIFO Queue"},{"location":"AWS/SQS/SQS%20Consuming%20Messages/","text":"SQS - Consuming Messages \u00b6 Poll AWS SQS for messages (receive up to 10 messages at a time) Process the message within the visibility timeout Delete the message using the message ID & receipt handle","title":"SQS Consuming Messages"},{"location":"AWS/SQS/SQS%20Consuming%20Messages/#sqs-consuming-messages","text":"Poll AWS SQS for messages (receive up to 10 messages at a time) Process the message within the visibility timeout Delete the message using the message ID & receipt handle","title":"SQS - Consuming Messages"},{"location":"AWS/SQS/SQS%20Dead%20Letter%20Queue/","text":"AWS SQS - Dead Letter Queue \u00b6 If a consumer fails to process a message within the visibility timeout the message goes back to the queue. We can set a threshold of how many times a message can go back to the queue - it's called a \"[[redrive policy]]\" After the threshold is exceeded, the message goes into a [[dead letter queue]] (DLQ) We have to create a DLQ first and then designate it [[dead letter queue]] Make sure to process the messages in the DLQ before they expire.","title":"SQS Dead Letter Queue"},{"location":"AWS/SQS/SQS%20Dead%20Letter%20Queue/#aws-sqs-dead-letter-queue","text":"If a consumer fails to process a message within the visibility timeout the message goes back to the queue. We can set a threshold of how many times a message can go back to the queue - it's called a \"[[redrive policy]]\" After the threshold is exceeded, the message goes into a [[dead letter queue]] (DLQ) We have to create a DLQ first and then designate it [[dead letter queue]] Make sure to process the messages in the DLQ before they expire.","title":"AWS SQS - Dead Letter Queue"},{"location":"AWS/SQS/SQS%20Delay%20Queue/","text":"AWS SQS - Delay Queue \u00b6 Delay a message (consumers don't see it immediately) up to 15 minutes Default is 0 seconds (message is available right away) Can set a default at queue level Can override the default using the DelaySeconds parameter","title":"SQS Delay Queue"},{"location":"AWS/SQS/SQS%20Delay%20Queue/#aws-sqs-delay-queue","text":"Delay a message (consumers don't see it immediately) up to 15 minutes Default is 0 seconds (message is available right away) Can set a default at queue level Can override the default using the DelaySeconds parameter","title":"AWS SQS - Delay Queue"},{"location":"AWS/SQS/SQS%20Hands%20On/","text":"SQS Hands on \u00b6 We can access SQS in [[AWS Console]]. Click on Configure Queue . Now the queue should be visible and there will be multiple options for it: We are going to send a message.","title":"SQS Hands On"},{"location":"AWS/SQS/SQS%20Hands%20On/#sqs-hands-on","text":"We can access SQS in [[AWS Console]]. Click on Configure Queue . Now the queue should be visible and there will be multiple options for it: We are going to send a message.","title":"SQS Hands on"},{"location":"AWS/SQS/SQS%20Long%20Polling/","text":"AWS SQS - Long Polling \u00b6 When a consumer requests message from the queue, it can optionally wait for messages to arrive if there are none in the queue. This is called [[long polling]] LongPolling decreases the number of [[API]] calls made to AWS SQS while increasing the efficiency and latency of your application. The wait time can be between 1 to 20 sec (20 sec preferable) Long Polling is preferable to short polling Long polling can be enabled at the queue level or at the API level using WaitTimeSeconds","title":"SQS Long Polling"},{"location":"AWS/SQS/SQS%20Long%20Polling/#aws-sqs-long-polling","text":"When a consumer requests message from the queue, it can optionally wait for messages to arrive if there are none in the queue. This is called [[long polling]] LongPolling decreases the number of [[API]] calls made to AWS SQS while increasing the efficiency and latency of your application. The wait time can be between 1 to 20 sec (20 sec preferable) Long Polling is preferable to short polling Long polling can be enabled at the queue level or at the API level using WaitTimeSeconds","title":"AWS SQS - Long Polling"},{"location":"AWS/SQS/SQS%20Message%20Consumption%20flow%20diagram/","text":"SQS Message Consumption flow diagram \u00b6","title":"SQS Message Consumption flow diagram"},{"location":"AWS/SQS/SQS%20Message%20Consumption%20flow%20diagram/#sqs-message-consumption-flow-diagram","text":"","title":"SQS Message Consumption flow diagram"},{"location":"AWS/SQS/SQS%20Producting%20messages/","text":"SQS - Producing messages \u00b6 Define Body Add message attributes (metadata, optional) Provide [[Delay Delivery]] (optional) Send to AWS SQS Get back [[Message identifier]] [[MD5]] has of the body","title":"SQS Producting messages"},{"location":"AWS/SQS/SQS%20Producting%20messages/#sqs-producing-messages","text":"Define Body Add message attributes (metadata, optional) Provide [[Delay Delivery]] (optional) Send to AWS SQS Get back [[Message identifier]] [[MD5]] has of the body","title":"SQS - Producing messages"},{"location":"AWS/SQS/SQS%20Standard%20Queue/","text":"AWS SQS - Standard Queue \u00b6 Oldest offering (over 10 years old) [[Fully managed]] Scales from 1 message per second to 10,000s per second Default retention of messages: 4 days, maximum of 14 days No limit to how many messages can be in the queue Low latency (< 10 ms on publish and receive) Horizontal scalability in terms of number of consumers Can have duplicate messages (at least once delivery, occasionally) Can have out of order messages (best effort ordering) Limitation of 256KB per message sent","title":"SQS Standard Queue"},{"location":"AWS/SQS/SQS%20Standard%20Queue/#aws-sqs-standard-queue","text":"Oldest offering (over 10 years old) [[Fully managed]] Scales from 1 message per second to 10,000s per second Default retention of messages: 4 days, maximum of 14 days No limit to how many messages can be in the queue Low latency (< 10 ms on publish and receive) Horizontal scalability in terms of number of consumers Can have duplicate messages (at least once delivery, occasionally) Can have out of order messages (best effort ordering) Limitation of 256KB per message sent","title":"AWS SQS - Standard Queue"},{"location":"AWS/SQS/SQS%20Visibility%20Timeout/","text":"SQS - Visibility Timeout \u00b6 When a consumer polls a message from a queue, the message is \"invisible\"'to other consumers for a defined period. The Visibility Timeout. You can set between 0 seconds and 12 hours (default to 30 seconds). If you set too high (15 minutes) and consumer fails to process the message, you must wait a long time before processing the message again. If you set too low (30 seconds) and consumer needs time to process the message (2 minutes), another consumer will receive the message will be processed more than once. ChangeMessageVisibility API to change the visibility while processing a message. DeleteMessage API to tell SQS the message was successfully processed.","title":"SQS Visibility Timeout"},{"location":"AWS/SQS/SQS%20Visibility%20Timeout/#sqs-visibility-timeout","text":"When a consumer polls a message from a queue, the message is \"invisible\"'to other consumers for a defined period. The Visibility Timeout. You can set between 0 seconds and 12 hours (default to 30 seconds). If you set too high (15 minutes) and consumer fails to process the message, you must wait a long time before processing the message again. If you set too low (30 seconds) and consumer needs time to process the message (2 minutes), another consumer will receive the message will be processed more than once. ChangeMessageVisibility API to change the visibility while processing a message. DeleteMessage API to tell SQS the message was successfully processed.","title":"SQS - Visibility Timeout"},{"location":"AWS/SSM/Parameter%20Store/AWS%20Parameter%20Store%20Hierarchy/","text":"AWS Parameter Store Hierarchy \u00b6 /my-departments my-app/ dev/ db-url db-password prod/ db-url db-password other-app/ other-department use with GetParameters or GetParametersByPath API in AWS Lambda functions.","title":"AWS Parameter Store Hierarchy"},{"location":"AWS/SSM/Parameter%20Store/AWS%20Parameter%20Store%20Hierarchy/#aws-parameter-store-hierarchy","text":"/my-departments my-app/ dev/ db-url db-password prod/ db-url db-password other-app/ other-department use with GetParameters or GetParametersByPath API in AWS Lambda functions.","title":"AWS Parameter Store Hierarchy"},{"location":"AWS/SSM/Parameter%20Store/SSM%20Parameter%20Store%20Hands%20On%20Lambda/","text":"SSM Parameter Store Hands On Lambda \u00b6 Create a new AWS Lambda function Insert code: import json import boto3 ssm = boto3.client('ssm', region_name=\"eu-west-1\") def lambda_handler(event, context): db_url = ssm.get_parameters(Names=['/my-app/dev/db-url']) print(db_url) db_password = ssm.get_parameters(Names=['/my-app/dev/db-password'], WithDecryption=True) print(db_password) When testing, we are going to get an error: An error occurred (AccessDeniedException) when calling the GetParameters operation: User: arn:aws:sts::539690530154:assumed-role/hello-world-ssm-role-ta2hmdtc/hello-world-ssm is not authorized to perform: ssm:GetParameters on resource: arn:aws:ssm:eu-west-1:539690530154:parameter/my-app/dev/db-url We are going to open up the AWS Lambda role and Add Inline Policy : Now if we test, we get: An error occurred (AccessDeniedException) when calling the GetParameters operation: The ciphertext refers to a customer master key that does not exist, does not exist in this region, or you are not allowed to access. (Service: AWSKMS; Status Code: 400; Error Code: AccessDeniedException; Request ID: 59647aab-d88f-4bf5-8ac7-b358b03f4722) We are going to add one more Inline policy : Now, when testing, we get START RequestId: 80755a69-a761-4fe2-832b-3148a123e8be Version: $LATEST {'Parameters': [{'Name': '/my-app/dev/db-url', 'Type': 'String', 'Value': 'dev.db.domain.com', 'Version': 1, 'LastModifiedDate': datetime.datetime(2020, 1, 1, 12, 45, 35, 442000, tzinfo=tzlocal()), 'ARN': 'arn:aws:ssm:eu-west-1:539690530154:parameter/my-app/dev/db-url'}], 'InvalidParameters': [], 'ResponseMetadata': {'RequestId': 'fcf16f9e-fce5-4e4c-a98d-80bf5ce6299a', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'fcf16f9e-fce5-4e4c-a98d-80bf5ce6299a', 'content-type': 'application/x-amz-json-1.1', 'content-length': '250', 'date': 'Wed, 01 Jan 2020 13:07:22 GMT'}, 'RetryAttempts': 0}} {'Parameters': [{'Name': '/my-app/dev/db-password', 'Type': 'SecureString', 'Value': 'mysupersecretdevpassword', 'Version': 1, 'LastModifiedDate': datetime.datetime(2020, 1, 1, 12, 46, 24, 389000, tzinfo=tzlocal()), 'ARN': 'arn:aws:ssm:eu-west-1:539690530154:parameter/my-app/dev/db-password'}], 'InvalidParameters': [], 'ResponseMetadata': {'RequestId': '95d2f060-3fe5-413e-94eb-5de5f16542b7', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '95d2f060-3fe5-413e-94eb-5de5f16542b7', 'content-type': 'application/x-amz-json-1.1', 'content-length': '273', 'date': 'Wed, 01 Jan 2020 13:07:22 GMT'}, 'RetryAttempts': 0}} it works! END RequestId: 80755a69-a761-4fe2-832b-3148a123e8be REPORT RequestId: 80755a69-a761-4fe2-832b-3148a123e8be Duration: 243.28 ms Billed Duration: 300 ms Memory Size: 128 MB Max Memory Used: 79 MB","title":"SSM Parameter Store Hands On Lambda"},{"location":"AWS/SSM/Parameter%20Store/SSM%20Parameter%20Store%20Hands%20On%20Lambda/#ssm-parameter-store-hands-on-lambda","text":"Create a new AWS Lambda function Insert code: import json import boto3 ssm = boto3.client('ssm', region_name=\"eu-west-1\") def lambda_handler(event, context): db_url = ssm.get_parameters(Names=['/my-app/dev/db-url']) print(db_url) db_password = ssm.get_parameters(Names=['/my-app/dev/db-password'], WithDecryption=True) print(db_password) When testing, we are going to get an error: An error occurred (AccessDeniedException) when calling the GetParameters operation: User: arn:aws:sts::539690530154:assumed-role/hello-world-ssm-role-ta2hmdtc/hello-world-ssm is not authorized to perform: ssm:GetParameters on resource: arn:aws:ssm:eu-west-1:539690530154:parameter/my-app/dev/db-url We are going to open up the AWS Lambda role and Add Inline Policy : Now if we test, we get: An error occurred (AccessDeniedException) when calling the GetParameters operation: The ciphertext refers to a customer master key that does not exist, does not exist in this region, or you are not allowed to access. (Service: AWSKMS; Status Code: 400; Error Code: AccessDeniedException; Request ID: 59647aab-d88f-4bf5-8ac7-b358b03f4722) We are going to add one more Inline policy : Now, when testing, we get START RequestId: 80755a69-a761-4fe2-832b-3148a123e8be Version: $LATEST {'Parameters': [{'Name': '/my-app/dev/db-url', 'Type': 'String', 'Value': 'dev.db.domain.com', 'Version': 1, 'LastModifiedDate': datetime.datetime(2020, 1, 1, 12, 45, 35, 442000, tzinfo=tzlocal()), 'ARN': 'arn:aws:ssm:eu-west-1:539690530154:parameter/my-app/dev/db-url'}], 'InvalidParameters': [], 'ResponseMetadata': {'RequestId': 'fcf16f9e-fce5-4e4c-a98d-80bf5ce6299a', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'fcf16f9e-fce5-4e4c-a98d-80bf5ce6299a', 'content-type': 'application/x-amz-json-1.1', 'content-length': '250', 'date': 'Wed, 01 Jan 2020 13:07:22 GMT'}, 'RetryAttempts': 0}} {'Parameters': [{'Name': '/my-app/dev/db-password', 'Type': 'SecureString', 'Value': 'mysupersecretdevpassword', 'Version': 1, 'LastModifiedDate': datetime.datetime(2020, 1, 1, 12, 46, 24, 389000, tzinfo=tzlocal()), 'ARN': 'arn:aws:ssm:eu-west-1:539690530154:parameter/my-app/dev/db-password'}], 'InvalidParameters': [], 'ResponseMetadata': {'RequestId': '95d2f060-3fe5-413e-94eb-5de5f16542b7', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '95d2f060-3fe5-413e-94eb-5de5f16542b7', 'content-type': 'application/x-amz-json-1.1', 'content-length': '273', 'date': 'Wed, 01 Jan 2020 13:07:22 GMT'}, 'RetryAttempts': 0}} it works! END RequestId: 80755a69-a761-4fe2-832b-3148a123e8be REPORT RequestId: 80755a69-a761-4fe2-832b-3148a123e8be Duration: 243.28 ms Billed Duration: 300 ms Memory Size: 128 MB Max Memory Used: 79 MB","title":"SSM Parameter Store Hands On Lambda"},{"location":"AWS/SSM/Parameter%20Store/SSM%20Parameter%20Store%20hands%20on%20CLI/","text":"SSM Parameter Store hands on CLI \u00b6 We can search for [[Systems Manager]] in [[AWS Console]] and then navigate to SSM Parameter Store section. And add params: Then query via [[AWS CLI]] \u279c ~ aws ssm get-parameters --names /my-app/dev/db-url /my-app/dev/db-password { \"Parameters\": [ { \"Name\": \"/my-app/dev/db-password\", \"Type\": \"SecureString\", \"Value\": \"AQICAHhSgKv4yUSMN7xTX6TV5+owFLPqV7u3thrIPV83oQBh6AGQBrWbtKy9FhfLFZmp1aBlAAAAdjB0BgkqhkiG9w0BBwagZzBlAgEAMGAGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMRUzwDLakNxxfAoZLAgEQgDMt5tCgJphvuAtDng+Lhuas/aaR0ecOpy42PGq7uK5Fxik8routwmNgIT/2H0ubJiAkhuY=\", \"Version\": 1, \"LastModifiedDate\": 1577882784.389, \"ARN\": \"arn:aws:ssm:eu-west-1:539690530154:parameter/my-app/dev/db-password\" }, { \"Name\": \"/my-app/dev/db-url\", \"Type\": \"String\", \"Value\": \"dev.db.domain.com\", \"Version\": 1, \"LastModifiedDate\": 1577882735.442, \"ARN\": \"arn:aws:ssm:eu-west-1:539690530154:parameter/my-app/dev/db-url\" } ], \"InvalidParameters\": [] } Decrypt password: \u279c ~ aws ssm get-parameters --names /my-app/dev/db-url /my-app/dev/db-password --with-decryption { \"Parameters\": [ { \"Name\": \"/my-app/dev/db-password\", \"Type\": \"SecureString\", \"Value\": \"mysupersecretdevpassword\", \"Version\": 1, \"LastModifiedDate\": 1577882784.389, \"ARN\": \"arn:aws:ssm:eu-west-1:539690530154:parameter/my-app/dev/db-password\" }, { \"Name\": \"/my-app/dev/db-url\", \"Type\": \"String\", \"Value\": \"dev.db.domain.com\", \"Version\": 1, \"LastModifiedDate\": 1577882735.442, \"ARN\": \"arn:aws:ssm:eu-west-1:539690530154:parameter/my-app/dev/db-url\" } ], \"InvalidParameters\": [] } Via path query: \u279c ~ aws ssm get-parameters-by-path --path /my-app/ --recursive --with-decryption { \"Parameters\": [ { \"Name\": \"/my-app/dev/db-password\", \"Type\": \"SecureString\", \"Value\": \"mysupersecretdevpassword\", \"Version\": 1, \"LastModifiedDate\": 1577882784.389, \"ARN\": \"arn:aws:ssm:eu-west-1:539690530154:parameter/my-app/dev/db-password\" }, { \"Name\": \"/my-app/dev/db-url\", \"Type\": \"String\", \"Value\": \"dev.db.domain.com\", \"Version\": 1, \"LastModifiedDate\": 1577882735.442, \"ARN\": \"arn:aws:ssm:eu-west-1:539690530154:parameter/my-app/dev/db-url\" }, { \"Name\": \"/my-app/prod/db-password\", \"Type\": \"SecureString\", \"Value\": \"mysupersecretprodpassword\", \"Version\": 1, \"LastModifiedDate\": 1577882871.1, \"ARN\": \"arn:aws:ssm:eu-west-1:539690530154:parameter/my-app/prod/db-password\" }, { \"Name\": \"/my-app/prod/db-url\", \"Type\": \"String\", \"Value\": \"prod.db.domain.com\", \"Version\": 1, \"LastModifiedDate\": 1577882848.24, \"ARN\": \"arn:aws:ssm:eu-west-1:539690530154:parameter/my-app/prod/db-url\" } ] }","title":"SSM Parameter Store hands on CLI"},{"location":"AWS/SSM/Parameter%20Store/SSM%20Parameter%20Store%20hands%20on%20CLI/#ssm-parameter-store-hands-on-cli","text":"We can search for [[Systems Manager]] in [[AWS Console]] and then navigate to SSM Parameter Store section. And add params: Then query via [[AWS CLI]] \u279c ~ aws ssm get-parameters --names /my-app/dev/db-url /my-app/dev/db-password { \"Parameters\": [ { \"Name\": \"/my-app/dev/db-password\", \"Type\": \"SecureString\", \"Value\": \"AQICAHhSgKv4yUSMN7xTX6TV5+owFLPqV7u3thrIPV83oQBh6AGQBrWbtKy9FhfLFZmp1aBlAAAAdjB0BgkqhkiG9w0BBwagZzBlAgEAMGAGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMRUzwDLakNxxfAoZLAgEQgDMt5tCgJphvuAtDng+Lhuas/aaR0ecOpy42PGq7uK5Fxik8routwmNgIT/2H0ubJiAkhuY=\", \"Version\": 1, \"LastModifiedDate\": 1577882784.389, \"ARN\": \"arn:aws:ssm:eu-west-1:539690530154:parameter/my-app/dev/db-password\" }, { \"Name\": \"/my-app/dev/db-url\", \"Type\": \"String\", \"Value\": \"dev.db.domain.com\", \"Version\": 1, \"LastModifiedDate\": 1577882735.442, \"ARN\": \"arn:aws:ssm:eu-west-1:539690530154:parameter/my-app/dev/db-url\" } ], \"InvalidParameters\": [] } Decrypt password: \u279c ~ aws ssm get-parameters --names /my-app/dev/db-url /my-app/dev/db-password --with-decryption { \"Parameters\": [ { \"Name\": \"/my-app/dev/db-password\", \"Type\": \"SecureString\", \"Value\": \"mysupersecretdevpassword\", \"Version\": 1, \"LastModifiedDate\": 1577882784.389, \"ARN\": \"arn:aws:ssm:eu-west-1:539690530154:parameter/my-app/dev/db-password\" }, { \"Name\": \"/my-app/dev/db-url\", \"Type\": \"String\", \"Value\": \"dev.db.domain.com\", \"Version\": 1, \"LastModifiedDate\": 1577882735.442, \"ARN\": \"arn:aws:ssm:eu-west-1:539690530154:parameter/my-app/dev/db-url\" } ], \"InvalidParameters\": [] } Via path query: \u279c ~ aws ssm get-parameters-by-path --path /my-app/ --recursive --with-decryption { \"Parameters\": [ { \"Name\": \"/my-app/dev/db-password\", \"Type\": \"SecureString\", \"Value\": \"mysupersecretdevpassword\", \"Version\": 1, \"LastModifiedDate\": 1577882784.389, \"ARN\": \"arn:aws:ssm:eu-west-1:539690530154:parameter/my-app/dev/db-password\" }, { \"Name\": \"/my-app/dev/db-url\", \"Type\": \"String\", \"Value\": \"dev.db.domain.com\", \"Version\": 1, \"LastModifiedDate\": 1577882735.442, \"ARN\": \"arn:aws:ssm:eu-west-1:539690530154:parameter/my-app/dev/db-url\" }, { \"Name\": \"/my-app/prod/db-password\", \"Type\": \"SecureString\", \"Value\": \"mysupersecretprodpassword\", \"Version\": 1, \"LastModifiedDate\": 1577882871.1, \"ARN\": \"arn:aws:ssm:eu-west-1:539690530154:parameter/my-app/prod/db-password\" }, { \"Name\": \"/my-app/prod/db-url\", \"Type\": \"String\", \"Value\": \"prod.db.domain.com\", \"Version\": 1, \"LastModifiedDate\": 1577882848.24, \"ARN\": \"arn:aws:ssm:eu-west-1:539690530154:parameter/my-app/prod/db-url\" } ] }","title":"SSM Parameter Store hands on CLI"},{"location":"AWS/SSM/Parameter%20Store/SSM%20Parameter%20Store/","text":"SSM Parameter Store \u00b6 Secure storage for [[configuration]] and [[secrets]] Optional Seamless Encryption using AWS KMS (Key Management Service) Serverless , [[scalable]], [[durable]], easy SDK, free Version tracking of configurations / secrets -[[ Configuration management ]]using path & IAM Notifications with CloudWatch Events s Integration with CloudFormation SSM Parameter Store hands on CLI SSM Parameter Store Hands On Lambda","title":"SSM Parameter Store"},{"location":"AWS/SSM/Parameter%20Store/SSM%20Parameter%20Store/#ssm-parameter-store","text":"Secure storage for [[configuration]] and [[secrets]] Optional Seamless Encryption using AWS KMS (Key Management Service) Serverless , [[scalable]], [[durable]], easy SDK, free Version tracking of configurations / secrets -[[ Configuration management ]]using path & IAM Notifications with CloudWatch Events s Integration with CloudFormation SSM Parameter Store hands on CLI SSM Parameter Store Hands On Lambda","title":"SSM Parameter Store"},{"location":"AWS/SSO/AWS%20Single%20Sign%20ON%20%28SSO%29/","text":"AWS Single Sign ON (SSO) \u00b6 Centrally Managed [[Single Sign On (SSO)]] across AWS Accounts and Business Applications (Office 365, Salesforce, Box) One login gets you access to everything securely Integrated with [[Microsoft Active Directory]] Helps reduce the process of setting up [[Single Sign On (SSO)]] in a company Only helpful for [[Web Browser]], [[SAML 2.0]] enabled applications","title":"AWS Single Sign ON (SSO)"},{"location":"AWS/SSO/AWS%20Single%20Sign%20ON%20%28SSO%29/#aws-single-sign-on-sso","text":"Centrally Managed [[Single Sign On (SSO)]] across AWS Accounts and Business Applications (Office 365, Salesforce, Box) One login gets you access to everything securely Integrated with [[Microsoft Active Directory]] Helps reduce the process of setting up [[Single Sign On (SSO)]] in a company Only helpful for [[Web Browser]], [[SAML 2.0]] enabled applications","title":"AWS Single Sign ON (SSO)"},{"location":"AWS/STS/AWS%20STS%20-%20Security%20Token%20Service/","text":"AWS STS - Security Token Service \u00b6 Allows to grant limited and temporary access to AWS resources Token is valid for up to one hour (must be refreshed) Cross Account Access Allows users from one AWS account to access resources in another [[Identity Federation]] ([[Active Directory]]) Provides a non-AWS user with temporary AWS access by linking users [[Active Directory]] credentials Uses [[SAML]] (Security Assertion Markup Language) Allows [[Single Sign On (SSO)]] which enables users to log into [[AWS console]] without assigning IAM credentials Federation with third part providers / Programming/AWS/Cognito/AWS Cognito used mainly in web and mobile applications Makes use of Facebook/Google/Amazon etc to federate them","title":"AWS STS - Security Token Service"},{"location":"AWS/STS/AWS%20STS%20-%20Security%20Token%20Service/#aws-sts-security-token-service","text":"Allows to grant limited and temporary access to AWS resources Token is valid for up to one hour (must be refreshed) Cross Account Access Allows users from one AWS account to access resources in another [[Identity Federation]] ([[Active Directory]]) Provides a non-AWS user with temporary AWS access by linking users [[Active Directory]] credentials Uses [[SAML]] (Security Assertion Markup Language) Allows [[Single Sign On (SSO)]] which enables users to log into [[AWS console]] without assigning IAM credentials Federation with third part providers / Programming/AWS/Cognito/AWS Cognito used mainly in web and mobile applications Makes use of Facebook/Google/Amazon etc to federate them","title":"AWS STS - Security Token Service"},{"location":"AWS/STS/Cross%20Account%20Access/","text":"Cross Account Access \u00b6 Define an [[IAM Role]] for another account to access Define which accounts can access this [[IAM Role]] Use AWS STS - Security Token Service to retrieve credentials and impersonate the [[IAM Role]] you have access to (AssumeRole API) Temporary credentials can be valid between 15 minutes to 1 hour","title":"Cross Account Access"},{"location":"AWS/STS/Cross%20Account%20Access/#cross-account-access","text":"Define an [[IAM Role]] for another account to access Define which accounts can access this [[IAM Role]] Use AWS STS - Security Token Service to retrieve credentials and impersonate the [[IAM Role]] you have access to (AssumeRole API) Temporary credentials can be valid between 15 minutes to 1 hour","title":"Cross Account Access"},{"location":"AWS/Simple%20Workflow%20Service/Simple%20Workflow%20Service/","text":"AWS SWF - Simple Workflow Service \u00b6 Coordinate work amongst applications Code runs on AWS EC2 (not serverless ) 1 year max runtime Concept of \"[[activity step]]\" and \"[[decision step]]\" Has built-in \"[[human intervention]]\" step Example: order fulfilment from web to warehouse to deliver AWS Step Functions is recommended to be used for new applications except: If you need [[external signals]] to intervene in the [[process]] If you need [[child process]]es that return values to the [[parent process]]es","title":"Simple Workflow Service"},{"location":"AWS/Simple%20Workflow%20Service/Simple%20Workflow%20Service/#aws-swf-simple-workflow-service","text":"Coordinate work amongst applications Code runs on AWS EC2 (not serverless ) 1 year max runtime Concept of \"[[activity step]]\" and \"[[decision step]]\" Has built-in \"[[human intervention]]\" step Example: order fulfilment from web to warehouse to deliver AWS Step Functions is recommended to be used for new applications except: If you need [[external signals]] to intervene in the [[process]] If you need [[child process]]es that return values to the [[parent process]]es","title":"AWS SWF - Simple Workflow Service"},{"location":"AWS/Solution%20Architecture%20Decisions/Big%20Data%20Ingestion%20Pipeline/","text":"Big Data Ingestion Pipeline \u00b6 We want the ingestion pipeline to be fully Serverless We want to collect data in real time We want to transform the data We want to query the transformed data using [[SQL]] The reports created using the queries should be in AWS S3 We want to load that data into a [[Data warehouse]] and create [[dashboards]] [[IoT core]] allows you to harvest data from [[IoT device]]s Kinesis is great for [[real-time data collection]] Kinesis Firehose helps with data delivery to AWS S3 in near real-time (1 minute) Lambda can help Kinesis Firehose with [[data transformations]] AWS S3 can trigger notifications to AWS SQS AWS Lambda can subscribe to AWS SQS (we could have connected AWS S3 to AWS Lambda ) AWS Athena is a Serverless [[SQL]] service and results are stored in AWS S3 The reporting AWS S3 Bucket contains analysed data and can be used by reporting tool such as [[AWS QuickSight]], Redshift etc.","title":"Big Data Ingestion Pipeline"},{"location":"AWS/Solution%20Architecture%20Decisions/Big%20Data%20Ingestion%20Pipeline/#big-data-ingestion-pipeline","text":"We want the ingestion pipeline to be fully Serverless We want to collect data in real time We want to transform the data We want to query the transformed data using [[SQL]] The reports created using the queries should be in AWS S3 We want to load that data into a [[Data warehouse]] and create [[dashboards]] [[IoT core]] allows you to harvest data from [[IoT device]]s Kinesis is great for [[real-time data collection]] Kinesis Firehose helps with data delivery to AWS S3 in near real-time (1 minute) Lambda can help Kinesis Firehose with [[data transformations]] AWS S3 can trigger notifications to AWS SQS AWS Lambda can subscribe to AWS SQS (we could have connected AWS S3 to AWS Lambda ) AWS Athena is a Serverless [[SQL]] service and results are stored in AWS S3 The reporting AWS S3 Bucket contains analysed data and can be used by reporting tool such as [[AWS QuickSight]], Redshift etc.","title":"Big Data Ingestion Pipeline"},{"location":"AWS/Solution%20Architecture%20Decisions/Distributing%20Paid%20Content/","text":"Distributing Paid Content \u00b6 We sell videos online and users have to pay to buy videos Each video can be bough by many customers We only want to distribute videos to users who are premium users We have a [[database]] of premium users Links we send to premium users should be short lived Our application is global We want to be fully Serverless","title":"Distributing Paid Content"},{"location":"AWS/Solution%20Architecture%20Decisions/Distributing%20Paid%20Content/#distributing-paid-content","text":"We sell videos online and users have to pay to buy videos Each video can be bough by many customers We only want to distribute videos to users who are premium users We have a [[database]] of premium users Links we send to premium users should be short lived Our application is global We want to be fully Serverless","title":"Distributing Paid Content"},{"location":"AWS/Solution%20Architecture%20Decisions/Micro%20Services%20architecture/","text":"Micro Services architecture \u00b6 We want to switch to a [[micro-service architecture]] Many services interact with each other directly using a [[REST API]] Each architecture for each micro service may vary in form and shape We want a [[micro-service architecture]] so we can have a leaner [[development lifecycle]] for each service. Discussions on Micro Services \u00b6 You are free to design each [[micro-service]] the way you want Synchronous patterns: AWS API Gateway , [[Load Balancer]]s Asynchronous patterns: AWS SQS , Kinesis , AWS SNS , [[Lambda Triggers]] (S3) Challenges with micro-services: Repeated [[overhead for creating each new microservice]] Issues with [[optimizing server density, utilizaion]] Complexity of [[running multiple versions of multiple micro-services simultaneously]] [[proliferation of client-side code requirements to integrate with many seperate services]]. Some of the challenges are solved by Serverless patterns: AWS API Gateway , AWS Lambda scale automatically and you pay per usage You can easily clone [[API]], [[reproduce environments]] [[Generate client SDK through Swagger integration for the API Gateway]]","title":"Micro Services architecture"},{"location":"AWS/Solution%20Architecture%20Decisions/Micro%20Services%20architecture/#micro-services-architecture","text":"We want to switch to a [[micro-service architecture]] Many services interact with each other directly using a [[REST API]] Each architecture for each micro service may vary in form and shape We want a [[micro-service architecture]] so we can have a leaner [[development lifecycle]] for each service.","title":"Micro Services architecture"},{"location":"AWS/Solution%20Architecture%20Decisions/Micro%20Services%20architecture/#discussions-on-micro-services","text":"You are free to design each [[micro-service]] the way you want Synchronous patterns: AWS API Gateway , [[Load Balancer]]s Asynchronous patterns: AWS SQS , Kinesis , AWS SNS , [[Lambda Triggers]] (S3) Challenges with micro-services: Repeated [[overhead for creating each new microservice]] Issues with [[optimizing server density, utilizaion]] Complexity of [[running multiple versions of multiple micro-services simultaneously]] [[proliferation of client-side code requirements to integrate with many seperate services]]. Some of the challenges are solved by Serverless patterns: AWS API Gateway , AWS Lambda scale automatically and you pay per usage You can easily clone [[API]], [[reproduce environments]] [[Generate client SDK through Swagger integration for the API Gateway]]","title":"Discussions on Micro Services"},{"location":"AWS/Solution%20Architecture%20Decisions/Serverless%20Mobile%20Application%20-%20MyTodoList/","text":"Serverless Mobile Application: MyTodoList \u00b6 We want to create a mobile application with the following requirements Expose as [[REST API]] with [[HTTPS]] [[Serverless architecture]] Users should be able to directly interact with their own folder in AWS S3 Users should authenticate through a managed Serverless service The users can write and read to-dos, but they mostly read them The [[database]] should scale, and have some [[high read throughput]]","title":"Serverless Mobile Application: MyTodoList"},{"location":"AWS/Solution%20Architecture%20Decisions/Serverless%20Mobile%20Application%20-%20MyTodoList/#serverless-mobile-application-mytodolist","text":"We want to create a mobile application with the following requirements Expose as [[REST API]] with [[HTTPS]] [[Serverless architecture]] Users should be able to directly interact with their own folder in AWS S3 Users should authenticate through a managed Serverless service The users can write and read to-dos, but they mostly read them The [[database]] should scale, and have some [[high read throughput]]","title":"Serverless Mobile Application: MyTodoList"},{"location":"AWS/Solution%20Architecture%20Decisions/Serverless%20hosted%20website%20-%20MyBlog.com/","text":"Serverless hosted website: MyBlog.com \u00b6 This website should scale globally Blogs are rarely written, but often read Some of the website is purely static files, the rest is a dynamic [[REST API]] [[Caching]] must be implemented where possible Any new users that subscribe should receive a welcome email Any photo uploaded to the blog should have a thumbnail generated","title":"Serverless hosted website: MyBlog.com"},{"location":"AWS/Solution%20Architecture%20Decisions/Serverless%20hosted%20website%20-%20MyBlog.com/#serverless-hosted-website-myblogcom","text":"This website should scale globally Blogs are rarely written, but often read Some of the website is purely static files, the rest is a dynamic [[REST API]] [[Caching]] must be implemented where possible Any new users that subscribe should receive a welcome email Any photo uploaded to the blog should have a thumbnail generated","title":"Serverless hosted website: MyBlog.com"},{"location":"AWS/Solution%20Architecture%20Decisions/Software%20updates%20offloading/","text":"Software updates offloading \u00b6 We have an application running on AWS EC2 that distributes software updates once in a while When a new [[software update]] is out, we get a lot of requests and the content is [[distributed]] in mass over the network. It's very costly We don't want to change our [[application]], but want to [[optimise]] our cost and [[CPU]], how can we do it? Current state: Easy fix: Why Programming/AWS/CloudFront/AWS CloudFront ? \u00b6 No changes in [[architecture]] Will [[cache]] software update files at the [[edge]] [[Software update]] files are not dynamic, they're static (never changing) Our AWS EC2 instances aren't Serverless But Programming/AWS/CloudFront/AWS CloudFront is, and will scale for us Our Auto Scaling Group (ASG) will not scale as much, and we'll save tremendously in AWS EC2 We'll also save in [[availability]], [[network bandwidth]] cost etc","title":"Software updates offloading"},{"location":"AWS/Solution%20Architecture%20Decisions/Software%20updates%20offloading/#software-updates-offloading","text":"We have an application running on AWS EC2 that distributes software updates once in a while When a new [[software update]] is out, we get a lot of requests and the content is [[distributed]] in mass over the network. It's very costly We don't want to change our [[application]], but want to [[optimise]] our cost and [[CPU]], how can we do it? Current state: Easy fix:","title":"Software updates offloading"},{"location":"AWS/Solution%20Architecture%20Decisions/Software%20updates%20offloading/#why-programmingawscloudfrontaws-cloudfront","text":"No changes in [[architecture]] Will [[cache]] software update files at the [[edge]] [[Software update]] files are not dynamic, they're static (never changing) Our AWS EC2 instances aren't Serverless But Programming/AWS/CloudFront/AWS CloudFront is, and will scale for us Our Auto Scaling Group (ASG) will not scale as much, and we'll save tremendously in AWS EC2 We'll also save in [[availability]], [[network bandwidth]] cost etc","title":"Why Programming/AWS/CloudFront/AWS CloudFront?"},{"location":"AWS/Solution%20Architecture%20Decisions/Stateful%20Web%20App%20-%20MyClothes.com/","text":"Stateful Web App: MyClothes.com \u00b6 MyClothes.com allows people to buy clothes online. Theres a [[shopping cart]] Our [[website]] is having hundreds of users at the same time We need to [[scale]], maintain Horizontal scalability and keep our [[web application]] as [[stateless]] as possible Users should not lose their[[ shopping cart]] Users should have their details (address, etc) in a [[database]] We can use [[Multi AZ]] setup with a [[Load Balancer]], Auto Scaling Group (ASG) . There are multiple ways to keep the sessions: - Use load Load Balancer stickiness - Introduce user cookies - Store session data into ElastiCache or DynamoDB - [[Stateless]] - [[Multi AZ]] - Can also be used for [[caching]] data from AWS RDS - Tighter security with Security Group s referencing each other","title":"Stateful Web App: MyClothes.com"},{"location":"AWS/Solution%20Architecture%20Decisions/Stateful%20Web%20App%20-%20MyClothes.com/#stateful-web-app-myclothescom","text":"MyClothes.com allows people to buy clothes online. Theres a [[shopping cart]] Our [[website]] is having hundreds of users at the same time We need to [[scale]], maintain Horizontal scalability and keep our [[web application]] as [[stateless]] as possible Users should not lose their[[ shopping cart]] Users should have their details (address, etc) in a [[database]] We can use [[Multi AZ]] setup with a [[Load Balancer]], Auto Scaling Group (ASG) . There are multiple ways to keep the sessions: - Use load Load Balancer stickiness - Introduce user cookies - Store session data into ElastiCache or DynamoDB - [[Stateless]] - [[Multi AZ]] - Can also be used for [[caching]] data from AWS RDS - Tighter security with Security Group s referencing each other","title":"Stateful Web App: MyClothes.com"},{"location":"AWS/Solution%20Architecture%20Decisions/Stateful%20Web%20App%20-%20MyWordpress.com/","text":"Stateful Web App: MyWordpress.com \u00b6 We are trying to create a fully scalable [[Wordpress]] website We want that website to access and correctly display picture [[uploads]] Our user data, an the blog content should be stored in a [[MySQL]] [[database]] Use multiple instances with a [[Load Balancer]]. For file uploads, use EFS - Elastic File System . For database, use AWS Aurora database to have easy [[Multi AZ]] and [[Read Replica]]s.","title":"Stateful Web App: MyWordpress.com"},{"location":"AWS/Solution%20Architecture%20Decisions/Stateful%20Web%20App%20-%20MyWordpress.com/#stateful-web-app-mywordpresscom","text":"We are trying to create a fully scalable [[Wordpress]] website We want that website to access and correctly display picture [[uploads]] Our user data, an the blog content should be stored in a [[MySQL]] [[database]] Use multiple instances with a [[Load Balancer]]. For file uploads, use EFS - Elastic File System . For database, use AWS Aurora database to have easy [[Multi AZ]] and [[Read Replica]]s.","title":"Stateful Web App: MyWordpress.com"},{"location":"AWS/Solution%20Architecture%20Decisions/Stateless%20Web%20App%20-%20WhatIsTheTime.com/","text":"Stateless Web App: WhatIsTheTime.com \u00b6 WhatIsTheTime.com allows people to know what time it is We don't need a [[database]] We want to start small and can accept [[downtime]] We want to full Vertical scalability and Horizontal scalability , no [[downtime]] Let's go through the Solutions Architect journey for this app Starting simple \u00b6 Create a single t2.micro instance Scaling vertically \u00b6 When we have more traffic for the t2.micro instance to handle, we scale it vertically to something like m5.large. [[Downtime]] while upgrading to new instance. Scaling horizontally \u00b6 Instead of scaling vertically, we can add multiple instances to balance the load between them. In one instance goes away, we can leverage [[Load Balancer]] and Health Checks to route clients to a different instance. We can use Auto Scaling Group (ASG) to manage the instance count automatically In order to make our app disaster tolerant, we can also make it [[Multi AZ]].","title":"Stateless Web App: WhatIsTheTime.com"},{"location":"AWS/Solution%20Architecture%20Decisions/Stateless%20Web%20App%20-%20WhatIsTheTime.com/#stateless-web-app-whatisthetimecom","text":"WhatIsTheTime.com allows people to know what time it is We don't need a [[database]] We want to start small and can accept [[downtime]] We want to full Vertical scalability and Horizontal scalability , no [[downtime]] Let's go through the Solutions Architect journey for this app","title":"Stateless Web App: WhatIsTheTime.com"},{"location":"AWS/Solution%20Architecture%20Decisions/Stateless%20Web%20App%20-%20WhatIsTheTime.com/#starting-simple","text":"Create a single t2.micro instance","title":"Starting simple"},{"location":"AWS/Solution%20Architecture%20Decisions/Stateless%20Web%20App%20-%20WhatIsTheTime.com/#scaling-vertically","text":"When we have more traffic for the t2.micro instance to handle, we scale it vertically to something like m5.large. [[Downtime]] while upgrading to new instance.","title":"Scaling vertically"},{"location":"AWS/Solution%20Architecture%20Decisions/Stateless%20Web%20App%20-%20WhatIsTheTime.com/#scaling-horizontally","text":"Instead of scaling vertically, we can add multiple instances to balance the load between them. In one instance goes away, we can leverage [[Load Balancer]] and Health Checks to route clients to a different instance. We can use Auto Scaling Group (ASG) to manage the instance count automatically In order to make our app disaster tolerant, we can also make it [[Multi AZ]].","title":"Scaling horizontally"},{"location":"AWS/Step%20Functions/AWS%20Step%20Functions/","text":"AWS Step Functions \u00b6 Build serverless visual workflow to orchestrate your AWS Lambda functions Represent flow as a [[JSON]] [[state machine]] Features: [[sequence]], [[parallel]], [[conditions]], [[timeouts]], [[error handling]] Can also integrate with AWS EC2 , [[AWS ECS]], [[On premise server]]s, AWS API Gateway Maximum execution time of 1 year Possibility to implement [[human approval]] feature Use cases [[Order fulfilment]] [[Data processing]] [[Web application]]s Any [[workflow]]","title":"AWS Step Functions"},{"location":"AWS/Step%20Functions/AWS%20Step%20Functions/#aws-step-functions","text":"Build serverless visual workflow to orchestrate your AWS Lambda functions Represent flow as a [[JSON]] [[state machine]] Features: [[sequence]], [[parallel]], [[conditions]], [[timeouts]], [[error handling]] Can also integrate with AWS EC2 , [[AWS ECS]], [[On premise server]]s, AWS API Gateway Maximum execution time of 1 year Possibility to implement [[human approval]] feature Use cases [[Order fulfilment]] [[Data processing]] [[Web application]]s Any [[workflow]]","title":"AWS Step Functions"},{"location":"AWS/Step%20Functions/Step%20functions%20%26%20Simple%20Workflow%20Service/","text":"Step functions & SWF \u00b6","title":"Step functions & SWF"},{"location":"AWS/Step%20Functions/Step%20functions%20%26%20Simple%20Workflow%20Service/#step-functions-swf","text":"","title":"Step functions &amp; SWF"},{"location":"AWS/Trusted%20Advisor/AWS%20Trusted%20Advisor/","text":"Trusted Advisor \u00b6 No need to install anything - high level AWS account assessment Analyse your AWS accounts and provides recommendations [[Cost Optimization]] [[Performance]] [[Security]] [[Fault Tolerance]] [[Service Limits]] Core Checks and recommendations - all customers Can enable weekly [[email notification]] from the console Full Trusted Advisor - Available for Business & Enterprise support plans Ability to set CloudWatch Alarm when reaching limits","title":"Trusted Advisor"},{"location":"AWS/Trusted%20Advisor/AWS%20Trusted%20Advisor/#trusted-advisor","text":"No need to install anything - high level AWS account assessment Analyse your AWS accounts and provides recommendations [[Cost Optimization]] [[Performance]] [[Security]] [[Fault Tolerance]] [[Service Limits]] Core Checks and recommendations - all customers Can enable weekly [[email notification]] from the console Full Trusted Advisor - Available for Business & Enterprise support plans Ability to set CloudWatch Alarm when reaching limits","title":"Trusted Advisor"},{"location":"AWS/VPC/Bastion%20Host/","text":"Bastion Hosts \u00b6 We can use a Bastion Host to [[SSH]] into our [[private instance]]s The bastion is in the [[public subnet]] which is then connected to all other [[private subnet]]s Bastion Host security group must be tightened Tip: Make sure the bastion host port only has 22 traffic from the IP you need, not from the security group s of your other instances.","title":"Bastion Hosts"},{"location":"AWS/VPC/Bastion%20Host/#bastion-hosts","text":"We can use a Bastion Host to [[SSH]] into our [[private instance]]s The bastion is in the [[public subnet]] which is then connected to all other [[private subnet]]s Bastion Host security group must be tightened Tip: Make sure the bastion host port only has 22 traffic from the IP you need, not from the security group s of your other instances.","title":"Bastion Hosts"},{"location":"AWS/VPC/CIDR/","text":"CIDR s are used for Security Group s rules, or AWS networking in general They help to define an [[IP address range]] We've seen WW.XX.YY.ZZ/32 == one [[IP]] We've seen 0.0.0.0/0 == all IPs But we can define 192.168.0.0/26: 192.168.0.0-192.168.0.63 (64 IPs) Understanding CIDR \u00b6 A CIDR has two components The base IP (XX.XX.XX.XX) The [[Subnet Mask]] (/26) The base IP represents an IP contained in the range The [[subnet mask]]s defines how many bits can change in the IP The subnet mask can take two forms. Examples: 255.255.255.0 (less common) /24 (more common) Understanding CIDRs Subnet Masks \u00b6 The subnet masks basically allows part of the underlying IP to get additional next values from the base IP /32 allows for 1 IP = 2^0 /31 allows for 2 IP = 2^1 /30 allows for 4 IP = 2^2 /29 allows for 8 IP = 2^3 /28 allows for 16 IP = 2^4 /27 allows for 32 IP = 2^5 /26 allows for 64 IP = 2^6 /25 allows for 128 IP = 2^7 /24 allows for 256 IP = 2^8 /16 allows for 65,536 IP = 2^16 /0 allows for all IPs = 2^32 Quick memo: \u00b6 /32 - no IP number can change /24 - last IP number can change /16 - last two IP numbers can change /8 - last three numbers can change /0 - all IP numbers can change Little exercise \u00b6 192.168.0.0/24 192.168.0.0 - 192.168.0.255 (256 IPs) 192.168.0.0/16 192.168.0.0 - 192.168.255.255 (65.536 IPs) 134.56.78.123/32 Just 134.56.78.123/32 0.0.0.0/0 All IPs When in doubt, use this website: https://www.ipaddressguide.com/cidr Private vs Public IP (IPv4) \u00b6 Allowed ranges \u00b6 The [[Internet Assigned Numbers Authority]] (IANA) established certain blocks of [[IPv4]] addresses for the use of private ([[LAN]]) and public ([[internet]]) addresses. Private IP can only allow certain values 10.0.0.0 - 10.255.255.255 (10.0.0.0/8) - in big networks 172.16.0.0 - 172.31.255.255 (172.16.0.0/12) - default AWS one 192.168.0.0-192.168.255.255 (192.168.0.0/16) - example: home networks All the rest of the IP one the internet are public IP s","title":"CIDR"},{"location":"AWS/VPC/CIDR/#understanding-cidr","text":"A CIDR has two components The base IP (XX.XX.XX.XX) The [[Subnet Mask]] (/26) The base IP represents an IP contained in the range The [[subnet mask]]s defines how many bits can change in the IP The subnet mask can take two forms. Examples: 255.255.255.0 (less common) /24 (more common)","title":"Understanding CIDR"},{"location":"AWS/VPC/CIDR/#understanding-cidrs-subnet-masks","text":"The subnet masks basically allows part of the underlying IP to get additional next values from the base IP /32 allows for 1 IP = 2^0 /31 allows for 2 IP = 2^1 /30 allows for 4 IP = 2^2 /29 allows for 8 IP = 2^3 /28 allows for 16 IP = 2^4 /27 allows for 32 IP = 2^5 /26 allows for 64 IP = 2^6 /25 allows for 128 IP = 2^7 /24 allows for 256 IP = 2^8 /16 allows for 65,536 IP = 2^16 /0 allows for all IPs = 2^32","title":"Understanding CIDRs Subnet Masks"},{"location":"AWS/VPC/CIDR/#quick-memo","text":"/32 - no IP number can change /24 - last IP number can change /16 - last two IP numbers can change /8 - last three numbers can change /0 - all IP numbers can change","title":"Quick memo:"},{"location":"AWS/VPC/CIDR/#little-exercise","text":"192.168.0.0/24 192.168.0.0 - 192.168.0.255 (256 IPs) 192.168.0.0/16 192.168.0.0 - 192.168.255.255 (65.536 IPs) 134.56.78.123/32 Just 134.56.78.123/32 0.0.0.0/0 All IPs When in doubt, use this website: https://www.ipaddressguide.com/cidr","title":"Little exercise"},{"location":"AWS/VPC/CIDR/#private-vs-public-ip-ipv4","text":"","title":"Private vs Public IP (IPv4)"},{"location":"AWS/VPC/CIDR/#allowed-ranges","text":"The [[Internet Assigned Numbers Authority]] (IANA) established certain blocks of [[IPv4]] addresses for the use of private ([[LAN]]) and public ([[internet]]) addresses. Private IP can only allow certain values 10.0.0.0 - 10.255.255.255 (10.0.0.0/8) - in big networks 172.16.0.0 - 172.31.255.255 (172.16.0.0/12) - default AWS one 192.168.0.0-192.168.255.255 (192.168.0.0/16) - example: home networks All the rest of the IP one the internet are public IP s","title":"Allowed ranges"},{"location":"AWS/VPC/Customer%20Gateway/","text":"Customer Gateway \u00b6 [[Software application]] or [[physical device]] on [[customer side]] of the [[VPN connection]] https://docs.aws.amazon.com/vpc/latest/adminguide/Introduction.html#DevicesTested [[IP Address]]: Use [[static]], internet-routable [[IP address]] for your [[customer gateway device]] If behind a CGW behind [[NAT]] (with NAT-T), use the public IP address of the [[NAT]]","title":"Customer Gateway"},{"location":"AWS/VPC/Customer%20Gateway/#customer-gateway","text":"[[Software application]] or [[physical device]] on [[customer side]] of the [[VPN connection]] https://docs.aws.amazon.com/vpc/latest/adminguide/Introduction.html#DevicesTested [[IP Address]]: Use [[static]], internet-routable [[IP address]] for your [[customer gateway device]] If behind a CGW behind [[NAT]] (with NAT-T), use the public IP address of the [[NAT]]","title":"Customer Gateway"},{"location":"AWS/VPC/DNS%20Resolution%20Options/","text":"DNS Resolution Options \u00b6 DNS Resolution in VPC \u00b6 enableDnsSupport (= [[DNS Resolution Setting]]) Default True Helps decide if DNS resolution is supported for the VPC if True, queries the [[AWS DNS server]] at 169.254.169.253 enableDnsHostname (= [[DNS hostname setting]]) False by default for newly created VPC , true by default for default VPC Won't do anything unless enable DnsSupport=True If True, assign [[public hostname]] to AWS EC2 instance if it has a public IP If you use custom [[DNS domain name]]s in a [[private zone]] in AWS Route 53 , you must set both these attributes to true If we enable it, we can refresh our AWS EC2 instance list and see that there will be [[Public DNS hostname]]s associated with instances: We can go to AWS Route 53 and create a [[private zone]]: Then we can create [[record set]]s like these: Now we'll have a demo.foobar.internal hostname that will resolve in our VPC .","title":"DNS Resolution Options"},{"location":"AWS/VPC/DNS%20Resolution%20Options/#dns-resolution-options","text":"","title":"DNS Resolution Options"},{"location":"AWS/VPC/DNS%20Resolution%20Options/#dns-resolution-in-vpc","text":"enableDnsSupport (= [[DNS Resolution Setting]]) Default True Helps decide if DNS resolution is supported for the VPC if True, queries the [[AWS DNS server]] at 169.254.169.253 enableDnsHostname (= [[DNS hostname setting]]) False by default for newly created VPC , true by default for default VPC Won't do anything unless enable DnsSupport=True If True, assign [[public hostname]] to AWS EC2 instance if it has a public IP If you use custom [[DNS domain name]]s in a [[private zone]] in AWS Route 53 , you must set both these attributes to true If we enable it, we can refresh our AWS EC2 instance list and see that there will be [[Public DNS hostname]]s associated with instances: We can go to AWS Route 53 and create a [[private zone]]: Then we can create [[record set]]s like these: Now we'll have a demo.foobar.internal hostname that will resolve in our VPC .","title":"DNS Resolution in VPC"},{"location":"AWS/VPC/Default%20VPC/","text":"Default VPC overview \u00b6 All new accounts have a Default VPC New instances are launched into Default VPC is no Subnet is specified Default VPC have [[internet connectivity]] and all instances have public IP We also get a public and a private [[DNS name]] Our account has one default VPC, available hosts - 65536. Has 3 subnets - 3 availability zones, each has 4091 available [[IP]]s. We have a [[VPC route table]], which defines that one of the IPs has an Internet Gateway & Route tables and has associations to our 3 availability zone subnets Then our 3 Availability Zone s allow all [[inbound traffic]] and all [[outbound traffic]] (through Network ACL ):","title":"Default VPC overview"},{"location":"AWS/VPC/Default%20VPC/#default-vpc-overview","text":"All new accounts have a Default VPC New instances are launched into Default VPC is no Subnet is specified Default VPC have [[internet connectivity]] and all instances have public IP We also get a public and a private [[DNS name]] Our account has one default VPC, available hosts - 65536. Has 3 subnets - 3 availability zones, each has 4091 available [[IP]]s. We have a [[VPC route table]], which defines that one of the IPs has an Internet Gateway & Route tables and has associations to our 3 availability zone subnets Then our 3 Availability Zone s allow all [[inbound traffic]] and all [[outbound traffic]] (through Network ACL ):","title":"Default VPC overview"},{"location":"AWS/VPC/Direct%20Connect%20Gateway/","text":"Direct Connect Gateway \u00b6 If you want to setup a Direct Connect to one or more VPC in many different AWS Region s (same account), you must use a Direct Connect Gateway https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways.html","title":"Direct Connect Gateway"},{"location":"AWS/VPC/Direct%20Connect%20Gateway/#direct-connect-gateway","text":"If you want to setup a Direct Connect to one or more VPC in many different AWS Region s (same account), you must use a Direct Connect Gateway https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways.html","title":"Direct Connect Gateway"},{"location":"AWS/VPC/Direct%20Connect/","text":"Direct Connect \u00b6 Provides a dedicated [[private connection]] from a [[remote network]] to your VPC Dedicated connection must be setup between your DC and [[AWS Direct Connect locations]] You need to setup a Virtual Private Gateway on your VPC Access public resources ( AWS S3 ) and private ( AWS EC2 ) on same connection Use cases: Increase bandwidth throughput - working with large data sets - lower cost More consistent network experience - applications using real-time data feeds Hybrid environments (on-premises + cloud) Supports both [[IPv4]] and [[IPv6]]","title":"Direct Connect"},{"location":"AWS/VPC/Direct%20Connect/#direct-connect","text":"Provides a dedicated [[private connection]] from a [[remote network]] to your VPC Dedicated connection must be setup between your DC and [[AWS Direct Connect locations]] You need to setup a Virtual Private Gateway on your VPC Access public resources ( AWS S3 ) and private ( AWS EC2 ) on same connection Use cases: Increase bandwidth throughput - working with large data sets - lower cost More consistent network experience - applications using real-time data feeds Hybrid environments (on-premises + cloud) Supports both [[IPv4]] and [[IPv6]]","title":"Direct Connect"},{"location":"AWS/VPC/Egress%20Only%20Internet%20Gateway/","text":"Egress Only Internet Gateway \u00b6 Egress only Internet Gateway & Route tables if for [[IPv6]] only Similar function as a [[NAT]], but a NAT is for [[IPv4]] Good to know: [[IPV6]] are all public addresses Therefore all our instances with [[IPv6]] are publicly accessible Egress only Internet Gateway gives our [[IPv6]] instances access to the internet, but they won't be directly reachable by the internet After creating an Egress Only Internet Gateway, edit the Internet Gateway & Route tables","title":"Egress Only Internet Gateway"},{"location":"AWS/VPC/Egress%20Only%20Internet%20Gateway/#egress-only-internet-gateway","text":"Egress only Internet Gateway & Route tables if for [[IPv6]] only Similar function as a [[NAT]], but a NAT is for [[IPv4]] Good to know: [[IPV6]] are all public addresses Therefore all our instances with [[IPv6]] are publicly accessible Egress only Internet Gateway gives our [[IPv6]] instances access to the internet, but they won't be directly reachable by the internet After creating an Egress Only Internet Gateway, edit the Internet Gateway & Route tables","title":"Egress Only Internet Gateway"},{"location":"AWS/VPC/Internet%20Gateway%20%26%20Route%20tables/","text":"Internet Gateways & Route tables \u00b6 If we want to create an AWS EC2 instance in the subnet with a public IP address, we need to modify the [[public subnet]] to [[auto-assign IP]]s. When created, we can see that we have a private IP of 10.0.0.250 and a public IP of 54.246.162.86 . Now, even though our security group allows this, if we were to try to connect to it via [[ssh]], the connection would time out. This is because the subnet does not have an Internet Gateway & Route tables .","title":"Internet Gateways & Route tables"},{"location":"AWS/VPC/Internet%20Gateway%20%26%20Route%20tables/#internet-gateways-route-tables","text":"If we want to create an AWS EC2 instance in the subnet with a public IP address, we need to modify the [[public subnet]] to [[auto-assign IP]]s. When created, we can see that we have a private IP of 10.0.0.250 and a public IP of 54.246.162.86 . Now, even though our security group allows this, if we were to try to connect to it via [[ssh]], the connection would time out. This is because the subnet does not have an Internet Gateway & Route tables .","title":"Internet Gateways &amp; Route tables"},{"location":"AWS/VPC/Internet%20Gateway/","text":"Internet Gateway \u00b6 Internet Gateways help our VPC instances connect with the internet It scales horizontally and is [[Highly Available]] and [[redundant]] Must be created separately from VPC One VPC can only have attached to one Internet Gateway and vice versa Internet Gateway is also a [[NAT]] for the instances that have a [[public IPv4]] internet gateways on their own do not allow [[internet access]] Route Table s must also be edited Creating an Internet Gateway \u00b6 It will show as detached Now if we would try to [[ssh]], it still wouldn't work, because we have to edit the Route Table .","title":"Internet Gateway"},{"location":"AWS/VPC/Internet%20Gateway/#internet-gateway","text":"Internet Gateways help our VPC instances connect with the internet It scales horizontally and is [[Highly Available]] and [[redundant]] Must be created separately from VPC One VPC can only have attached to one Internet Gateway and vice versa Internet Gateway is also a [[NAT]] for the instances that have a [[public IPv4]] internet gateways on their own do not allow [[internet access]] Route Table s must also be edited","title":"Internet Gateway"},{"location":"AWS/VPC/Internet%20Gateway/#creating-an-internet-gateway","text":"It will show as detached Now if we would try to [[ssh]], it still wouldn't work, because we have to edit the Route Table .","title":"Creating an Internet Gateway"},{"location":"AWS/VPC/NAT%20Gateway/","text":"NAT Gateway \u00b6 AWS Managed [[NAT]], higher bandwidth, better availability, no admin Pay by the hour for usage and bandwidth [[NAT]] is created in a specific Availability Zone , uses an Elastic IP Cannot be used by an instance in that subnet (only from other subnets) Requires Internet Gateway & Route tables ([[Private Subnet]] => NAT Gateway => Internet Gateway & Route tables ) 5 Gbps of bandwidth with automatic scaling up to 45 Gbps No security group to manage / required The difference between NAT Gateway and NAT Instance can be found here https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html We are going to create a new NAT Gateway on PublicSubnetA Then we are going to edit the route table of the PrivateRouteTable","title":"NAT Gateway"},{"location":"AWS/VPC/NAT%20Gateway/#nat-gateway","text":"AWS Managed [[NAT]], higher bandwidth, better availability, no admin Pay by the hour for usage and bandwidth [[NAT]] is created in a specific Availability Zone , uses an Elastic IP Cannot be used by an instance in that subnet (only from other subnets) Requires Internet Gateway & Route tables ([[Private Subnet]] => NAT Gateway => Internet Gateway & Route tables ) 5 Gbps of bandwidth with automatic scaling up to 45 Gbps No security group to manage / required The difference between NAT Gateway and NAT Instance can be found here https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html We are going to create a new NAT Gateway on PublicSubnetA Then we are going to edit the route table of the PrivateRouteTable","title":"NAT Gateway"},{"location":"AWS/VPC/NAT%20Instance/","text":"NAT Instances - Network Address Translation (outdated) \u00b6 Allows instances in the [[private subnet]]s to connect to the internet Must be launched in a [[public subnet]] Must disable AWS EC2 flag: [[Source - Destination Check]] Must have Elastic IP attached to it Route Table must be configured to route traffic from [[private subnet]]s to NAT Instance To do this, we are going to add a new AWS EC2 instance with a [[NAT template]]. Place it in the [[public subnet]] And we are going to launch a private instance in the [[private subnet]]. Make sure to disable the [[Source - Destination Check]] for the [[NAT]]. Also would be recommended that the key pair is a different from the public ones. Then edit the routes of the Private Instance and add the NAT Instance . By doing this, the private instance will have an access to the internet, through the NAT instance. NAT Instances - Comments \u00b6 [[Amazon Linux AMI]] - preconfigured available Not [[Highly Available]] / [[resilient]] setup out of the box Would need to create Auto Scaling Group (ASG) in [[Multi AZ]] + resilient EC2 User Data script Internet traffic bandwidth depends on AWS EC2 instance performance Must manage security group s & rules Inbound Allow [[HTTP]] / [[HTTPS]] traffic coming from [[private subnet]]s Allow [[SSH]] from your home network (access is provided though Internet Gateway & Route tables ) Outbound Allow [[HTTP]] / [[HTTPS]] traffic to the internet","title":"NAT Instances - Network Address Translation (outdated)"},{"location":"AWS/VPC/NAT%20Instance/#nat-instances-network-address-translation-outdated","text":"Allows instances in the [[private subnet]]s to connect to the internet Must be launched in a [[public subnet]] Must disable AWS EC2 flag: [[Source - Destination Check]] Must have Elastic IP attached to it Route Table must be configured to route traffic from [[private subnet]]s to NAT Instance To do this, we are going to add a new AWS EC2 instance with a [[NAT template]]. Place it in the [[public subnet]] And we are going to launch a private instance in the [[private subnet]]. Make sure to disable the [[Source - Destination Check]] for the [[NAT]]. Also would be recommended that the key pair is a different from the public ones. Then edit the routes of the Private Instance and add the NAT Instance . By doing this, the private instance will have an access to the internet, through the NAT instance.","title":"NAT Instances - Network Address Translation (outdated)"},{"location":"AWS/VPC/NAT%20Instance/#nat-instances-comments","text":"[[Amazon Linux AMI]] - preconfigured available Not [[Highly Available]] / [[resilient]] setup out of the box Would need to create Auto Scaling Group (ASG) in [[Multi AZ]] + resilient EC2 User Data script Internet traffic bandwidth depends on AWS EC2 instance performance Must manage security group s & rules Inbound Allow [[HTTP]] / [[HTTPS]] traffic coming from [[private subnet]]s Allow [[SSH]] from your home network (access is provided though Internet Gateway & Route tables ) Outbound Allow [[HTTP]] / [[HTTPS]] traffic to the internet","title":"NAT Instances - Comments"},{"location":"AWS/VPC/Network%20ACL/","text":"Network ACLs & Security Groups \u00b6 Network ACLs \u00b6 [[NACL]] are like a [[firewall]] which control traffic from and to subnet [[Default NACL]] allows everything outbound and everything inbound One NACL per Subnet , new Subnets are assigned to the Default NACL Define NACL rules Rules have a number(1-32766) and higher precedence with a lower number If you define #100 ALLOW and #200 DENY , [[IP]] will be allowed Last rule is an asterisk (*) and denies a request in case of no rule match AWS recommends adding rules by increment of 100 Newly created NACL will deny everything NACL are a great way of blocking a specific IP at the subnet level https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html Incoming Request \u00b6 Outgoing Request \u00b6 If we look at our DemoVPC NACL, we can see that all the inbound traffic is allowed, as well as all [[outbound network]]. Network ACLs vs Security Groups \u00b6 https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Security.html#VPC_Security_Comparison","title":"Network ACLs & Security Groups"},{"location":"AWS/VPC/Network%20ACL/#network-acls-security-groups","text":"","title":"Network ACLs &amp; Security Groups"},{"location":"AWS/VPC/Network%20ACL/#network-acls","text":"[[NACL]] are like a [[firewall]] which control traffic from and to subnet [[Default NACL]] allows everything outbound and everything inbound One NACL per Subnet , new Subnets are assigned to the Default NACL Define NACL rules Rules have a number(1-32766) and higher precedence with a lower number If you define #100 ALLOW and #200 DENY , [[IP]] will be allowed Last rule is an asterisk (*) and denies a request in case of no rule match AWS recommends adding rules by increment of 100 Newly created NACL will deny everything NACL are a great way of blocking a specific IP at the subnet level https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html","title":"Network ACLs"},{"location":"AWS/VPC/Network%20ACL/#incoming-request","text":"","title":"Incoming Request"},{"location":"AWS/VPC/Network%20ACL/#outgoing-request","text":"If we look at our DemoVPC NACL, we can see that all the inbound traffic is allowed, as well as all [[outbound network]].","title":"Outgoing Request"},{"location":"AWS/VPC/Network%20ACL/#network-acls-vs-security-groups","text":"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Security.html#VPC_Security_Comparison","title":"Network ACLs vs Security Groups"},{"location":"AWS/VPC/Route%20Table/","text":"Editing Route tables \u00b6 We are going to create a public route table for [[public subnet]]s. And a private Route Table For [[private subnet]]s. Then, we want to edit the associations and add public subnets to the public route table, private to private. Then we are going to edit the routes of the public route table and add Internet Gateway . Now, we can connect to the [[SSH]].","title":"Route Table"},{"location":"AWS/VPC/Route%20Table/#editing-route-tables","text":"We are going to create a public route table for [[public subnet]]s. And a private Route Table For [[private subnet]]s. Then, we want to edit the associations and add public subnets to the public route table, private to private. Then we are going to edit the routes of the public route table and add Internet Gateway . Now, we can connect to the [[SSH]].","title":"Editing Route tables"},{"location":"AWS/VPC/Site%20to%20Site%20VPN/","text":"Site to Site VPN \u00b6","title":"Site to Site VPN"},{"location":"AWS/VPC/Site%20to%20Site%20VPN/#site-to-site-vpn","text":"","title":"Site to Site VPN"},{"location":"AWS/VPC/Subnet/","text":"Subnet overview hands on \u00b6 We are navigating to subnets section and creating a new [[public subnet]] with 256 IP addresses ON Availability Zone A. Then the same thing for Availability Zone B (change the Availability zone and CIDR block) Then a [[Private Subnet]] A with 4096 IPs and a [[Private Subnet]] B. When created, we notice that there are less available [[IP]]s than it should. This is because AWS reserves 5 [[IP]] addresses (first 4 and last 1 IP) in each Subnet . - These 5 IPs are not available for use and cannot be assigned to an instance - Ex, if CIDR block 10.0.0.0/24 reserved IP are: - 10.0.0.0 - [[network address]] - 10.0.0.1 - reserved by AWS for the [[VPC router]] - 10.0.0.2 - reserved by AWS for mapping to Amazon-provided DNS - 10.0.0.3 - reserved by AWS for future use - 10.0.0.255 - [[Network broadcast address]]. AWS does not support broadcast in a VPC , therefore the address is reserved. Tip: If you need 29 IP addresses for AWS EC2 instances, you can't choose a subnet of size /27 (32 IPs) You need at least 64 [[IP]], [[Subnet size]] /26 (64-5=59 > 29, but 32-5=27<29)","title":"Subnet overview hands on"},{"location":"AWS/VPC/Subnet/#subnet-overview-hands-on","text":"We are navigating to subnets section and creating a new [[public subnet]] with 256 IP addresses ON Availability Zone A. Then the same thing for Availability Zone B (change the Availability zone and CIDR block) Then a [[Private Subnet]] A with 4096 IPs and a [[Private Subnet]] B. When created, we notice that there are less available [[IP]]s than it should. This is because AWS reserves 5 [[IP]] addresses (first 4 and last 1 IP) in each Subnet . - These 5 IPs are not available for use and cannot be assigned to an instance - Ex, if CIDR block 10.0.0.0/24 reserved IP are: - 10.0.0.0 - [[network address]] - 10.0.0.1 - reserved by AWS for the [[VPC router]] - 10.0.0.2 - reserved by AWS for mapping to Amazon-provided DNS - 10.0.0.3 - reserved by AWS for future use - 10.0.0.255 - [[Network broadcast address]]. AWS does not support broadcast in a VPC , therefore the address is reserved. Tip: If you need 29 IP addresses for AWS EC2 instances, you can't choose a subnet of size /27 (32 IPs) You need at least 64 [[IP]], [[Subnet size]] /26 (64-5=59 > 29, but 32-5=27<29)","title":"Subnet overview hands on"},{"location":"AWS/VPC/VPC%20Flow%20Logs%20%2B%20Athena/","text":"VPC Flow Logs + Athena \u00b6 Set up VPC Flow Logs . We can use the documentation at https://docs.aws.amazon.com/athena/latest/ug/vpc-flow-logs.html to analyse the flow logs with AWS Athena .","title":"VPC Flow Logs + Athena"},{"location":"AWS/VPC/VPC%20Flow%20Logs%20%2B%20Athena/#vpc-flow-logs-athena","text":"Set up VPC Flow Logs . We can use the documentation at https://docs.aws.amazon.com/athena/latest/ug/vpc-flow-logs.html to analyse the flow logs with AWS Athena .","title":"VPC Flow Logs + Athena"},{"location":"AWS/VPC/VPC%20Flow%20Logs/","text":"Capture information about IP traffic going into your interfaces VPC Flow Logs [[Subnet Flow Logs]] [[Elastic network Interface Flow Logs]] Helps to [[monitor]] & [[troubleshoot]] [[connectivity issues]] Flow logs data can go to AWS S3 / CloudWatch Logs Captures network information from AWS managed interfaces too: [[Elastic Load Balancer]], AWS RDS , ElastiCache , Redshift , AWS WorkSpaces . Flow Log Syntax \u00b6 <version> <account-id> <interface-id> <srcaddr> <dstaddr> <srcport> <dstport> <protocol> <packets> <bytes> <start> <end> <action> <log-status> srcaddr, dstaddr help identify problematic IP srcport, dstport help identify problematic ports Action: success of failure of the request due to Security Group / Network ACL Can be used for analytics on usage patterns or malicious behaviour Flow logs example https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html#flow-log-records Query VPC flow logs using AWS Athena on AWS S3 or [[CloudWatch Logs Insights]] VPC Flow Logs + Athena","title":"VPC Flow Logs"},{"location":"AWS/VPC/VPC%20Flow%20Logs/#flow-log-syntax","text":"<version> <account-id> <interface-id> <srcaddr> <dstaddr> <srcport> <dstport> <protocol> <packets> <bytes> <start> <end> <action> <log-status> srcaddr, dstaddr help identify problematic IP srcport, dstport help identify problematic ports Action: success of failure of the request due to Security Group / Network ACL Can be used for analytics on usage patterns or malicious behaviour Flow logs example https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html#flow-log-records Query VPC flow logs using AWS Athena on AWS S3 or [[CloudWatch Logs Insights]] VPC Flow Logs + Athena","title":"Flow Log Syntax"},{"location":"AWS/VPC/VPC%20Peering/","text":"VPC Peering \u00b6 Connect two VPC , privately using AWS' network Make them behave as if they were in the same network Must not have overlapping CIDR [[VPC Peering connection]] is not transitive (must be established for each VPC that need to communicate with one another) You must update Route Table in each VPC 's Subnet s to ensure instances can communicate We can create a peering connection to connect our Default VPC with our DemoVPC. Now we have to update the PublicRouteTable and DefaultRouteTable","title":"VPC Peering"},{"location":"AWS/VPC/VPC%20Peering/#vpc-peering","text":"Connect two VPC , privately using AWS' network Make them behave as if they were in the same network Must not have overlapping CIDR [[VPC Peering connection]] is not transitive (must be established for each VPC that need to communicate with one another) You must update Route Table in each VPC 's Subnet s to ensure instances can communicate We can create a peering connection to connect our Default VPC with our DemoVPC. Now we have to update the PublicRouteTable and DefaultRouteTable","title":"VPC Peering"},{"location":"AWS/VPC/VPC%20Summary/","text":"CIDR - [[IP Range]] VPC Summary - VPC => We define a list of [[IPv4]] & [[IPv6]] CIDR Subnet s - tied to an Availability Zone , we define a CIDR Internet Gateway & Route tables - at the VPC Summary level provide [[IPv4]] & [[IPv6]] internet access Route Table s - must be edited to add routes from Subnet s to the Internet Gateway & Route tables s, [[VPC peering connection]]s, VPC endpoint s etc NAT Instance s - gives internet access to instances in [[private subnet]]s. Old, must be setup in a [[public subnet]], disable Source / Destination check flag. NAT Gateway - managed by AWS, provides [[scalable internet access]] to private instances, [[IPv4]] only [[Private DNS]] + AWS Route 53 - enable [[DNS resolution]] + [[DNS hostname]]s ( VPC ) Network ACL - [[stateless]], [[subnet rules]] for inbound and outbound, don't forget [[ephemeral port]]s Security Group s: [[stateful]], operate at the AWS EC2 instance level VPC Peering : Connect two VPC with non overlapping CIDR non transitive VPC endpoint s: Provide private access to AWS Services ( AWS S3 , DynamoDB , CloudFormation , SSM Parameter Store ) within VPC VPC Flow Logs : Can be setup at the VPC / Subnet / [[ENI]] Level for ACCEPT and REJECT traffic, helps identifying attacks, analyse using AWS Athena or [[CloudWatch Logs Insights]] Bastion Host : Public instance to [[ssh]] into, that has SSH connectivity to instances in [[private subnet]]s Site to Site VPN : Setup a Customer Gateway on DC, a Virtual Private Gateway on VPC , and site-to-site VPN over [[public internet]] Direct Connect : Setup a Virtual Private Gateway on VPC , and establish a [[direct private connection]] to an [[AWS Direct Connect Location]] Direct Connect Gateway : setup a Direct Connect to many VPC in different AWS Region Egress Only Internet Gateway : like a NAT Gateway , but for [[IPv6]]","title":"VPC Summary"},{"location":"AWS/VPC/VPC%20endpoint/","text":"VPC endpoint \u00b6 Endpoints allow you to connect to AWS Services using a [[private network]] instead of the [[public network]]. They scale horizontally and are [[redundant]] They remove the need of Internet Gateway & Route tables , [[NAT]], etc to access AWS Services Interface: provisions an [[ENI]] (private IP address) as an entry point (must attach Security Group ) - most AWS service. [[Gateway]]: provisions a target and must be used in a Route Table - AWS S3 and DynamoDB In case of issues: Check [[DNS Setting Resolution]] in your VPC Check Internet Gateway & Route tables Now the [[Private Subnet]] should have access to AWS S3 without any internet access (if we remove that Internet Gateway & Route tables ), provided the AWS EC2 instance has the correct IAM permissions.","title":"VPC endpoint"},{"location":"AWS/VPC/VPC%20endpoint/#vpc-endpoint","text":"Endpoints allow you to connect to AWS Services using a [[private network]] instead of the [[public network]]. They scale horizontally and are [[redundant]] They remove the need of Internet Gateway & Route tables , [[NAT]], etc to access AWS Services Interface: provisions an [[ENI]] (private IP address) as an entry point (must attach Security Group ) - most AWS service. [[Gateway]]: provisions a target and must be used in a Route Table - AWS S3 and DynamoDB In case of issues: Check [[DNS Setting Resolution]] in your VPC Check Internet Gateway & Route tables Now the [[Private Subnet]] should have access to AWS S3 without any internet access (if we remove that Internet Gateway & Route tables ), provided the AWS EC2 instance has the correct IAM permissions.","title":"VPC endpoint"},{"location":"AWS/VPC/VPC/","text":"VPC in AWS - IPv4 \u00b6 VPC = Virtual Private Cloud You can have multiple VPC s in a AWS Region (max 5 per region, but you can place a support ticket to increase the amount) Max CIDR per VPC - 5. For each CIDR : Min size is /28 = 16 IP addresses Max is /16 = 65536 IP addresses Because VPC is private, only the Private IP ranges are allowed 10.0.0.0 - 10.255.255.255 (10.0.0.0/8) 172.16.0.0 - 172.31.255.255 (172.16.0.0/12) 192.168.0.0 - 192.168.255.255 (192.168.0.0/16) Your VPC CIDR should not overlap with your other networks (ex: corporate) Hands on \u00b6 We are going to create an empty VPC, without any wizard. To do this, we are going to navigate to VPC -> Your VPCs in [[AWS Console]]. In the tenancy option we are going to select default, this means that when that we launch AWS EC2 instances, we are going to want [[shared hardware]], not dedicated one that will cost us a lot more money. When created, we can see that it also has a Main Route Table and Main Network ACL created for us as well. Also, we are not limited to only one CIDR Block, we can edit it and put multiple in it (up to 5 blocks).","title":"VPC in AWS - IPv4"},{"location":"AWS/VPC/VPC/#vpc-in-aws-ipv4","text":"VPC = Virtual Private Cloud You can have multiple VPC s in a AWS Region (max 5 per region, but you can place a support ticket to increase the amount) Max CIDR per VPC - 5. For each CIDR : Min size is /28 = 16 IP addresses Max is /16 = 65536 IP addresses Because VPC is private, only the Private IP ranges are allowed 10.0.0.0 - 10.255.255.255 (10.0.0.0/8) 172.16.0.0 - 172.31.255.255 (172.16.0.0/12) 192.168.0.0 - 192.168.255.255 (192.168.0.0/16) Your VPC CIDR should not overlap with your other networks (ex: corporate)","title":"VPC in AWS - IPv4"},{"location":"AWS/VPC/VPC/#hands-on","text":"We are going to create an empty VPC, without any wizard. To do this, we are going to navigate to VPC -> Your VPCs in [[AWS Console]]. In the tenancy option we are going to select default, this means that when that we launch AWS EC2 instances, we are going to want [[shared hardware]], not dedicated one that will cost us a lot more money. When created, we can see that it also has a Main Route Table and Main Network ACL created for us as well. Also, we are not limited to only one CIDR Block, we can edit it and put multiple in it (up to 5 blocks).","title":"Hands on"},{"location":"AWS/VPC/Virtual%20Private%20Gateway/","text":"Virtual Private Gateway: \u00b6 VPN concentrator on the AWS side of the [[VPN connection]] VGW is created and attached to the VPC from which you want to create the Site to Site VPN Possibility to customise the [[ASN]]","title":"Virtual Private Gateway"},{"location":"AWS/VPC/Virtual%20Private%20Gateway/#virtual-private-gateway","text":"VPN concentrator on the AWS side of the [[VPN connection]] VGW is created and attached to the VPC from which you want to create the Site to Site VPN Possibility to customise the [[ASN]]","title":"Virtual Private Gateway:"},{"location":"AWS/WorkSpaces/AWS%20WorkSpaces/","text":"AWS WorkSpaces \u00b6 Managed, Secure Cloud Desktop Great to eliminate management of on-premise VDI ([[Virtual Desktop Infrastructure]]) On demand, pay per by usage Secure, [[Encrypted]], [[Network Isolation]] Integrated with [[Microsoft Active Directory]]","title":"AWS WorkSpaces"},{"location":"AWS/WorkSpaces/AWS%20WorkSpaces/#aws-workspaces","text":"Managed, Secure Cloud Desktop Great to eliminate management of on-premise VDI ([[Virtual Desktop Infrastructure]]) On demand, pay per by usage Secure, [[Encrypted]], [[Network Isolation]] Integrated with [[Microsoft Active Directory]]","title":"AWS WorkSpaces"},{"location":"AWS/aws-lambda-and-the-serverless-framework/","text":"AWS Lambda and the Serverless Framework \u00b6 Sources: - AWS Lambda and the Serverless Framework - Hands On Learning! The code for this is located HERE .","title":"AWS Lambda and the Serverless Framework"},{"location":"AWS/aws-lambda-and-the-serverless-framework/#aws-lambda-and-the-serverless-framework","text":"Sources: - AWS Lambda and the Serverless Framework - Hands On Learning! The code for this is located HERE .","title":"AWS Lambda and the Serverless Framework"},{"location":"AWS/aws-lambda-and-the-serverless-framework/01-getting-started/01-aws-lambda-overview/","text":"AWS Lambda Overview \u00b6 What is AWS \u00b6 AWS is a Cloud Provider They provide you with servers and services that you can use on demand and scale easily. AWS has revolutionized IT over time AWS powers some of the biggest websites in the world (for example Netflix) Recently (Nov 2014) they introduced AWS Lambda Why AWS Lambda \u00b6 EC2 \u00b6 Virtual Servers in the Cloud Limited by RAM and CPU Continuously running Scaling means intervention to add / remove servers Lambda \u00b6 Virtual functions - no servers to manage. Limited by time - short executions Run on-demand Scaling is automated Benefits of AWS Lambda \u00b6 Easy pricing Pay per request and compute time Free tier of 1,000,000 AWS Lambda requests and 400,000 GBs of compute time Integrated with the whole AWS Stack Integrated with many programming languages Easy monitoring through AWS CloudWatch Easy to get more resources per functions (up to 1.5GB of ram) Increasing RAM will also improve CPU and network AWS Lambda Languages \u00b6 aws-nodejs aws-python aws-python3 aws-groovy-gradle aws-java-gradle aws-java-maven aws-scala-sbt aws-csharp AWS Lambda Integrations - main ones \u00b6 API Gateway Kinesis DynamoDB AWS S3 AWS IoT CloudWatch Events CloudWatch Logs AWS SNS AWS Congnito Example: thumbnail creation \u00b6 A new image gets uploaded to S3 Triggers AWS Lambda Function that creates a thumbnail New thumbnail gets uploaded to S3 Metadata uploaded into DynamoDB","title":"AWS Lambda Overview"},{"location":"AWS/aws-lambda-and-the-serverless-framework/01-getting-started/01-aws-lambda-overview/#aws-lambda-overview","text":"","title":"AWS Lambda Overview"},{"location":"AWS/aws-lambda-and-the-serverless-framework/01-getting-started/01-aws-lambda-overview/#what-is-aws","text":"AWS is a Cloud Provider They provide you with servers and services that you can use on demand and scale easily. AWS has revolutionized IT over time AWS powers some of the biggest websites in the world (for example Netflix) Recently (Nov 2014) they introduced AWS Lambda","title":"What is AWS"},{"location":"AWS/aws-lambda-and-the-serverless-framework/01-getting-started/01-aws-lambda-overview/#why-aws-lambda","text":"","title":"Why AWS Lambda"},{"location":"AWS/aws-lambda-and-the-serverless-framework/01-getting-started/01-aws-lambda-overview/#ec2","text":"Virtual Servers in the Cloud Limited by RAM and CPU Continuously running Scaling means intervention to add / remove servers","title":"EC2"},{"location":"AWS/aws-lambda-and-the-serverless-framework/01-getting-started/01-aws-lambda-overview/#lambda","text":"Virtual functions - no servers to manage. Limited by time - short executions Run on-demand Scaling is automated","title":"Lambda"},{"location":"AWS/aws-lambda-and-the-serverless-framework/01-getting-started/01-aws-lambda-overview/#benefits-of-aws-lambda","text":"Easy pricing Pay per request and compute time Free tier of 1,000,000 AWS Lambda requests and 400,000 GBs of compute time Integrated with the whole AWS Stack Integrated with many programming languages Easy monitoring through AWS CloudWatch Easy to get more resources per functions (up to 1.5GB of ram) Increasing RAM will also improve CPU and network","title":"Benefits of AWS Lambda"},{"location":"AWS/aws-lambda-and-the-serverless-framework/01-getting-started/01-aws-lambda-overview/#aws-lambda-languages","text":"aws-nodejs aws-python aws-python3 aws-groovy-gradle aws-java-gradle aws-java-maven aws-scala-sbt aws-csharp","title":"AWS Lambda Languages"},{"location":"AWS/aws-lambda-and-the-serverless-framework/01-getting-started/01-aws-lambda-overview/#aws-lambda-integrations-main-ones","text":"API Gateway Kinesis DynamoDB AWS S3 AWS IoT CloudWatch Events CloudWatch Logs AWS SNS AWS Congnito","title":"AWS Lambda Integrations - main ones"},{"location":"AWS/aws-lambda-and-the-serverless-framework/01-getting-started/01-aws-lambda-overview/#example-thumbnail-creation","text":"A new image gets uploaded to S3 Triggers AWS Lambda Function that creates a thumbnail New thumbnail gets uploaded to S3 Metadata uploaded into DynamoDB","title":"Example: thumbnail creation"},{"location":"AWS/aws-lambda-and-the-serverless-framework/01-getting-started/02-aws-lambda-console/","text":"AWS Lambda Console \u00b6 In this lecture we are going to deploy our first AWS Lambda Function. We are going to do this through the AWS Console, later we'll move on to the serverless framework. Deploying Hello World Function Using AWS Lambda \u00b6 Sign Up for an AWS Account Create our first lambda function Test and run it Creating our Lambda \u00b6 We can access lambda by searching it in the AWS Console. There we can click on Create Function . There we can choose from different types of lambda functions: - Author from scratch - Start with a simple hello world example and change it - Use a blueprint - Build a lambda application from sample code and configuration presets for common use cases - Container Image - Select a container image to deploy for your function - Browse serverless app repository - Deploy a sample lambda application from the AWS Serverless Application Repository We are going to use the hello-world-python blueprint. In the next step we are going to just enter the function name as hello-world-python and leave everything else as-is. Once that's done, our function is successfully created. In this window we can configure the triggers, destinations for the lambda as well as change the code itself. Currently, we are going to test it manually. In the configuration tab we can set things like Permissions, Environment Variables and all kinds of settings. At the bottom we can change the runtime settings, where we can choose the runtime environment, the handler (which method is called) and the architecture. Testing the Lambda \u00b6 We can go to the Test tab and we'll see a screen to configure a new event. We can click the save changes and test. It should succeed: Deleting Lambda \u00b6 We can delete the function by clicking Action and then choosing the delete.","title":"AWS Lambda Console"},{"location":"AWS/aws-lambda-and-the-serverless-framework/01-getting-started/02-aws-lambda-console/#aws-lambda-console","text":"In this lecture we are going to deploy our first AWS Lambda Function. We are going to do this through the AWS Console, later we'll move on to the serverless framework.","title":"AWS Lambda Console"},{"location":"AWS/aws-lambda-and-the-serverless-framework/01-getting-started/02-aws-lambda-console/#deploying-hello-world-function-using-aws-lambda","text":"Sign Up for an AWS Account Create our first lambda function Test and run it","title":"Deploying Hello World Function Using AWS Lambda"},{"location":"AWS/aws-lambda-and-the-serverless-framework/01-getting-started/02-aws-lambda-console/#creating-our-lambda","text":"We can access lambda by searching it in the AWS Console. There we can click on Create Function . There we can choose from different types of lambda functions: - Author from scratch - Start with a simple hello world example and change it - Use a blueprint - Build a lambda application from sample code and configuration presets for common use cases - Container Image - Select a container image to deploy for your function - Browse serverless app repository - Deploy a sample lambda application from the AWS Serverless Application Repository We are going to use the hello-world-python blueprint. In the next step we are going to just enter the function name as hello-world-python and leave everything else as-is. Once that's done, our function is successfully created. In this window we can configure the triggers, destinations for the lambda as well as change the code itself. Currently, we are going to test it manually. In the configuration tab we can set things like Permissions, Environment Variables and all kinds of settings. At the bottom we can change the runtime settings, where we can choose the runtime environment, the handler (which method is called) and the architecture.","title":"Creating our Lambda"},{"location":"AWS/aws-lambda-and-the-serverless-framework/01-getting-started/02-aws-lambda-console/#testing-the-lambda","text":"We can go to the Test tab and we'll see a screen to configure a new event. We can click the save changes and test. It should succeed:","title":"Testing the Lambda"},{"location":"AWS/aws-lambda-and-the-serverless-framework/01-getting-started/02-aws-lambda-console/#deleting-lambda","text":"We can delete the function by clicking Action and then choosing the delete.","title":"Deleting Lambda"},{"location":"AWS/aws-lambda-and-the-serverless-framework/02-aws-lambda-and-serverless/01-installing-serverless/","text":"The Serverless Framwork \u00b6 Serverless Framework (https://serverless.com) aims to ease the pain of creating, deploying, managing and debugging lambda functions. It integrates well with CI/CD tools It has CloudFormation support so your entire stack can be deployed using this Framework Installing Serverless \u00b6 Install dependencies (node & AWS CLI) Install the serverless framework Setting up AWS for the serverless-admin user Download credentials on your machine Setup Serverless to use these credentials # 1. install node # ... # 2. install serverless sudo npm i -g serverless # 3. setup serverless serverless config credentials --provider aws --key XXX --secret YYY --profile serverless-admin For the access credentials we are going to create a new IAM user serverless-admin .","title":"The Serverless Framwork"},{"location":"AWS/aws-lambda-and-the-serverless-framework/02-aws-lambda-and-serverless/01-installing-serverless/#the-serverless-framwork","text":"Serverless Framework (https://serverless.com) aims to ease the pain of creating, deploying, managing and debugging lambda functions. It integrates well with CI/CD tools It has CloudFormation support so your entire stack can be deployed using this Framework","title":"The Serverless Framwork"},{"location":"AWS/aws-lambda-and-the-serverless-framework/02-aws-lambda-and-serverless/01-installing-serverless/#installing-serverless","text":"Install dependencies (node & AWS CLI) Install the serverless framework Setting up AWS for the serverless-admin user Download credentials on your machine Setup Serverless to use these credentials # 1. install node # ... # 2. install serverless sudo npm i -g serverless # 3. setup serverless serverless config credentials --provider aws --key XXX --secret YYY --profile serverless-admin For the access credentials we are going to create a new IAM user serverless-admin .","title":"Installing Serverless"},{"location":"AWS/aws-lambda-and-the-serverless-framework/02-aws-lambda-and-serverless/02-deploying-our-first-function/","text":"Deploying Hello World Function Using Serverless \u00b6 We are going to create an serverless project. We can use either serverless or sls command to do that. \u279c learning-serverless sls create --template aws-python --path hello-world-python Serverless: Generating boilerplate... Serverless: Generating boilerplate in \"/home/davis/projects/learning-serverless/hello-world-python\" Serverless: Successfully generated boilerplate for template: \"aws-python\" The command created a directory hello-world-python with 3 files - .gitignore , handler.py and serverless.yaml . We are going to replace all the contents of the handler.py with: def hello(event, context): print(\"Hi!\") return \"hello-world\" Then we are going to add the profile and region to the serverless.yaml service: hello-world-python frameworkVersion: '2' provider: name: aws runtime: python2.7 lambdaHashingVersion: 20201221 profile: serverless-admin region: eu-west-1 functions: hello: handler: handler.hello Then, we are going to deploy it: \u279c learning-serverless git:(master) cd hello-world-python \u279c hello-world-python git:(master) sls deploy -v Serverless: Packaging service... Serverless: Excluding development dependencies... Serverless: Creating Stack... Serverless: Checking Stack create progress... CloudFormation - CREATE_IN_PROGRESS - AWS::CloudFormation::Stack - hello-world-python-dev CloudFormation - CREATE_IN_PROGRESS - AWS::S3::Bucket - ServerlessDeploymentBucket CloudFormation - CREATE_IN_PROGRESS - AWS::S3::Bucket - ServerlessDeploymentBucket CloudFormation - CREATE_COMPLETE - AWS::S3::Bucket - ServerlessDeploymentBucket CloudFormation - CREATE_IN_PROGRESS - AWS::S3::BucketPolicy - ServerlessDeploymentBucketPolicy CloudFormation - CREATE_IN_PROGRESS - AWS::S3::BucketPolicy - ServerlessDeploymentBucketPolicy CloudFormation - CREATE_COMPLETE - AWS::S3::BucketPolicy - ServerlessDeploymentBucketPolicy CloudFormation - CREATE_COMPLETE - AWS::CloudFormation::Stack - hello-world-python-dev Serverless: Stack create finished... Serverless: Uploading CloudFormation file to S3... Serverless: Uploading artifacts... Serverless: Uploading service hello-world-python.zip file to S3 (454 B)... Serverless: Validating template... Serverless: Updating Stack... Serverless: Checking Stack update progress... CloudFormation - UPDATE_IN_PROGRESS - AWS::CloudFormation::Stack - hello-world-python-dev CloudFormation - CREATE_IN_PROGRESS - AWS::Logs::LogGroup - HelloLogGroup CloudFormation - CREATE_IN_PROGRESS - AWS::IAM::Role - IamRoleLambdaExecution CloudFormation - CREATE_IN_PROGRESS - AWS::IAM::Role - IamRoleLambdaExecution CloudFormation - CREATE_IN_PROGRESS - AWS::Logs::LogGroup - HelloLogGroup CloudFormation - CREATE_COMPLETE - AWS::Logs::LogGroup - HelloLogGroup CloudFormation - CREATE_COMPLETE - AWS::IAM::Role - IamRoleLambdaExecution CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Function - HelloLambdaFunction CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Function - HelloLambdaFunction CloudFormation - CREATE_COMPLETE - AWS::Lambda::Function - HelloLambdaFunction CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Version - HelloLambdaVersionpC2AzkyzeFlxav73Mzt7weVZ7YH18lex6bRJ5zN0SE CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Version - HelloLambdaVersionpC2AzkyzeFlxav73Mzt7weVZ7YH18lex6bRJ5zN0SE CloudFormation - CREATE_COMPLETE - AWS::Lambda::Version - HelloLambdaVersionpC2AzkyzeFlxav73Mzt7weVZ7YH18lex6bRJ5zN0SE CloudFormation - UPDATE_COMPLETE_CLEANUP_IN_PROGRESS - AWS::CloudFormation::Stack - hello-world-python-dev CloudFormation - UPDATE_COMPLETE - AWS::CloudFormation::Stack - hello-world-python-dev Serverless: Stack update finished... Service Information service: hello-world-python stage: dev region: eu-west-1 stack: hello-world-python-dev resources: 6 api keys: None endpoints: functions: hello: hello-world-python-dev-hello layers: None Stack Outputs HelloLambdaFunctionQualifiedArn: arn:aws:lambda:eu-west-1:539690530154:function:hello-world-python-dev-hello:1 ServerlessDeploymentBucketName: hello-world-python-dev-serverlessdeploymentbucket-1gnp3uaf77yre Serverless: Deprecation warning: Starting with v3.0.0, \"-v\" will no longer be supported as alias for \"--verbose\" option. Please use \"--verbose\" flag instead. More Info: https://www.serverless.com/framework/docs/deprecations/#CLI_VERBOSE_OPTION_ALIAS Toggle on monitoring with the Serverless Dashboard: run \"serverless\" Now if we go to our Lambda console, we'll see the created function. And if we test it, it will print the Hi! , return hello-world .","title":"Deploying Hello World Function Using Serverless"},{"location":"AWS/aws-lambda-and-the-serverless-framework/02-aws-lambda-and-serverless/02-deploying-our-first-function/#deploying-hello-world-function-using-serverless","text":"We are going to create an serverless project. We can use either serverless or sls command to do that. \u279c learning-serverless sls create --template aws-python --path hello-world-python Serverless: Generating boilerplate... Serverless: Generating boilerplate in \"/home/davis/projects/learning-serverless/hello-world-python\" Serverless: Successfully generated boilerplate for template: \"aws-python\" The command created a directory hello-world-python with 3 files - .gitignore , handler.py and serverless.yaml . We are going to replace all the contents of the handler.py with: def hello(event, context): print(\"Hi!\") return \"hello-world\" Then we are going to add the profile and region to the serverless.yaml service: hello-world-python frameworkVersion: '2' provider: name: aws runtime: python2.7 lambdaHashingVersion: 20201221 profile: serverless-admin region: eu-west-1 functions: hello: handler: handler.hello Then, we are going to deploy it: \u279c learning-serverless git:(master) cd hello-world-python \u279c hello-world-python git:(master) sls deploy -v Serverless: Packaging service... Serverless: Excluding development dependencies... Serverless: Creating Stack... Serverless: Checking Stack create progress... CloudFormation - CREATE_IN_PROGRESS - AWS::CloudFormation::Stack - hello-world-python-dev CloudFormation - CREATE_IN_PROGRESS - AWS::S3::Bucket - ServerlessDeploymentBucket CloudFormation - CREATE_IN_PROGRESS - AWS::S3::Bucket - ServerlessDeploymentBucket CloudFormation - CREATE_COMPLETE - AWS::S3::Bucket - ServerlessDeploymentBucket CloudFormation - CREATE_IN_PROGRESS - AWS::S3::BucketPolicy - ServerlessDeploymentBucketPolicy CloudFormation - CREATE_IN_PROGRESS - AWS::S3::BucketPolicy - ServerlessDeploymentBucketPolicy CloudFormation - CREATE_COMPLETE - AWS::S3::BucketPolicy - ServerlessDeploymentBucketPolicy CloudFormation - CREATE_COMPLETE - AWS::CloudFormation::Stack - hello-world-python-dev Serverless: Stack create finished... Serverless: Uploading CloudFormation file to S3... Serverless: Uploading artifacts... Serverless: Uploading service hello-world-python.zip file to S3 (454 B)... Serverless: Validating template... Serverless: Updating Stack... Serverless: Checking Stack update progress... CloudFormation - UPDATE_IN_PROGRESS - AWS::CloudFormation::Stack - hello-world-python-dev CloudFormation - CREATE_IN_PROGRESS - AWS::Logs::LogGroup - HelloLogGroup CloudFormation - CREATE_IN_PROGRESS - AWS::IAM::Role - IamRoleLambdaExecution CloudFormation - CREATE_IN_PROGRESS - AWS::IAM::Role - IamRoleLambdaExecution CloudFormation - CREATE_IN_PROGRESS - AWS::Logs::LogGroup - HelloLogGroup CloudFormation - CREATE_COMPLETE - AWS::Logs::LogGroup - HelloLogGroup CloudFormation - CREATE_COMPLETE - AWS::IAM::Role - IamRoleLambdaExecution CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Function - HelloLambdaFunction CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Function - HelloLambdaFunction CloudFormation - CREATE_COMPLETE - AWS::Lambda::Function - HelloLambdaFunction CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Version - HelloLambdaVersionpC2AzkyzeFlxav73Mzt7weVZ7YH18lex6bRJ5zN0SE CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Version - HelloLambdaVersionpC2AzkyzeFlxav73Mzt7weVZ7YH18lex6bRJ5zN0SE CloudFormation - CREATE_COMPLETE - AWS::Lambda::Version - HelloLambdaVersionpC2AzkyzeFlxav73Mzt7weVZ7YH18lex6bRJ5zN0SE CloudFormation - UPDATE_COMPLETE_CLEANUP_IN_PROGRESS - AWS::CloudFormation::Stack - hello-world-python-dev CloudFormation - UPDATE_COMPLETE - AWS::CloudFormation::Stack - hello-world-python-dev Serverless: Stack update finished... Service Information service: hello-world-python stage: dev region: eu-west-1 stack: hello-world-python-dev resources: 6 api keys: None endpoints: functions: hello: hello-world-python-dev-hello layers: None Stack Outputs HelloLambdaFunctionQualifiedArn: arn:aws:lambda:eu-west-1:539690530154:function:hello-world-python-dev-hello:1 ServerlessDeploymentBucketName: hello-world-python-dev-serverlessdeploymentbucket-1gnp3uaf77yre Serverless: Deprecation warning: Starting with v3.0.0, \"-v\" will no longer be supported as alias for \"--verbose\" option. Please use \"--verbose\" flag instead. More Info: https://www.serverless.com/framework/docs/deprecations/#CLI_VERBOSE_OPTION_ALIAS Toggle on monitoring with the Serverless Dashboard: run \"serverless\" Now if we go to our Lambda console, we'll see the created function. And if we test it, it will print the Hi! , return hello-world .","title":"Deploying Hello World Function Using Serverless"},{"location":"AWS/aws-lambda-and-the-serverless-framework/02-aws-lambda-and-serverless/03-running-the-function-from-the-CLI/","text":"Running the function from the CLI \u00b6 Invoking the function from our computer using Serverless \u00b6 We can use the sls invoke to call the function from our local machine. \u279c hello-world-python git:(master) \u2717 sls invoke -f hello -l \"hello-world\" -------------------------------------------------------------------- START RequestId: 0c5349b9-c63a-4017-8f9f-9a8857ce0f91 Version: $LATEST Hi! END RequestId: 0c5349b9-c63a-4017-8f9f-9a8857ce0f91 REPORT RequestId: 0c5349b9-c63a-4017-8f9f-9a8857ce0f91 Duration: 0.36 ms Billed Duration: 1 ms Memory Size: 1024 MB Max Memory Used: 23 MB","title":"Running the function from the CLI"},{"location":"AWS/aws-lambda-and-the-serverless-framework/02-aws-lambda-and-serverless/03-running-the-function-from-the-CLI/#running-the-function-from-the-cli","text":"","title":"Running the function from the CLI"},{"location":"AWS/aws-lambda-and-the-serverless-framework/02-aws-lambda-and-serverless/03-running-the-function-from-the-CLI/#invoking-the-function-from-our-computer-using-serverless","text":"We can use the sls invoke to call the function from our local machine. \u279c hello-world-python git:(master) \u2717 sls invoke -f hello -l \"hello-world\" -------------------------------------------------------------------- START RequestId: 0c5349b9-c63a-4017-8f9f-9a8857ce0f91 Version: $LATEST Hi! END RequestId: 0c5349b9-c63a-4017-8f9f-9a8857ce0f91 REPORT RequestId: 0c5349b9-c63a-4017-8f9f-9a8857ce0f91 Duration: 0.36 ms Billed Duration: 1 ms Memory Size: 1024 MB Max Memory Used: 23 MB","title":"Invoking the function from our computer using Serverless"},{"location":"AWS/aws-lambda-and-the-serverless-framework/02-aws-lambda-and-serverless/04-updating-the-function-from-the-CLI/","text":"Updating the function from the CLI \u00b6 Updating the funciton and deploying the stack \u00b6 We are going to change the function: def hello(event, context): print(\"first update!\") return \"hello-world\" \u279c hello-world-python git:(master) \u2717 nvim \u279c hello-world-python git:(master) \u2717 sls invoke -f hello -l \"hello-world\" -------------------------------------------------------------------- START RequestId: ba4a4095-104a-4b9a-bacb-6a03593d41ba Version: $LATEST Hi! END RequestId: ba4a4095-104a-4b9a-bacb-6a03593d41ba REPORT RequestId: ba4a4095-104a-4b9a-bacb-6a03593d41ba Duration: 0.35 ms Billed Duration: 1 ms Memory Size: 1024 MB Max Memory Used: 23 MB \u279c hello-world-python git:(master) \u2717 sls deploy -v Serverless: Packaging service... Serverless: Excluding development dependencies... Serverless: Uploading CloudFormation file to S3... Serverless: Uploading artifacts... Serverless: Uploading service hello-world-python.zip file to S3 (464 B)... Serverless: Validating template... Serverless: Updating Stack... Serverless: Checking Stack update progress... CloudFormation - UPDATE_IN_PROGRESS - AWS::CloudFormation::Stack - hello-world-python-dev CloudFormation - UPDATE_IN_PROGRESS - AWS::Lambda::Function - HelloLambdaFunction CloudFormation - UPDATE_COMPLETE - AWS::Lambda::Function - HelloLambdaFunction CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Version - HelloLambdaVersionnYuVKWVqZo3ayPZ2VhECVLLDq2S1214n8jqgJs4e0g CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Version - HelloLambdaVersionnYuVKWVqZo3ayPZ2VhECVLLDq2S1214n8jqgJs4e0g CloudFormation - CREATE_COMPLETE - AWS::Lambda::Version - HelloLambdaVersionnYuVKWVqZo3ayPZ2VhECVLLDq2S1214n8jqgJs4e0g CloudFormation - UPDATE_COMPLETE_CLEANUP_IN_PROGRESS - AWS::CloudFormation::Stack - hello-world-python-dev CloudFormation - DELETE_SKIPPED - AWS::Lambda::Version - HelloLambdaVersionpC2AzkyzeFlxav73Mzt7weVZ7YH18lex6bRJ5zN0SE CloudFormation - UPDATE_COMPLETE - AWS::CloudFormation::Stack - hello-world-python-dev Serverless: Stack update finished... Service Information service: hello-world-python stage: dev region: eu-west-1 stack: hello-world-python-dev resources: 6 api keys: None endpoints: functions: hello: hello-world-python-dev-hello layers: None Stack Outputs HelloLambdaFunctionQualifiedArn: arn:aws:lambda:eu-west-1:539690530154:function:hello-world-python-dev-hello:2 ServerlessDeploymentBucketName: hello-world-python-dev-serverlessdeploymentbucket-1gnp3uaf77yre Serverless: Deprecation warning: Starting with v3.0.0, \"-v\" will no longer be supported as alias for \"--verbose\" option. Please use \"--verbose\" flag instead. More Info: https://www.serverless.com/framework/docs/deprecations/#CLI_VERBOSE_OPTION_ALIAS Toggle on monitoring with the Serverless Dashboard: run \"serverless\" \u279c hello-world-python git:(master) \u2717 sls invoke -f hello -l \"hello-world\" -------------------------------------------------------------------- START RequestId: 6592a35a-19cf-4ea4-974c-fee4d4a775f5 Version: $LATEST first update! END RequestId: 6592a35a-19cf-4ea4-974c-fee4d4a775f5 REPORT RequestId: 6592a35a-19cf-4ea4-974c-fee4d4a775f5 Duration: 0.22 ms Billed Duration: 1 ms Memory Size: 1024 MB Max Memory Used: 23 MB Init Duration: 0.80 ms \u279c hello-world-python git:(master) \u2717 Deploying the function without updating the whole stack \u00b6 Now we are going to update the function once more: def hello(event, context): print(\"second update!\") return \"hello-world\" \u279c hello-world-python git:(master) \u2717 sls invoke -f hello -l \"hello-world\" -------------------------------------------------------------------- START RequestId: a2bfe322-a4d7-450b-a2b8-e2f99fd41710 Version: $LATEST first update! END RequestId: a2bfe322-a4d7-450b-a2b8-e2f99fd41710 REPORT RequestId: a2bfe322-a4d7-450b-a2b8-e2f99fd41710 Duration: 0.27 ms Billed Duration: 1 ms Memory Size: 1024 MB Max Memory Used: 23 MB \u279c hello-world-python git:(master) \u2717 sls deploy function -f hello Serverless: Packaging function: hello... Serverless: Excluding development dependencies... Serverless: Uploading function: hello (464 B)... Serverless: Successfully deployed function: hello Serverless: Configuration did not change. Skipping function configuration update. \u279c hello-world-python git:(master) \u2717 sls invoke -f hello -l \"hello-world\" -------------------------------------------------------------------- START RequestId: 9c7252a0-58b9-4504-8be8-b52709a9af1c Version: $LATEST second update! END RequestId: 9c7252a0-58b9-4504-8be8-b52709a9af1c REPORT RequestId: 9c7252a0-58b9-4504-8be8-b52709a9af1c Duration: 0.25 ms Billed Duration: 1 ms Memory Size: 1024 MB Max Memory Used: 23 MB Init Duration: 0.81 ms \u279c hello-world-python git:(master) \u2717","title":"Updating the function from the CLI"},{"location":"AWS/aws-lambda-and-the-serverless-framework/02-aws-lambda-and-serverless/04-updating-the-function-from-the-CLI/#updating-the-function-from-the-cli","text":"","title":"Updating the function from the CLI"},{"location":"AWS/aws-lambda-and-the-serverless-framework/02-aws-lambda-and-serverless/04-updating-the-function-from-the-CLI/#updating-the-funciton-and-deploying-the-stack","text":"We are going to change the function: def hello(event, context): print(\"first update!\") return \"hello-world\" \u279c hello-world-python git:(master) \u2717 nvim \u279c hello-world-python git:(master) \u2717 sls invoke -f hello -l \"hello-world\" -------------------------------------------------------------------- START RequestId: ba4a4095-104a-4b9a-bacb-6a03593d41ba Version: $LATEST Hi! END RequestId: ba4a4095-104a-4b9a-bacb-6a03593d41ba REPORT RequestId: ba4a4095-104a-4b9a-bacb-6a03593d41ba Duration: 0.35 ms Billed Duration: 1 ms Memory Size: 1024 MB Max Memory Used: 23 MB \u279c hello-world-python git:(master) \u2717 sls deploy -v Serverless: Packaging service... Serverless: Excluding development dependencies... Serverless: Uploading CloudFormation file to S3... Serverless: Uploading artifacts... Serverless: Uploading service hello-world-python.zip file to S3 (464 B)... Serverless: Validating template... Serverless: Updating Stack... Serverless: Checking Stack update progress... CloudFormation - UPDATE_IN_PROGRESS - AWS::CloudFormation::Stack - hello-world-python-dev CloudFormation - UPDATE_IN_PROGRESS - AWS::Lambda::Function - HelloLambdaFunction CloudFormation - UPDATE_COMPLETE - AWS::Lambda::Function - HelloLambdaFunction CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Version - HelloLambdaVersionnYuVKWVqZo3ayPZ2VhECVLLDq2S1214n8jqgJs4e0g CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Version - HelloLambdaVersionnYuVKWVqZo3ayPZ2VhECVLLDq2S1214n8jqgJs4e0g CloudFormation - CREATE_COMPLETE - AWS::Lambda::Version - HelloLambdaVersionnYuVKWVqZo3ayPZ2VhECVLLDq2S1214n8jqgJs4e0g CloudFormation - UPDATE_COMPLETE_CLEANUP_IN_PROGRESS - AWS::CloudFormation::Stack - hello-world-python-dev CloudFormation - DELETE_SKIPPED - AWS::Lambda::Version - HelloLambdaVersionpC2AzkyzeFlxav73Mzt7weVZ7YH18lex6bRJ5zN0SE CloudFormation - UPDATE_COMPLETE - AWS::CloudFormation::Stack - hello-world-python-dev Serverless: Stack update finished... Service Information service: hello-world-python stage: dev region: eu-west-1 stack: hello-world-python-dev resources: 6 api keys: None endpoints: functions: hello: hello-world-python-dev-hello layers: None Stack Outputs HelloLambdaFunctionQualifiedArn: arn:aws:lambda:eu-west-1:539690530154:function:hello-world-python-dev-hello:2 ServerlessDeploymentBucketName: hello-world-python-dev-serverlessdeploymentbucket-1gnp3uaf77yre Serverless: Deprecation warning: Starting with v3.0.0, \"-v\" will no longer be supported as alias for \"--verbose\" option. Please use \"--verbose\" flag instead. More Info: https://www.serverless.com/framework/docs/deprecations/#CLI_VERBOSE_OPTION_ALIAS Toggle on monitoring with the Serverless Dashboard: run \"serverless\" \u279c hello-world-python git:(master) \u2717 sls invoke -f hello -l \"hello-world\" -------------------------------------------------------------------- START RequestId: 6592a35a-19cf-4ea4-974c-fee4d4a775f5 Version: $LATEST first update! END RequestId: 6592a35a-19cf-4ea4-974c-fee4d4a775f5 REPORT RequestId: 6592a35a-19cf-4ea4-974c-fee4d4a775f5 Duration: 0.22 ms Billed Duration: 1 ms Memory Size: 1024 MB Max Memory Used: 23 MB Init Duration: 0.80 ms \u279c hello-world-python git:(master) \u2717","title":"Updating the funciton and deploying the stack"},{"location":"AWS/aws-lambda-and-the-serverless-framework/02-aws-lambda-and-serverless/04-updating-the-function-from-the-CLI/#deploying-the-function-without-updating-the-whole-stack","text":"Now we are going to update the function once more: def hello(event, context): print(\"second update!\") return \"hello-world\" \u279c hello-world-python git:(master) \u2717 sls invoke -f hello -l \"hello-world\" -------------------------------------------------------------------- START RequestId: a2bfe322-a4d7-450b-a2b8-e2f99fd41710 Version: $LATEST first update! END RequestId: a2bfe322-a4d7-450b-a2b8-e2f99fd41710 REPORT RequestId: a2bfe322-a4d7-450b-a2b8-e2f99fd41710 Duration: 0.27 ms Billed Duration: 1 ms Memory Size: 1024 MB Max Memory Used: 23 MB \u279c hello-world-python git:(master) \u2717 sls deploy function -f hello Serverless: Packaging function: hello... Serverless: Excluding development dependencies... Serverless: Uploading function: hello (464 B)... Serverless: Successfully deployed function: hello Serverless: Configuration did not change. Skipping function configuration update. \u279c hello-world-python git:(master) \u2717 sls invoke -f hello -l \"hello-world\" -------------------------------------------------------------------- START RequestId: 9c7252a0-58b9-4504-8be8-b52709a9af1c Version: $LATEST second update! END RequestId: 9c7252a0-58b9-4504-8be8-b52709a9af1c REPORT RequestId: 9c7252a0-58b9-4504-8be8-b52709a9af1c Duration: 0.25 ms Billed Duration: 1 ms Memory Size: 1024 MB Max Memory Used: 23 MB Init Duration: 0.81 ms \u279c hello-world-python git:(master) \u2717","title":"Deploying the function without updating the whole stack"},{"location":"AWS/aws-lambda-and-the-serverless-framework/02-aws-lambda-and-serverless/05-fetching-function-logs-from-the-CLI/","text":"Fetching function logs from the CLI \u00b6 View function logs in AWS CloudWatch logs \u00b6 We can open up the Monitoring tab in the function and see some metrics. To see the logs, we can click on the View logs in CloudWatch button. There will be a list of requests that we can get logs for. Choose one of them. Stream function logs to our computer using Serverless \u00b6 \u279c hello-world-python git:(master) \u2717 sls logs -f hello -t START RequestId: 00d945d0-ea90-4a75-a935-dc895480109e Version: $LATEST second update! END RequestId: 00d945d0-ea90-4a75-a935-dc895480109e REPORT RequestId: 00d945d0-ea90-4a75-a935-dc895480109e Duration: 0.22 ms Billed Duration: 1 ms Memory Size: 1024 MB Max Memory Used: 23 MB Init Duration: 0.66 ms Note, that this will stream the logs as they happen. It will continuously poll for new logs and show them.","title":"Fetching function logs from the CLI"},{"location":"AWS/aws-lambda-and-the-serverless-framework/02-aws-lambda-and-serverless/05-fetching-function-logs-from-the-CLI/#fetching-function-logs-from-the-cli","text":"","title":"Fetching function logs from the CLI"},{"location":"AWS/aws-lambda-and-the-serverless-framework/02-aws-lambda-and-serverless/05-fetching-function-logs-from-the-CLI/#view-function-logs-in-aws-cloudwatch-logs","text":"We can open up the Monitoring tab in the function and see some metrics. To see the logs, we can click on the View logs in CloudWatch button. There will be a list of requests that we can get logs for. Choose one of them.","title":"View function logs in AWS CloudWatch logs"},{"location":"AWS/aws-lambda-and-the-serverless-framework/02-aws-lambda-and-serverless/05-fetching-function-logs-from-the-CLI/#stream-function-logs-to-our-computer-using-serverless","text":"\u279c hello-world-python git:(master) \u2717 sls logs -f hello -t START RequestId: 00d945d0-ea90-4a75-a935-dc895480109e Version: $LATEST second update! END RequestId: 00d945d0-ea90-4a75-a935-dc895480109e REPORT RequestId: 00d945d0-ea90-4a75-a935-dc895480109e Duration: 0.22 ms Billed Duration: 1 ms Memory Size: 1024 MB Max Memory Used: 23 MB Init Duration: 0.66 ms Note, that this will stream the logs as they happen. It will continuously poll for new logs and show them.","title":"Stream function logs to our computer using Serverless"},{"location":"AWS/aws-lambda-and-the-serverless-framework/02-aws-lambda-and-serverless/06-removing-the-function/","text":"Removing the funciton \u00b6 We need to clean up following things: - Function - Dependencies of the function - CloudWatch Log Groups - IAM Roles - Everything else the framework has created Thankfully, the serverless solves this problem for us. \u279c hello-world-python git:(master) sls remove Serverless: Getting all objects in S3 bucket... Serverless: Removing objects in S3 bucket... Serverless: Removing Stack... Serverless: Checking Stack delete progress... ..... Serverless: Stack delete finished... Serverless: Stack delete finished...","title":"Removing the funciton"},{"location":"AWS/aws-lambda-and-the-serverless-framework/02-aws-lambda-and-serverless/06-removing-the-function/#removing-the-funciton","text":"We need to clean up following things: - Function - Dependencies of the function - CloudWatch Log Groups - IAM Roles - Everything else the framework has created Thankfully, the serverless solves this problem for us. \u279c hello-world-python git:(master) sls remove Serverless: Getting all objects in S3 bucket... Serverless: Removing objects in S3 bucket... Serverless: Removing Stack... Serverless: Checking Stack delete progress... ..... Serverless: Stack delete finished... Serverless: Stack delete finished...","title":"Removing the funciton"},{"location":"AWS/aws-lambda-and-the-serverless-framework/02-aws-lambda-and-serverless/07-section-summary/","text":"Section summary \u00b6 We learned how to deploy, run, stream logs and destroy Lambda functions. Advantages of using this framework: - Everything using programming (not manual clicks in the AWS Console) - Can be integrated with Continous Integration Frameworks - Can be integrated with Continous Delivery frameworks - We barely scratched the surface of this framework!","title":"Section summary"},{"location":"AWS/aws-lambda-and-the-serverless-framework/02-aws-lambda-and-serverless/07-section-summary/#section-summary","text":"We learned how to deploy, run, stream logs and destroy Lambda functions. Advantages of using this framework: - Everything using programming (not manual clicks in the AWS Console) - Can be integrated with Continous Integration Frameworks - Can be integrated with Continous Delivery frameworks - We barely scratched the surface of this framework!","title":"Section summary"},{"location":"AWS/aws-lambda-and-the-serverless-framework/03-aws-lambda-and-the-serverless-in-depth/01-function-timeout-and-memory/","text":"AWS Functions Timeout and Memory \u00b6 We are going to change the serverless.yaml configuration like so: service: hello-world-python frameworkVersion: '2' provider: name: aws runtime: python2.7 lambdaHashingVersion: 20201221 profile: serverless-admin region: eu-west-1 functions: hello-short-timeout: handler: handler.hello memorySize: 128 timeout: 3 hello-long-timeout: handler: handler.hello memorySize: 256 timeout: 6 And update the function: import time def hello(event, context): print(\"second update!\") time.sleep(4) return \"hello-world\" And deploy it: \u279c hello-world-python git:(master) \u2717 sls deploy -v [15/491] Serverless: Packaging service... Serverless: Excluding development dependencies... Serverless: Creating Stack... Serverless: Checking Stack create progress... CloudFormation - CREATE_IN_PROGRESS - AWS::CloudFormation::Stack - hello-world-python-dev CloudFormation - CREATE_IN_PROGRESS - AWS::S3::Bucket - ServerlessDeploymentBucket CloudFormation - CREATE_IN_PROGRESS - AWS::S3::Bucket - ServerlessDeploymentBucket CloudFormation - CREATE_COMPLETE - AWS::S3::Bucket - ServerlessDeploymentBucket CloudFormation - CREATE_IN_PROGRESS - AWS::S3::BucketPolicy - ServerlessDeploymentBucketPolicy CloudFormation - CREATE_IN_PROGRESS - AWS::S3::BucketPolicy - ServerlessDeploymentBucketPolicy CloudFormation - CREATE_COMPLETE - AWS::S3::BucketPolicy - ServerlessDeploymentBucketPolicy CloudFormation - CREATE_COMPLETE - AWS::CloudFormation::Stack - hello-world-python-dev Serverless: Stack create finished... Serverless: Uploading CloudFormation file to S3... Serverless: Uploading artifacts... Serverless: Uploading service hello-world-python.zip file to S3 (481 B)... Serverless: Validating template... Serverless: Updating Stack... Serverless: Checking Stack update progress... CloudFormation - UPDATE_IN_PROGRESS - AWS::CloudFormation::Stack - hello-world-python-dev CloudFormation - CREATE_IN_PROGRESS - AWS::Logs::LogGroup - HelloDashlongDashtimeoutLogGroup CloudFormation - CREATE_IN_PROGRESS - AWS::IAM::Role - IamRoleLambdaExecution CloudFormation - CREATE_IN_PROGRESS - AWS::Logs::LogGroup - HelloDashshortDashtimeoutLogGroup CloudFormation - CREATE_IN_PROGRESS - AWS::IAM::Role - IamRoleLambdaExecution CloudFormation - CREATE_IN_PROGRESS - AWS::Logs::LogGroup - HelloDashlongDashtimeoutLogGroup CloudFormation - CREATE_COMPLETE - AWS::Logs::LogGroup - HelloDashlongDashtimeoutLogGroup CloudFormation - CREATE_IN_PROGRESS - AWS::Logs::LogGroup - HelloDashshortDashtimeoutLogGroup CloudFormation - CREATE_COMPLETE - AWS::Logs::LogGroup - HelloDashshortDashtimeoutLogGroup CloudFormation - CREATE_COMPLETE - AWS::IAM::Role - IamRoleLambdaExecution CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Function - HelloDashlongDashtimeoutLambdaFunction CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Function - HelloDashshortDashtimeoutLambdaFunction CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Function - HelloDashlongDashtimeoutLambdaFunction CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Function - HelloDashshortDashtimeoutLambdaFunction CloudFormation - CREATE_COMPLETE - AWS::Lambda::Function - HelloDashlongDashtimeoutLambdaFunction CloudFormation - CREATE_COMPLETE - AWS::Lambda::Function - HelloDashshortDashtimeoutLambdaFunction CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Version - HelloDashlongDashtimeoutLambdaVersion3kQV2oC0POBMifyDGFV4BsSfc7frlb3RPwdnjzQPFgM CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Version - HelloDashlongDashtimeoutLambdaVersion3kQV2oC0POBMifyDGFV4BsSfc7frlb3RPwdnjzQPFgM CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Version - HelloDashshortDashtimeoutLambdaVersionznsbIl9hJRSWGiJqJdhDyz4W89MX5Gwoo9dGTsCttw CloudFormation - CREATE_COMPLETE - AWS::Lambda::Version - HelloDashlongDashtimeoutLambdaVersion3kQV2oC0POBMifyDGFV4BsSfc7frlb3RPwdnjzQPFgM CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Version - HelloDashshortDashtimeoutLambdaVersionznsbIl9hJRSWGiJqJdhDyz4W89MX5Gwoo9dGTsCttw CloudFormation - CREATE_COMPLETE - AWS::Lambda::Version - HelloDashshortDashtimeoutLambdaVersionznsb Il9hJRSWGiJqJdhDyz4W89MX5Gwoo9dGTsCttw CloudFormation - UPDATE_COMPLETE_CLEANUP_IN_PROGRESS - AWS::CloudFormation::Stack - hello-world-pyth on-dev CloudFormation - UPDATE_COMPLETE - AWS::CloudFormation::Stack - hello-world-python-dev Serverless: Stack update finished... Service Information service: hello-world-python stage: dev region: eu-west-1 stack: hello-world-python-dev resources: 9 api keys: None endpoints: functions: hello-short-timeout: hello-world-python-dev-hello-short-timeout hello-long-timeout: hello-world-python-dev-hello-long-timeout layers: None Stack Outputs HelloDashshortDashtimeoutLambdaFunctionQualifiedArn: arn:aws:lambda:eu-west-1:539690530154:function:hello-world-python-dev-hello-short-timeout:1 ServerlessDeploymentBucketName: hello-world-python-dev-serverlessdeploymentbucket-13qatxs10r6fv HelloDashlongDashtimeoutLambdaFunctionQualifiedArn: arn:aws:lambda:eu-west-1:539690530154:function:hello-world-python-dev-hello-long-timeout:1 Serverless: Deprecation warning: Starting with v3.0.0, \"-v\" will no longer be supported as alias for \"--verbose\" option. Please use \"--verbose\" flag instead. More Info: https://www.serverless.com/framework/docs/deprecations/#CLI_VERBOSE_OPTION_ALIAS Toggle on monitoring with the Serverless Dashboard: run \"serverless\" Now we can test them: \u279c hello-world-python git:(master) \u2717 sls invoke -f hello-short-timeout -l { \"errorMessage\": \"2021-10-12T11:24:59.786Z 4fd2df20-4cd4-4883-b84c-6b8f8b38f0bc Task timed out after 3.00 seconds\" } -------------------------------------------------------------------- START RequestId: 4fd2df20-4cd4-4883-b84c-6b8f8b38f0bc Version: $LATEST second update! END RequestId: 4fd2df20-4cd4-4883-b84c-6b8f8b38f0bc REPORT RequestId: 4fd2df20-4cd4-4883-b84c-6b8f8b38f0bc Duration: 3003.17 ms Billed Duration: 3000 ms Memory Size: 128 MB Max Memory Used: 23 MB Init Duration: 2.01 ms 2021-10-12T11:24:59.786Z 4fd2df20-4cd4-4883-b84c-6b8f8b38f0bc Task timed out after 3.00 seconds Serverless Error ---------------------------------------- Invoked function failed Get Support -------------------------------------------- Docs: docs.serverless.com Bugs: github.com/serverless/serverless/issues Issues: forum.serverless.com Your Environment Information --------------------------- Operating System: linux Node Version: 16.10.0 Framework Version: 2.62.0 Plugin Version: 5.4.6 SDK Version: 4.3.0 Components Version: 3.17.1 \u279c hello-world-python git:(master) \u2717 sls invoke -f hello-long-timeout -l \"hello-world\" -------------------------------------------------------------------- START RequestId: 21066957-d8a2-44b9-8c69-110c31030b1b Version: $LATEST second update! END RequestId: 21066957-d8a2-44b9-8c69-110c31030b1b REPORT RequestId: 21066957-d8a2-44b9-8c69-110c31030b1b Duration: 4004.47 ms Billed Duration: 4005 ms Memory Size: 256 MB Max Memory Used: 23 MB Init Duration: 0.86 ms \u279c hello-world-python git:(master) \u2717 So, the short timeout function failed while the long one succeeded. The morale of the story - you can limit your timeouts to limit your spending, but make sure to track your run times as it can kill your functions as well. We can also configure this under provider: service: hello-world-python frameworkVersion: '2' provider: name: aws runtime: python2.7 lambdaHashingVersion: 20201221 profile: serverless-admin region: eu-west-1 memorySize: 512 timeout: 2 functions: hello-short-timeout: handler: handler.hello hello-long-timeout: handler: handler.hello memorySize: 256 timeout: 6","title":"AWS Functions Timeout and Memory"},{"location":"AWS/aws-lambda-and-the-serverless-framework/03-aws-lambda-and-the-serverless-in-depth/01-function-timeout-and-memory/#aws-functions-timeout-and-memory","text":"We are going to change the serverless.yaml configuration like so: service: hello-world-python frameworkVersion: '2' provider: name: aws runtime: python2.7 lambdaHashingVersion: 20201221 profile: serverless-admin region: eu-west-1 functions: hello-short-timeout: handler: handler.hello memorySize: 128 timeout: 3 hello-long-timeout: handler: handler.hello memorySize: 256 timeout: 6 And update the function: import time def hello(event, context): print(\"second update!\") time.sleep(4) return \"hello-world\" And deploy it: \u279c hello-world-python git:(master) \u2717 sls deploy -v [15/491] Serverless: Packaging service... Serverless: Excluding development dependencies... Serverless: Creating Stack... Serverless: Checking Stack create progress... CloudFormation - CREATE_IN_PROGRESS - AWS::CloudFormation::Stack - hello-world-python-dev CloudFormation - CREATE_IN_PROGRESS - AWS::S3::Bucket - ServerlessDeploymentBucket CloudFormation - CREATE_IN_PROGRESS - AWS::S3::Bucket - ServerlessDeploymentBucket CloudFormation - CREATE_COMPLETE - AWS::S3::Bucket - ServerlessDeploymentBucket CloudFormation - CREATE_IN_PROGRESS - AWS::S3::BucketPolicy - ServerlessDeploymentBucketPolicy CloudFormation - CREATE_IN_PROGRESS - AWS::S3::BucketPolicy - ServerlessDeploymentBucketPolicy CloudFormation - CREATE_COMPLETE - AWS::S3::BucketPolicy - ServerlessDeploymentBucketPolicy CloudFormation - CREATE_COMPLETE - AWS::CloudFormation::Stack - hello-world-python-dev Serverless: Stack create finished... Serverless: Uploading CloudFormation file to S3... Serverless: Uploading artifacts... Serverless: Uploading service hello-world-python.zip file to S3 (481 B)... Serverless: Validating template... Serverless: Updating Stack... Serverless: Checking Stack update progress... CloudFormation - UPDATE_IN_PROGRESS - AWS::CloudFormation::Stack - hello-world-python-dev CloudFormation - CREATE_IN_PROGRESS - AWS::Logs::LogGroup - HelloDashlongDashtimeoutLogGroup CloudFormation - CREATE_IN_PROGRESS - AWS::IAM::Role - IamRoleLambdaExecution CloudFormation - CREATE_IN_PROGRESS - AWS::Logs::LogGroup - HelloDashshortDashtimeoutLogGroup CloudFormation - CREATE_IN_PROGRESS - AWS::IAM::Role - IamRoleLambdaExecution CloudFormation - CREATE_IN_PROGRESS - AWS::Logs::LogGroup - HelloDashlongDashtimeoutLogGroup CloudFormation - CREATE_COMPLETE - AWS::Logs::LogGroup - HelloDashlongDashtimeoutLogGroup CloudFormation - CREATE_IN_PROGRESS - AWS::Logs::LogGroup - HelloDashshortDashtimeoutLogGroup CloudFormation - CREATE_COMPLETE - AWS::Logs::LogGroup - HelloDashshortDashtimeoutLogGroup CloudFormation - CREATE_COMPLETE - AWS::IAM::Role - IamRoleLambdaExecution CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Function - HelloDashlongDashtimeoutLambdaFunction CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Function - HelloDashshortDashtimeoutLambdaFunction CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Function - HelloDashlongDashtimeoutLambdaFunction CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Function - HelloDashshortDashtimeoutLambdaFunction CloudFormation - CREATE_COMPLETE - AWS::Lambda::Function - HelloDashlongDashtimeoutLambdaFunction CloudFormation - CREATE_COMPLETE - AWS::Lambda::Function - HelloDashshortDashtimeoutLambdaFunction CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Version - HelloDashlongDashtimeoutLambdaVersion3kQV2oC0POBMifyDGFV4BsSfc7frlb3RPwdnjzQPFgM CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Version - HelloDashlongDashtimeoutLambdaVersion3kQV2oC0POBMifyDGFV4BsSfc7frlb3RPwdnjzQPFgM CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Version - HelloDashshortDashtimeoutLambdaVersionznsbIl9hJRSWGiJqJdhDyz4W89MX5Gwoo9dGTsCttw CloudFormation - CREATE_COMPLETE - AWS::Lambda::Version - HelloDashlongDashtimeoutLambdaVersion3kQV2oC0POBMifyDGFV4BsSfc7frlb3RPwdnjzQPFgM CloudFormation - CREATE_IN_PROGRESS - AWS::Lambda::Version - HelloDashshortDashtimeoutLambdaVersionznsbIl9hJRSWGiJqJdhDyz4W89MX5Gwoo9dGTsCttw CloudFormation - CREATE_COMPLETE - AWS::Lambda::Version - HelloDashshortDashtimeoutLambdaVersionznsb Il9hJRSWGiJqJdhDyz4W89MX5Gwoo9dGTsCttw CloudFormation - UPDATE_COMPLETE_CLEANUP_IN_PROGRESS - AWS::CloudFormation::Stack - hello-world-pyth on-dev CloudFormation - UPDATE_COMPLETE - AWS::CloudFormation::Stack - hello-world-python-dev Serverless: Stack update finished... Service Information service: hello-world-python stage: dev region: eu-west-1 stack: hello-world-python-dev resources: 9 api keys: None endpoints: functions: hello-short-timeout: hello-world-python-dev-hello-short-timeout hello-long-timeout: hello-world-python-dev-hello-long-timeout layers: None Stack Outputs HelloDashshortDashtimeoutLambdaFunctionQualifiedArn: arn:aws:lambda:eu-west-1:539690530154:function:hello-world-python-dev-hello-short-timeout:1 ServerlessDeploymentBucketName: hello-world-python-dev-serverlessdeploymentbucket-13qatxs10r6fv HelloDashlongDashtimeoutLambdaFunctionQualifiedArn: arn:aws:lambda:eu-west-1:539690530154:function:hello-world-python-dev-hello-long-timeout:1 Serverless: Deprecation warning: Starting with v3.0.0, \"-v\" will no longer be supported as alias for \"--verbose\" option. Please use \"--verbose\" flag instead. More Info: https://www.serverless.com/framework/docs/deprecations/#CLI_VERBOSE_OPTION_ALIAS Toggle on monitoring with the Serverless Dashboard: run \"serverless\" Now we can test them: \u279c hello-world-python git:(master) \u2717 sls invoke -f hello-short-timeout -l { \"errorMessage\": \"2021-10-12T11:24:59.786Z 4fd2df20-4cd4-4883-b84c-6b8f8b38f0bc Task timed out after 3.00 seconds\" } -------------------------------------------------------------------- START RequestId: 4fd2df20-4cd4-4883-b84c-6b8f8b38f0bc Version: $LATEST second update! END RequestId: 4fd2df20-4cd4-4883-b84c-6b8f8b38f0bc REPORT RequestId: 4fd2df20-4cd4-4883-b84c-6b8f8b38f0bc Duration: 3003.17 ms Billed Duration: 3000 ms Memory Size: 128 MB Max Memory Used: 23 MB Init Duration: 2.01 ms 2021-10-12T11:24:59.786Z 4fd2df20-4cd4-4883-b84c-6b8f8b38f0bc Task timed out after 3.00 seconds Serverless Error ---------------------------------------- Invoked function failed Get Support -------------------------------------------- Docs: docs.serverless.com Bugs: github.com/serverless/serverless/issues Issues: forum.serverless.com Your Environment Information --------------------------- Operating System: linux Node Version: 16.10.0 Framework Version: 2.62.0 Plugin Version: 5.4.6 SDK Version: 4.3.0 Components Version: 3.17.1 \u279c hello-world-python git:(master) \u2717 sls invoke -f hello-long-timeout -l \"hello-world\" -------------------------------------------------------------------- START RequestId: 21066957-d8a2-44b9-8c69-110c31030b1b Version: $LATEST second update! END RequestId: 21066957-d8a2-44b9-8c69-110c31030b1b REPORT RequestId: 21066957-d8a2-44b9-8c69-110c31030b1b Duration: 4004.47 ms Billed Duration: 4005 ms Memory Size: 256 MB Max Memory Used: 23 MB Init Duration: 0.86 ms \u279c hello-world-python git:(master) \u2717 So, the short timeout function failed while the long one succeeded. The morale of the story - you can limit your timeouts to limit your spending, but make sure to track your run times as it can kill your functions as well. We can also configure this under provider: service: hello-world-python frameworkVersion: '2' provider: name: aws runtime: python2.7 lambdaHashingVersion: 20201221 profile: serverless-admin region: eu-west-1 memorySize: 512 timeout: 2 functions: hello-short-timeout: handler: handler.hello hello-long-timeout: handler: handler.hello memorySize: 256 timeout: 6","title":"AWS Functions Timeout and Memory"},{"location":"AWS/aws-lambda-and-the-serverless-framework/03-aws-lambda-and-the-serverless-in-depth/02-iam-permissions-for-lambda-functions/","text":"IAM Permissions for Lambda Functions \u00b6 Our lambda functions access other services like S3 to store images or dynamoDB to store and retrieve data. By default our lambda functions are not authorized to do that. For this, we provide an IAM policy which allows you to entirely secure your AWS setup. For this we are going to create a new project. \u279c learning-serverless git:(master) sls create --template aws-python --path python-example-iam Serverless: Generating boilerplate... Serverless: Generating boilerplate in \"/home/davis/projects/learning-serverless/python-example-iam\" Serverless: Successfully generated boilerplate for template: \"aws-python\" And create a following function: import boto3 def hello(event, context): client = boto3.client('lambda') response = client.list_functions() return response This will get the list of all the lambda functions we have and return them. First off, we are going to try to run it without any IAM permissions. \u279c learning-serverless git:(master) \u2717 cd python-example-iam \u279c python-example-iam git:(master) \u2717 sls deploy Serverless: Packaging service... Serverless: Excluding development dependencies... Serverless: Creating Stack... Serverless: Checking Stack create progress... ........ Serverless: Stack create finished... Serverless: Uploading CloudFormation file to S3... Serverless: Uploading artifacts... Serverless: Uploading service python-example-iam.zip file to S3 (490 B)... Serverless: Validating template... Serverless: Updating Stack... Serverless: Checking Stack update progress... ............... Serverless: Stack update finished... Service Information service: python-example-iam stage: dev region: us-east-1 stack: python-example-iam-dev resources: 6 api keys: None endpoints: functions: hello: python-example-iam-dev-hello layers: None Toggle on monitoring with the Serverless Dashboard: run \"serverless\" \u279c python-example-iam git:(master) sls invoke -f hello -l { \"stackTrace\": [ [ \"/var/task/handler.py\", 5, \"hello\", \"response = client.list_functions()\" ], [ \"/var/runtime/botocore/client.py\", 386, \"_api_call\", \"return self._make_api_call(operation_name, kwargs)\" ], [ \"/var/runtime/botocore/client.py\", 705, \"_make_api_call\", \"raise error_class(parsed_response, operation_name)\" ] ], \"errorType\": \"ClientError\", \"errorMessage\": \"An error occurred (AccessDeniedException) when calling the ListFunctions operation: User: arn:aws:sts::357261687744:assumed-role/python-example-iam-dev-us-east-1-lambdaRole/python-example-iam-dev-hello is not authorized to perform: lambda:ListFunctions on resource: *\" } -------------------------------------------------------------------- START RequestId: 4e318fca-ab2b-4ed4-8303-75957bb26a64 Version: $LATEST /var/runtime/boto3/compat.py:86: PythonDeprecationWarning: Boto3 will no longer support Python 2.7 starting July 15, 2021. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.6 or later. More information can be found here: https://aws.amazon.com/blogs/developer/announcing-end-of-support-for-python-2-7-in-aws-sdk-for-python-and-aws-cli-v1/ warnings.warn(warning, PythonDeprecationWarning) An error occurred (AccessDeniedException) when calling the ListFunctions operation: User: arn:aws:sts::357261687744:assumed-role/python-example-iam-dev-us-east-1-lambdaRole/python-example-iam-dev-hello is not authorized to perform: lambda:ListFunctions on resource: *: ClientError Traceback (most recent call last): File \"/var/task/handler.py\", line 5, in hello response = client.list_functions() File \"/var/runtime/botocore/client.py\", line 386, in _api_call return self._make_api_call(operation_name, kwargs) File \"/var/runtime/botocore/client.py\", line 705, in _make_api_call raise error_class(parsed_response, operation_name) ClientError: An error occurred (AccessDeniedException) when calling the ListFunctions operation: User: arn:aws:sts::357261687744:assumed-role/python-example-iam-dev-us-east-1-lambdaRole/python-example-iam-dev-hello is not authorized to perform: lambda:ListFunctions on resource: * END RequestId: 4e318fca-ab2b-4ed4-8303-75957bb26a64 REPORT RequestId: 4e318fca-ab2b-4ed4-8303-75957bb26a64 Duration: 306.51 ms Billed Duration: 307 ms Memory Size: 1024 MB Max Memory Used: 63 MB Init Duration: 186.54 ms Serverless Error ---------------------------------------- Invoked function failed Get Support -------------------------------------------- Docs: docs.serverless.com Bugs: github.com/serverless/serverless/issues Issues: forum.serverless.com Your Environment Information --------------------------- Operating System: linux Node Version: 16.10.0 Framework Version: 2.62.0 Plugin Version: 5.4.6 SDK Version: 4.3.0 Components Version: 3.17.1 So the request fails. We are going to configure the iam in the serverless.yaml. service: python-example-iam frameworkVersion: '2' provider: name: aws runtime: python2.7 lambdaHashingVersion: 20201221 profile: serverless-admin region: eu-west-1 iamRoleStatements: - Effect: Allow Action: - lambda:* Resource: - \"*\" functions: hello: handler: handler.hello And then deploy and invoke again: \u279c python-example-iam git:(master) \u2717 sls invoke -f hello -l { \"Functions\": [ { \"TracingConfig\": { \"Mode\": \"PassThrough\" }, \"Version\": \"$LATEST\", \"CodeSha256\": \"Nn8prCQ4VjwPWFA+Wi/pTckoZ9R7hWEZWZymn/gI3Fc=\", \"FunctionName\": \"python-example-iam-dev-hello\", \"MemorySize\": 1024, \"RevisionId\": \"eb571f80-37b9-4174-b283-7249f9df2859\", \"CodeSize\": 490, \"PackageType\": \"Zip\", \"FunctionArn\": \"arn:aws:lambda:eu-west-1:539690530154:function:python-example-iam-dev-hello\", \"Handler\": \"handler.hello\", \"Role\": \"arn:aws:iam::539690530154:role/python-example-iam-dev-eu-west-1-lambdaRole\", \"Timeout\": 6, \"LastModified\": \"2021-10-12T11:41:50.296+0000\", \"Runtime\": \"python2.7\", \"Description\": \"\" }, { \"TracingConfig\": { \"Mode\": \"PassThrough\" }, \"Version\": \"$LATEST\", \"CodeSha256\": \"FJI+X2rjl83woBoRp+ozHqHBRNlvxDCvvE0Zj9POW6Q=\", \"FunctionName\": \"hello-world-python-dev-hello-long-timeout\", \"MemorySize\": 256, \"RevisionId\": \"215e0ac3-7712-45d7-89a7-f814640d78a0\", \"CodeSize\": 481, \"PackageType\": \"Zip\", \"FunctionArn\": \"arn:aws:lambda:eu-west-1:539690530154:function:hello-world-python-dev-hello-long-timeout\", \"Handler\": \"handler.hello\", \"Role\": \"arn:aws:iam::539690530154:role/hello-world-python-dev-eu-west-1-lambdaRole\", \"Timeout\": 6, \"LastModified\": \"2021-10-12T11:29:42.426+0000\", \"Runtime\": \"python2.7\", \"Description\": \"\" }, { \"TracingConfig\": { \"Mode\": \"PassThrough\" }, \"Version\": \"$LATEST\", \"CodeSha256\": \"FJI+X2rjl83woBoRp+ozHqHBRNlvxDCvvE0Zj9POW6Q=\", \"FunctionName\": \"hello-world-python-dev-hello-short-timeout\", \"VpcConfig\": { \"SubnetIds\": [], \"VpcId\": \"\", \"SecurityGroupIds\": [] }, \"MemorySize\": 512, \"RevisionId\": \"118ef4c8-541a-4e24-8466-9c802468f477\", \"CodeSize\": 481, \"PackageType\": \"Zip\", \"FunctionArn\": \"arn:aws:lambda:eu-west-1:539690530154:function:hello-world-python-dev-hello-short-timeout\", \"Handler\": \"handler.hello\", \"Role\": \"arn:aws:iam::539690530154:role/hello-world-python-dev-eu-west-1-lambdaRole\", \"Timeout\": 2, \"LastModified\": \"2021-10-12T11:29:42.304+0000\", \"Runtime\": \"python2.7\", \"Description\": \"\" } ], \"ResponseMetadata\": { \"RetryAttempts\": 0, \"HTTPStatusCode\": 200, \"RequestId\": \"1383966a-249a-4b49-b493-dadb219b2b45\", \"HTTPHeaders\": { \"date\": \"Tue, 12 Oct 2021 11:42:12 GMT\", \"x-amzn-requestid\": \"1383966a-249a-4b49-b493-dadb219b2b45\", \"content-length\": \"2992\", \"content-type\": \"application/json\", \"connection\": \"keep-alive\" } } } -------------------------------------------------------------------- START RequestId: 13622bc9-a2ad-44b7-bdb1-faea58c913bf Version: $LATEST /var/runtime/boto3/compat.py:86: PythonDeprecationWarning: Boto3 will no longer support Python 2.7 starting July 15, 2021. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.6 or later. More information can be found here: https://aws.amazon.com/blogs/developer/announcing-end-of-support-for-python-2-7-in-aws-sdk-for-python-and-aws-cli-v1/ warnings.warn(warning, PythonDeprecationWarning) END RequestId: 13622bc9-a2ad-44b7-bdb1-faea58c913bf REPORT RequestId: 13622bc9-a2ad-44b7-bdb1-faea58c913bf Duration: 289.32 ms Billed Duration: 290 ms Memory Size: 1024 MB Max Memory Used: 63 MB Init Duration: 167.40 ms","title":"IAM Permissions for Lambda Functions"},{"location":"AWS/aws-lambda-and-the-serverless-framework/03-aws-lambda-and-the-serverless-in-depth/02-iam-permissions-for-lambda-functions/#iam-permissions-for-lambda-functions","text":"Our lambda functions access other services like S3 to store images or dynamoDB to store and retrieve data. By default our lambda functions are not authorized to do that. For this, we provide an IAM policy which allows you to entirely secure your AWS setup. For this we are going to create a new project. \u279c learning-serverless git:(master) sls create --template aws-python --path python-example-iam Serverless: Generating boilerplate... Serverless: Generating boilerplate in \"/home/davis/projects/learning-serverless/python-example-iam\" Serverless: Successfully generated boilerplate for template: \"aws-python\" And create a following function: import boto3 def hello(event, context): client = boto3.client('lambda') response = client.list_functions() return response This will get the list of all the lambda functions we have and return them. First off, we are going to try to run it without any IAM permissions. \u279c learning-serverless git:(master) \u2717 cd python-example-iam \u279c python-example-iam git:(master) \u2717 sls deploy Serverless: Packaging service... Serverless: Excluding development dependencies... Serverless: Creating Stack... Serverless: Checking Stack create progress... ........ Serverless: Stack create finished... Serverless: Uploading CloudFormation file to S3... Serverless: Uploading artifacts... Serverless: Uploading service python-example-iam.zip file to S3 (490 B)... Serverless: Validating template... Serverless: Updating Stack... Serverless: Checking Stack update progress... ............... Serverless: Stack update finished... Service Information service: python-example-iam stage: dev region: us-east-1 stack: python-example-iam-dev resources: 6 api keys: None endpoints: functions: hello: python-example-iam-dev-hello layers: None Toggle on monitoring with the Serverless Dashboard: run \"serverless\" \u279c python-example-iam git:(master) sls invoke -f hello -l { \"stackTrace\": [ [ \"/var/task/handler.py\", 5, \"hello\", \"response = client.list_functions()\" ], [ \"/var/runtime/botocore/client.py\", 386, \"_api_call\", \"return self._make_api_call(operation_name, kwargs)\" ], [ \"/var/runtime/botocore/client.py\", 705, \"_make_api_call\", \"raise error_class(parsed_response, operation_name)\" ] ], \"errorType\": \"ClientError\", \"errorMessage\": \"An error occurred (AccessDeniedException) when calling the ListFunctions operation: User: arn:aws:sts::357261687744:assumed-role/python-example-iam-dev-us-east-1-lambdaRole/python-example-iam-dev-hello is not authorized to perform: lambda:ListFunctions on resource: *\" } -------------------------------------------------------------------- START RequestId: 4e318fca-ab2b-4ed4-8303-75957bb26a64 Version: $LATEST /var/runtime/boto3/compat.py:86: PythonDeprecationWarning: Boto3 will no longer support Python 2.7 starting July 15, 2021. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.6 or later. More information can be found here: https://aws.amazon.com/blogs/developer/announcing-end-of-support-for-python-2-7-in-aws-sdk-for-python-and-aws-cli-v1/ warnings.warn(warning, PythonDeprecationWarning) An error occurred (AccessDeniedException) when calling the ListFunctions operation: User: arn:aws:sts::357261687744:assumed-role/python-example-iam-dev-us-east-1-lambdaRole/python-example-iam-dev-hello is not authorized to perform: lambda:ListFunctions on resource: *: ClientError Traceback (most recent call last): File \"/var/task/handler.py\", line 5, in hello response = client.list_functions() File \"/var/runtime/botocore/client.py\", line 386, in _api_call return self._make_api_call(operation_name, kwargs) File \"/var/runtime/botocore/client.py\", line 705, in _make_api_call raise error_class(parsed_response, operation_name) ClientError: An error occurred (AccessDeniedException) when calling the ListFunctions operation: User: arn:aws:sts::357261687744:assumed-role/python-example-iam-dev-us-east-1-lambdaRole/python-example-iam-dev-hello is not authorized to perform: lambda:ListFunctions on resource: * END RequestId: 4e318fca-ab2b-4ed4-8303-75957bb26a64 REPORT RequestId: 4e318fca-ab2b-4ed4-8303-75957bb26a64 Duration: 306.51 ms Billed Duration: 307 ms Memory Size: 1024 MB Max Memory Used: 63 MB Init Duration: 186.54 ms Serverless Error ---------------------------------------- Invoked function failed Get Support -------------------------------------------- Docs: docs.serverless.com Bugs: github.com/serverless/serverless/issues Issues: forum.serverless.com Your Environment Information --------------------------- Operating System: linux Node Version: 16.10.0 Framework Version: 2.62.0 Plugin Version: 5.4.6 SDK Version: 4.3.0 Components Version: 3.17.1 So the request fails. We are going to configure the iam in the serverless.yaml. service: python-example-iam frameworkVersion: '2' provider: name: aws runtime: python2.7 lambdaHashingVersion: 20201221 profile: serverless-admin region: eu-west-1 iamRoleStatements: - Effect: Allow Action: - lambda:* Resource: - \"*\" functions: hello: handler: handler.hello And then deploy and invoke again: \u279c python-example-iam git:(master) \u2717 sls invoke -f hello -l { \"Functions\": [ { \"TracingConfig\": { \"Mode\": \"PassThrough\" }, \"Version\": \"$LATEST\", \"CodeSha256\": \"Nn8prCQ4VjwPWFA+Wi/pTckoZ9R7hWEZWZymn/gI3Fc=\", \"FunctionName\": \"python-example-iam-dev-hello\", \"MemorySize\": 1024, \"RevisionId\": \"eb571f80-37b9-4174-b283-7249f9df2859\", \"CodeSize\": 490, \"PackageType\": \"Zip\", \"FunctionArn\": \"arn:aws:lambda:eu-west-1:539690530154:function:python-example-iam-dev-hello\", \"Handler\": \"handler.hello\", \"Role\": \"arn:aws:iam::539690530154:role/python-example-iam-dev-eu-west-1-lambdaRole\", \"Timeout\": 6, \"LastModified\": \"2021-10-12T11:41:50.296+0000\", \"Runtime\": \"python2.7\", \"Description\": \"\" }, { \"TracingConfig\": { \"Mode\": \"PassThrough\" }, \"Version\": \"$LATEST\", \"CodeSha256\": \"FJI+X2rjl83woBoRp+ozHqHBRNlvxDCvvE0Zj9POW6Q=\", \"FunctionName\": \"hello-world-python-dev-hello-long-timeout\", \"MemorySize\": 256, \"RevisionId\": \"215e0ac3-7712-45d7-89a7-f814640d78a0\", \"CodeSize\": 481, \"PackageType\": \"Zip\", \"FunctionArn\": \"arn:aws:lambda:eu-west-1:539690530154:function:hello-world-python-dev-hello-long-timeout\", \"Handler\": \"handler.hello\", \"Role\": \"arn:aws:iam::539690530154:role/hello-world-python-dev-eu-west-1-lambdaRole\", \"Timeout\": 6, \"LastModified\": \"2021-10-12T11:29:42.426+0000\", \"Runtime\": \"python2.7\", \"Description\": \"\" }, { \"TracingConfig\": { \"Mode\": \"PassThrough\" }, \"Version\": \"$LATEST\", \"CodeSha256\": \"FJI+X2rjl83woBoRp+ozHqHBRNlvxDCvvE0Zj9POW6Q=\", \"FunctionName\": \"hello-world-python-dev-hello-short-timeout\", \"VpcConfig\": { \"SubnetIds\": [], \"VpcId\": \"\", \"SecurityGroupIds\": [] }, \"MemorySize\": 512, \"RevisionId\": \"118ef4c8-541a-4e24-8466-9c802468f477\", \"CodeSize\": 481, \"PackageType\": \"Zip\", \"FunctionArn\": \"arn:aws:lambda:eu-west-1:539690530154:function:hello-world-python-dev-hello-short-timeout\", \"Handler\": \"handler.hello\", \"Role\": \"arn:aws:iam::539690530154:role/hello-world-python-dev-eu-west-1-lambdaRole\", \"Timeout\": 2, \"LastModified\": \"2021-10-12T11:29:42.304+0000\", \"Runtime\": \"python2.7\", \"Description\": \"\" } ], \"ResponseMetadata\": { \"RetryAttempts\": 0, \"HTTPStatusCode\": 200, \"RequestId\": \"1383966a-249a-4b49-b493-dadb219b2b45\", \"HTTPHeaders\": { \"date\": \"Tue, 12 Oct 2021 11:42:12 GMT\", \"x-amzn-requestid\": \"1383966a-249a-4b49-b493-dadb219b2b45\", \"content-length\": \"2992\", \"content-type\": \"application/json\", \"connection\": \"keep-alive\" } } } -------------------------------------------------------------------- START RequestId: 13622bc9-a2ad-44b7-bdb1-faea58c913bf Version: $LATEST /var/runtime/boto3/compat.py:86: PythonDeprecationWarning: Boto3 will no longer support Python 2.7 starting July 15, 2021. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.6 or later. More information can be found here: https://aws.amazon.com/blogs/developer/announcing-end-of-support-for-python-2-7-in-aws-sdk-for-python-and-aws-cli-v1/ warnings.warn(warning, PythonDeprecationWarning) END RequestId: 13622bc9-a2ad-44b7-bdb1-faea58c913bf REPORT RequestId: 13622bc9-a2ad-44b7-bdb1-faea58c913bf Duration: 289.32 ms Billed Duration: 290 ms Memory Size: 1024 MB Max Memory Used: 63 MB Init Duration: 167.40 ms","title":"IAM Permissions for Lambda Functions"},{"location":"AWS/aws-lambda-and-the-serverless-framework/03-aws-lambda-and-the-serverless-in-depth/03-environment-variables-in-aws-lambda/","text":"Environment Variables in AWS Lambda \u00b6 Environment variables are good because they provide external configuration to your functions. This way, we can change a function's behavior without even changing the code of the function. \u279c python-example-iam git:(master) sls create --template aws-python --path python-example-environment-variables Serverless: Generating boilerplate... Serverless: Generating boilerplate in \"/home/davis/projects/learning-serverless/python-example-iam/python-example-environment-variables\" Serverless: Successfully generated boilerplate for template: \"aws-python\" We are going to set up a following funciton: import os def hello(event, context): return os.environ['FIRST_NAME'] And the configuration: service: python-example-environment-variables frameworkVersion: '2' provider: name: aws runtime: python2.7 lambdaHashingVersion: 20201221 profile: serverless-admin region: eu-west-1 environment: variable1: value1 variable2: value2 FIRST_NAME: John functions: hello-env-john: handler: handler.hello hello-env-marc: handler: handler.hello environment: FIRST_NAME: Marc And then we can invoke it: \u279c python-example-environment-variables git:(master) \u2717 sls invoke -f hello-env-john \"John\" \u279c python-example-environment-variables git:(master) \u2717 sls invoke -f hello-env-marc \"Marc\"","title":"Environment Variables in AWS Lambda"},{"location":"AWS/aws-lambda-and-the-serverless-framework/03-aws-lambda-and-the-serverless-in-depth/03-environment-variables-in-aws-lambda/#environment-variables-in-aws-lambda","text":"Environment variables are good because they provide external configuration to your functions. This way, we can change a function's behavior without even changing the code of the function. \u279c python-example-iam git:(master) sls create --template aws-python --path python-example-environment-variables Serverless: Generating boilerplate... Serverless: Generating boilerplate in \"/home/davis/projects/learning-serverless/python-example-iam/python-example-environment-variables\" Serverless: Successfully generated boilerplate for template: \"aws-python\" We are going to set up a following funciton: import os def hello(event, context): return os.environ['FIRST_NAME'] And the configuration: service: python-example-environment-variables frameworkVersion: '2' provider: name: aws runtime: python2.7 lambdaHashingVersion: 20201221 profile: serverless-admin region: eu-west-1 environment: variable1: value1 variable2: value2 FIRST_NAME: John functions: hello-env-john: handler: handler.hello hello-env-marc: handler: handler.hello environment: FIRST_NAME: Marc And then we can invoke it: \u279c python-example-environment-variables git:(master) \u2717 sls invoke -f hello-env-john \"John\" \u279c python-example-environment-variables git:(master) \u2717 sls invoke -f hello-env-marc \"Marc\"","title":"Environment Variables in AWS Lambda"},{"location":"AWS/aws-lambda-and-the-serverless-framework/03-aws-lambda-and-the-serverless-in-depth/04-vpc-for-lambda-functions/","text":"VPC for lambda functions \u00b6 VPC are Virtual Private Clouds Many companies use VPC to privately deploy their applications By default Lambda functions are not launched in a VPC But you can launch lambda in your VPC, so that your lambda functions can securely access your resources. You can also assign security groups to your lambda functions as well for enhanced network security. You can choose to deploy your lambda function in any subnets you like. This will allow your lambda function to inherit a private IP from that subnet. service: service-04-python-example-vpc frameworkVersion: '2' provider: name: aws runtime: python2.7 lambdaHashingVersion: 20201221 profile: serverless-admin vpc: securityGroupIds: - sg-XXXX subnetIds: - subnet-XXX - subnet-XXX functions: hello: handler: handler.hello","title":"VPC for lambda functions"},{"location":"AWS/aws-lambda-and-the-serverless-framework/03-aws-lambda-and-the-serverless-in-depth/04-vpc-for-lambda-functions/#vpc-for-lambda-functions","text":"VPC are Virtual Private Clouds Many companies use VPC to privately deploy their applications By default Lambda functions are not launched in a VPC But you can launch lambda in your VPC, so that your lambda functions can securely access your resources. You can also assign security groups to your lambda functions as well for enhanced network security. You can choose to deploy your lambda function in any subnets you like. This will allow your lambda function to inherit a private IP from that subnet. service: service-04-python-example-vpc frameworkVersion: '2' provider: name: aws runtime: python2.7 lambdaHashingVersion: 20201221 profile: serverless-admin vpc: securityGroupIds: - sg-XXXX subnetIds: - subnet-XXX - subnet-XXX functions: hello: handler: handler.hello","title":"VPC for lambda functions"},{"location":"AWS/aws-lambda-and-the-serverless-framework/03-aws-lambda-and-the-serverless-in-depth/05-aws-lambda-pricing/","text":"AWS Lambda Pricing \u00b6 You can find overall pricing information here: https://aws.amazon.com/lambda/pricing For us-east1 region as of June 2017 it is: Pay per calls First 1,000,000 requests are free 0.20 per 1 million requests after Pay per duration (in increment of 100ms) 400,000 GB-seconds of compute time per month are free 400,000 seconds if function is 1 GB RAM 3,200,000 seconds if function is 128MB RAM After that $1.00 for 600,000 GB-seconds It is usually very cheap to run AWS Lambda so it's very popular.","title":"AWS Lambda Pricing"},{"location":"AWS/aws-lambda-and-the-serverless-framework/03-aws-lambda-and-the-serverless-in-depth/05-aws-lambda-pricing/#aws-lambda-pricing","text":"You can find overall pricing information here: https://aws.amazon.com/lambda/pricing For us-east1 region as of June 2017 it is: Pay per calls First 1,000,000 requests are free 0.20 per 1 million requests after Pay per duration (in increment of 100ms) 400,000 GB-seconds of compute time per month are free 400,000 seconds if function is 1 GB RAM 3,200,000 seconds if function is 128MB RAM After that $1.00 for 600,000 GB-seconds It is usually very cheap to run AWS Lambda so it's very popular.","title":"AWS Lambda Pricing"},{"location":"AWS/aws-lambda-and-the-serverless-framework/04-real-world-example-s3-thumbnails/01-overview-of-s3-thumbnail-generator-service/","text":"Overview of S3 Thumbnail Generator Service \u00b6 So, we'll have an S3 bucket where will upload a new image. This event will trigger an AWS Lambda function that will creatre a thumbnail of the image. The new thumbnail will be uploaded into S3 bucket. What we'll use \u00b6 S3 events Function timeouts and memory IAM Permissions Plugins to deploy python dependencies (need to have docker installed) Custom variables Environment variables","title":"Overview of S3 Thumbnail Generator Service"},{"location":"AWS/aws-lambda-and-the-serverless-framework/04-real-world-example-s3-thumbnails/01-overview-of-s3-thumbnail-generator-service/#overview-of-s3-thumbnail-generator-service","text":"So, we'll have an S3 bucket where will upload a new image. This event will trigger an AWS Lambda function that will creatre a thumbnail of the image. The new thumbnail will be uploaded into S3 bucket.","title":"Overview of S3 Thumbnail Generator Service"},{"location":"AWS/aws-lambda-and-the-serverless-framework/04-real-world-example-s3-thumbnails/01-overview-of-s3-thumbnail-generator-service/#what-well-use","text":"S3 events Function timeouts and memory IAM Permissions Plugins to deploy python dependencies (need to have docker installed) Custom variables Environment variables","title":"What we'll use"},{"location":"AWS/aws-lambda-and-the-serverless-framework/04-real-world-example-s3-thumbnails/02-thumbnail-service-implementation/","text":"Thumbnail service implementation \u00b6 We are going to create a new project. \u279c learning-serverless git:(master) sls create --template aws-python --path 05-python-s3-thumbnail Serverless: Generating boilerplate... Serverless: Generating boilerplate in \"/home/davis/projects/learning-serverless/05-python-s3-thumbnail\" Serverless: Successfully generated boilerplate for template: \"aws-python\" We are going to populate the serverless.yaml file: service: service-05-python-s3-thumbnail frameworkVersion: '2' provider: name: aws runtime: python2.7 lambdaHashingVersion: 20201221 region: eu-west-1 profile: serverless-admin timeout: 10 memorySize: 128 iamRoleStatements: - Effect: Allow Action: - \"s3:*\" Resource: \"*\" environment: THUMBNAIL_SIZE: 128 custom: bucket: davis-s3-thumbnail-generator pythonRequirements: dockerizePip: true functions: s3-thumbnail-generator: handler: handler.s3_thumbnail_generator events: - s3: bucket: ${self:custom.bucket} event: s3:ObjectCreated:* rules: - suffix: .png plugins: - serverless-python-requirements And the handler.py file: import boto3 import cStringIO from PIL import Image, ImageOps import os s3 = boto.client('s3') size = int(os.environ['THUMBNAIL_SIZE']) def s3_thumbnail_generator(event, context): print(event) bucket = event['Records'][0]['s3']['bucket']['name'] key = event['Records'][0]['s3']['object']['key'] image = get_s3_image(bucket, key) thumbnail = image_to_thumbnail(image) thumbnail_key = new_filename(key) url = upload_to_s3(bucket, thumbnail_key, thumbnail) return url def get s3_image(bucket, key): response = s3.get_object(Bucket=bucket, Key=key) imagecontent = response['Body'].read() file = cStringIO.StringIO(imagecontent) img = Image.open(file) return img def image_to_thumbnail(image): return ImageOps.fit(image, (size, size), Image.ANTIALIAS) def new_filename(key) key_split = key.rsplit('.', 1) return key_split + \"_thumbnail.png\" def upload_to_s3(bucket, key, image): out_thumbnail = cStringIO.StringIO() image.save(out_thumbnail, 'PNG') out_thumbnail.seek(0) response = s3.put_object( ACL='public-read', Body=out_thumbnail, Bucket=bucket, ContentType='image/png', Key=key ) print(response) url = '{}/{}/{}'.format(s3.meta.endpoint_url, bucket, key) return url Also, create requirements.txt file: Pillow Then, we are going to deploy it! \u279c 05-python-s3-thumbnail git:(master) \u2717 sls plugin install -n serverless-python-requirements Serverless: Installing plugin \"serverless-python-requirements@latest\" (this might take a few seconds...) Serverless: Successfully installed \"serverless-python-requirements@latest\" \u279c 05-python-s3-thumbnail git:(master) \u2717 sls deploy Now we can test it by uploading an image: After we refresh the bucket, the thumbnail is available:","title":"Thumbnail service implementation"},{"location":"AWS/aws-lambda-and-the-serverless-framework/04-real-world-example-s3-thumbnails/02-thumbnail-service-implementation/#thumbnail-service-implementation","text":"We are going to create a new project. \u279c learning-serverless git:(master) sls create --template aws-python --path 05-python-s3-thumbnail Serverless: Generating boilerplate... Serverless: Generating boilerplate in \"/home/davis/projects/learning-serverless/05-python-s3-thumbnail\" Serverless: Successfully generated boilerplate for template: \"aws-python\" We are going to populate the serverless.yaml file: service: service-05-python-s3-thumbnail frameworkVersion: '2' provider: name: aws runtime: python2.7 lambdaHashingVersion: 20201221 region: eu-west-1 profile: serverless-admin timeout: 10 memorySize: 128 iamRoleStatements: - Effect: Allow Action: - \"s3:*\" Resource: \"*\" environment: THUMBNAIL_SIZE: 128 custom: bucket: davis-s3-thumbnail-generator pythonRequirements: dockerizePip: true functions: s3-thumbnail-generator: handler: handler.s3_thumbnail_generator events: - s3: bucket: ${self:custom.bucket} event: s3:ObjectCreated:* rules: - suffix: .png plugins: - serverless-python-requirements And the handler.py file: import boto3 import cStringIO from PIL import Image, ImageOps import os s3 = boto.client('s3') size = int(os.environ['THUMBNAIL_SIZE']) def s3_thumbnail_generator(event, context): print(event) bucket = event['Records'][0]['s3']['bucket']['name'] key = event['Records'][0]['s3']['object']['key'] image = get_s3_image(bucket, key) thumbnail = image_to_thumbnail(image) thumbnail_key = new_filename(key) url = upload_to_s3(bucket, thumbnail_key, thumbnail) return url def get s3_image(bucket, key): response = s3.get_object(Bucket=bucket, Key=key) imagecontent = response['Body'].read() file = cStringIO.StringIO(imagecontent) img = Image.open(file) return img def image_to_thumbnail(image): return ImageOps.fit(image, (size, size), Image.ANTIALIAS) def new_filename(key) key_split = key.rsplit('.', 1) return key_split + \"_thumbnail.png\" def upload_to_s3(bucket, key, image): out_thumbnail = cStringIO.StringIO() image.save(out_thumbnail, 'PNG') out_thumbnail.seek(0) response = s3.put_object( ACL='public-read', Body=out_thumbnail, Bucket=bucket, ContentType='image/png', Key=key ) print(response) url = '{}/{}/{}'.format(s3.meta.endpoint_url, bucket, key) return url Also, create requirements.txt file: Pillow Then, we are going to deploy it! \u279c 05-python-s3-thumbnail git:(master) \u2717 sls plugin install -n serverless-python-requirements Serverless: Installing plugin \"serverless-python-requirements@latest\" (this might take a few seconds...) Serverless: Successfully installed \"serverless-python-requirements@latest\" \u279c 05-python-s3-thumbnail git:(master) \u2717 sls deploy Now we can test it by uploading an image: After we refresh the bucket, the thumbnail is available:","title":"Thumbnail service implementation"},{"location":"AWS/aws-lambda-and-the-serverless-framework/04-real-world-example-s3-thumbnails/03-thumbnail-servcice-next-steps/","text":"Ideas to make our service better \u00b6 Make sure we handle errors Corrupted images Non image files Make sure we correctly measure timeouts Do not create a thumbnail if the image is too big (over >5MB) Build alerting using AWS SNS if an image is not successfully processed Put image metadata in DynamoDB (the metadata you want)","title":"Ideas to make our service better"},{"location":"AWS/aws-lambda-and-the-serverless-framework/04-real-world-example-s3-thumbnails/03-thumbnail-servcice-next-steps/#ideas-to-make-our-service-better","text":"Make sure we handle errors Corrupted images Non image files Make sure we correctly measure timeouts Do not create a thumbnail if the image is too big (over >5MB) Build alerting using AWS SNS if an image is not successfully processed Put image metadata in DynamoDB (the metadata you want)","title":"Ideas to make our service better"},{"location":"AWS/aws-lambda-and-the-serverless-framework/05-real-world-example-rest-api/01-overview-of-the-rest-api-service/","text":"Overview of the REST API service \u00b6 We are going to create an API gateway and it will have multiple lambdas attached to it to serve CREATE/LIST/GET/UPDATE/DELETE endpoints. The data will be persisted in a dynamo table. What we'll use: \u00b6 NodeJS runtime API Gateway DynamoDB table Function timeouts and memory IAM permissions CloudFormation Resources","title":"Overview of the REST API service"},{"location":"AWS/aws-lambda-and-the-serverless-framework/05-real-world-example-rest-api/01-overview-of-the-rest-api-service/#overview-of-the-rest-api-service","text":"We are going to create an API gateway and it will have multiple lambdas attached to it to serve CREATE/LIST/GET/UPDATE/DELETE endpoints. The data will be persisted in a dynamo table.","title":"Overview of the REST API service"},{"location":"AWS/aws-lambda-and-the-serverless-framework/05-real-world-example-rest-api/01-overview-of-the-rest-api-service/#what-well-use","text":"NodeJS runtime API Gateway DynamoDB table Function timeouts and memory IAM permissions CloudFormation Resources","title":"What we'll use:"},{"location":"AWS/aws-lambda-and-the-serverless-framework/05-real-world-example-rest-api/02-building-the-rest-api/","text":"Building the REST API \u00b6 The code is from https://github.com/serverless/examples We are going to download that repository and do the following things: \u279c 06-aws-node-rest-api-with-dynamodb git:(master) \u2717 npm i npm WARN deprecated uuid@2.0.3: Please upgrade to version 7 or higher. Older versions may use Math.random() in certain circumstances, which is known to be problematic. See https://v8.dev/blog/math-random for details. added 1 package, and audited 2 packages in 610ms found 0 vulnerabilities We can examine the serverless.yml file and see that there are some interesting things there, like: DYNAMODB_TABLE: ${self:service}-${opt:stage, self:provider.stage} The table name will be the same as the service name defined at the root, with a suffix of it's stage. The stage comes from option that are specified either when deploying sls or it will use the provider stage which is dev by default. Also, the IAM roles are specifying only the permissions we need on one specific table. iamRoleStatements: - Effect: Allow Action: - dynamodb:Query - dynamodb:Scan - dynamodb:GetItem - dynamodb:PutItem - dynamodb:UpdateItem - dynamodb:DeleteItem Resource: \"arn:aws:dynamodb:${opt:region, self:provider.region}:*:table/${self:provider.environment.DYNAMODB_TABLE}\" The functions are defined like this: get: handler: todos/get.get events: - http: path: todos/{id} method: get cors: true The handler in this case is located at the todos/get.js , it has a method get . The event is coming from API Gateway, with a path of todo/{id} , method GET . The CORS variable specifies if the endpoint can be called from other domains. Finally, at the end of the file we can see the Resources which basically is a CloudFormation template. resources: Resources: TodosDynamoDbTable: Type: 'AWS::DynamoDB::Table' DeletionPolicy: Retain Properties: AttributeDefinitions: - AttributeName: id AttributeType: S KeySchema: - AttributeName: id KeyType: HASH ProvisionedThroughput: ReadCapacityUnits: 1 WriteCapacityUnits: 1 TableName: ${self:provider.environment.DYNAMODB_TABLE} It lists resources we need to create. In this case we create a TodosDynamoDbTable table.","title":"Building the REST API"},{"location":"AWS/aws-lambda-and-the-serverless-framework/05-real-world-example-rest-api/02-building-the-rest-api/#building-the-rest-api","text":"The code is from https://github.com/serverless/examples We are going to download that repository and do the following things: \u279c 06-aws-node-rest-api-with-dynamodb git:(master) \u2717 npm i npm WARN deprecated uuid@2.0.3: Please upgrade to version 7 or higher. Older versions may use Math.random() in certain circumstances, which is known to be problematic. See https://v8.dev/blog/math-random for details. added 1 package, and audited 2 packages in 610ms found 0 vulnerabilities We can examine the serverless.yml file and see that there are some interesting things there, like: DYNAMODB_TABLE: ${self:service}-${opt:stage, self:provider.stage} The table name will be the same as the service name defined at the root, with a suffix of it's stage. The stage comes from option that are specified either when deploying sls or it will use the provider stage which is dev by default. Also, the IAM roles are specifying only the permissions we need on one specific table. iamRoleStatements: - Effect: Allow Action: - dynamodb:Query - dynamodb:Scan - dynamodb:GetItem - dynamodb:PutItem - dynamodb:UpdateItem - dynamodb:DeleteItem Resource: \"arn:aws:dynamodb:${opt:region, self:provider.region}:*:table/${self:provider.environment.DYNAMODB_TABLE}\" The functions are defined like this: get: handler: todos/get.get events: - http: path: todos/{id} method: get cors: true The handler in this case is located at the todos/get.js , it has a method get . The event is coming from API Gateway, with a path of todo/{id} , method GET . The CORS variable specifies if the endpoint can be called from other domains. Finally, at the end of the file we can see the Resources which basically is a CloudFormation template. resources: Resources: TodosDynamoDbTable: Type: 'AWS::DynamoDB::Table' DeletionPolicy: Retain Properties: AttributeDefinitions: - AttributeName: id AttributeType: S KeySchema: - AttributeName: id KeyType: HASH ProvisionedThroughput: ReadCapacityUnits: 1 WriteCapacityUnits: 1 TableName: ${self:provider.environment.DYNAMODB_TABLE} It lists resources we need to create. In this case we create a TodosDynamoDbTable table.","title":"Building the REST API"},{"location":"AWS/aws-lambda-and-the-serverless-framework/05-real-world-example-rest-api/03-deploying-the-rest-api/","text":"Deploying the REST API \u00b6 Once the service is set up, we can deploy it. sls deploy -v Once deployed, we'll see our 5 lambda functions: There will be a dynamoDB table setup: And API Gateway routes:","title":"Deploying the REST API"},{"location":"AWS/aws-lambda-and-the-serverless-framework/05-real-world-example-rest-api/03-deploying-the-rest-api/#deploying-the-rest-api","text":"Once the service is set up, we can deploy it. sls deploy -v Once deployed, we'll see our 5 lambda functions: There will be a dynamoDB table setup: And API Gateway routes:","title":"Deploying the REST API"},{"location":"AWS/aws-lambda-and-the-serverless-framework/05-real-world-example-rest-api/04-testing-the-rest-api/","text":"Testing the REST API \u00b6 When deployed we got the list of our endpoints as outputs: endpoints: POST - https://wj34feq9gl.execute-api.eu-west-1.amazonaws.com/dev/todos GET - https://wj34feq9gl.execute-api.eu-west-1.amazonaws.com/dev/todos GET - https://wj34feq9gl.execute-api.eu-west-1.amazonaws.com/dev/todos/{id} PUT - https://wj34feq9gl.execute-api.eu-west-1.amazonaws.com/dev/todos/{id} DELETE - https://wj34feq9gl.execute-api.eu-west-1.amazonaws.com/dev/todos/{id} We can use cUrl to test them: \u279c 06-aws-node-rest-api-with-dynamodb git:(master) \u2717 curl --location --request POST 'https://wj34feq9gl.execute-api.eu-west-1.amazonaws.com/dev/todos' \\ --header 'Content-Type: application/json' \\ --data-raw '{\"text\":\"Do something!\"}' {\"id\":\"6a86c070-2b65-11ec-8afa-cb984a671a49\",\"text\":\"Do something!\",\"checked\":false,\"createdAt\":1634047512055,\"updatedAt\":1634047512055}% \u279c 06-aws-node-rest-api-with-dynamodb git:(master) \u2717 curl --location --request POST 'https://wj34feq9gl.execute-api.eu-west-1.amazonaws.com/dev/todos' \\ --header 'Content-Type: application/json' \\ --data-raw '{\"text\":\"Do something else!\"}' {\"id\":\"86ae3da0-2b65-11ec-8afa-cb984a671a49\",\"text\":\"Do something else!\",\"checked\":false,\"createdAt\":1634047559290,\"updatedAt\":1634047559290}% \u279c 06-aws-node-rest-api-with-dynamodb git:(master) \u2717 curl --location --request GET 'https://wj34feq9gl.execute-api.eu-west-1.amazonaws.com/dev/todos' [{\"checked\":false,\"createdAt\":1634047559290,\"text\":\"Do something else!\",\"id\":\"86ae3da0-2b65-11ec-8afa-cb984a671a49\",\"updatedAt\":1634047559290},{\"checked\":false,\"createdAt\":1634047512055,\"text\":\"Do something!\",\"id\":\"6a86c070-2b65-11ec-8afa-cb984a671a49\",\"updatedAt\":1634047512055}]% \u279c 06-aws-node-rest-api-with-dynamodb git:(master) \u2717 curl --location --request GET 'https://wj34feq9gl.execute-api.eu-west-1.amazonaws.com/dev/todos/86ae3da0-2b65-11ec-8afa-cb984a671a49' {\"checked\":false,\"createdAt\":1634047559290,\"text\":\"Do something else!\",\"id\":\"86ae3da0-2b65-11ec-8afa-cb984a671a49\",\"updatedAt\":1634047559290}% \u279c 06-aws-node-rest-api-with-dynamodb git:(master) \u2717 curl --location --request DELETE 'https://wj34feq9gl.execute-api.eu-west-1.amazonaws.com/dev/todos/86ae3da0-2b65-11ec-8afa-cb984a671a49' {}% \u279c 06-aws-node-rest-api-with-dynamodb git:(master) \u2717 curl --location --request GET 'https://wj34feq9gl.execute-api.eu-west-1.amazonaws.com/dev/todos/86ae3da0-2b65-11ec-8afa-cb984a671a49' \u279c 06-aws-node-rest-api-with-dynamodb git:(master) \u2717 curl --location --request GET 'https://wj34feq9gl.execute-api.eu-west-1.amazonaws.com/dev/todos' [{\"checked\":false,\"createdAt\":1634047512055,\"text\":\"Do something!\",\"id\":\"6a86c070-2b65-11ec-8afa-cb984a671a49\",\"updatedAt\":1634047512055}]% \u279c 06-aws-node-rest-api-with-dynamodb git:(master) \u2717 We can also see the items in our DynamoDB table.","title":"Testing the REST API"},{"location":"AWS/aws-lambda-and-the-serverless-framework/05-real-world-example-rest-api/04-testing-the-rest-api/#testing-the-rest-api","text":"When deployed we got the list of our endpoints as outputs: endpoints: POST - https://wj34feq9gl.execute-api.eu-west-1.amazonaws.com/dev/todos GET - https://wj34feq9gl.execute-api.eu-west-1.amazonaws.com/dev/todos GET - https://wj34feq9gl.execute-api.eu-west-1.amazonaws.com/dev/todos/{id} PUT - https://wj34feq9gl.execute-api.eu-west-1.amazonaws.com/dev/todos/{id} DELETE - https://wj34feq9gl.execute-api.eu-west-1.amazonaws.com/dev/todos/{id} We can use cUrl to test them: \u279c 06-aws-node-rest-api-with-dynamodb git:(master) \u2717 curl --location --request POST 'https://wj34feq9gl.execute-api.eu-west-1.amazonaws.com/dev/todos' \\ --header 'Content-Type: application/json' \\ --data-raw '{\"text\":\"Do something!\"}' {\"id\":\"6a86c070-2b65-11ec-8afa-cb984a671a49\",\"text\":\"Do something!\",\"checked\":false,\"createdAt\":1634047512055,\"updatedAt\":1634047512055}% \u279c 06-aws-node-rest-api-with-dynamodb git:(master) \u2717 curl --location --request POST 'https://wj34feq9gl.execute-api.eu-west-1.amazonaws.com/dev/todos' \\ --header 'Content-Type: application/json' \\ --data-raw '{\"text\":\"Do something else!\"}' {\"id\":\"86ae3da0-2b65-11ec-8afa-cb984a671a49\",\"text\":\"Do something else!\",\"checked\":false,\"createdAt\":1634047559290,\"updatedAt\":1634047559290}% \u279c 06-aws-node-rest-api-with-dynamodb git:(master) \u2717 curl --location --request GET 'https://wj34feq9gl.execute-api.eu-west-1.amazonaws.com/dev/todos' [{\"checked\":false,\"createdAt\":1634047559290,\"text\":\"Do something else!\",\"id\":\"86ae3da0-2b65-11ec-8afa-cb984a671a49\",\"updatedAt\":1634047559290},{\"checked\":false,\"createdAt\":1634047512055,\"text\":\"Do something!\",\"id\":\"6a86c070-2b65-11ec-8afa-cb984a671a49\",\"updatedAt\":1634047512055}]% \u279c 06-aws-node-rest-api-with-dynamodb git:(master) \u2717 curl --location --request GET 'https://wj34feq9gl.execute-api.eu-west-1.amazonaws.com/dev/todos/86ae3da0-2b65-11ec-8afa-cb984a671a49' {\"checked\":false,\"createdAt\":1634047559290,\"text\":\"Do something else!\",\"id\":\"86ae3da0-2b65-11ec-8afa-cb984a671a49\",\"updatedAt\":1634047559290}% \u279c 06-aws-node-rest-api-with-dynamodb git:(master) \u2717 curl --location --request DELETE 'https://wj34feq9gl.execute-api.eu-west-1.amazonaws.com/dev/todos/86ae3da0-2b65-11ec-8afa-cb984a671a49' {}% \u279c 06-aws-node-rest-api-with-dynamodb git:(master) \u2717 curl --location --request GET 'https://wj34feq9gl.execute-api.eu-west-1.amazonaws.com/dev/todos/86ae3da0-2b65-11ec-8afa-cb984a671a49' \u279c 06-aws-node-rest-api-with-dynamodb git:(master) \u2717 curl --location --request GET 'https://wj34feq9gl.execute-api.eu-west-1.amazonaws.com/dev/todos' [{\"checked\":false,\"createdAt\":1634047512055,\"text\":\"Do something!\",\"id\":\"6a86c070-2b65-11ec-8afa-cb984a671a49\",\"updatedAt\":1634047512055}]% \u279c 06-aws-node-rest-api-with-dynamodb git:(master) \u2717 We can also see the items in our DynamoDB table.","title":"Testing the REST API"},{"location":"AWS/aws-lambda-and-the-serverless-framework/05-real-world-example-rest-api/05-rest-api-next-step-ideas/","text":"REST API next step ideas \u00b6 Build another service to have a REST API on users Build another service to get the user's todos Implement correct error validation (constraints) Try building a webpage that leverages that REST API","title":"REST API next step ideas"},{"location":"AWS/aws-lambda-and-the-serverless-framework/05-real-world-example-rest-api/05-rest-api-next-step-ideas/#rest-api-next-step-ideas","text":"Build another service to have a REST API on users Build another service to get the user's todos Implement correct error validation (constraints) Try building a webpage that leverages that REST API","title":"REST API next step ideas"},{"location":"AWS/aws-lambda-and-the-serverless-framework/06-real-world-example-aws-automation-ec2-start-stop/01-overview-of-the-lambda-service/","text":"Overview of EC2 start & stop service \u00b6 We are going to make a service that leverages CloudWatch schedule and every day at 9am start an EC2 instance. Every day at 5pm it will stop it. What we'll use \u00b6 Python runtime CloudWatch events IAM Permissions to stop and start ec2 instances Function timeouts and memory","title":"Overview of EC2 start & stop service"},{"location":"AWS/aws-lambda-and-the-serverless-framework/06-real-world-example-aws-automation-ec2-start-stop/01-overview-of-the-lambda-service/#overview-of-ec2-start-stop-service","text":"We are going to make a service that leverages CloudWatch schedule and every day at 9am start an EC2 instance. Every day at 5pm it will stop it.","title":"Overview of EC2 start &amp; stop service"},{"location":"AWS/aws-lambda-and-the-serverless-framework/06-real-world-example-aws-automation-ec2-start-stop/01-overview-of-the-lambda-service/#what-well-use","text":"Python runtime CloudWatch events IAM Permissions to stop and start ec2 instances Function timeouts and memory","title":"What we'll use"},{"location":"AWS/aws-lambda-and-the-serverless-framework/06-real-world-example-aws-automation-ec2-start-stop/02-deploying-the-ec2-start-and-stop-lambda-functions/","text":"Deploying the EC2 start and stop lambda functions \u00b6 We are going to create a new project: \u279c learning-serverless git:(master) sls create --template aws-python --path 07-ec2-start-stop Serverless: Generating boilerplate... Serverless: Generating boilerplate in \"/home/davis/projects/learning-serverless/07-ec2-start-stop\" Serverless: Successfully generated boilerplate for template: \"aws-python\" We are going to set up the serverless.yaml like this: service: service-07-ec2-start-stop frameworkVersion: '2' provider: name: aws runtime: python2.7 lambdaHashingVersion: 20201221 region: eu-west-1 profile: serverless-admin memorySize: 128 iamRoleStatements: - Effect: Allow Action: - ec2:* Resource: \"*\" functions: start-ec2: handler: handler.start_ec2 timeout: 60 events: - schedule: cron(0 13 * * ? *) stop-ec2: handler: handler.stop_ec2 timeout: 60 events: - schedule: cron(0 21 * * ? *) And the handler file, like so: import boto3 ec2 = boto3.client('ec2') def start_ec2(event, context): ec2_instances = get_all_ec2_ids() = get_all_ec2_ids() response = ec2.start_instances( InstanceIds=ec2_instances, DryRun=False) return response def stop_ec2(event, context): ec2_instances = get_all_ec2_ids() response = ec2.stop_instances( InstanceIds = ec2_instances, DryRun = False ) return response def get_all_ec2_ids(): response = ec2.describe_instances(DryRun=False) instances = [] for reservation in response[\"Reservations\"]: for instance in reservation[\"Instances\"]: instances.append(instance[\"InstanceId\"]) return instances","title":"Deploying the EC2 start and stop lambda functions"},{"location":"AWS/aws-lambda-and-the-serverless-framework/06-real-world-example-aws-automation-ec2-start-stop/02-deploying-the-ec2-start-and-stop-lambda-functions/#deploying-the-ec2-start-and-stop-lambda-functions","text":"We are going to create a new project: \u279c learning-serverless git:(master) sls create --template aws-python --path 07-ec2-start-stop Serverless: Generating boilerplate... Serverless: Generating boilerplate in \"/home/davis/projects/learning-serverless/07-ec2-start-stop\" Serverless: Successfully generated boilerplate for template: \"aws-python\" We are going to set up the serverless.yaml like this: service: service-07-ec2-start-stop frameworkVersion: '2' provider: name: aws runtime: python2.7 lambdaHashingVersion: 20201221 region: eu-west-1 profile: serverless-admin memorySize: 128 iamRoleStatements: - Effect: Allow Action: - ec2:* Resource: \"*\" functions: start-ec2: handler: handler.start_ec2 timeout: 60 events: - schedule: cron(0 13 * * ? *) stop-ec2: handler: handler.stop_ec2 timeout: 60 events: - schedule: cron(0 21 * * ? *) And the handler file, like so: import boto3 ec2 = boto3.client('ec2') def start_ec2(event, context): ec2_instances = get_all_ec2_ids() = get_all_ec2_ids() response = ec2.start_instances( InstanceIds=ec2_instances, DryRun=False) return response def stop_ec2(event, context): ec2_instances = get_all_ec2_ids() response = ec2.stop_instances( InstanceIds = ec2_instances, DryRun = False ) return response def get_all_ec2_ids(): response = ec2.describe_instances(DryRun=False) instances = [] for reservation in response[\"Reservations\"]: for instance in reservation[\"Instances\"]: instances.append(instance[\"InstanceId\"]) return instances","title":"Deploying the EC2 start and stop lambda functions"},{"location":"AWS/aws-lambda-and-the-serverless-framework/06-real-world-example-aws-automation-ec2-start-stop/03-next-steps-and-ideas/","text":"Next steps and ideas \u00b6 Leverage tags to exclude EC2 instances Leverage tags to let each EC2 instance have it's own schedule Improve CRON Job to only run on weekdays Looping strategy if you have over 100 EC2 isntances Leveraging environment variables to include or exclude instances Run across any region Deploy as many function as regions","title":"Next steps and ideas"},{"location":"AWS/aws-lambda-and-the-serverless-framework/06-real-world-example-aws-automation-ec2-start-stop/03-next-steps-and-ideas/#next-steps-and-ideas","text":"Leverage tags to exclude EC2 instances Leverage tags to let each EC2 instance have it's own schedule Improve CRON Job to only run on weekdays Looping strategy if you have over 100 EC2 isntances Leveraging environment variables to include or exclude instances Run across any region Deploy as many function as regions","title":"Next steps and ideas"},{"location":"AWS/developer-associate/","text":"Learning AWS Certified Developer Associate Course \u00b6 Sources: - Ultimate AWS Certified Developer Associate 2021","title":"Learning AWS Certified Developer Associate Course"},{"location":"AWS/developer-associate/#learning-aws-certified-developer-associate-course","text":"Sources: - Ultimate AWS Certified Developer Associate 2021","title":"Learning AWS Certified Developer Associate Course"},{"location":"AWS/developer-associate/02-getting-started-with-aws/01-aws-cloud-overview-regions-and-azs/","text":"AWS Cloud Overview - Regions & AZ \u00b6 AWS Cloud history \u00b6 2002 - AWS was launched internally. 2003 - Amazon infrastructure is one of their core strength. Idea to market. 2004 - Launched publicly with SQS 2006 - Re-launched publicly with SQS, S3 & EC2 2007 - Launched in Europe In 2019, AWS had $35.02 billion in annual revenue AWS accounts for 47% of market in 2019 (microsoft is 2nd with 22%) Pioneer and Leader of the AWS Cloud Market for the 9th consecutive year Over 1 million active users AWS Cloud Use Cases \u00b6 AWS Enables you to build sophisticaed, scalable applications Applicable to a diverse set of industries MCDonalds 21st century fox Activision Netflix Use cases include Enterprise IT Backup & Storage, Big Data Analytics Website hosting, Mobile & Social Apps Gaming AWS Global infrastructure \u00b6 AWS Regions It has regions all around the world Names can be us-east-1, eu-west-3 A region is a cluster of data centers Most AWS Services are region scoped You might want to choose an AWS region based on following criteria: Compliance - with data governance and legal requirements: data never leaves a region without your explicit permission. Proximity to customers: reduced latency Available services within a region: new services and new features aren' t available in every region. Pricing: pricing varies region to region and is transparent in the service pricing page. AWS Availability Zones Each region has many availability zones (usually 3, min is 2, max is 6). ap-southeast-2a ap-southeast-2b ap-southeast-2c Each availability zone (AZ) is one or more discrete data centers with redundant power, networking and connectivity. They're separate from each other, so that they' re isolated from disasters They're connected with high bandwidth, ultra-low latency networking AWS Edge Locations / Points of Presence Amazon has 216 Points of Presence (205 Edge Locations & || regional caches) in 84 cities across 42 countries. Content is delivered to end users with lower latency. https://infrastructure.aws/ Tour of the AWS Console \u00b6 AWS has Global Services: Identity and Access Management (IAM) Route 53 (DNS Service) CloudFront (Content Delivery Network) WAF (Web Application Firewall) Most AWS Services are region-scoped: Amazon EC2 (Infrastructure as a Service) Elastic Beanstalk (Platform as a Service) Lambda (Function as a Service) Rekognition (Software as a Service) Region table: https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services","title":"AWS Cloud Overview - Regions & AZ"},{"location":"AWS/developer-associate/02-getting-started-with-aws/01-aws-cloud-overview-regions-and-azs/#aws-cloud-overview-regions-az","text":"","title":"AWS Cloud Overview - Regions &amp; AZ"},{"location":"AWS/developer-associate/02-getting-started-with-aws/01-aws-cloud-overview-regions-and-azs/#aws-cloud-history","text":"2002 - AWS was launched internally. 2003 - Amazon infrastructure is one of their core strength. Idea to market. 2004 - Launched publicly with SQS 2006 - Re-launched publicly with SQS, S3 & EC2 2007 - Launched in Europe In 2019, AWS had $35.02 billion in annual revenue AWS accounts for 47% of market in 2019 (microsoft is 2nd with 22%) Pioneer and Leader of the AWS Cloud Market for the 9th consecutive year Over 1 million active users","title":"AWS Cloud history"},{"location":"AWS/developer-associate/02-getting-started-with-aws/01-aws-cloud-overview-regions-and-azs/#aws-cloud-use-cases","text":"AWS Enables you to build sophisticaed, scalable applications Applicable to a diverse set of industries MCDonalds 21st century fox Activision Netflix Use cases include Enterprise IT Backup & Storage, Big Data Analytics Website hosting, Mobile & Social Apps Gaming","title":"AWS Cloud Use Cases"},{"location":"AWS/developer-associate/02-getting-started-with-aws/01-aws-cloud-overview-regions-and-azs/#aws-global-infrastructure","text":"AWS Regions It has regions all around the world Names can be us-east-1, eu-west-3 A region is a cluster of data centers Most AWS Services are region scoped You might want to choose an AWS region based on following criteria: Compliance - with data governance and legal requirements: data never leaves a region without your explicit permission. Proximity to customers: reduced latency Available services within a region: new services and new features aren' t available in every region. Pricing: pricing varies region to region and is transparent in the service pricing page. AWS Availability Zones Each region has many availability zones (usually 3, min is 2, max is 6). ap-southeast-2a ap-southeast-2b ap-southeast-2c Each availability zone (AZ) is one or more discrete data centers with redundant power, networking and connectivity. They're separate from each other, so that they' re isolated from disasters They're connected with high bandwidth, ultra-low latency networking AWS Edge Locations / Points of Presence Amazon has 216 Points of Presence (205 Edge Locations & || regional caches) in 84 cities across 42 countries. Content is delivered to end users with lower latency. https://infrastructure.aws/","title":"AWS Global infrastructure"},{"location":"AWS/developer-associate/02-getting-started-with-aws/01-aws-cloud-overview-regions-and-azs/#tour-of-the-aws-console","text":"AWS has Global Services: Identity and Access Management (IAM) Route 53 (DNS Service) CloudFront (Content Delivery Network) WAF (Web Application Firewall) Most AWS Services are region-scoped: Amazon EC2 (Infrastructure as a Service) Elastic Beanstalk (Platform as a Service) Lambda (Function as a Service) Rekognition (Software as a Service) Region table: https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services","title":"Tour of the AWS Console"},{"location":"AWS/developer-associate/03-iam-and-aws-cli/04-iam-policies-hands-on/","text":"IAM Policies Hands On \u00b6 If we remove the previously made user from the admin group, we would see a following error when viting the IAM page. We are going to create a new group developers and attach the a random permission to it like AWSDirectConnectReadOnlyAccess , attach the user to it. We are going to re-attach the user to the admin group. We are going to add permission straight to the user: Now we can see that the user has 3 policies - one attached directly, one from admin group and one from the developers group. If we go to the policy section, open one up and click on the JSON button, we can see that these policies are written in the format we previously looked at: You can also create your own policies which can be done either with a simple JSON or a visual editor. If we click back to the JSON , it will be populated:","title":"IAM Policies Hands On"},{"location":"AWS/developer-associate/03-iam-and-aws-cli/04-iam-policies-hands-on/#iam-policies-hands-on","text":"If we remove the previously made user from the admin group, we would see a following error when viting the IAM page. We are going to create a new group developers and attach the a random permission to it like AWSDirectConnectReadOnlyAccess , attach the user to it. We are going to re-attach the user to the admin group. We are going to add permission straight to the user: Now we can see that the user has 3 policies - one attached directly, one from admin group and one from the developers group. If we go to the policy section, open one up and click on the JSON button, we can see that these policies are written in the format we previously looked at: You can also create your own policies which can be done either with a simple JSON or a visual editor. If we click back to the JSON , it will be populated:","title":"IAM Policies Hands On"},{"location":"AWS/developer-associate/03-iam-and-aws-cli/05-iam-mfa-overview/","text":"IAM MFA overview \u00b6 In order to protect our users and groups from being compromised we can utilize 2 defense mechanisms: Password Policy Strong passwords = better security Set minimum password length Require specific character types Including uppercase letters lowercase letters numbers non-alphanumeric characters Allow all IAM users to change their own passwords Require users to change their passwords after some time (password expiration) Prevent password re-use Multi Factor Authentication - MFA MFA = password you know + security device you own You can have several MFA device options in AWS like: Virtual MFA device Google Authenticator Authy Universal 2nd Factor (U2F) Security Key YubiKey Hardware Key Fob MFA Device Gemalto Hardware Key Fob MFA Device for AWS GovCloud (US) SurePassID","title":"IAM MFA overview"},{"location":"AWS/developer-associate/03-iam-and-aws-cli/05-iam-mfa-overview/#iam-mfa-overview","text":"In order to protect our users and groups from being compromised we can utilize 2 defense mechanisms: Password Policy Strong passwords = better security Set minimum password length Require specific character types Including uppercase letters lowercase letters numbers non-alphanumeric characters Allow all IAM users to change their own passwords Require users to change their passwords after some time (password expiration) Prevent password re-use Multi Factor Authentication - MFA MFA = password you know + security device you own You can have several MFA device options in AWS like: Virtual MFA device Google Authenticator Authy Universal 2nd Factor (U2F) Security Key YubiKey Hardware Key Fob MFA Device Gemalto Hardware Key Fob MFA Device for AWS GovCloud (US) SurePassID","title":"IAM MFA overview"},{"location":"AWS/developer-associate/03-iam-and-aws-cli/06-aws-access-keys-cli-and-sdk/","text":"AWS Access keys, CLI and SDK \u00b6 To access AWS, you have 3 options: - AWS Management Console (protected by password + MFA) - AWS Command Line Interface (CLI): protected by access keys - AWS Software Developer Kit (SDK) - for code: protected by access keys Access Keys are generated through AWS console Users manage their own access keys Access keys are secret, just like a password. Don't share them What's a CLI? \u00b6 A tool that enables you to interact with AWS services using commands in your command-line shell. Direct access to the public APIs of AWS services You can develop scripts to manage your resources It's open source https://github.com/aws/aws-cli Alternative to using AWS Management Console What's the AWS SDK? \u00b6 AWS Software Development Kit (AWS SDK) Language-specific APIs (set of libraries) Enables you to access and manage AWS services programmatically Embedded within your application Supports SDKs (Javascript, Python, PHP, .NET, Ruby, Java, Go, Node.js, C++) Mobile SDKs (Android, iOS...) IoT Device SDKs (Embedded C, Arduino, ...) Example: AWS CLI is built on AWS SDK for Python","title":"AWS Access keys, CLI and SDK"},{"location":"AWS/developer-associate/03-iam-and-aws-cli/06-aws-access-keys-cli-and-sdk/#aws-access-keys-cli-and-sdk","text":"To access AWS, you have 3 options: - AWS Management Console (protected by password + MFA) - AWS Command Line Interface (CLI): protected by access keys - AWS Software Developer Kit (SDK) - for code: protected by access keys Access Keys are generated through AWS console Users manage their own access keys Access keys are secret, just like a password. Don't share them","title":"AWS Access keys, CLI and SDK"},{"location":"AWS/developer-associate/03-iam-and-aws-cli/06-aws-access-keys-cli-and-sdk/#whats-a-cli","text":"A tool that enables you to interact with AWS services using commands in your command-line shell. Direct access to the public APIs of AWS services You can develop scripts to manage your resources It's open source https://github.com/aws/aws-cli Alternative to using AWS Management Console","title":"What's a CLI?"},{"location":"AWS/developer-associate/03-iam-and-aws-cli/06-aws-access-keys-cli-and-sdk/#whats-the-aws-sdk","text":"AWS Software Development Kit (AWS SDK) Language-specific APIs (set of libraries) Enables you to access and manage AWS services programmatically Embedded within your application Supports SDKs (Javascript, Python, PHP, .NET, Ruby, Java, Go, Node.js, C++) Mobile SDKs (Android, iOS...) IoT Device SDKs (Embedded C, Arduino, ...) Example: AWS CLI is built on AWS SDK for Python","title":"What's the AWS SDK?"},{"location":"AWS/developer-associate/03-iam-and-aws-cli/07-aws-cli-setup/","text":"AWS Setup \u00b6 Windows \u00b6 You can follow the steps listed in: https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-windows.html#cliv2-windows-install Mac \u00b6 You can followw the steps listed in: https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-mac.html Linux \u00b6 You can follow the steps listed in: https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-linux.html","title":"AWS Setup"},{"location":"AWS/developer-associate/03-iam-and-aws-cli/07-aws-cli-setup/#aws-setup","text":"","title":"AWS Setup"},{"location":"AWS/developer-associate/03-iam-and-aws-cli/07-aws-cli-setup/#windows","text":"You can follow the steps listed in: https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-windows.html#cliv2-windows-install","title":"Windows"},{"location":"AWS/developer-associate/03-iam-and-aws-cli/07-aws-cli-setup/#mac","text":"You can followw the steps listed in: https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-mac.html","title":"Mac"},{"location":"AWS/developer-associate/03-iam-and-aws-cli/07-aws-cli-setup/#linux","text":"You can follow the steps listed in: https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-linux.html","title":"Linux"},{"location":"AWS/developer-associate/03-iam-and-aws-cli/08-aws-cli-hands-on/","text":"AWS CLI Hands on \u00b6 You can obtain access key in your IAM account, under Security Credentials -> Access Keys -> Create Access Keys . Do not generate access credentials for your root account! Only add them for your IAM accounts. \u279c /tmp aws --version aws-cli/2.2.22 Python/3.8.8 Linux/5.12.15-arch1-1 exe/x86_64.arch prompt/off \u279c /tmp aws configure AWS Access Key ID [None]: **REDACTED** AWS Secret Access Key [None]: **REDACTED** Default region name [None]: eu-west-1 Default output format [None]: \u279c /tmp aws iam list-users { \"Users\": [ { \"Path\": \"/\", \"UserName\": \"davis\", \"UserId\": \"SOME-ID\", \"Arn\": \"arn:aws:iam::SOME-ID:user/davis\", \"CreateDate\": \"2021-06-28T10:26:17+00:00\", \"PasswordLastUsed\": \"2021-07-26T15:15:40+00:00\" } ] } Note that in order to receive the list of users previously executed, you need to have the permissions to do so in the IAM. If the request is denied - it might return nothing.","title":"AWS CLI Hands on"},{"location":"AWS/developer-associate/03-iam-and-aws-cli/08-aws-cli-hands-on/#aws-cli-hands-on","text":"You can obtain access key in your IAM account, under Security Credentials -> Access Keys -> Create Access Keys . Do not generate access credentials for your root account! Only add them for your IAM accounts. \u279c /tmp aws --version aws-cli/2.2.22 Python/3.8.8 Linux/5.12.15-arch1-1 exe/x86_64.arch prompt/off \u279c /tmp aws configure AWS Access Key ID [None]: **REDACTED** AWS Secret Access Key [None]: **REDACTED** Default region name [None]: eu-west-1 Default output format [None]: \u279c /tmp aws iam list-users { \"Users\": [ { \"Path\": \"/\", \"UserName\": \"davis\", \"UserId\": \"SOME-ID\", \"Arn\": \"arn:aws:iam::SOME-ID:user/davis\", \"CreateDate\": \"2021-06-28T10:26:17+00:00\", \"PasswordLastUsed\": \"2021-07-26T15:15:40+00:00\" } ] } Note that in order to receive the list of users previously executed, you need to have the permissions to do so in the IAM. If the request is denied - it might return nothing.","title":"AWS CLI Hands on"},{"location":"AWS/developer-associate/03-iam-and-aws-cli/10-iam-roles-for-services/","text":"IAM Roles for Services \u00b6 Some services in AWS need to perform actions on your behalf. For this, just like users, they will need permissions. To do this, we will assign permissions to AWS services with IAM Roles. For example, we will create an EC2 instance. This EC2 instance may want to perform some actions on AWS. In order to do this, we need to give this EC2 instance permissions to do these actions. Some of the most common roles we are going to use: - EC2 Instance Roles - Lambda Function Roles - Roles for CloudFormation","title":"IAM Roles for Services"},{"location":"AWS/developer-associate/03-iam-and-aws-cli/10-iam-roles-for-services/#iam-roles-for-services","text":"Some services in AWS need to perform actions on your behalf. For this, just like users, they will need permissions. To do this, we will assign permissions to AWS services with IAM Roles. For example, we will create an EC2 instance. This EC2 instance may want to perform some actions on AWS. In order to do this, we need to give this EC2 instance permissions to do these actions. Some of the most common roles we are going to use: - EC2 Instance Roles - Lambda Function Roles - Roles for CloudFormation","title":"IAM Roles for Services"},{"location":"AWS/developer-associate/03-iam-and-aws-cli/11-iam-roles-hands-on/","text":"IAM Roles Hands On \u00b6 On the left side of the IAM we have a section called Roles . We are going to create a new role. There is a button on the right side for it. This will be a role for an AWS Service, we are going to select the EC2 Instance type below. In the next step we can attach the necessary permissions, we are going to select IAMReadOnlyAccess and click next. We are going to skip the tag step and then we are going to give it a name DemoRoleForEC2 .","title":"IAM Roles Hands On"},{"location":"AWS/developer-associate/03-iam-and-aws-cli/11-iam-roles-hands-on/#iam-roles-hands-on","text":"On the left side of the IAM we have a section called Roles . We are going to create a new role. There is a button on the right side for it. This will be a role for an AWS Service, we are going to select the EC2 Instance type below. In the next step we can attach the necessary permissions, we are going to select IAMReadOnlyAccess and click next. We are going to skip the tag step and then we are going to give it a name DemoRoleForEC2 .","title":"IAM Roles Hands On"},{"location":"AWS/developer-associate/03-iam-and-aws-cli/15-iam-summary/","text":"IAM Summary \u00b6 Users: mapped to a physical user; has a password for AWS Console. Groups: contains users only Policies: JSON document that outlines permissions for users or groups Roles: for EC2 instance or AWS services Security: MFA + Password Policy Access Keys: access AWS using the CLI or SDK Audit: IAM Credential Reports & IAM Access Advisor","title":"IAM Summary"},{"location":"AWS/developer-associate/03-iam-and-aws-cli/15-iam-summary/#iam-summary","text":"Users: mapped to a physical user; has a password for AWS Console. Groups: contains users only Policies: JSON document that outlines permissions for users or groups Roles: for EC2 instance or AWS services Security: MFA + Password Policy Access Keys: access AWS using the CLI or SDK Audit: IAM Credential Reports & IAM Access Advisor","title":"IAM Summary"},{"location":"AWS/developer-associate/03-iam-and-aws-cli/AWS%20CloudShell/","text":"AWS CloudShell \u00b6 AWS CloudShell is an alternative for using a terminal issue aws commands. Currently it is only available on a few regions though: https://docs.aws.amazon.com/cloudshell/latest/userguide/supported-aws-regions.html You can access it via the management console. It will probably take a minute to load on the first time. But after that a terminal will open up. It can be used like any ordinary linux shell, not just for aws commands. You can also upload and download files from the shell via the actions button on the right side.","title":"AWS CloudShell"},{"location":"AWS/developer-associate/03-iam-and-aws-cli/AWS%20CloudShell/#aws-cloudshell","text":"AWS CloudShell is an alternative for using a terminal issue aws commands. Currently it is only available on a few regions though: https://docs.aws.amazon.com/cloudshell/latest/userguide/supported-aws-regions.html You can access it via the management console. It will probably take a minute to load on the first time. But after that a terminal will open up. It can be used like any ordinary linux shell, not just for aws commands. You can also upload and download files from the shell via the actions button on the right side.","title":"AWS CloudShell"},{"location":"AWS/developer-associate/04-ec2-fundamentals/01-aws-budget-setup/","text":"AWS Budget Setup \u00b6 In order not to spend too much, we are first going to set up a budget. When you are logged into your AWS account, click on your username and go to My Billing Dashboard . When you are visiting the billing page, an error might come up saying that you need permissions. This can be fixed by accessing your root account. Then click on our username and go to My Account . We are going to scroll down and access the page called IAM User And Role Access To Billing Information . Then we are going to activate it. After that, we can access the billing information with users that have the IAM permissions set, e.g., administrators. Here we cann view all the bills that we have previously had, explore the costs etc. We are going to open up the Budgets section and add a new budget, with a type Cost Budget . We are going to set up a monthly recurring budget with a fixed amount that we are willing to spend each month. In the next step we can configure thresholds. This is for configuring notifications that will alert you that you have reached a certain threshold in your spending.","title":"AWS Budget Setup"},{"location":"AWS/developer-associate/04-ec2-fundamentals/01-aws-budget-setup/#aws-budget-setup","text":"In order not to spend too much, we are first going to set up a budget. When you are logged into your AWS account, click on your username and go to My Billing Dashboard . When you are visiting the billing page, an error might come up saying that you need permissions. This can be fixed by accessing your root account. Then click on our username and go to My Account . We are going to scroll down and access the page called IAM User And Role Access To Billing Information . Then we are going to activate it. After that, we can access the billing information with users that have the IAM permissions set, e.g., administrators. Here we cann view all the bills that we have previously had, explore the costs etc. We are going to open up the Budgets section and add a new budget, with a type Cost Budget . We are going to set up a monthly recurring budget with a fixed amount that we are willing to spend each month. In the next step we can configure thresholds. This is for configuring notifications that will alert you that you have reached a certain threshold in your spending.","title":"AWS Budget Setup"},{"location":"AWS/developer-associate/04-ec2-fundamentals/02-ec2-basics/","text":"EC2 Basics \u00b6 EC2 is one of the most popular of AWS' offerings. It stands for Elastic Compute Cloud, is used for Infrastructure as a Service on AWS. It mainly consists of capability of: - Renting virtual machines (EC2) - Storing data on virtual drives (EBS volumes) - Distributing load accross machines (ELB - Elastic Load Balancer) - Scaling the services using an auto-scaling group (ASG) - Knowing EC2 is fundamental to understand how the Cloud works EC2 sizing & configuration options \u00b6 Operating system (OS): Linux, Windows or Mac OS How much compute power & cores (CPU) How much random-access memory (RAM) How much storage space: Network-attached (EBS&EFS) Hardware (EC2 Instance Store) Network card: speed of the card, public IP address Firewall rules: security group Bootstrap script (configure at first launch): EC2 User Data EC2 User Data \u00b6 It's possible to bootstrap our instances using an EC2 User data script. This means launching commands when a machine starts on the first launch of the instance. EC2 user data is used to automate boot tasks like: - Installing updates - Installing software - Downloading common files from the internet One thing to remember is that the EC2 data script runs with the root user. EC2 instance types: example \u00b6 The t2.micro is part of the AWS free tier (up to 750 hours per month).","title":"EC2 Basics"},{"location":"AWS/developer-associate/04-ec2-fundamentals/02-ec2-basics/#ec2-basics","text":"EC2 is one of the most popular of AWS' offerings. It stands for Elastic Compute Cloud, is used for Infrastructure as a Service on AWS. It mainly consists of capability of: - Renting virtual machines (EC2) - Storing data on virtual drives (EBS volumes) - Distributing load accross machines (ELB - Elastic Load Balancer) - Scaling the services using an auto-scaling group (ASG) - Knowing EC2 is fundamental to understand how the Cloud works","title":"EC2 Basics"},{"location":"AWS/developer-associate/04-ec2-fundamentals/02-ec2-basics/#ec2-sizing-configuration-options","text":"Operating system (OS): Linux, Windows or Mac OS How much compute power & cores (CPU) How much random-access memory (RAM) How much storage space: Network-attached (EBS&EFS) Hardware (EC2 Instance Store) Network card: speed of the card, public IP address Firewall rules: security group Bootstrap script (configure at first launch): EC2 User Data","title":"EC2 sizing &amp; configuration options"},{"location":"AWS/developer-associate/04-ec2-fundamentals/02-ec2-basics/#ec2-user-data","text":"It's possible to bootstrap our instances using an EC2 User data script. This means launching commands when a machine starts on the first launch of the instance. EC2 user data is used to automate boot tasks like: - Installing updates - Installing software - Downloading common files from the internet One thing to remember is that the EC2 data script runs with the root user.","title":"EC2 User Data"},{"location":"AWS/developer-associate/04-ec2-fundamentals/02-ec2-basics/#ec2-instance-types-example","text":"The t2.micro is part of the AWS free tier (up to 750 hours per month).","title":"EC2 instance types: example"},{"location":"AWS/developer-associate/04-ec2-fundamentals/03-ec2-instance-hands-on/","text":"Create an EC2 instance with EC2 user data to have a webiste hands on \u00b6 We'll be launchin our first virtual server using AWS Console. We'll get a first high-level approach to the various parameters We'll see that our web server is launched using EC2 user data We'll learn how to start / stop / terminate our instances. We are going to the EC2 console by typing EC2 in the search bar. Once that's done we are going to launch a new instance, but before you do - make sure you have selected a region that is most suitable for you. Creating a new instance can be done by navigating to Instances page and clicking on the launch instance button. First, we have to choose the AMI - Amazon Machine Image. We can use the quick start AMIs like Amazon Linux, macOs etc. There is also an option to choose from my own created AMIs, market places AMIs and community AMIs. We are going to select the Amazon Linux from the quick start page as it is free tier eligible. On the next step we can select the instance type which basically tells how much compute power, memory and network capability we have. We are going to use the t2.micro one. On the next step we have an option to change the instance details. Here we can configure things like how many instances we want to launch, the network configuration, IAM roles, file systems. We are going to leave everything as is except for one thing we are interested in - the user data field. We are going to use the following script: #!/bin/bash # Use this for your user data yum update -y yum install -y httpd systemctl start httpd systemctl enable httpd echo \"<h1>Hello World from $(hostname -f)</h1>\" > /var/www/html/index.html Then, the next step is to configure storage. We can configure how much storage we will have available on the instance, the speed of it as well as an option to encrypt it, delete on instance termination. The next step is to add tags to our EC2 instance. Then, we can configure the security group. We can use an existing security group or configure a new one. We are going to create a new one and allow the HTTP (port 80) to go through. The next step is to simply review what we are creating. Once we're ready, we click on launch. When we launch it, we need to select a key pair. It is used for SSH when we are trying to log into our instance. We are going to create a new one and download it. Make sure not to lose the file. After that, our instance will be in the creation process. It will probably take a minute for it to go live. Once it's ready, we can view the details of the instance. There will be a field with a public IP address. If we open up that IP address in the browser, it will show our new website. (note that the link next to the public ip address might not work as it will use https:// instead of http:// ) We can stop the instance by selecting it and and choosing clicking on the Instance State , selecting the Stop instance option. (This can also be done by right-clicking on the instance). If we try to connect to the same webpage again, we won't be able to as the instance is stopped. In order to get rid of the instance - we can select the Terminate Instance option. It will delete it. Note, that when you stop the instance and start it again, you'll get a new Public IP Address.","title":"Create an EC2 instance with EC2 user data to have a webiste hands on"},{"location":"AWS/developer-associate/04-ec2-fundamentals/03-ec2-instance-hands-on/#create-an-ec2-instance-with-ec2-user-data-to-have-a-webiste-hands-on","text":"We'll be launchin our first virtual server using AWS Console. We'll get a first high-level approach to the various parameters We'll see that our web server is launched using EC2 user data We'll learn how to start / stop / terminate our instances. We are going to the EC2 console by typing EC2 in the search bar. Once that's done we are going to launch a new instance, but before you do - make sure you have selected a region that is most suitable for you. Creating a new instance can be done by navigating to Instances page and clicking on the launch instance button. First, we have to choose the AMI - Amazon Machine Image. We can use the quick start AMIs like Amazon Linux, macOs etc. There is also an option to choose from my own created AMIs, market places AMIs and community AMIs. We are going to select the Amazon Linux from the quick start page as it is free tier eligible. On the next step we can select the instance type which basically tells how much compute power, memory and network capability we have. We are going to use the t2.micro one. On the next step we have an option to change the instance details. Here we can configure things like how many instances we want to launch, the network configuration, IAM roles, file systems. We are going to leave everything as is except for one thing we are interested in - the user data field. We are going to use the following script: #!/bin/bash # Use this for your user data yum update -y yum install -y httpd systemctl start httpd systemctl enable httpd echo \"<h1>Hello World from $(hostname -f)</h1>\" > /var/www/html/index.html Then, the next step is to configure storage. We can configure how much storage we will have available on the instance, the speed of it as well as an option to encrypt it, delete on instance termination. The next step is to add tags to our EC2 instance. Then, we can configure the security group. We can use an existing security group or configure a new one. We are going to create a new one and allow the HTTP (port 80) to go through. The next step is to simply review what we are creating. Once we're ready, we click on launch. When we launch it, we need to select a key pair. It is used for SSH when we are trying to log into our instance. We are going to create a new one and download it. Make sure not to lose the file. After that, our instance will be in the creation process. It will probably take a minute for it to go live. Once it's ready, we can view the details of the instance. There will be a field with a public IP address. If we open up that IP address in the browser, it will show our new website. (note that the link next to the public ip address might not work as it will use https:// instead of http:// ) We can stop the instance by selecting it and and choosing clicking on the Instance State , selecting the Stop instance option. (This can also be done by right-clicking on the instance). If we try to connect to the same webpage again, we won't be able to as the instance is stopped. In order to get rid of the instance - we can select the Terminate Instance option. It will delete it. Note, that when you stop the instance and start it again, you'll get a new Public IP Address.","title":"Create an EC2 instance with EC2 user data to have a webiste hands on"},{"location":"AWS/developer-associate/04-ec2-fundamentals/05-security-groups-and-classing-ports-overview/","text":"Security Groups & Classic Ports Overview \u00b6 Security Groups are fundamental of network security in AWS. They control how traffic is allowed into or out of our EC2 instances. The security groups only contain allow rules Security groups rules can reference by IP or by security group. Security groups are acting as a firewall on EC2 instances They regulate: Access to ports Authorized IP ranges - IPv4 and IPv6 Control of inbound network (from other to the instance) Control of outbound network (from the instance to other) The security groups can be attached to multiple instances. Locked down to a region / VPC combination security groups live outside the EC21 - if traffic is blocked, the EC2 instance won't see it It's good to maintain one separate security group for SSH access If your application is not accessible (time out), it's a security group issue. If your application gives a \"connection refused\" error - it's an application error or it's not launched. All inbound traffic is blocked by default All outbound traffic is authorised by default","title":"Security Groups & Classic Ports Overview"},{"location":"AWS/developer-associate/04-ec2-fundamentals/05-security-groups-and-classing-ports-overview/#security-groups-classic-ports-overview","text":"Security Groups are fundamental of network security in AWS. They control how traffic is allowed into or out of our EC2 instances. The security groups only contain allow rules Security groups rules can reference by IP or by security group. Security groups are acting as a firewall on EC2 instances They regulate: Access to ports Authorized IP ranges - IPv4 and IPv6 Control of inbound network (from other to the instance) Control of outbound network (from the instance to other) The security groups can be attached to multiple instances. Locked down to a region / VPC combination security groups live outside the EC21 - if traffic is blocked, the EC2 instance won't see it It's good to maintain one separate security group for SSH access If your application is not accessible (time out), it's a security group issue. If your application gives a \"connection refused\" error - it's an application error or it's not launched. All inbound traffic is blocked by default All outbound traffic is authorised by default","title":"Security Groups &amp; Classic Ports Overview"},{"location":"AWS/developer-associate/04-ec2-fundamentals/06-security-groups-hands-on/","text":"Security Groups Hands On \u00b6 We can go into AWS EC2 -> Security Groups to find our security groups. They are the way to control access to your EC2 instances. The launch-wizard-1 security group is a security group that was created when we launched an EC2 instance. The default security group is created when your AWS account is created. If we select the launch-wizard-1 we can see that we have 3 bound rules: And 1 outbound rule: The inbound rules specify that we can connect to the machine by using HTTP (port 80) on both IPv4 and IPv6. As well as ssh into it via the port 2. The outbound rules specify that everything is allowed to go out of the instance. We can edit the rules by clicking on the edit inbound|outbound rules button next to the table. If we were to delete the port 80 inbound rules, once we open our instance in our browser, it will get a timeout as the instance will not respond to us. We can also open up ports by adding new entries: We can select the type (port) as well as the source which can be from anywhere, can be specific IP addresses or even other security groups.","title":"Security Groups Hands On"},{"location":"AWS/developer-associate/04-ec2-fundamentals/06-security-groups-hands-on/#security-groups-hands-on","text":"We can go into AWS EC2 -> Security Groups to find our security groups. They are the way to control access to your EC2 instances. The launch-wizard-1 security group is a security group that was created when we launched an EC2 instance. The default security group is created when your AWS account is created. If we select the launch-wizard-1 we can see that we have 3 bound rules: And 1 outbound rule: The inbound rules specify that we can connect to the machine by using HTTP (port 80) on both IPv4 and IPv6. As well as ssh into it via the port 2. The outbound rules specify that everything is allowed to go out of the instance. We can edit the rules by clicking on the edit inbound|outbound rules button next to the table. If we were to delete the port 80 inbound rules, once we open our instance in our browser, it will get a timeout as the instance will not respond to us. We can also open up ports by adding new entries: We can select the type (port) as well as the source which can be from anywhere, can be specific IP addresses or even other security groups.","title":"Security Groups Hands On"},{"location":"AWS/developer-associate/04-ec2-fundamentals/07-ssh-overview/","text":"SSH Overview \u00b6 The SSH is used to connect to the cloud instances to perform maintenance or other actions. Depending on your operating system, there can be several ways to connect to the instance: SSH Putty EC2 Instance Connect Mac + + Linux + + Windows < 10 + + Windows >= 10 + + + SSH using linux or mac \u00b6 Get the public IP of the instance Make sure in the inbound rules that the port 22 is open and the source can be from your IP address. Locate the PEM key you downloaded when the instance was created Ssh into your instance: ssh -i EC2Tutorial.pem ec2-user@PUBLIC-IP If there is an error saying Permissions 0644 for 'EC2Tutorial.pem' are too open - you need to change the key permissions. chmod 0400 EC2Tutorial.pem SSH using windows \u00b6 You can configure all the required parameters necessary for doing SSH on windows using the free tool called Putty . Install Putty Open up a program called PuttyGen or Putty Key Generator to convert the PEM key file to PPK. Click on File -> Load private key and select the PEM file. (make sure that the file type filter is selected to All Files). Set a password if wanted Click on save private key Open up Putty Under the host name enter ec2-user@PUBLIC-IP-ADDRESS Save it under the saved sessions On the left side pane we go to Connection -> SSH -> Auth , there will be an option to choose a private keyfile for authentication - use the previously generated key. Save the profile once more Click on open SSH using windows 10 \u00b6 Open up PowerShell/cmd and type ssh - it should return a usage documentation. If it does - you can proceed. If not - use putty. Locate the PEM key you downloaded when the instance was created Ssh into your instance: ssh -i C:\\Users\\Davis\\Downloads\\Ec2Tutorial.pem EC2Tutorial.pem ec2-user@PUBLIC-IP If there is an error saying Permissions 0644 for 'EC2Tutorial.pem' are too open - you need to change the key permissions. You can do that by right-clicking on the file Properties -> Security -> Advanced . - The owner of the key should be you. - Disable inheritance - Remove any other users","title":"SSH Overview"},{"location":"AWS/developer-associate/04-ec2-fundamentals/07-ssh-overview/#ssh-overview","text":"The SSH is used to connect to the cloud instances to perform maintenance or other actions. Depending on your operating system, there can be several ways to connect to the instance: SSH Putty EC2 Instance Connect Mac + + Linux + + Windows < 10 + + Windows >= 10 + + +","title":"SSH Overview"},{"location":"AWS/developer-associate/04-ec2-fundamentals/07-ssh-overview/#ssh-using-linux-or-mac","text":"Get the public IP of the instance Make sure in the inbound rules that the port 22 is open and the source can be from your IP address. Locate the PEM key you downloaded when the instance was created Ssh into your instance: ssh -i EC2Tutorial.pem ec2-user@PUBLIC-IP If there is an error saying Permissions 0644 for 'EC2Tutorial.pem' are too open - you need to change the key permissions. chmod 0400 EC2Tutorial.pem","title":"SSH using linux or mac"},{"location":"AWS/developer-associate/04-ec2-fundamentals/07-ssh-overview/#ssh-using-windows","text":"You can configure all the required parameters necessary for doing SSH on windows using the free tool called Putty . Install Putty Open up a program called PuttyGen or Putty Key Generator to convert the PEM key file to PPK. Click on File -> Load private key and select the PEM file. (make sure that the file type filter is selected to All Files). Set a password if wanted Click on save private key Open up Putty Under the host name enter ec2-user@PUBLIC-IP-ADDRESS Save it under the saved sessions On the left side pane we go to Connection -> SSH -> Auth , there will be an option to choose a private keyfile for authentication - use the previously generated key. Save the profile once more Click on open","title":"SSH using windows"},{"location":"AWS/developer-associate/04-ec2-fundamentals/07-ssh-overview/#ssh-using-windows-10","text":"Open up PowerShell/cmd and type ssh - it should return a usage documentation. If it does - you can proceed. If not - use putty. Locate the PEM key you downloaded when the instance was created Ssh into your instance: ssh -i C:\\Users\\Davis\\Downloads\\Ec2Tutorial.pem EC2Tutorial.pem ec2-user@PUBLIC-IP If there is an error saying Permissions 0644 for 'EC2Tutorial.pem' are too open - you need to change the key permissions. You can do that by right-clicking on the file Properties -> Security -> Advanced . - The owner of the key should be you. - Disable inheritance - Remove any other users","title":"SSH using windows 10"},{"location":"AWS/developer-associate/04-ec2-fundamentals/08-ssh-troubleshooting/","text":"SSH troubleshooting \u00b6 1. There's a connection timeout \u00b6 This is a security group issue. Any timeout (not just for SSH) is related to security groups or a firewall. 2. There's still a connection timeout issue \u00b6 If your security group is properly configured as above, and you still have connection timeout issues, then that means a corporate firewall or a personal firewall is blocking the connection. Use the EC2 Instance Connect. 3. SSH does not work on Windows \u00b6 If it says ssh command not found , that means that you have to use Putty. If that doesn't work - use the EC2 instance connect. 4. There's a connection refused \u00b6 This means that the instance is reachable, but no SSH utility is running on the instance Try to restart the instance If it doesn't work, terminate the instance and create a new one. Make sure you're using Amazon Linux 2 5. Permission denied (publickey, gssapi-keyex,gssapi-with-mic) \u00b6 This means either two things: - You are using wrong security key or not using a security key. - You are using the wrong user. Make sure you have started an Amazon Linux 2 EC2 instance, and make sure you're using the user ec2-user.","title":"SSH troubleshooting"},{"location":"AWS/developer-associate/04-ec2-fundamentals/08-ssh-troubleshooting/#ssh-troubleshooting","text":"","title":"SSH troubleshooting"},{"location":"AWS/developer-associate/04-ec2-fundamentals/08-ssh-troubleshooting/#1-theres-a-connection-timeout","text":"This is a security group issue. Any timeout (not just for SSH) is related to security groups or a firewall.","title":"1. There's a connection timeout"},{"location":"AWS/developer-associate/04-ec2-fundamentals/08-ssh-troubleshooting/#2-theres-still-a-connection-timeout-issue","text":"If your security group is properly configured as above, and you still have connection timeout issues, then that means a corporate firewall or a personal firewall is blocking the connection. Use the EC2 Instance Connect.","title":"2. There's still a connection timeout issue"},{"location":"AWS/developer-associate/04-ec2-fundamentals/08-ssh-troubleshooting/#3-ssh-does-not-work-on-windows","text":"If it says ssh command not found , that means that you have to use Putty. If that doesn't work - use the EC2 instance connect.","title":"3. SSH does not work on Windows"},{"location":"AWS/developer-associate/04-ec2-fundamentals/08-ssh-troubleshooting/#4-theres-a-connection-refused","text":"This means that the instance is reachable, but no SSH utility is running on the instance Try to restart the instance If it doesn't work, terminate the instance and create a new one. Make sure you're using Amazon Linux 2","title":"4. There's a connection refused"},{"location":"AWS/developer-associate/04-ec2-fundamentals/08-ssh-troubleshooting/#5-permission-denied-publickey-gssapi-keyexgssapi-with-mic","text":"This means either two things: - You are using wrong security key or not using a security key. - You are using the wrong user. Make sure you have started an Amazon Linux 2 EC2 instance, and make sure you're using the user ec2-user.","title":"5. Permission denied (publickey, gssapi-keyex,gssapi-with-mic)"},{"location":"AWS/developer-associate/04-ec2-fundamentals/09-ec2-instance-connect/","text":"EC2 Instance Connect \u00b6 You can use an alternative to ssh for connecting to your ec2 instances - the EC2 Instance Connect. In the EC2 Instances view you can select an instance and click on the connect button. Then the first tab should be the EC2 Instance Connect . This is going to open up a terminal to the EC2 instance: Note that this still relies on SSH. If you have disabled the SSH inbound rules in your security groups - you will get a timeout.","title":"EC2 Instance Connect"},{"location":"AWS/developer-associate/04-ec2-fundamentals/09-ec2-instance-connect/#ec2-instance-connect","text":"You can use an alternative to ssh for connecting to your ec2 instances - the EC2 Instance Connect. In the EC2 Instances view you can select an instance and click on the connect button. Then the first tab should be the EC2 Instance Connect . This is going to open up a terminal to the EC2 instance: Note that this still relies on SSH. If you have disabled the SSH inbound rules in your security groups - you will get a timeout.","title":"EC2 Instance Connect"},{"location":"AWS/developer-associate/04-ec2-fundamentals/10-ec2-instance-roles-demo/","text":"EC2 Instance Roles Demo \u00b6 For this demo we are going to SSH into our EC2 instance. The Amazon Linux comes with aws cli pre-installed. [ec2-user@ip-172-31-16-73 ~]$ aws --version aws-cli/1.18.147 Python/2.7.18 Linux/4.14.243-185.433.amzn2.x86_64 botocore/1.18.6 So, we can start using it. [ec2-user@ip-172-31-16-73 ~]$ aws iam list-users Unable to locate credentials. You can configure credentials by running \"aws configure\". We can configure this by using the aws configure and providing the access credentials, but but that really is a bad idea . The reason for that is that anyone with the access to the aws account can connect into the instance and retrieve your private credentials. The other way to do this is to use IAM roles. In the previous sections we created an IAM role with a read only access to IAM. We are going to attach it to the EC2 instance. That can be done by selecting an EC2 instance and clicking on Actions -> Security -> Modify IAM role . And there we are going to select the role: It should be visible under instance details now: If we try try the same command now, it will work: [ec2-user@ip-172-31-16-73 ~]$ aws iam list-users { \"Users\": [ { \"UserName\": \"davis\", ... If we were to detach the permission now. [ec2-user@ip-172-31-16-73 ~]$ aws iam list-users An error occurred (AccessDenied) when calling the ListUsers operation: User: arn:aws:sts::539690530154:assumed-role/DemoRoleForEc2/i-0db772bcf6e955804 is not authorized to perform: iam:ListUsers on resource: arn:aws:iam::539690530154:user/ If you were to add it once more, there might be a situation that you cannot connect right away. It might take a second to apply the changes.","title":"EC2 Instance Roles Demo"},{"location":"AWS/developer-associate/04-ec2-fundamentals/10-ec2-instance-roles-demo/#ec2-instance-roles-demo","text":"For this demo we are going to SSH into our EC2 instance. The Amazon Linux comes with aws cli pre-installed. [ec2-user@ip-172-31-16-73 ~]$ aws --version aws-cli/1.18.147 Python/2.7.18 Linux/4.14.243-185.433.amzn2.x86_64 botocore/1.18.6 So, we can start using it. [ec2-user@ip-172-31-16-73 ~]$ aws iam list-users Unable to locate credentials. You can configure credentials by running \"aws configure\". We can configure this by using the aws configure and providing the access credentials, but but that really is a bad idea . The reason for that is that anyone with the access to the aws account can connect into the instance and retrieve your private credentials. The other way to do this is to use IAM roles. In the previous sections we created an IAM role with a read only access to IAM. We are going to attach it to the EC2 instance. That can be done by selecting an EC2 instance and clicking on Actions -> Security -> Modify IAM role . And there we are going to select the role: It should be visible under instance details now: If we try try the same command now, it will work: [ec2-user@ip-172-31-16-73 ~]$ aws iam list-users { \"Users\": [ { \"UserName\": \"davis\", ... If we were to detach the permission now. [ec2-user@ip-172-31-16-73 ~]$ aws iam list-users An error occurred (AccessDenied) when calling the ListUsers operation: User: arn:aws:sts::539690530154:assumed-role/DemoRoleForEc2/i-0db772bcf6e955804 is not authorized to perform: iam:ListUsers on resource: arn:aws:iam::539690530154:user/ If you were to add it once more, there might be a situation that you cannot connect right away. It might take a second to apply the changes.","title":"EC2 Instance Roles Demo"},{"location":"AWS/developer-associate/04-ec2-fundamentals/11-ec2-instance-launch-types/","text":"EC2 Instance Launch Types \u00b6 EC2 Instances Purchasing Options \u00b6 On-demand Instances: short workload, predictable pricing Reserved: minimum 1 year Reserved Instances: long workloads Convertible Reserved Instances: long workloads with flexible instances (can be converted to another size) Scheduled Reserve Instances: example - every Thursday between 3 and 6pm Spot Instances: short workloads, cheap, can lose instances (less reliable) Dedicated Hosts: book an entire physical server, control instance placement EC2 on demand \u00b6 Pay for what you use: Linux - billing per second, after the first minute All other OSes - billing per hour Has the highest cost but no upfront payment No long-term commitment Recommended for short-term and un-interrupted workloads, where you can't predict how the application will behave. EC2 Reserved Instances \u00b6 Up to 75% discount compared to on-demand Reservation period: 1 year = + discount | 3 years = +++ discount Purchasing options: no upfront | partial upfront = + | All upfront = ++ discount Reserve a specific instance type Recommended for steady-state usage applications (like databases) Convertible Reserved Instance can change the EC2 instance type Up to 54% discount Scheduled Reserved Instances launch within time window you reserve When you require a fraction of day / week / month Still commitment over 1 to 3 years EC2 Spot Instances \u00b6 Can get a discount of up to 90% compared to on-demand Instances that you can \"lose\" at any point of time if your max price is less than the current spot price. The MOST cost-efficient instance in AWS Useful for workloads that are resilient to failure Batch jobs data analysis Image processing Any distributed workloads Workloads with a flexible start and end time not suited for critical jobs or databases EC2 Dedicated Hosts \u00b6 An Amazon EC2 Decicated Host is a physical server with EC2 instance capability fully dedicated to your use. Dedicated Hosts can help you address compliance requirements and reduce costs by allowing you to use your existing server-bound software licenses. Allocated for your account for a 3-year period reservation. More expensive Useful for software that have complicated licensing model (BYOL - Bring Your Own License) Or for companies that have strong regulatory or compliance needs EC2 Dedicated Instances \u00b6 Instances running on hardware that's dedicated to you. May share hardware with other instances in same account. No control over instance placement (can move hardware after Stop/Start)","title":"EC2 Instance Launch Types"},{"location":"AWS/developer-associate/04-ec2-fundamentals/11-ec2-instance-launch-types/#ec2-instance-launch-types","text":"","title":"EC2 Instance Launch Types"},{"location":"AWS/developer-associate/04-ec2-fundamentals/11-ec2-instance-launch-types/#ec2-instances-purchasing-options","text":"On-demand Instances: short workload, predictable pricing Reserved: minimum 1 year Reserved Instances: long workloads Convertible Reserved Instances: long workloads with flexible instances (can be converted to another size) Scheduled Reserve Instances: example - every Thursday between 3 and 6pm Spot Instances: short workloads, cheap, can lose instances (less reliable) Dedicated Hosts: book an entire physical server, control instance placement","title":"EC2 Instances Purchasing Options"},{"location":"AWS/developer-associate/04-ec2-fundamentals/11-ec2-instance-launch-types/#ec2-on-demand","text":"Pay for what you use: Linux - billing per second, after the first minute All other OSes - billing per hour Has the highest cost but no upfront payment No long-term commitment Recommended for short-term and un-interrupted workloads, where you can't predict how the application will behave.","title":"EC2 on demand"},{"location":"AWS/developer-associate/04-ec2-fundamentals/11-ec2-instance-launch-types/#ec2-reserved-instances","text":"Up to 75% discount compared to on-demand Reservation period: 1 year = + discount | 3 years = +++ discount Purchasing options: no upfront | partial upfront = + | All upfront = ++ discount Reserve a specific instance type Recommended for steady-state usage applications (like databases) Convertible Reserved Instance can change the EC2 instance type Up to 54% discount Scheduled Reserved Instances launch within time window you reserve When you require a fraction of day / week / month Still commitment over 1 to 3 years","title":"EC2 Reserved Instances"},{"location":"AWS/developer-associate/04-ec2-fundamentals/11-ec2-instance-launch-types/#ec2-spot-instances","text":"Can get a discount of up to 90% compared to on-demand Instances that you can \"lose\" at any point of time if your max price is less than the current spot price. The MOST cost-efficient instance in AWS Useful for workloads that are resilient to failure Batch jobs data analysis Image processing Any distributed workloads Workloads with a flexible start and end time not suited for critical jobs or databases","title":"EC2 Spot Instances"},{"location":"AWS/developer-associate/04-ec2-fundamentals/11-ec2-instance-launch-types/#ec2-dedicated-hosts","text":"An Amazon EC2 Decicated Host is a physical server with EC2 instance capability fully dedicated to your use. Dedicated Hosts can help you address compliance requirements and reduce costs by allowing you to use your existing server-bound software licenses. Allocated for your account for a 3-year period reservation. More expensive Useful for software that have complicated licensing model (BYOL - Bring Your Own License) Or for companies that have strong regulatory or compliance needs","title":"EC2 Dedicated Hosts"},{"location":"AWS/developer-associate/04-ec2-fundamentals/11-ec2-instance-launch-types/#ec2-dedicated-instances","text":"Instances running on hardware that's dedicated to you. May share hardware with other instances in same account. No control over instance placement (can move hardware after Stop/Start)","title":"EC2 Dedicated Instances"},{"location":"AWS/developer-associate/04-ec2-fundamentals/EC2%20Instance%20Types/","text":"EC2 Instance Types Overview \u00b6 You can use different types of EC2 instances that are optimized for different use cases: https://aws.amazon.com/ec2/instance-types/ Currently we have 7 different types of EC2 instances: - General Purpose - Compute Optimized - Memory Optimized - Accelerated Computing - Storage Optimized - Instance Features - Measuring Instance Performance Each of these types have a different set of families. The AWS has the following naming convention like: - m5.2xlarge - m: instance class - 5: generation (AWS Improves them over time) - 2xlarge: the size within the instance class. The more the size, the more the CPU, memory etc. EC2 Instance Types - General Purpose \u00b6 Great for a diversity of workloads such as web servers or code repositories. Balance between Compute, Memory, Networking. In this course, we will be using the t2.micro which is a General Purpose EC2 instance. EC2 Instance Types - Compute Optimized \u00b6 Great for compute-intensive tasks that require high performance processors: Batch processing workloads Media transcoding High performance web servers High performance computing (HPC) Scientific modeling & machine learning Dedicated gaming servers EC2 Instance Types - Memory Optimized \u00b6 Fast performance for workloads that process large data sets in memory. Use cases: High performance, relational/non-relational databases Distributed web scale cache stores In-memory databases optimized for BI (Business Intelligence) Applications performing real-time processing on big unstructured data EC2 Instance Types - Storage Optimized \u00b6 Great for storage-intensive tasks that require high, sequential read and write access to large data sets on local storage. High frequency online transaction processing (OLTP) systems Relationsl & NoSQL databases Cache for in-memory databases (for example, Redis) Data warehousing applications Distributed file systems There is a great resource https://instances.vantage.sh/ that lists all of the instance types and you can compare them.","title":"EC2 Instance Types Overview"},{"location":"AWS/developer-associate/04-ec2-fundamentals/EC2%20Instance%20Types/#ec2-instance-types-overview","text":"You can use different types of EC2 instances that are optimized for different use cases: https://aws.amazon.com/ec2/instance-types/ Currently we have 7 different types of EC2 instances: - General Purpose - Compute Optimized - Memory Optimized - Accelerated Computing - Storage Optimized - Instance Features - Measuring Instance Performance Each of these types have a different set of families. The AWS has the following naming convention like: - m5.2xlarge - m: instance class - 5: generation (AWS Improves them over time) - 2xlarge: the size within the instance class. The more the size, the more the CPU, memory etc.","title":"EC2 Instance Types Overview"},{"location":"AWS/developer-associate/04-ec2-fundamentals/EC2%20Instance%20Types/#ec2-instance-types-general-purpose","text":"Great for a diversity of workloads such as web servers or code repositories. Balance between Compute, Memory, Networking. In this course, we will be using the t2.micro which is a General Purpose EC2 instance.","title":"EC2 Instance Types - General Purpose"},{"location":"AWS/developer-associate/04-ec2-fundamentals/EC2%20Instance%20Types/#ec2-instance-types-compute-optimized","text":"Great for compute-intensive tasks that require high performance processors: Batch processing workloads Media transcoding High performance web servers High performance computing (HPC) Scientific modeling & machine learning Dedicated gaming servers","title":"EC2 Instance Types - Compute Optimized"},{"location":"AWS/developer-associate/04-ec2-fundamentals/EC2%20Instance%20Types/#ec2-instance-types-memory-optimized","text":"Fast performance for workloads that process large data sets in memory. Use cases: High performance, relational/non-relational databases Distributed web scale cache stores In-memory databases optimized for BI (Business Intelligence) Applications performing real-time processing on big unstructured data","title":"EC2 Instance Types - Memory Optimized"},{"location":"AWS/developer-associate/04-ec2-fundamentals/EC2%20Instance%20Types/#ec2-instance-types-storage-optimized","text":"Great for storage-intensive tasks that require high, sequential read and write access to large data sets on local storage. High frequency online transaction processing (OLTP) systems Relationsl & NoSQL databases Cache for in-memory databases (for example, Redis) Data warehousing applications Distributed file systems There is a great resource https://instances.vantage.sh/ that lists all of the instance types and you can compare them.","title":"EC2 Instance Types - Storage Optimized"},{"location":"AWS/developer-associate/05-ec2-instance-storage/02-ebs-hands-on/","text":"EBS Hands On \u00b6 If we go to EC2 and inspect our instance, under the storage tab, we'll see that it has a root device with a type of EBS volume. Also there is a section Block devices that will lost all attached storages. Currently we only have the root device. We can also go to EC2 -> Elastic Block Store -> Volumes and create a new volume. We are going to create a 2 GB General purpose storage in the same availability zone that our ec2 instance is located in. Now we'll see 2 volumes in our list - one is in use (the root volume) and one that is available (the one we created just now). We can now attach our volume to the EC2 instance by right-clicking. Now under the instance, we'll see both of these volumes:","title":"EBS Hands On"},{"location":"AWS/developer-associate/05-ec2-instance-storage/02-ebs-hands-on/#ebs-hands-on","text":"If we go to EC2 and inspect our instance, under the storage tab, we'll see that it has a root device with a type of EBS volume. Also there is a section Block devices that will lost all attached storages. Currently we only have the root device. We can also go to EC2 -> Elastic Block Store -> Volumes and create a new volume. We are going to create a 2 GB General purpose storage in the same availability zone that our ec2 instance is located in. Now we'll see 2 volumes in our list - one is in use (the root volume) and one that is available (the one we created just now). We can now attach our volume to the EC2 instance by right-clicking. Now under the instance, we'll see both of these volumes:","title":"EBS Hands On"},{"location":"AWS/developer-associate/05-ec2-instance-storage/03-ebs-snapshots-overview/","text":"EBS Snapsots \u00b6 Make a backup (snapshot) of your EBS volume at a point in time Not necessary to detach volume to do a snapshot, but recommended Can copy snapshots across AZ or Region","title":"EBS Snapsots"},{"location":"AWS/developer-associate/05-ec2-instance-storage/03-ebs-snapshots-overview/#ebs-snapsots","text":"Make a backup (snapshot) of your EBS volume at a point in time Not necessary to detach volume to do a snapshot, but recommended Can copy snapshots across AZ or Region","title":"EBS Snapsots"},{"location":"AWS/developer-associate/05-ec2-instance-storage/04-ebs-snapshots-hands-on/","text":"EBS Snapshots hands on \u00b6 When working with volumes, there is an option to create a snapshot for it. To verify that the snapshot was created we can go to EC2 -> Elastic Block Storage -> Snapshots Now when it's available we can do multiple things with it: copy it across regions: create a volume from it (also in a different AZ)","title":"EBS Snapshots hands on"},{"location":"AWS/developer-associate/05-ec2-instance-storage/04-ebs-snapshots-hands-on/#ebs-snapshots-hands-on","text":"When working with volumes, there is an option to create a snapshot for it. To verify that the snapshot was created we can go to EC2 -> Elastic Block Storage -> Snapshots Now when it's available we can do multiple things with it: copy it across regions: create a volume from it (also in a different AZ)","title":"EBS Snapshots hands on"},{"location":"AWS/developer-associate/05-ec2-instance-storage/05-ami-overview/","text":"AMI Overview \u00b6 AMI - Amazon Machine Image AMI are a customization of an EC2 instance You add your own software, configuration, operating system, monitoring... Faster boot / configuration time because all your sofware is pre-packaged AMI are built for a specific region (and can be copied across regions) You can launch EC2 instances from: A public AMI: AWS provided Your own AMI: you make and maintain them yourself An AWS Marketplace AMI: an AMI someone else made (and potentially sells) AMI Process (from an EC2 instance) \u00b6 Start an EC2 instance and customize it Stop the instance (for data integrity) Build an AMI - this will also create EBS snapshots Launch instances from other AMIs","title":"AMI Overview"},{"location":"AWS/developer-associate/05-ec2-instance-storage/05-ami-overview/#ami-overview","text":"AMI - Amazon Machine Image AMI are a customization of an EC2 instance You add your own software, configuration, operating system, monitoring... Faster boot / configuration time because all your sofware is pre-packaged AMI are built for a specific region (and can be copied across regions) You can launch EC2 instances from: A public AMI: AWS provided Your own AMI: you make and maintain them yourself An AWS Marketplace AMI: an AMI someone else made (and potentially sells)","title":"AMI Overview"},{"location":"AWS/developer-associate/05-ec2-instance-storage/05-ami-overview/#ami-process-from-an-ec2-instance","text":"Start an EC2 instance and customize it Stop the instance (for data integrity) Build an AMI - this will also create EBS snapshots Launch instances from other AMIs","title":"AMI Process (from an EC2 instance)"},{"location":"AWS/developer-associate/05-ec2-instance-storage/06-ami-hands-on/","text":"AMI Hands on \u00b6 In order to play with AMI we are going to create a new EC2 instance. Will all the default settings except for the User data field. In it we are going to input the following: #!/bin/bash yum update -y yum install -y httpd systemctl start httpd systemctl enable httd Once the instance is up and running, we can open it's public IP in a browser. Now we are going to create an AMI from it. We'll have to wait some time until the AMI becomes available. We can check that in EC2 -> Images -> AMIs . Now we can go to Launch instances -> My AMIs and select our AMI. Then in the user data we input this: #!/bin/bash echo \"<h1>Hello World from $(hostname -f)</h1>\" > /var/www/html/index.html And once it's up, we can visit in the browser and see the custom hello text.","title":"AMI Hands on"},{"location":"AWS/developer-associate/05-ec2-instance-storage/06-ami-hands-on/#ami-hands-on","text":"In order to play with AMI we are going to create a new EC2 instance. Will all the default settings except for the User data field. In it we are going to input the following: #!/bin/bash yum update -y yum install -y httpd systemctl start httpd systemctl enable httd Once the instance is up and running, we can open it's public IP in a browser. Now we are going to create an AMI from it. We'll have to wait some time until the AMI becomes available. We can check that in EC2 -> Images -> AMIs . Now we can go to Launch instances -> My AMIs and select our AMI. Then in the user data we input this: #!/bin/bash echo \"<h1>Hello World from $(hostname -f)</h1>\" > /var/www/html/index.html And once it's up, we can visit in the browser and see the custom hello text.","title":"AMI Hands on"},{"location":"AWS/developer-associate/05-ec2-instance-storage/08-ebs-volume-types/","text":"EBS Volume Types \u00b6 EBS Volumes come in 6 types gp2 / gp3 (SSD): General purpose SSD volume that balances price and performance for a wide variety of workloads io1 / io2 (SSD): Highest performance SSD volume for mission-critical low latency or high-throughput workloads st1 (HDD): Low cost HDD volume designed for frequently accessed, throughput-intensive workloads sc1 (HDD): Lowest cost HDD volume designed for less frequently accessed workloads EBS Voluimes are characterized in Size | Thoughput | IOPS When in doubt always consult the AWS documentation Only gp2/gp3 and io1/io2 can be used as boot volumes General Purpose SSD \u00b6 Cost efective storage, low latency System boot volumes, Virtual desktops, Development and test environments 1 GiB - 16 TiB gp3: Baseline of 3000 IOPS and throughput of 125MiB/s Can increase IOPS up to 16000 and throughput up to 1000MiB/s independently gp2: Small gp2 volumes can burst IOPS to 3000 Size of the volume and IOPS are linked, max IOPS is 16,000 3 IOPS per GB, means at 5334 GB we are at the max IOPS Provisioned IOPS (PIOPS) SSD \u00b6 Critical business application with sustained IOPS performance Or applications that need more than 16000 IOPS Great for database workloads (sensitive to storage performance and consistency) io1/io2 (4GiB-16TiB): Max PIOPS: 64000 for Nitro EC2 instances & 32000 for other Can increase PIOPS independently from storage size io2 have more durability and more IOPS per GiB (at the same price as io1) io2 Block Express (4 GiB - 64TiB): sub-millisecond latency max PIOPS: 256000 with an IOPS:GiB ratio of 1,000:1 Support EBS multi-attach Hard Disk Drives (HDD) \u00b6 Cannot be a boot volume 125 MiB to 16 TiB Throughput optimized HDD (st1) Big Data, Data Warehouses, Log Processing Max throughput 500MiB/s - max IOPS 500 Cold HDD (sc1): For data that is infrequently accessed Scenarios where lowest cost is important Max throughput 250MiB/s - Max IOPS 250","title":"EBS Volume Types"},{"location":"AWS/developer-associate/05-ec2-instance-storage/08-ebs-volume-types/#ebs-volume-types","text":"EBS Volumes come in 6 types gp2 / gp3 (SSD): General purpose SSD volume that balances price and performance for a wide variety of workloads io1 / io2 (SSD): Highest performance SSD volume for mission-critical low latency or high-throughput workloads st1 (HDD): Low cost HDD volume designed for frequently accessed, throughput-intensive workloads sc1 (HDD): Lowest cost HDD volume designed for less frequently accessed workloads EBS Voluimes are characterized in Size | Thoughput | IOPS When in doubt always consult the AWS documentation Only gp2/gp3 and io1/io2 can be used as boot volumes","title":"EBS Volume Types"},{"location":"AWS/developer-associate/05-ec2-instance-storage/08-ebs-volume-types/#general-purpose-ssd","text":"Cost efective storage, low latency System boot volumes, Virtual desktops, Development and test environments 1 GiB - 16 TiB gp3: Baseline of 3000 IOPS and throughput of 125MiB/s Can increase IOPS up to 16000 and throughput up to 1000MiB/s independently gp2: Small gp2 volumes can burst IOPS to 3000 Size of the volume and IOPS are linked, max IOPS is 16,000 3 IOPS per GB, means at 5334 GB we are at the max IOPS","title":"General Purpose SSD"},{"location":"AWS/developer-associate/05-ec2-instance-storage/08-ebs-volume-types/#provisioned-iops-piops-ssd","text":"Critical business application with sustained IOPS performance Or applications that need more than 16000 IOPS Great for database workloads (sensitive to storage performance and consistency) io1/io2 (4GiB-16TiB): Max PIOPS: 64000 for Nitro EC2 instances & 32000 for other Can increase PIOPS independently from storage size io2 have more durability and more IOPS per GiB (at the same price as io1) io2 Block Express (4 GiB - 64TiB): sub-millisecond latency max PIOPS: 256000 with an IOPS:GiB ratio of 1,000:1 Support EBS multi-attach","title":"Provisioned IOPS (PIOPS) SSD"},{"location":"AWS/developer-associate/05-ec2-instance-storage/08-ebs-volume-types/#hard-disk-drives-hdd","text":"Cannot be a boot volume 125 MiB to 16 TiB Throughput optimized HDD (st1) Big Data, Data Warehouses, Log Processing Max throughput 500MiB/s - max IOPS 500 Cold HDD (sc1): For data that is infrequently accessed Scenarios where lowest cost is important Max throughput 250MiB/s - Max IOPS 250","title":"Hard Disk Drives (HDD)"},{"location":"AWS/developer-associate/05-ec2-instance-storage/09-ebs-multi-attach/","text":"EBS Multi-Attach - io1/io2 family \u00b6 Attach the same EBS volume to multiple EC2 instances in the same AZ Each instance has full read&write permissions to the volume Use case Achieve higher application availability in clustered Linux applications (ex: Teradata) Applications must manage concurrent write opperations Must use a file system that's cluster aware: Not XFS, EX4 etc","title":"EBS Multi-Attach - io1/io2 family"},{"location":"AWS/developer-associate/05-ec2-instance-storage/09-ebs-multi-attach/#ebs-multi-attach-io1io2-family","text":"Attach the same EBS volume to multiple EC2 instances in the same AZ Each instance has full read&write permissions to the volume Use case Achieve higher application availability in clustered Linux applications (ex: Teradata) Applications must manage concurrent write opperations Must use a file system that's cluster aware: Not XFS, EX4 etc","title":"EBS Multi-Attach - io1/io2 family"},{"location":"AWS/developer-associate/05-ec2-instance-storage/11-efs-hands-on/","text":"EFS hands on \u00b6 In order to work with EFS we can navigate to it in the search bar. Then we are going to click on the Create a file system , to see all the options we are going to use the Customize button. On the first step we can set the name as well as various settings like regions, lifecycles and performance. The next step is where we can configure mounting it accross multiple availability zones. Before we make any changes we are going to create an empty security grop called my-efs-demo . Then we are going to remove all the default security groups and change them to the newly created one. The next step is file system policy which is optional. We are going to skip it. Once that is created, we are going to create 2 EC2 instances that are going to access the EFS volume. The first one will be located in eu-west-1b. The secon one will be placed in eu-west-1a. Both of them will reuse the same security grop ec2-to-efs . Now we can open up the EFS and click on attach. This will give us instructions on how to mount the file system. In order to use the efs mount helper we need to install it on our ec2 instance: sudo yum install -y amazon-efs-utils Then we can set it up mkdir efs sudo mount -t efs -o tls fs-de1b88ea:/ efs We might receive a timeout when trying to mount it. To fix that, we are going to edit the my-efs-demo security group and add an inboud NFS rule with a source of ec2-to-efs security group. Now it should be working:","title":"EFS hands on"},{"location":"AWS/developer-associate/05-ec2-instance-storage/11-efs-hands-on/#efs-hands-on","text":"In order to work with EFS we can navigate to it in the search bar. Then we are going to click on the Create a file system , to see all the options we are going to use the Customize button. On the first step we can set the name as well as various settings like regions, lifecycles and performance. The next step is where we can configure mounting it accross multiple availability zones. Before we make any changes we are going to create an empty security grop called my-efs-demo . Then we are going to remove all the default security groups and change them to the newly created one. The next step is file system policy which is optional. We are going to skip it. Once that is created, we are going to create 2 EC2 instances that are going to access the EFS volume. The first one will be located in eu-west-1b. The secon one will be placed in eu-west-1a. Both of them will reuse the same security grop ec2-to-efs . Now we can open up the EFS and click on attach. This will give us instructions on how to mount the file system. In order to use the efs mount helper we need to install it on our ec2 instance: sudo yum install -y amazon-efs-utils Then we can set it up mkdir efs sudo mount -t efs -o tls fs-de1b88ea:/ efs We might receive a timeout when trying to mount it. To fix that, we are going to edit the my-efs-demo security group and add an inboud NFS rule with a source of ec2-to-efs security group. Now it should be working:","title":"EFS hands on"},{"location":"AWS/developer-associate/05-ec2-instance-storage/12-efs-vs-ebs/","text":"EBS vs EFS \u00b6 EBS volumes can be attached to only one instance at a time are locked at the Availability Zone (AZ) level gp2: IO increases if the disk size increases io1: can increase IO independently To migrate an EBS volume accross AZ take a snapshot Restore the snapshot to another AZ EBS backups use IO and you shouldn't run them while your application is handling a lot of traffic Root EBS volumes of instances get terminated by default if the EC2 instance gets terminated. (you can disable that). EFS volumes Mounting 100s of instances accross AZ EFS share website files (wordpress) Only for linux instances (POSIX) EFS has a higher price point than EBS Can leverage EFS-IA for cost savings Rembmer: EFS vs EBS vs Instance Store","title":"EBS vs EFS"},{"location":"AWS/developer-associate/05-ec2-instance-storage/12-efs-vs-ebs/#ebs-vs-efs","text":"EBS volumes can be attached to only one instance at a time are locked at the Availability Zone (AZ) level gp2: IO increases if the disk size increases io1: can increase IO independently To migrate an EBS volume accross AZ take a snapshot Restore the snapshot to another AZ EBS backups use IO and you shouldn't run them while your application is handling a lot of traffic Root EBS volumes of instances get terminated by default if the EC2 instance gets terminated. (you can disable that). EFS volumes Mounting 100s of instances accross AZ EFS share website files (wordpress) Only for linux instances (POSIX) EFS has a higher price point than EBS Can leverage EFS-IA for cost savings Rembmer: EFS vs EBS vs Instance Store","title":"EBS vs EFS"},{"location":"AWS/developer-associate/05-ec2-instance-storage/Amazon%20Elastic%20Block%20Store%20%28EBS%29/","text":"EBS Overview \u00b6 What's an EBS Volume? \u00b6 An EBS (Elastic Block Storage) Volume is a network drive you can attach to your instances while they run It allows your instances to persist data, event after their termination They can only be mounted to one instance at a time (at the Certified Cloud Practicioner level, for Solutions Architect, Developer, SysOpes - there is multi-attach feature for some EBS). They are bound to a specific availability zone Free tier: 30 GB of free EBS storage of type General Purpose (SSD) or Magnetic per month. EBS Volume \u00b6 It's a network drive (i.e. not a physical drive) It uses the network to communicate to the instance, which means there might be a bit of latency It can be detached from an EC2 instance and attached to another one quickly It's locked to an Availability Zone (AZ) An EBS Volume in us-east-1a cannot be attached to us-east-1b To move a volume across, you first need to snapshot it Have a provisioned capacity (size in GBs, and IOPS) You get billed for all the provisioned capacity You can increase the capacity of the drive over time EBS - Delete on Termination attribute \u00b6 When creating EC2 instances, there is an option to choose Delete on Termination option, which will delete the EBS volume once the instance is terminated. By default it is enabled for the root volume, but isn't for any custom added volume.","title":"EBS Overview"},{"location":"AWS/developer-associate/05-ec2-instance-storage/Amazon%20Elastic%20Block%20Store%20%28EBS%29/#ebs-overview","text":"","title":"EBS Overview"},{"location":"AWS/developer-associate/05-ec2-instance-storage/Amazon%20Elastic%20Block%20Store%20%28EBS%29/#whats-an-ebs-volume","text":"An EBS (Elastic Block Storage) Volume is a network drive you can attach to your instances while they run It allows your instances to persist data, event after their termination They can only be mounted to one instance at a time (at the Certified Cloud Practicioner level, for Solutions Architect, Developer, SysOpes - there is multi-attach feature for some EBS). They are bound to a specific availability zone Free tier: 30 GB of free EBS storage of type General Purpose (SSD) or Magnetic per month.","title":"What's an EBS Volume?"},{"location":"AWS/developer-associate/05-ec2-instance-storage/Amazon%20Elastic%20Block%20Store%20%28EBS%29/#ebs-volume","text":"It's a network drive (i.e. not a physical drive) It uses the network to communicate to the instance, which means there might be a bit of latency It can be detached from an EC2 instance and attached to another one quickly It's locked to an Availability Zone (AZ) An EBS Volume in us-east-1a cannot be attached to us-east-1b To move a volume across, you first need to snapshot it Have a provisioned capacity (size in GBs, and IOPS) You get billed for all the provisioned capacity You can increase the capacity of the drive over time","title":"EBS Volume"},{"location":"AWS/developer-associate/05-ec2-instance-storage/Amazon%20Elastic%20Block%20Store%20%28EBS%29/#ebs-delete-on-termination-attribute","text":"When creating EC2 instances, there is an option to choose Delete on Termination option, which will delete the EBS volume once the instance is terminated. By default it is enabled for the root volume, but isn't for any custom added volume.","title":"EBS - Delete on Termination attribute"},{"location":"AWS/developer-associate/05-ec2-instance-storage/EC2%20Instance%20Store/","text":"EC2 Instance Store \u00b6 EBS volumes are network drives with good but \"limited\" performance If you need a high performance hardware disk, use EC2 instance store Better I/O performance EC2 Instance Store lose their storage if they're stopped (ephemeral) Good for buffer / cache / scratch data / temporary content Risk of data loss if hardware fails","title":"EC2 Instance Store"},{"location":"AWS/developer-associate/05-ec2-instance-storage/EC2%20Instance%20Store/#ec2-instance-store","text":"EBS volumes are network drives with good but \"limited\" performance If you need a high performance hardware disk, use EC2 instance store Better I/O performance EC2 Instance Store lose their storage if they're stopped (ephemeral) Good for buffer / cache / scratch data / temporary content Risk of data loss if hardware fails","title":"EC2 Instance Store"},{"location":"AWS/developer-associate/05-ec2-instance-storage/Elastic%20File%20System%20%28EFS%29/","text":"EFS Overview (Elastic File System) \u00b6 Managed NFS (network file system) that can be mounted on many EC2 instances EFS works with EC2 instances in multi-AZ Highly available, scalable, expensive (3xgp2), pay per use Use cases: content management, web serving, data sharing, Wordpress - Uses NFSv4.1 protocol - Uses security group to controll access to EFS - Compatible with Linux based AMI (not windows) - Encryption at rest using KMS - POSIX file system (~Linux) that has a standard file API - File system scales automatically, pay-peruse, no capacity planning. EFS Scale 1000s of concurrent NFS clients, 10GB+/s throughput Grow to petabyte-scale network file system, automatically Performance mode (set at EFS creation time) General purpose (defaullt): latency-sensitive use cases (web server, CMS, etc...) Max I/O - higher latency, throughput, highly parralel (big data, media processing) Throughput mode Bursting (1TB = 50MiB/s + burst of up to 100MiB/s) Provisioned: set your throughput regardless of storage size, ex: 1Gib/s for 1TB storage Storage Tiers (lifecycle management feature - move file after N days) Standard: for frequently accessed files Infrequent acces (EFS-IA): cost to retrieve files, lower price to store","title":"EFS Overview (Elastic File System)"},{"location":"AWS/developer-associate/05-ec2-instance-storage/Elastic%20File%20System%20%28EFS%29/#efs-overview-elastic-file-system","text":"Managed NFS (network file system) that can be mounted on many EC2 instances EFS works with EC2 instances in multi-AZ Highly available, scalable, expensive (3xgp2), pay per use Use cases: content management, web serving, data sharing, Wordpress - Uses NFSv4.1 protocol - Uses security group to controll access to EFS - Compatible with Linux based AMI (not windows) - Encryption at rest using KMS - POSIX file system (~Linux) that has a standard file API - File system scales automatically, pay-peruse, no capacity planning. EFS Scale 1000s of concurrent NFS clients, 10GB+/s throughput Grow to petabyte-scale network file system, automatically Performance mode (set at EFS creation time) General purpose (defaullt): latency-sensitive use cases (web server, CMS, etc...) Max I/O - higher latency, throughput, highly parralel (big data, media processing) Throughput mode Bursting (1TB = 50MiB/s + burst of up to 100MiB/s) Provisioned: set your throughput regardless of storage size, ex: 1Gib/s for 1TB storage Storage Tiers (lifecycle management feature - move file after N days) Standard: for frequently accessed files Infrequent acces (EFS-IA): cost to retrieve files, lower price to store","title":"EFS Overview (Elastic File System)"},{"location":"AWS/developer-associate/05-ec2-instance-storage/img/01-high-availability-and-scalability/","text":"High Availability and Scalability \u00b6 Scalability means that an application / systen can handle greater loads by adapting. There are two kinds of scalability: Vertical scalability Horizontal scalability (= elasticity) Scalability is linked but different to High Availability Vertican Scalability \u00b6 Vertical scalability means increasing the size of the instance For example, your application runs on a t2.micro Scaling that application vertically means running it on t2.large Vertical scalability is very common for non distributed systems, such as a database. RDS, ElastiCache are services that can scale vertically. There's usually a limit to how much you can vertically scale (hardware limit) Horizontal Scalability \u00b6 Horizontal Scalability means increasing the number of instances / systems for your application Horizontal scaling implies distributed systems This is very common for web applications / modern applications It's easy to horizontally scale thanks to the cloud offerings such as Amazon EC2 High Availability \u00b6 High availability usually goes hand in hand with horizontal scalin High availability means running your application /system in at least 2 data centers (AZs) The goal of high availability is to survive a data center loss The high availability can be passive (for RDS Multi AZ for example) The high availability can be active (for horizontal scaling) High Availability & Scalability for EC2 \u00b6 Vertical Scaling: Increase instance size (= scale up/down) From: t2.nano - 0.5G of ram, 1 vCPU To: u-12tb1.metal - 12.3TB of ram, 448 vCPUs Horizontal scaling: increase number of isntances (= scale out/in) Auto Scaling Group Load Balancer High Availability: Run instances for the same application across multi AZ Auto Scaling Group multi AZ Load Balancer multi AZ","title":"High Availability and Scalability"},{"location":"AWS/developer-associate/05-ec2-instance-storage/img/01-high-availability-and-scalability/#high-availability-and-scalability","text":"Scalability means that an application / systen can handle greater loads by adapting. There are two kinds of scalability: Vertical scalability Horizontal scalability (= elasticity) Scalability is linked but different to High Availability","title":"High Availability and Scalability"},{"location":"AWS/developer-associate/05-ec2-instance-storage/img/01-high-availability-and-scalability/#vertican-scalability","text":"Vertical scalability means increasing the size of the instance For example, your application runs on a t2.micro Scaling that application vertically means running it on t2.large Vertical scalability is very common for non distributed systems, such as a database. RDS, ElastiCache are services that can scale vertically. There's usually a limit to how much you can vertically scale (hardware limit)","title":"Vertican Scalability"},{"location":"AWS/developer-associate/05-ec2-instance-storage/img/01-high-availability-and-scalability/#horizontal-scalability","text":"Horizontal Scalability means increasing the number of instances / systems for your application Horizontal scaling implies distributed systems This is very common for web applications / modern applications It's easy to horizontally scale thanks to the cloud offerings such as Amazon EC2","title":"Horizontal Scalability"},{"location":"AWS/developer-associate/05-ec2-instance-storage/img/01-high-availability-and-scalability/#high-availability","text":"High availability usually goes hand in hand with horizontal scalin High availability means running your application /system in at least 2 data centers (AZs) The goal of high availability is to survive a data center loss The high availability can be passive (for RDS Multi AZ for example) The high availability can be active (for horizontal scaling)","title":"High Availability"},{"location":"AWS/developer-associate/05-ec2-instance-storage/img/01-high-availability-and-scalability/#high-availability-scalability-for-ec2","text":"Vertical Scaling: Increase instance size (= scale up/down) From: t2.nano - 0.5G of ram, 1 vCPU To: u-12tb1.metal - 12.3TB of ram, 448 vCPUs Horizontal scaling: increase number of isntances (= scale out/in) Auto Scaling Group Load Balancer High Availability: Run instances for the same application across multi AZ Auto Scaling Group multi AZ Load Balancer multi AZ","title":"High Availability &amp; Scalability for EC2"},{"location":"AWS/developer-associate/06-elb-asg/01-high-availability-and-scalability/","text":"Scalability & High Availability \u00b6 Scalabiliy means that an application / system can handle greater loads by adapting. There are two kinds of scalability: Vertical scalability Horizontal scalability (elasticity) Scalability is linked but different to High Availability Vertical Scalability \u00b6 Vertical scalability means increasing the size of the instance. For example, your application runs on t2.micro, scaling that application vertically means running it on t2.large. Vertical scalability is very common for non distributed systems, such as database. RDS, ElastiCache are services that can scale vertically. There's usually a limit to gow much you can vertically scale (hardware limit). Horizontal Scalability \u00b6 Horizontal Scalability means increasing the number of instances / systems for your application. Horizontal scaling implies distributed systems. This is very common for web applications / modern applicaitons. It's easy to horizontally scale thanks to cloud offerings such as Amazon EC2. High Availability \u00b6 High Availability usually goes hand in hand with horizontal scaling High Availability means running your application / system in at least 2 data centers (Availability Zones) The goal of High Availability is to survive a data center loss The High Availability can be passive (for RDS Multi AZ for example). The High Availability can be active (for horizontal scaling). High Availability & Scalability for EC2 \u00b6 Vertical Scaling: Increase instance size (scale up/down) From: t2.namo - 0.5G RAM, 1 vCPU To: u-l2tgb1.metal - 12.3TB RAM, 448 vCPUS Horizontal Scaling: Increase number of instances (scale in/out) Auto Scaling Group Load Balancer High Availability: Run instances for the same application across multi AZ Auto Scaling Group multi AZ Load Balancer multi AZ","title":"Scalability & High Availability"},{"location":"AWS/developer-associate/06-elb-asg/01-high-availability-and-scalability/#scalability-high-availability","text":"Scalabiliy means that an application / system can handle greater loads by adapting. There are two kinds of scalability: Vertical scalability Horizontal scalability (elasticity) Scalability is linked but different to High Availability","title":"Scalability &amp; High Availability"},{"location":"AWS/developer-associate/06-elb-asg/01-high-availability-and-scalability/#vertical-scalability","text":"Vertical scalability means increasing the size of the instance. For example, your application runs on t2.micro, scaling that application vertically means running it on t2.large. Vertical scalability is very common for non distributed systems, such as database. RDS, ElastiCache are services that can scale vertically. There's usually a limit to gow much you can vertically scale (hardware limit).","title":"Vertical Scalability"},{"location":"AWS/developer-associate/06-elb-asg/01-high-availability-and-scalability/#horizontal-scalability","text":"Horizontal Scalability means increasing the number of instances / systems for your application. Horizontal scaling implies distributed systems. This is very common for web applications / modern applicaitons. It's easy to horizontally scale thanks to cloud offerings such as Amazon EC2.","title":"Horizontal Scalability"},{"location":"AWS/developer-associate/06-elb-asg/01-high-availability-and-scalability/#high-availability","text":"High Availability usually goes hand in hand with horizontal scaling High Availability means running your application / system in at least 2 data centers (Availability Zones) The goal of High Availability is to survive a data center loss The High Availability can be passive (for RDS Multi AZ for example). The High Availability can be active (for horizontal scaling).","title":"High Availability"},{"location":"AWS/developer-associate/06-elb-asg/01-high-availability-and-scalability/#high-availability-scalability-for-ec2","text":"Vertical Scaling: Increase instance size (scale up/down) From: t2.namo - 0.5G RAM, 1 vCPU To: u-l2tgb1.metal - 12.3TB RAM, 448 vCPUS Horizontal Scaling: Increase number of instances (scale in/out) Auto Scaling Group Load Balancer High Availability: Run instances for the same application across multi AZ Auto Scaling Group multi AZ Load Balancer multi AZ","title":"High Availability &amp; Scalability for EC2"},{"location":"AWS/developer-associate/06-elb-asg/02-elastic-load-balancing-overview/","text":"Elastic Load Balancing (ELB) Overview \u00b6 What is load balancing? \u00b6 Load Balances are servers that forward traffic to multiple servers (e.g. EC2 instances) downstream. Why use a load balancer? \u00b6 Spread load accross multiple downstream instances Expose a single point of access (DNS) to your application Seamlessly handle failures of downstream instances Do regular health checks to your instances Provide SSL termination (HTTPS) for your websites Enforce stickiness with cookies High Availability across zones Separate public traffic from private traffic Why use an Elastic Load Balancer? \u00b6 An Elastic Load Balancer is a managed load balancer AWS guarantees that it will be working AWS takes care of upgrades, maintenance, high availability AWS provides only a few configuration knobs It costs less to setup your own load balancer but it will be a lot more effort on your end It is integrated with many AWS offerings / services EC2, EC2 Auto Scaling Groups, Amazon ECS AWS Certificate Manager (ACM), CloudWatch Route 53, AWS WAF, AWS Global Accelerator Health Checks \u00b6 Health Checks are crucial for Load Balancers They enable the load balancer to know if instances it forwards traffic to are available to reply to requests The health check is done on a port and a route (/health is common) If the response is not 200 (OK), then the instance is unhealthy Types of load balancers on AWS \u00b6 AWS has 4 kinds of managed Load Balancers: - Classic Load Balancer (v1 old generation) - 2009 - CLB - HTTP, HTTPS, TCP, SSL (secure TCP) - Application Load Balancer (v2 - new generation) - 2016 - ALB - HTTP, HTTPS, WebSocket - Network Load Balancer (v2 - new generation) - 2017 - NLB - TCP, TLS (secure TCP), UDP - Gateway Load Balancer - 2020 - GWLB - Operates at layer 3 (Network layer) - IP Protocol Overall, it is recommended to use the newergeneration load balancers as they provide more features. Some load balancers can be setup as internal (private) or external (public) ELBs.","title":"Elastic Load Balancing (ELB) Overview"},{"location":"AWS/developer-associate/06-elb-asg/02-elastic-load-balancing-overview/#elastic-load-balancing-elb-overview","text":"","title":"Elastic Load Balancing (ELB) Overview"},{"location":"AWS/developer-associate/06-elb-asg/02-elastic-load-balancing-overview/#what-is-load-balancing","text":"Load Balances are servers that forward traffic to multiple servers (e.g. EC2 instances) downstream.","title":"What is load balancing?"},{"location":"AWS/developer-associate/06-elb-asg/02-elastic-load-balancing-overview/#why-use-a-load-balancer","text":"Spread load accross multiple downstream instances Expose a single point of access (DNS) to your application Seamlessly handle failures of downstream instances Do regular health checks to your instances Provide SSL termination (HTTPS) for your websites Enforce stickiness with cookies High Availability across zones Separate public traffic from private traffic","title":"Why use a load balancer?"},{"location":"AWS/developer-associate/06-elb-asg/02-elastic-load-balancing-overview/#why-use-an-elastic-load-balancer","text":"An Elastic Load Balancer is a managed load balancer AWS guarantees that it will be working AWS takes care of upgrades, maintenance, high availability AWS provides only a few configuration knobs It costs less to setup your own load balancer but it will be a lot more effort on your end It is integrated with many AWS offerings / services EC2, EC2 Auto Scaling Groups, Amazon ECS AWS Certificate Manager (ACM), CloudWatch Route 53, AWS WAF, AWS Global Accelerator","title":"Why use an Elastic Load Balancer?"},{"location":"AWS/developer-associate/06-elb-asg/02-elastic-load-balancing-overview/#health-checks","text":"Health Checks are crucial for Load Balancers They enable the load balancer to know if instances it forwards traffic to are available to reply to requests The health check is done on a port and a route (/health is common) If the response is not 200 (OK), then the instance is unhealthy","title":"Health Checks"},{"location":"AWS/developer-associate/06-elb-asg/02-elastic-load-balancing-overview/#types-of-load-balancers-on-aws","text":"AWS has 4 kinds of managed Load Balancers: - Classic Load Balancer (v1 old generation) - 2009 - CLB - HTTP, HTTPS, TCP, SSL (secure TCP) - Application Load Balancer (v2 - new generation) - 2016 - ALB - HTTP, HTTPS, WebSocket - Network Load Balancer (v2 - new generation) - 2017 - NLB - TCP, TLS (secure TCP), UDP - Gateway Load Balancer - 2020 - GWLB - Operates at layer 3 (Network layer) - IP Protocol Overall, it is recommended to use the newergeneration load balancers as they provide more features. Some load balancers can be setup as internal (private) or external (public) ELBs.","title":"Types of load balancers on AWS"},{"location":"AWS/developer-associate/06-elb-asg/03-classic-load-balancers/","text":"Classic Load Balancers (v1) \u00b6 Supports TCP (layer 4), HTTP & HTTPS (layer 7) Health checks are TCP or HTTP based Fixed hostname - XXX.region.elb.amazonaws.com","title":"Classic Load Balancers (v1)"},{"location":"AWS/developer-associate/06-elb-asg/03-classic-load-balancers/#classic-load-balancers-v1","text":"Supports TCP (layer 4), HTTP & HTTPS (layer 7) Health checks are TCP or HTTP based Fixed hostname - XXX.region.elb.amazonaws.com","title":"Classic Load Balancers (v1)"},{"location":"AWS/developer-associate/06-elb-asg/05-application-load-balancer/","text":"Application Load Balancer (v2) \u00b6 Application load balancers is Layer 7 (HTTP) Load balancing to multiple HTTP applications applications across machines (target groups) Load balancing to multiple applications on the same machine (ex: containers). Support for HTTP/2 and WebSockets Support redirects (from HTTP to HTTPS for example) Routing tables to different target groups: Routing based on path in URL (example.com/users and example.com/posts) Routing based on hostname in url (one.example.com and other.example.com) Routing based on Query String, Headers (Example.com/users?id=123&order=false) ALB are a great fit for micro services & container-based applications (example: Docker & Amazon ECS) Has a port mapping feature to redirect to a dynamic port in ECS In comparison, we'd need multiple Classic Load Balancers per application. Application Load Balancer Target Groups \u00b6 EC2 instances (can be managed by an Auto Scaling Group) - HTTP ECS tasks (managed by ECS itself) - HTTP Lambda functions - HTTP request is translated into a JSON event IP addresses - must be private IPs ALB can route to multiple target groups Health checks are at the target group level Good to know \u00b6 Fixed hostname (XXX.region.elb.amazonaws.com) The application servers don't see the IP of the client directly The true IP of the client is inserted in the header X-Forwarded-For We can also get Port (X-Forwarded-Port) and proto (X-Forwarded-Proto)","title":"Application Load Balancer (v2)"},{"location":"AWS/developer-associate/06-elb-asg/05-application-load-balancer/#application-load-balancer-v2","text":"Application load balancers is Layer 7 (HTTP) Load balancing to multiple HTTP applications applications across machines (target groups) Load balancing to multiple applications on the same machine (ex: containers). Support for HTTP/2 and WebSockets Support redirects (from HTTP to HTTPS for example) Routing tables to different target groups: Routing based on path in URL (example.com/users and example.com/posts) Routing based on hostname in url (one.example.com and other.example.com) Routing based on Query String, Headers (Example.com/users?id=123&order=false) ALB are a great fit for micro services & container-based applications (example: Docker & Amazon ECS) Has a port mapping feature to redirect to a dynamic port in ECS In comparison, we'd need multiple Classic Load Balancers per application.","title":"Application Load Balancer (v2)"},{"location":"AWS/developer-associate/06-elb-asg/05-application-load-balancer/#application-load-balancer-target-groups","text":"EC2 instances (can be managed by an Auto Scaling Group) - HTTP ECS tasks (managed by ECS itself) - HTTP Lambda functions - HTTP request is translated into a JSON event IP addresses - must be private IPs ALB can route to multiple target groups Health checks are at the target group level","title":"Application Load Balancer Target Groups"},{"location":"AWS/developer-associate/06-elb-asg/05-application-load-balancer/#good-to-know","text":"Fixed hostname (XXX.region.elb.amazonaws.com) The application servers don't see the IP of the client directly The true IP of the client is inserted in the header X-Forwarded-For We can also get Port (X-Forwarded-Port) and proto (X-Forwarded-Proto)","title":"Good to know"},{"location":"AWS/developer-associate/06-elb-asg/07-network-load-balancer/","text":"Network Load Balancer (NLB) \u00b6 Network load balancers (layer 4) allow to: Forward TCP & UDP traffic to your instances Handle millions of requests per second Less latency ~ 100ms (vs 400ms for ALB) NLB has one static IP per AZ, and supports assigning Elastic IP (helpful for whitelisting specific IP) NLB are used for extreme performance, TCP or UDP traffic Not included in the AWS free tier","title":"Network Load Balancer (NLB)"},{"location":"AWS/developer-associate/06-elb-asg/07-network-load-balancer/#network-load-balancer-nlb","text":"Network load balancers (layer 4) allow to: Forward TCP & UDP traffic to your instances Handle millions of requests per second Less latency ~ 100ms (vs 400ms for ALB) NLB has one static IP per AZ, and supports assigning Elastic IP (helpful for whitelisting specific IP) NLB are used for extreme performance, TCP or UDP traffic Not included in the AWS free tier","title":"Network Load Balancer (NLB)"},{"location":"AWS/developer-associate/06-elb-asg/09-elastic-load-balancer-sticky-sessions/","text":"Elastic Load Balancer - Sticky Sessions \u00b6 It is possible to implement stickiness so that the same client is always redirected to the same instance behind a load balancer This works for Classic Load Balancers & Application Load Balancers The \"cookie\" used for stickiness has an expiration date you control Use case: make sure the user doesn't lose his session data Enabling stickiness may bring imbalance to the load over the backend EC2 instances Cookie Names \u00b6 Application-based cookies Custom cookie Generated by the target Can include any custom attributes required by the application Cookie name must be specified individually for each target group Don't use AWSALB, AWSALBAPP or AWSALBTG (reserved for use by the ELB) Application cookie Generated by the load balancer Cookie name is AWSALBAPP Duration-based cookies Cookie generated by the load balancer Cookie name is AWSALB for ALB, AWSELB for CLB","title":"Elastic Load Balancer - Sticky Sessions"},{"location":"AWS/developer-associate/06-elb-asg/09-elastic-load-balancer-sticky-sessions/#elastic-load-balancer-sticky-sessions","text":"It is possible to implement stickiness so that the same client is always redirected to the same instance behind a load balancer This works for Classic Load Balancers & Application Load Balancers The \"cookie\" used for stickiness has an expiration date you control Use case: make sure the user doesn't lose his session data Enabling stickiness may bring imbalance to the load over the backend EC2 instances","title":"Elastic Load Balancer - Sticky Sessions"},{"location":"AWS/developer-associate/06-elb-asg/09-elastic-load-balancer-sticky-sessions/#cookie-names","text":"Application-based cookies Custom cookie Generated by the target Can include any custom attributes required by the application Cookie name must be specified individually for each target group Don't use AWSALB, AWSALBAPP or AWSALBTG (reserved for use by the ELB) Application cookie Generated by the load balancer Cookie name is AWSALBAPP Duration-based cookies Cookie generated by the load balancer Cookie name is AWSALB for ALB, AWSELB for CLB","title":"Cookie Names"},{"location":"AWS/developer-associate/06-elb-asg/10-elastic-load-balancer-cross-zone-load-balancing/","text":"Elastic Load Balancer - Cross Zone Load Balancing \u00b6 With Cross Zone Load Balancing: - each load balancer instance distributes evenly across all registered instances in all AZ Without Cross Zone Load Balancing: - Requests are distributed in the instances of the node of the Elastic Load Balancer Application Load Balancer: - Always on (can't be disabled) - No charges for inter AZ data Network Load Balancer - Disabled by default - You pay charges ($) for inter AZ data if enabled Classic Load Balancer - Through Console - enabled by default - Through CLI/API - disabled by default - No charges for inter AZ data if enabled","title":"Elastic Load Balancer - Cross Zone Load Balancing"},{"location":"AWS/developer-associate/06-elb-asg/10-elastic-load-balancer-cross-zone-load-balancing/#elastic-load-balancer-cross-zone-load-balancing","text":"With Cross Zone Load Balancing: - each load balancer instance distributes evenly across all registered instances in all AZ Without Cross Zone Load Balancing: - Requests are distributed in the instances of the node of the Elastic Load Balancer Application Load Balancer: - Always on (can't be disabled) - No charges for inter AZ data Network Load Balancer - Disabled by default - You pay charges ($) for inter AZ data if enabled Classic Load Balancer - Through Console - enabled by default - Through CLI/API - disabled by default - No charges for inter AZ data if enabled","title":"Elastic Load Balancer - Cross Zone Load Balancing"},{"location":"AWS/developer-associate/06-elb-asg/11-elastic-load-balancer-ssl-certificates/","text":"Elastic Load Balancer - SSL Certificates \u00b6 Basics \u00b6 An SSL Certificate allows traffic between your clients and your load balancer to be encrypted in transit (in-flight encryption) SSL refers to Secure Sockets Layer, used to encrypy connections TLS refers to Transport Layer Security, which is a newer version Nowadays, TLS certificates are mainly used, but people still refer as SSL Public SSL certificatres are issued by Certificate Authorities (CA) Comodo, Symantec, GoDaddy, GlobalSign, Digicert, Letsencrypt, etc... SSL certificates have an expirateion date (you set) and must be renewed Load Balancer - SSL Certificates \u00b6 The load balancer uses an X.509 certificate (SSL/TLS server certificate) You can manage certificates using ACM (AWS Certificate Manager) You can upload your own certificates alternatively HTTPS listener: You must specify a default certificate You can use SNI (Server Name Indication) to specify the hostname they reach Ability to specify a security policy to support older version of SSL / TLS (legacy clients) SSL - Server Name Indication \u00b6 SNI solves the problem of loading multiple SSL certificates onto one web server (to serve multiple websites) It's a newer protocol, and requires the client to indicate the hostname of the target server in the inital SSL handshake The server will then find the correct certificate, or return the default one Note: - Only works for ALB & NLB (newer generation), CloudFront - Does not work for CLB (older gen) Elastic Load Balancers - SSL Certificates \u00b6 Classic Load Balancer (v1) Support only one SSL certificate Must use multiple CLB for multiple hostname with multiple SSL certificates Application Load Balancer (v2) Supports multiple listeners with multiple SSL certificates Uses Server Nam Indication (SNI) to make it work Network Load Balancer (v2) Supports multiple listeners with multiple SSL certificates Uses Server Name Indication (SNI) to make it work","title":"Elastic Load Balancer - SSL Certificates"},{"location":"AWS/developer-associate/06-elb-asg/11-elastic-load-balancer-ssl-certificates/#elastic-load-balancer-ssl-certificates","text":"","title":"Elastic Load Balancer - SSL Certificates"},{"location":"AWS/developer-associate/06-elb-asg/11-elastic-load-balancer-ssl-certificates/#basics","text":"An SSL Certificate allows traffic between your clients and your load balancer to be encrypted in transit (in-flight encryption) SSL refers to Secure Sockets Layer, used to encrypy connections TLS refers to Transport Layer Security, which is a newer version Nowadays, TLS certificates are mainly used, but people still refer as SSL Public SSL certificatres are issued by Certificate Authorities (CA) Comodo, Symantec, GoDaddy, GlobalSign, Digicert, Letsencrypt, etc... SSL certificates have an expirateion date (you set) and must be renewed","title":"Basics"},{"location":"AWS/developer-associate/06-elb-asg/11-elastic-load-balancer-ssl-certificates/#load-balancer-ssl-certificates","text":"The load balancer uses an X.509 certificate (SSL/TLS server certificate) You can manage certificates using ACM (AWS Certificate Manager) You can upload your own certificates alternatively HTTPS listener: You must specify a default certificate You can use SNI (Server Name Indication) to specify the hostname they reach Ability to specify a security policy to support older version of SSL / TLS (legacy clients)","title":"Load Balancer - SSL Certificates"},{"location":"AWS/developer-associate/06-elb-asg/11-elastic-load-balancer-ssl-certificates/#ssl-server-name-indication","text":"SNI solves the problem of loading multiple SSL certificates onto one web server (to serve multiple websites) It's a newer protocol, and requires the client to indicate the hostname of the target server in the inital SSL handshake The server will then find the correct certificate, or return the default one Note: - Only works for ALB & NLB (newer generation), CloudFront - Does not work for CLB (older gen)","title":"SSL - Server Name Indication"},{"location":"AWS/developer-associate/06-elb-asg/11-elastic-load-balancer-ssl-certificates/#elastic-load-balancers-ssl-certificates","text":"Classic Load Balancer (v1) Support only one SSL certificate Must use multiple CLB for multiple hostname with multiple SSL certificates Application Load Balancer (v2) Supports multiple listeners with multiple SSL certificates Uses Server Nam Indication (SNI) to make it work Network Load Balancer (v2) Supports multiple listeners with multiple SSL certificates Uses Server Name Indication (SNI) to make it work","title":"Elastic Load Balancers - SSL Certificates"},{"location":"AWS/developer-associate/06-elb-asg/12-elastic-load-balancer-connection-draining/","text":"Elastic Load Balancer - Connection Draining \u00b6 Feature naming Connection Draining - for CLB Deregistration Delay - for ALB & NBL Time to complete in-flight requests while the instance is de-registering or unhealthy Stops sending new requests to the EC2 instance which is de-registering Between 1 to 3600 seconds (default: 300 seconds) Can be disabled (set value to 0) Set to a low value if your requests are short","title":"Elastic Load Balancer - Connection Draining"},{"location":"AWS/developer-associate/06-elb-asg/12-elastic-load-balancer-connection-draining/#elastic-load-balancer-connection-draining","text":"Feature naming Connection Draining - for CLB Deregistration Delay - for ALB & NBL Time to complete in-flight requests while the instance is de-registering or unhealthy Stops sending new requests to the EC2 instance which is de-registering Between 1 to 3600 seconds (default: 300 seconds) Can be disabled (set value to 0) Set to a low value if your requests are short","title":"Elastic Load Balancer - Connection Draining"},{"location":"AWS/developer-associate/06-elb-asg/13-auto-scaling-groups-overview/","text":"Auto Scaling Groups Overview \u00b6 What's an Auto Scaling Group? \u00b6 In real-life, the load on your websites and applications can change In the cloud, you can create and get rid of servers very quickly The goal of an Auto Scaling Group (ASG) is to: Scale out (add EC2 instances) to match an increased load Scale in (remove EC2 instances) to match a decreased load Ensure we have a minimum and maximum number of machines running Automatically Register new instances to a load balancer ASGs have the following attributes \u00b6 A launch configuration AMI + Instance type EC2 user data EBS Volumes Security Groups SSH Key Pair Min Size / Max Size / Initial Capacity Network + Subnets Information Load Balancer Information Scaling Policies Auto Scaling Alarms \u00b6 It is possible to scale an ASG based on CloudWatch alarms An Alarm monitors a metric (such as Average CPU) Metrics are computed for the overall ASG instances Based on the alarm: We can create scale-out policies (increase the number of instances) We can create scale-in policies (decrease the number of instances) Auto Scaling New Rules \u00b6 It is now possible to define \"better\" auto scaling rules that are directly managed by EC2 Target Average CPU Usage Number of requests on the ELB per instance Average Network In Average Network Out These rules are easier to set up and can make more sense Auto Scaling Custom Metric \u00b6 We can auto scale based on a custom metric (ex: number of connected users) Send custom metric from application on EC2 to CloudWatch Create CloudWatch alarm to react to low/high values Use the CloudWatch alaram as the scaling policy for ASG ASG Brain Dump \u00b6 Scaling policies can be on CPU, Network... and can even be on custom metrics or based on a schedule (if you know your visitors patterns) ASGs use Launch configuration or Launch Templates (newer) To update an ASG, you must provide a new launch configuration / launch template IAM roles attached to an ASG will get assigned to EC2 instances ASG are free. You pay for the underlying resources being launched Having instances under an ASG means that if they get terminated for whatever reason, the ASG will automatically create new ones as a replacement. ASG can terminate instances marked as unhealthy by an LB (and hence replace them)","title":"Auto Scaling Groups Overview"},{"location":"AWS/developer-associate/06-elb-asg/13-auto-scaling-groups-overview/#auto-scaling-groups-overview","text":"","title":"Auto Scaling Groups Overview"},{"location":"AWS/developer-associate/06-elb-asg/13-auto-scaling-groups-overview/#whats-an-auto-scaling-group","text":"In real-life, the load on your websites and applications can change In the cloud, you can create and get rid of servers very quickly The goal of an Auto Scaling Group (ASG) is to: Scale out (add EC2 instances) to match an increased load Scale in (remove EC2 instances) to match a decreased load Ensure we have a minimum and maximum number of machines running Automatically Register new instances to a load balancer","title":"What's an Auto Scaling Group?"},{"location":"AWS/developer-associate/06-elb-asg/13-auto-scaling-groups-overview/#asgs-have-the-following-attributes","text":"A launch configuration AMI + Instance type EC2 user data EBS Volumes Security Groups SSH Key Pair Min Size / Max Size / Initial Capacity Network + Subnets Information Load Balancer Information Scaling Policies","title":"ASGs have the following attributes"},{"location":"AWS/developer-associate/06-elb-asg/13-auto-scaling-groups-overview/#auto-scaling-alarms","text":"It is possible to scale an ASG based on CloudWatch alarms An Alarm monitors a metric (such as Average CPU) Metrics are computed for the overall ASG instances Based on the alarm: We can create scale-out policies (increase the number of instances) We can create scale-in policies (decrease the number of instances)","title":"Auto Scaling Alarms"},{"location":"AWS/developer-associate/06-elb-asg/13-auto-scaling-groups-overview/#auto-scaling-new-rules","text":"It is now possible to define \"better\" auto scaling rules that are directly managed by EC2 Target Average CPU Usage Number of requests on the ELB per instance Average Network In Average Network Out These rules are easier to set up and can make more sense","title":"Auto Scaling New Rules"},{"location":"AWS/developer-associate/06-elb-asg/13-auto-scaling-groups-overview/#auto-scaling-custom-metric","text":"We can auto scale based on a custom metric (ex: number of connected users) Send custom metric from application on EC2 to CloudWatch Create CloudWatch alarm to react to low/high values Use the CloudWatch alaram as the scaling policy for ASG","title":"Auto Scaling Custom Metric"},{"location":"AWS/developer-associate/06-elb-asg/13-auto-scaling-groups-overview/#asg-brain-dump","text":"Scaling policies can be on CPU, Network... and can even be on custom metrics or based on a schedule (if you know your visitors patterns) ASGs use Launch configuration or Launch Templates (newer) To update an ASG, you must provide a new launch configuration / launch template IAM roles attached to an ASG will get assigned to EC2 instances ASG are free. You pay for the underlying resources being launched Having instances under an ASG means that if they get terminated for whatever reason, the ASG will automatically create new ones as a replacement. ASG can terminate instances marked as unhealthy by an LB (and hence replace them)","title":"ASG Brain Dump"},{"location":"AWS/developer-associate/06-elb-asg/15-auto-scaling-groups-scaling-policies/","text":"Auto Scaling Groups - Scaling Policies \u00b6 Dynamic Scaling Policies \u00b6 Target Tracking Scaling Most simple and easy to set-up Example: I want the average ASG CPU to stay at around 40% Simple / Step Scaling When a CloudWatch alarm is triggered (example CPU > 70%) - add 2 units. When a CloudWatch alarm is triggered (example CPU < 30%), then remove 1 unit Scheduled Actions Anticipate a scaling based on known usage patterns Example: increase the min capacity to 10 at 5pm on Fridays Predictive Scaling \u00b6 Continuously forecast load and schedule scaling ahead Analyze historical load Generate forecast Schedule scaling actions Good metrics to scale on \u00b6 CPUUtilization: Average CPU utilization across your instances RequestCountPerTarget: to make sure the number of requests per EC2 isntances is stable Average Network In / Out (if your application is network bound) Any custom metric (that you push using CloudWatch) Scaling Cooldowns \u00b6 After a scaling activity happens, you are in the cooldown period (default 300 seconds) During the cooldown period, the ASG will not launch or terminate additional instances (to allow for metrics to stabilize) Advice: Use a ready-to-use AMI to redice configuration time in order to be service requests faster and reduce the cooldown period","title":"Auto Scaling Groups - Scaling Policies"},{"location":"AWS/developer-associate/06-elb-asg/15-auto-scaling-groups-scaling-policies/#auto-scaling-groups-scaling-policies","text":"","title":"Auto Scaling Groups - Scaling Policies"},{"location":"AWS/developer-associate/06-elb-asg/15-auto-scaling-groups-scaling-policies/#dynamic-scaling-policies","text":"Target Tracking Scaling Most simple and easy to set-up Example: I want the average ASG CPU to stay at around 40% Simple / Step Scaling When a CloudWatch alarm is triggered (example CPU > 70%) - add 2 units. When a CloudWatch alarm is triggered (example CPU < 30%), then remove 1 unit Scheduled Actions Anticipate a scaling based on known usage patterns Example: increase the min capacity to 10 at 5pm on Fridays","title":"Dynamic Scaling Policies"},{"location":"AWS/developer-associate/06-elb-asg/15-auto-scaling-groups-scaling-policies/#predictive-scaling","text":"Continuously forecast load and schedule scaling ahead Analyze historical load Generate forecast Schedule scaling actions","title":"Predictive Scaling"},{"location":"AWS/developer-associate/06-elb-asg/15-auto-scaling-groups-scaling-policies/#good-metrics-to-scale-on","text":"CPUUtilization: Average CPU utilization across your instances RequestCountPerTarget: to make sure the number of requests per EC2 isntances is stable Average Network In / Out (if your application is network bound) Any custom metric (that you push using CloudWatch)","title":"Good metrics to scale on"},{"location":"AWS/developer-associate/06-elb-asg/15-auto-scaling-groups-scaling-policies/#scaling-cooldowns","text":"After a scaling activity happens, you are in the cooldown period (default 300 seconds) During the cooldown period, the ASG will not launch or terminate additional instances (to allow for metrics to stabilize) Advice: Use a ready-to-use AMI to redice configuration time in order to be service requests faster and reduce the cooldown period","title":"Scaling Cooldowns"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/01-amazon-rds-overview/","text":"AWS RDS Overview \u00b6 RDS Stands for Relatational Database Service It's a managed DB service It allows you to create databases in the cloud that are managed by AWS Postgresql MySQL MariaDB Oracle Microsoft SQL Server Aurora (AWS PRoprietary Database) Advantage over using RDS versus deploying DB on EC2 \u00b6 RDS is a managed service: Automated provisioning, OS pathching Continous backups and restore to specific timestamp (Point in Time Restore) Monitoring dashboards Read replicas for improved read performance Multi AZ setup for DR (Disaster Recovery) Maintenance windows for upgrades Scaling capability (vertical and horizontal) Storage backed by EBS (gp2 or io1) But you cannot SSH into your instances RDS Backups \u00b6 Backups are automatically enabled in RDS Automated backups: Daily full backup of the database (during the maintenance window) Transaction logs are backed-up by RDS every 5 minutes Ability to restore to any point in time (from oldest backup to 5 minutes ago) 7 day retention (can be increased to 35 days) DB Snapshots Manually triggered by the user Retention of backup for as long as you want RDS - Storage Auto Scaling \u00b6 Helps you increase storage on your RDS DB instance dynamically When RDS detects you are running out of free database storage, it scales automatically Avoid manually scaling your database storage You have to set Maximum Storage Threshold (maximum limit for DB Storage) Automatically modify storage if: Free storage less than 10% of allocated storage Low-storage lasts at least 5 minutes 6 hours have passed since last modification Useful for applications with unpredicatable loads Supports all RDS database engines (MariaDB, MySQL, PostgreSQL, SQL Server, Oracle).","title":"AWS RDS Overview"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/01-amazon-rds-overview/#aws-rds-overview","text":"RDS Stands for Relatational Database Service It's a managed DB service It allows you to create databases in the cloud that are managed by AWS Postgresql MySQL MariaDB Oracle Microsoft SQL Server Aurora (AWS PRoprietary Database)","title":"AWS RDS Overview"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/01-amazon-rds-overview/#advantage-over-using-rds-versus-deploying-db-on-ec2","text":"RDS is a managed service: Automated provisioning, OS pathching Continous backups and restore to specific timestamp (Point in Time Restore) Monitoring dashboards Read replicas for improved read performance Multi AZ setup for DR (Disaster Recovery) Maintenance windows for upgrades Scaling capability (vertical and horizontal) Storage backed by EBS (gp2 or io1) But you cannot SSH into your instances","title":"Advantage over using RDS versus deploying DB on EC2"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/01-amazon-rds-overview/#rds-backups","text":"Backups are automatically enabled in RDS Automated backups: Daily full backup of the database (during the maintenance window) Transaction logs are backed-up by RDS every 5 minutes Ability to restore to any point in time (from oldest backup to 5 minutes ago) 7 day retention (can be increased to 35 days) DB Snapshots Manually triggered by the user Retention of backup for as long as you want","title":"RDS Backups"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/01-amazon-rds-overview/#rds-storage-auto-scaling","text":"Helps you increase storage on your RDS DB instance dynamically When RDS detects you are running out of free database storage, it scales automatically Avoid manually scaling your database storage You have to set Maximum Storage Threshold (maximum limit for DB Storage) Automatically modify storage if: Free storage less than 10% of allocated storage Low-storage lasts at least 5 minutes 6 hours have passed since last modification Useful for applications with unpredicatable loads Supports all RDS database engines (MariaDB, MySQL, PostgreSQL, SQL Server, Oracle).","title":"RDS - Storage Auto Scaling"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/02-rds-read-replicas-vs-multi-az/","text":"RDS Read Replicas vs Multi AZ \u00b6 RDS Read Replicas for read scalability \u00b6 Up to 5 read replicas Within AZ, Cross AZ or Cross Region Replication is ASYNC so reads are eventually consistent Replicas can be promoted to their own DB Application must update the connection string to leverage read replicas RDS Read Replicas - Use Cases \u00b6 You have a production database that is taking on normal load You want to run a reporting application to run some analytics You create a read replica to run the new workload there The production application is unaffected Read replicas are used for SELECT (read) only kind of statements (not INSERT, UPDATE, DELETE) RDS Read Replicas - network cost \u00b6 In AWS there's a network cost when data goes from one AZ to another For RDS Read Replicas within the same region, you don't pay that fee. So, if you have an RDS instance in us-east-1a and a read replica in us-east-1b - the replication will be free. If you have an RDS instance in us-east-1a and you make cross-region replication to eu-west-1b , it will cost you the network fee. RDS Multi AZ (Disaster Recovery) \u00b6 SYNC replication One DNS name - automatic app failover to standby Increase availability Failover in case of loss of AZ, loss of network, instance or storage failure No manual intervention in apps Not used for scaling The read replicas can be setup as Multi AZ for Disaster Recovery (DR). RDS - From Single-AZ to Multi-AZ \u00b6 Zero downtime operation (no need to stop the DB) Just click on \"modify\" for the database The following happens internally: A snapshot is taken A new DB is restored from the snapshot in a new AZ Synchronization is established between the two databases","title":"RDS Read Replicas vs Multi AZ"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/02-rds-read-replicas-vs-multi-az/#rds-read-replicas-vs-multi-az","text":"","title":"RDS Read Replicas vs Multi AZ"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/02-rds-read-replicas-vs-multi-az/#rds-read-replicas-for-read-scalability","text":"Up to 5 read replicas Within AZ, Cross AZ or Cross Region Replication is ASYNC so reads are eventually consistent Replicas can be promoted to their own DB Application must update the connection string to leverage read replicas","title":"RDS Read Replicas for read scalability"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/02-rds-read-replicas-vs-multi-az/#rds-read-replicas-use-cases","text":"You have a production database that is taking on normal load You want to run a reporting application to run some analytics You create a read replica to run the new workload there The production application is unaffected Read replicas are used for SELECT (read) only kind of statements (not INSERT, UPDATE, DELETE)","title":"RDS Read Replicas - Use Cases"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/02-rds-read-replicas-vs-multi-az/#rds-read-replicas-network-cost","text":"In AWS there's a network cost when data goes from one AZ to another For RDS Read Replicas within the same region, you don't pay that fee. So, if you have an RDS instance in us-east-1a and a read replica in us-east-1b - the replication will be free. If you have an RDS instance in us-east-1a and you make cross-region replication to eu-west-1b , it will cost you the network fee.","title":"RDS Read Replicas - network cost"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/02-rds-read-replicas-vs-multi-az/#rds-multi-az-disaster-recovery","text":"SYNC replication One DNS name - automatic app failover to standby Increase availability Failover in case of loss of AZ, loss of network, instance or storage failure No manual intervention in apps Not used for scaling The read replicas can be setup as Multi AZ for Disaster Recovery (DR).","title":"RDS Multi AZ (Disaster Recovery)"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/02-rds-read-replicas-vs-multi-az/#rds-from-single-az-to-multi-az","text":"Zero downtime operation (no need to stop the DB) Just click on \"modify\" for the database The following happens internally: A snapshot is taken A new DB is restored from the snapshot in a new AZ Synchronization is established between the two databases","title":"RDS - From Single-AZ to Multi-AZ"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/03-amazon-rds-hands-on/","text":"Amazon RDS Hands On \u00b6 We can open up the RDS service in AWS And go to the databases secion and click on create database There will we have different options: We can choose the creation method: We are going to choose the standard one. In it the first choice to make is to pick a database engine: Amazon Aurora MySQL MariaDB PostgreSQL Oracle Microsoft SQL Server We are going to select MySQL and select the latest version. Next we can choose between some templates that will be pre-filling the following configurations with best suited values. We are going to use the Production one but modify it to be compliant with free tier. Next we can choose our availability and durability settings. For free tier pick Single DB Instance. In the settings section we can set the identifier, username and password. Next we can choose the instance types. For free tier use db.t2.micro Next we can choose the storage type. For free tier use gp2. You can also set the allocated storage and enable storage autoscaling. In the connectivity section we can choose the VPC the database will be placed into. Whether the database is publicly available. The security groups and port under additional configuration. We can also choose the authentication methods. Under the additional configuration we can: Set the initial database name, parameter groups, option groups. Configure backups Configure encryption Enable performance insights Configure retention period Configure monitoring Configure maintenance and toggle deletion protection Once done setting up, we can click on create the database and it will show up on the databases section wth a status of creating . It can take a few minutes for it to be ready. Once it's done, we can open it up and see the connectivity tab which will list the endpoint and the port t connect to: In the next tab we can see monitoring There are also several actions we can take: Create a read replica for larger read capacity Take a snapshot - create a backup of the database which you can spin up elsewhere. Restore to point in time Migrate snapshot - in a different region for example Stop Reboot Delete","title":"Amazon RDS Hands On"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/03-amazon-rds-hands-on/#amazon-rds-hands-on","text":"We can open up the RDS service in AWS And go to the databases secion and click on create database There will we have different options: We can choose the creation method: We are going to choose the standard one. In it the first choice to make is to pick a database engine: Amazon Aurora MySQL MariaDB PostgreSQL Oracle Microsoft SQL Server We are going to select MySQL and select the latest version. Next we can choose between some templates that will be pre-filling the following configurations with best suited values. We are going to use the Production one but modify it to be compliant with free tier. Next we can choose our availability and durability settings. For free tier pick Single DB Instance. In the settings section we can set the identifier, username and password. Next we can choose the instance types. For free tier use db.t2.micro Next we can choose the storage type. For free tier use gp2. You can also set the allocated storage and enable storage autoscaling. In the connectivity section we can choose the VPC the database will be placed into. Whether the database is publicly available. The security groups and port under additional configuration. We can also choose the authentication methods. Under the additional configuration we can: Set the initial database name, parameter groups, option groups. Configure backups Configure encryption Enable performance insights Configure retention period Configure monitoring Configure maintenance and toggle deletion protection Once done setting up, we can click on create the database and it will show up on the databases section wth a status of creating . It can take a few minutes for it to be ready. Once it's done, we can open it up and see the connectivity tab which will list the endpoint and the port t connect to: In the next tab we can see monitoring There are also several actions we can take: Create a read replica for larger read capacity Take a snapshot - create a backup of the database which you can spin up elsewhere. Restore to point in time Migrate snapshot - in a different region for example Stop Reboot Delete","title":"Amazon RDS Hands On"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/04-rds-encryption-and-security/","text":"RDS Encryption & Security \u00b6 RDS Security - Encryption \u00b6 At rest encryption Possibility to encrypt the master & read replicas with AWS KMS - AES-256 encryption Encryption has to be defined at launch time If the master is not encrypted, the read replicas cannot be encrypted Transparent Data Encryption (TDE) available for Oracle and SQL Server In-flight encryption SSL certificates to encrypt data to RDS in flight Provide SSL options with trust certificate when connecting to database To enforce SSL: PostgreSQL: rds.force_ssl = 1 in the AWS RDS COnsole (paramater groups) MySQL: GRANT USAGE ON . to 'mysqluser'@'%' REQUIRE SSL; RDS Encryption Operations \u00b6 Encrypting RDS Backups Snapshots of un-encrypted RDS databases are un-encrypted Snapshots of encrypted RDS databases are encrypted Can copy a snapshot into an encrypted one To encrypt an un-encrypted RDS database: Create a snapshot of the un-encrypted database Copy the snapshot and enable encryption for the snapshot Restore the database from the encrypted snapshot Migrate applications to the new database, and delete the old database RDS Security - Network & IAM \u00b6 Network security RDS databases are usually deployed within a private subnet, not in a public one RDS security works by leveraging security groups (the same concept as for EC2 instances) - it controls which IP / security group can communicate with RDS Access management IAM policies help control who can manage AWS RDS (through RDS API) Traditional Username and Password can be used to log into the database IAM-based authentication can be used to log into RDS MySQL & PostgreSQL RDS - IAM Authentication \u00b6 IAM database authentication works with MySQL and PostgreSQL You don't need a password, just an authentication token obtained through IAM & RDS API calls Auth toiken has a liftime of 15 minutes Benefits: Network in/out must be encrypted using SSL IAM to centrally manage users instead of DB Can leverage IAM Roles and EC2 isntance profiles for easy integration RDS Security - Summary \u00b6 Envryption at rest Is done only when you first create the DB instance or unencrypted DB => snapshot => copy snapshot as encrypted => create DB from snapshot You responsibility: Check the ports / IP/ securityy group inbound rules in DB's SG In-database user creation and permissions or manage through IAM Creating a database with or without public access Ensure parameter groups or DB is configured to only allow SSL connections. AWS Responsibility: no SSH access no manual DB patching no manual OS patching no way to audit the underlying instance","title":"RDS Encryption & Security"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/04-rds-encryption-and-security/#rds-encryption-security","text":"","title":"RDS Encryption &amp; Security"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/04-rds-encryption-and-security/#rds-security-encryption","text":"At rest encryption Possibility to encrypt the master & read replicas with AWS KMS - AES-256 encryption Encryption has to be defined at launch time If the master is not encrypted, the read replicas cannot be encrypted Transparent Data Encryption (TDE) available for Oracle and SQL Server In-flight encryption SSL certificates to encrypt data to RDS in flight Provide SSL options with trust certificate when connecting to database To enforce SSL: PostgreSQL: rds.force_ssl = 1 in the AWS RDS COnsole (paramater groups) MySQL: GRANT USAGE ON . to 'mysqluser'@'%' REQUIRE SSL;","title":"RDS Security - Encryption"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/04-rds-encryption-and-security/#rds-encryption-operations","text":"Encrypting RDS Backups Snapshots of un-encrypted RDS databases are un-encrypted Snapshots of encrypted RDS databases are encrypted Can copy a snapshot into an encrypted one To encrypt an un-encrypted RDS database: Create a snapshot of the un-encrypted database Copy the snapshot and enable encryption for the snapshot Restore the database from the encrypted snapshot Migrate applications to the new database, and delete the old database","title":"RDS Encryption Operations"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/04-rds-encryption-and-security/#rds-security-network-iam","text":"Network security RDS databases are usually deployed within a private subnet, not in a public one RDS security works by leveraging security groups (the same concept as for EC2 instances) - it controls which IP / security group can communicate with RDS Access management IAM policies help control who can manage AWS RDS (through RDS API) Traditional Username and Password can be used to log into the database IAM-based authentication can be used to log into RDS MySQL & PostgreSQL","title":"RDS Security - Network &amp; IAM"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/04-rds-encryption-and-security/#rds-iam-authentication","text":"IAM database authentication works with MySQL and PostgreSQL You don't need a password, just an authentication token obtained through IAM & RDS API calls Auth toiken has a liftime of 15 minutes Benefits: Network in/out must be encrypted using SSL IAM to centrally manage users instead of DB Can leverage IAM Roles and EC2 isntance profiles for easy integration","title":"RDS - IAM Authentication"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/04-rds-encryption-and-security/#rds-security-summary","text":"Envryption at rest Is done only when you first create the DB instance or unencrypted DB => snapshot => copy snapshot as encrypted => create DB from snapshot You responsibility: Check the ports / IP/ securityy group inbound rules in DB's SG In-database user creation and permissions or manage through IAM Creating a database with or without public access Ensure parameter groups or DB is configured to only allow SSL connections. AWS Responsibility: no SSH access no manual DB patching no manual OS patching no way to audit the underlying instance","title":"RDS Security - Summary"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/05-aurora-overview/","text":"Amazon Aurora \u00b6 Aurora is a proprietary technology from AWS (not open sourced) Postgres and MySQL are both supported as Aurora DB (thaat means your drivers will work as if Aurora was a Postgres or MySQL database) Aurora is \"AWS cloud optimized\" and claims 5x performance improvement over MySQL on RDS, over 3x the performance of Postgres on RDS Aurora storage automatically grows in increments of 10GB up to 64TB. Aurora can have 15 replicas while MySQL has 5, and the replication process is faster (sub 10ms replica lag) Failover in Aurora is instantaneous. It's HA native. Aurora costs more than RDS (20% more) - bit is more efficient. Aurora High Availability and Read Scaling \u00b6 6 copies of your data across 3 AZ: 4 copies out of 6 needed for writes 3 copies out of 6 need for reads Self healing with peer-to-peer replication Storage is striped across 100s of volumes One aurora instance takes writes (master) Automated failover for master in less than 30 seconds Master + up to 15 aurora read replicas serve reads Support for cross region replication Features of Aurora \u00b6 Automatic failover Backup and Recovery Isolation and security Industry compliance Push-button scaling Automated Patching with Zero Downtime Advanced Monitoring Routine Maintenance Backtrack: restore data at any point in time without using backups Aurora Security \u00b6 Similar to RDS because uses the same engines Encryption at rest using KMS Automated backups, snapshots and replicas are also encrypted Encryption in flight using SSL (same process as MySQL or Postgres) Possibility to authenticate using IAM token (same method as RDS) You are responsible for protecting the instance with security groups You can't ssh into it","title":"Amazon Aurora"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/05-aurora-overview/#amazon-aurora","text":"Aurora is a proprietary technology from AWS (not open sourced) Postgres and MySQL are both supported as Aurora DB (thaat means your drivers will work as if Aurora was a Postgres or MySQL database) Aurora is \"AWS cloud optimized\" and claims 5x performance improvement over MySQL on RDS, over 3x the performance of Postgres on RDS Aurora storage automatically grows in increments of 10GB up to 64TB. Aurora can have 15 replicas while MySQL has 5, and the replication process is faster (sub 10ms replica lag) Failover in Aurora is instantaneous. It's HA native. Aurora costs more than RDS (20% more) - bit is more efficient.","title":"Amazon Aurora"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/05-aurora-overview/#aurora-high-availability-and-read-scaling","text":"6 copies of your data across 3 AZ: 4 copies out of 6 needed for writes 3 copies out of 6 need for reads Self healing with peer-to-peer replication Storage is striped across 100s of volumes One aurora instance takes writes (master) Automated failover for master in less than 30 seconds Master + up to 15 aurora read replicas serve reads Support for cross region replication","title":"Aurora High Availability and Read Scaling"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/05-aurora-overview/#features-of-aurora","text":"Automatic failover Backup and Recovery Isolation and security Industry compliance Push-button scaling Automated Patching with Zero Downtime Advanced Monitoring Routine Maintenance Backtrack: restore data at any point in time without using backups","title":"Features of Aurora"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/05-aurora-overview/#aurora-security","text":"Similar to RDS because uses the same engines Encryption at rest using KMS Automated backups, snapshots and replicas are also encrypted Encryption in flight using SSL (same process as MySQL or Postgres) Possibility to authenticate using IAM token (same method as RDS) You are responsible for protecting the instance with security groups You can't ssh into it","title":"Aurora Security"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/06-aurora-hands-on/","text":"Aurora Hands On \u00b6 Lets create an Aurora Database We have 2 editions available - the MySQL compatible version or PostgreSQL compatible version. Also, we can choose the capacity to be either provisioned or serverless. For the provisioned capacity we can also choose the replication. Also, we have multiple versions available. In the filters section we can filter out versions that support different features like span across multiple regions and parallel queries. For templates we have either production or development. No free tier is available. The other features are basically same as any other RDS. When spun up it will create a writer and reader instance - different endpoints for writes and reads. The aurora will have multiple actions: Add readers (add reading capacity) Create cross-region read replica Restore to point in time Add replica auto scaling We can create an autoscaling rule based on CPU or connections that will auto scale read replicas. Add AWS Region Enable Aurora to be global","title":"Aurora Hands On"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/06-aurora-hands-on/#aurora-hands-on","text":"Lets create an Aurora Database We have 2 editions available - the MySQL compatible version or PostgreSQL compatible version. Also, we can choose the capacity to be either provisioned or serverless. For the provisioned capacity we can also choose the replication. Also, we have multiple versions available. In the filters section we can filter out versions that support different features like span across multiple regions and parallel queries. For templates we have either production or development. No free tier is available. The other features are basically same as any other RDS. When spun up it will create a writer and reader instance - different endpoints for writes and reads. The aurora will have multiple actions: Add readers (add reading capacity) Create cross-region read replica Restore to point in time Add replica auto scaling We can create an autoscaling rule based on CPU or connections that will auto scale read replicas. Add AWS Region Enable Aurora to be global","title":"Aurora Hands On"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/07-elasticache-overview/","text":"Amazon ElastiCache Overview \u00b6 The same way RDS is to get managed Relational Databases the ElastiCache is to get a manged Redis or Memcached Caches are in-memory databases with really high performance, low latency Helps reduce load off of databases for read intensive workloads Helps make your application stateless AWS takes care of OS maintenence / patching, optimizations, setup, configuration, monitoring, failure recovery and backups. Using ElastiCache involves heavy application code changes. ElastiCache Solition Architecture - DB Cache \u00b6 Application queries ElastiCache, if not available, get from RDS and store in ElastiCache Helps relieve load in RDS Cache must have an invalidation strategy to make sure only the most current data is used in there. ElastiCache Solution Architecture - User Session Store \u00b6 User logs into any of the application The application writes the session data into ElastiCache The user hits another instance of our application The instance retrieves the data and the user is already logged in. ElastiCache - Redis vs Memcached \u00b6 Redis Multi AZ with Auto-Failover Read replicas to scale reads and have high availability Data Durability using AOF persistence Backup and restore features Memcached Multi-node for partitioning of data (sharding) No high availability (replication) Non persistent No backup and restore Multi-threaded architecture","title":"Amazon ElastiCache Overview"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/07-elasticache-overview/#amazon-elasticache-overview","text":"The same way RDS is to get managed Relational Databases the ElastiCache is to get a manged Redis or Memcached Caches are in-memory databases with really high performance, low latency Helps reduce load off of databases for read intensive workloads Helps make your application stateless AWS takes care of OS maintenence / patching, optimizations, setup, configuration, monitoring, failure recovery and backups. Using ElastiCache involves heavy application code changes.","title":"Amazon ElastiCache Overview"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/07-elasticache-overview/#elasticache-solition-architecture-db-cache","text":"Application queries ElastiCache, if not available, get from RDS and store in ElastiCache Helps relieve load in RDS Cache must have an invalidation strategy to make sure only the most current data is used in there.","title":"ElastiCache Solition Architecture - DB Cache"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/07-elasticache-overview/#elasticache-solution-architecture-user-session-store","text":"User logs into any of the application The application writes the session data into ElastiCache The user hits another instance of our application The instance retrieves the data and the user is already logged in.","title":"ElastiCache Solution Architecture - User Session Store"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/07-elasticache-overview/#elasticache-redis-vs-memcached","text":"Redis Multi AZ with Auto-Failover Read replicas to scale reads and have high availability Data Durability using AOF persistence Backup and restore features Memcached Multi-node for partitioning of data (sharding) No high availability (replication) Non persistent No backup and restore Multi-threaded architecture","title":"ElastiCache - Redis vs Memcached"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/08-elasticache-hands-on/","text":"ElastiCache Hands On \u00b6 Let's try creating an ElastiCache cluster Click on Get Started Now, ther will have 2 options for the cluster engine - redis or memcached. The redis can be a single node or cluster. In the Redis settings we can choose multiple settings like engine version, port, parameter group, instance type, number of replicas and multiAZ. In the advanced settings we can configure subnet, VPC and subnets. Choose availability zones. In security we can enable encryption at rest and in transit. In-transit will let you choose redis auth method as well. We also have options to log, import data into cluster and enable backups. Then we can enable maintenance windows, add tags.","title":"ElastiCache Hands On"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/08-elasticache-hands-on/#elasticache-hands-on","text":"Let's try creating an ElastiCache cluster Click on Get Started Now, ther will have 2 options for the cluster engine - redis or memcached. The redis can be a single node or cluster. In the Redis settings we can choose multiple settings like engine version, port, parameter group, instance type, number of replicas and multiAZ. In the advanced settings we can configure subnet, VPC and subnets. Choose availability zones. In security we can enable encryption at rest and in transit. In-transit will let you choose redis auth method as well. We also have options to log, import data into cluster and enable backups. Then we can enable maintenance windows, add tags.","title":"ElastiCache Hands On"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/09-elasticache-strategies/","text":"ElastiCache Strategies \u00b6 Caching Implementation Considerations \u00b6 Read more at https://aws.amazon.com/caching/implementation-considerations/ Is it safe to cache data? Data may be out of date, eventually consistent Is caching effective for that data? Pattern: data changing slowly, few keys are frequently needed Any patterns: data changing rapidly, all larke key space frequently needed Is data structured well for caching? Example: key value caching or caching of agggregations results Which caching design is the most appropriate? Lazy Loading / Cache-Aside / Lazy Population \u00b6 Pros Only requested data is cached (the cache isn't filled up with unused data) Node failures are not fatal (just increased latency to warm the cache) Cons Cache miss penalty that results in 3 round trips, noticeable delay for that request Stale data: data can be updated in the database and outdated in the cache def get_user(user_id): # check the cache record = cache.get(user_id) if record is None: # run a DB query record = db.query(\"select * from users where id = ?\", user_id) # populate the cache cache.set(user_id, record) else: return record user = get_user(17) Write Through - Add or Update cache when database is updated \u00b6 Pros: Data in cache is never stale, reads are quick Write penalty vs read penalty (each write requires 2 calls) Cons: Missing data until it is added / updated in the DB. mitigation is to implement lazy loading strategy as well. Cache churn - a lot of the data will never be read def save_user(user_id, values): # save to db record = db.query(\"update users ... where id = ?\", user_id, values) # push into cache cache.set(user_id, record) user = save_user(17, {\"name\": \"Some name\"}) Cache Evictions and Time-to-live (TTL) \u00b6 Cache eviction can occur in three ways: You delete the item explicitly in the cache Item is evicted because the memory is full and it's not recently used (LRU) You set an item time-to-live (TTL) TTL are helpful for any kind of data: Leaderboards Comments Activity streams TTL can range from few seconds to hours or days If too many evictions happen due to memory, you should scale up or out. Final Words \u00b6 Lazy Loading / Cache aside is easy to implement and works for many situations as a foundation, especially on the read side. Write-through is usually combined with Lazy Loading as targeted for the queries or workloads that benefit from this optimization Setting a TTL is usually not a bad idea, except when you're using Write-through. Set it to a sensible value for your application Only cache the data that makes sense (user profiles, blogs, etc) Quote: there are only two hard things in Computer Science: cache invalidation and naming things.","title":"ElastiCache Strategies"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/09-elasticache-strategies/#elasticache-strategies","text":"","title":"ElastiCache Strategies"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/09-elasticache-strategies/#caching-implementation-considerations","text":"Read more at https://aws.amazon.com/caching/implementation-considerations/ Is it safe to cache data? Data may be out of date, eventually consistent Is caching effective for that data? Pattern: data changing slowly, few keys are frequently needed Any patterns: data changing rapidly, all larke key space frequently needed Is data structured well for caching? Example: key value caching or caching of agggregations results Which caching design is the most appropriate?","title":"Caching Implementation Considerations"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/09-elasticache-strategies/#lazy-loading-cache-aside-lazy-population","text":"Pros Only requested data is cached (the cache isn't filled up with unused data) Node failures are not fatal (just increased latency to warm the cache) Cons Cache miss penalty that results in 3 round trips, noticeable delay for that request Stale data: data can be updated in the database and outdated in the cache def get_user(user_id): # check the cache record = cache.get(user_id) if record is None: # run a DB query record = db.query(\"select * from users where id = ?\", user_id) # populate the cache cache.set(user_id, record) else: return record user = get_user(17)","title":"Lazy Loading / Cache-Aside / Lazy Population"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/09-elasticache-strategies/#write-through-add-or-update-cache-when-database-is-updated","text":"Pros: Data in cache is never stale, reads are quick Write penalty vs read penalty (each write requires 2 calls) Cons: Missing data until it is added / updated in the DB. mitigation is to implement lazy loading strategy as well. Cache churn - a lot of the data will never be read def save_user(user_id, values): # save to db record = db.query(\"update users ... where id = ?\", user_id, values) # push into cache cache.set(user_id, record) user = save_user(17, {\"name\": \"Some name\"})","title":"Write Through - Add or Update cache when database is updated"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/09-elasticache-strategies/#cache-evictions-and-time-to-live-ttl","text":"Cache eviction can occur in three ways: You delete the item explicitly in the cache Item is evicted because the memory is full and it's not recently used (LRU) You set an item time-to-live (TTL) TTL are helpful for any kind of data: Leaderboards Comments Activity streams TTL can range from few seconds to hours or days If too many evictions happen due to memory, you should scale up or out.","title":"Cache Evictions and Time-to-live (TTL)"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/09-elasticache-strategies/#final-words","text":"Lazy Loading / Cache aside is easy to implement and works for many situations as a foundation, especially on the read side. Write-through is usually combined with Lazy Loading as targeted for the queries or workloads that benefit from this optimization Setting a TTL is usually not a bad idea, except when you're using Write-through. Set it to a sensible value for your application Only cache the data that makes sense (user profiles, blogs, etc) Quote: there are only two hard things in Computer Science: cache invalidation and naming things.","title":"Final Words"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/10-elasticache-redis-cluster-modes/","text":"ElastiCache Redis Cluster Modes \u00b6 ElastiCache Replication: Cluster Mode Disabled \u00b6 One primary node, up to 5 replicas Asynchronous replication The primary node is used for read/write The other nodes are read-only One shard, all nodes have all the data Guard against data loss if node failure Multi-AZ enabled by default for failover Helpful to scale read performance ElastiCache Replication: Cluster Mode Enabled \u00b6 Data is partitioned across shards (helpful to scale writes) Each shard has a primary and up to 5 replica nodes (same concept as before) Multi AZ capability Up to 500 nodes per cluster 500 shards with a single master 250 shards with 1 master and 1 replica ... 83 shards with one master and 5 replicas","title":"ElastiCache Redis Cluster Modes"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/10-elasticache-redis-cluster-modes/#elasticache-redis-cluster-modes","text":"","title":"ElastiCache Redis Cluster Modes"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/10-elasticache-redis-cluster-modes/#elasticache-replication-cluster-mode-disabled","text":"One primary node, up to 5 replicas Asynchronous replication The primary node is used for read/write The other nodes are read-only One shard, all nodes have all the data Guard against data loss if node failure Multi-AZ enabled by default for failover Helpful to scale read performance","title":"ElastiCache Replication: Cluster Mode Disabled"},{"location":"AWS/developer-associate/07-rds-aurora-elasticache/10-elasticache-redis-cluster-modes/#elasticache-replication-cluster-mode-enabled","text":"Data is partitioned across shards (helpful to scale writes) Each shard has a primary and up to 5 replica nodes (same concept as before) Multi AZ capability Up to 500 nodes per cluster 500 shards with a single master 250 shards with 1 master and 1 replica ... 83 shards with one master and 5 replicas","title":"ElastiCache Replication: Cluster Mode Enabled"},{"location":"AWS/developer-associate/08-route-53/01-what-is-dns/","text":"What is DNS? \u00b6 DNS stands for Domain Name System which translates the human friendly hostnames into the machin IP addresses www.google.com => 172.217.18.36 DNS is the backbone of the internet DNS uses hierarchical naming structure .com example.com www.example.com api.example.com DNS Terminologies \u00b6 Domain Registrar: Amazon Route 53, GoDaddy... DNS Records: A, AAAA, CNAME, NS, ... Zone File: contains DNS records Name Server: resolves DNS queries (Authoritative or Non-Authoritative) Top Level Domain (TLD): .com, .us, .in, .gov, .org... Second Level Domain (SLD): amazon.com, google.com, ... How DNS Works \u00b6","title":"What is DNS?"},{"location":"AWS/developer-associate/08-route-53/01-what-is-dns/#what-is-dns","text":"DNS stands for Domain Name System which translates the human friendly hostnames into the machin IP addresses www.google.com => 172.217.18.36 DNS is the backbone of the internet DNS uses hierarchical naming structure .com example.com www.example.com api.example.com","title":"What is DNS?"},{"location":"AWS/developer-associate/08-route-53/01-what-is-dns/#dns-terminologies","text":"Domain Registrar: Amazon Route 53, GoDaddy... DNS Records: A, AAAA, CNAME, NS, ... Zone File: contains DNS records Name Server: resolves DNS queries (Authoritative or Non-Authoritative) Top Level Domain (TLD): .com, .us, .in, .gov, .org... Second Level Domain (SLD): amazon.com, google.com, ...","title":"DNS Terminologies"},{"location":"AWS/developer-associate/08-route-53/01-what-is-dns/#how-dns-works","text":"","title":"How DNS Works"},{"location":"AWS/developer-associate/08-route-53/02-route-53-overview/","text":"Route 53 Overview \u00b6 The Amazon Route 53 is a highly available, scalable, fully managed and authoritative DNS. The authoritative means that the customer (you) can update the DNS records. Route 53 is also a Domain Registrar so you can register your domain names there as well. Provides an ability to check the health of your resources The only AWS service which provides 100% availability SLA (service level agreement) Route 53 - Records \u00b6 How you want to route traffic for a domain Each record contains: Domain/subdomain name - e.g. example.com Record Type - e.g. A or AAAA Value - e.g. 12.34.56.78 Routing Policy - how Route 53 responds to queries TTL - amount of time the record is cached at resolvers Route 53 supports the following DNS record types (must know) A / AAAA / CNAME / NS (advanced) CAA / DS / MX / NAPTR / PTR / SOA / TXT / SPF / SRV Route 53 - Record Types \u00b6 A - maps a hostname to IPv4 AAAA - maps a hostname to IPv6 CNAME - maps a hostname to another hostname The target is a domain name which must have an A or AAAA record Can't create a CNAME record for the top node of a DNS namespace (Zone Apex) Example: you can't create one for example.com but you can create for www.example.com NS - Name Servers for the Hosted Zone Control how traffic is routed for a domain Route 53 - Hosted Zones \u00b6 A container for records that define how to route traffic to a domain and its subdomains Public Hosted Zones - contains records that specify how to route traffic on the internet (public domain names) application1.mypublicdomain.com Private Hosted Zones - contain records that specify how you route traffic within one or more VPCs (private domain names) aplication1.comany.internal You pay $0.50 per month per hosted zone","title":"Route 53 Overview"},{"location":"AWS/developer-associate/08-route-53/02-route-53-overview/#route-53-overview","text":"The Amazon Route 53 is a highly available, scalable, fully managed and authoritative DNS. The authoritative means that the customer (you) can update the DNS records. Route 53 is also a Domain Registrar so you can register your domain names there as well. Provides an ability to check the health of your resources The only AWS service which provides 100% availability SLA (service level agreement)","title":"Route 53 Overview"},{"location":"AWS/developer-associate/08-route-53/02-route-53-overview/#route-53-records","text":"How you want to route traffic for a domain Each record contains: Domain/subdomain name - e.g. example.com Record Type - e.g. A or AAAA Value - e.g. 12.34.56.78 Routing Policy - how Route 53 responds to queries TTL - amount of time the record is cached at resolvers Route 53 supports the following DNS record types (must know) A / AAAA / CNAME / NS (advanced) CAA / DS / MX / NAPTR / PTR / SOA / TXT / SPF / SRV","title":"Route 53 - Records"},{"location":"AWS/developer-associate/08-route-53/02-route-53-overview/#route-53-record-types","text":"A - maps a hostname to IPv4 AAAA - maps a hostname to IPv6 CNAME - maps a hostname to another hostname The target is a domain name which must have an A or AAAA record Can't create a CNAME record for the top node of a DNS namespace (Zone Apex) Example: you can't create one for example.com but you can create for www.example.com NS - Name Servers for the Hosted Zone Control how traffic is routed for a domain","title":"Route 53 - Record Types"},{"location":"AWS/developer-associate/08-route-53/02-route-53-overview/#route-53-hosted-zones","text":"A container for records that define how to route traffic to a domain and its subdomains Public Hosted Zones - contains records that specify how to route traffic on the internet (public domain names) application1.mypublicdomain.com Private Hosted Zones - contain records that specify how you route traffic within one or more VPCs (private domain names) aplication1.comany.internal You pay $0.50 per month per hosted zone","title":"Route 53 - Hosted Zones"},{"location":"AWS/developer-associate/08-route-53/03-route-53-registering-a-domain/","text":"Registering a domain \u00b6 Open Route 53 service in AWS console Select register domain Search for a domain Place it in the cart Fill in the contact details Verify purchase Once you have made the purchase you should see a public hosted zone created in the Hosted Zones section as well where you can manage the DNS records for it.","title":"Registering a domain"},{"location":"AWS/developer-associate/08-route-53/03-route-53-registering-a-domain/#registering-a-domain","text":"Open Route 53 service in AWS console Select register domain Search for a domain Place it in the cart Fill in the contact details Verify purchase Once you have made the purchase you should see a public hosted zone created in the Hosted Zones section as well where you can manage the DNS records for it.","title":"Registering a domain"},{"location":"AWS/developer-associate/08-route-53/04-creating-our-first-records/","text":"Creating our first records \u00b6 When in the hosted zone we can click on create a new record. We can insert the record name, recort type, value, TTL and routing policy.","title":"Creating our first records"},{"location":"AWS/developer-associate/08-route-53/04-creating-our-first-records/#creating-our-first-records","text":"When in the hosted zone we can click on create a new record. We can insert the record name, recort type, value, TTL and routing policy.","title":"Creating our first records"},{"location":"AWS/developer-associate/08-route-53/05-ec2-setup/","text":"EC2 Setup \u00b6 We are going into EC2 and create 3 EC2 instances around the world with 3 different public IPs. Same as before. Go to the EC2 service, launch instance. Amazon EC2, t2.micro. The user data will be: #!/bin/bash yum update -y yum install -y httpd systemctl start httpd systemctl enable httpd EC2_AVAIL_ZONE=$(curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone) echo \"<h1>Hello World from $(hostname -f) in AZ $EC2_AVAIL_ZONE </h1>\" > /var/www/html/index.html Everything else will be default except for security group. We are going to enable both ssh and http. We can do the same for 2 different regions as well. Once it's done we can create an Application Load Balancer, target the instances.","title":"EC2 Setup"},{"location":"AWS/developer-associate/08-route-53/05-ec2-setup/#ec2-setup","text":"We are going into EC2 and create 3 EC2 instances around the world with 3 different public IPs. Same as before. Go to the EC2 service, launch instance. Amazon EC2, t2.micro. The user data will be: #!/bin/bash yum update -y yum install -y httpd systemctl start httpd systemctl enable httpd EC2_AVAIL_ZONE=$(curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone) echo \"<h1>Hello World from $(hostname -f) in AZ $EC2_AVAIL_ZONE </h1>\" > /var/www/html/index.html Everything else will be default except for security group. We are going to enable both ssh and http. We can do the same for 2 different regions as well. Once it's done we can create an Application Load Balancer, target the instances.","title":"EC2 Setup"},{"location":"AWS/developer-associate/08-route-53/06-route-53-records-ttl/","text":"Route 53 - Records TTL (Time to Live) \u00b6 The DNS clients will cache the result for the dns record, the TTL states for how long will it be cached. High TTL - e.g. 24 hours less traffic on route 53 possibly outdated records Low TTL - e.g. 60 seconds more traffic on route 53 ($$) Records are outdated for less time Easy to change records Except for alias records, TTL is mandatory for each DNS record.","title":"Route 53 - Records TTL (Time to Live)"},{"location":"AWS/developer-associate/08-route-53/06-route-53-records-ttl/#route-53-records-ttl-time-to-live","text":"The DNS clients will cache the result for the dns record, the TTL states for how long will it be cached. High TTL - e.g. 24 hours less traffic on route 53 possibly outdated records Low TTL - e.g. 60 seconds more traffic on route 53 ($$) Records are outdated for less time Easy to change records Except for alias records, TTL is mandatory for each DNS record.","title":"Route 53 - Records TTL (Time to Live)"},{"location":"AWS/developer-associate/08-route-53/07-route-53-cname-vs-alias/","text":"Route 53 - CNAME vs Alias \u00b6 AWS Resources (Load Balancer, CloudFront...) expose an AWS hostname: lb1-1234.us-east-2.elb.amazonaws.com and you want myapp.mydomain.com CNAME: Points to a hostname to any other hostname (app.mydomain.com => test.anything.com) Only for non root domains (aka something.mydomain.com) Alias: Points a hostname to an AWS Resource (app.mydomain.com => test.mydomain.com) Works for root domain and non root domains (aka mydomain.com) Free of charge Native health checks Route 53 - Alias Records \u00b6 Maps a hostname to an AWS Resource An extension to DNS functionality Automatically recognizes changes in the resource's IP address Unlike CNAME, it can be used for the top node of a DNS namespace (Zone Apex), e.g. example.com Alias Record is alwayts of type A/AAAA for AWS resources (IPv4 / IPv6) Cannot set TTL, it's set automatically by route 53 Route 53 - Alias Record Targets \u00b6 Elastic Load Balancers CloudFront Distributions API Gateway Elastic Beanstalk environments S3 Websites VPC Intergace Endpoints Global Accelerator Route 53 record in the same hosted zone You cannot set an ALIAS record for an EC2 DNS name","title":"Route 53 - CNAME vs Alias"},{"location":"AWS/developer-associate/08-route-53/07-route-53-cname-vs-alias/#route-53-cname-vs-alias","text":"AWS Resources (Load Balancer, CloudFront...) expose an AWS hostname: lb1-1234.us-east-2.elb.amazonaws.com and you want myapp.mydomain.com CNAME: Points to a hostname to any other hostname (app.mydomain.com => test.anything.com) Only for non root domains (aka something.mydomain.com) Alias: Points a hostname to an AWS Resource (app.mydomain.com => test.mydomain.com) Works for root domain and non root domains (aka mydomain.com) Free of charge Native health checks","title":"Route 53 - CNAME vs Alias"},{"location":"AWS/developer-associate/08-route-53/07-route-53-cname-vs-alias/#route-53-alias-records","text":"Maps a hostname to an AWS Resource An extension to DNS functionality Automatically recognizes changes in the resource's IP address Unlike CNAME, it can be used for the top node of a DNS namespace (Zone Apex), e.g. example.com Alias Record is alwayts of type A/AAAA for AWS resources (IPv4 / IPv6) Cannot set TTL, it's set automatically by route 53","title":"Route 53 - Alias Records"},{"location":"AWS/developer-associate/08-route-53/07-route-53-cname-vs-alias/#route-53-alias-record-targets","text":"Elastic Load Balancers CloudFront Distributions API Gateway Elastic Beanstalk environments S3 Websites VPC Intergace Endpoints Global Accelerator Route 53 record in the same hosted zone You cannot set an ALIAS record for an EC2 DNS name","title":"Route 53 - Alias Record Targets"},{"location":"AWS/developer-associate/08-route-53/11-route-53-health-checks/","text":"Route 53 - Health Checks \u00b6 HTTP Health Checks are only for public resources Health Check => Automated DNS Failover Health checks that monitor an endpoint (application, server, other AWS resource) Health checks that monitor other health checks (Calculated Health Checks) Health checks that monitor CloudWatch alarms (full control) - e.g. throttles of dynamodb, alarms on RDS, custom metrics (helpful for private resources) Health Checks are integrated with CloudWatch metrics Monitoring an Endpoint \u00b6 About 15 global health checkers will check the endpoint health Health/Unhealthy threshold - 3 (default) Interval - 30 seconds (can set up to 10 seconds - higher cost) Supported protocols: HTTP, HTTPS, TCP If > 18% of health checkers report the endpoint is healthy, route 53 considers it healthy. Otherwise, it's unhealthy. Ability to choose which locations you want route 53 to use. Health Checks pass only when the endpoint responds with 2xx or 3xx status codes Health Checks can be set up to pass / fail basedon the test in the first 5120 bytes of the response Configure your router/firewall to allo incoming requests from route 53 health checkers Calculated Health Checks \u00b6 Combine the results of multiple checks into a single health check You can use OR, AND or NOT Can monitor up to 256 Child Health Checks Specify how many of the health checks need to pass to make the parent class Usage: perform maintenance to your website without causing all health checks to fail Health Check - Private Hosted Zones \u00b6 Route 53 Health checkers are outside the VPC They can't access private endpoints (private VPC or on-premises resources) You can create a CloudWatch metric and associate CloudWatch Alarm, then create a Health Check that checks the alarm itself.","title":"Route 53 - Health Checks"},{"location":"AWS/developer-associate/08-route-53/11-route-53-health-checks/#route-53-health-checks","text":"HTTP Health Checks are only for public resources Health Check => Automated DNS Failover Health checks that monitor an endpoint (application, server, other AWS resource) Health checks that monitor other health checks (Calculated Health Checks) Health checks that monitor CloudWatch alarms (full control) - e.g. throttles of dynamodb, alarms on RDS, custom metrics (helpful for private resources) Health Checks are integrated with CloudWatch metrics","title":"Route 53 - Health Checks"},{"location":"AWS/developer-associate/08-route-53/11-route-53-health-checks/#monitoring-an-endpoint","text":"About 15 global health checkers will check the endpoint health Health/Unhealthy threshold - 3 (default) Interval - 30 seconds (can set up to 10 seconds - higher cost) Supported protocols: HTTP, HTTPS, TCP If > 18% of health checkers report the endpoint is healthy, route 53 considers it healthy. Otherwise, it's unhealthy. Ability to choose which locations you want route 53 to use. Health Checks pass only when the endpoint responds with 2xx or 3xx status codes Health Checks can be set up to pass / fail basedon the test in the first 5120 bytes of the response Configure your router/firewall to allo incoming requests from route 53 health checkers","title":"Monitoring an Endpoint"},{"location":"AWS/developer-associate/08-route-53/11-route-53-health-checks/#calculated-health-checks","text":"Combine the results of multiple checks into a single health check You can use OR, AND or NOT Can monitor up to 256 Child Health Checks Specify how many of the health checks need to pass to make the parent class Usage: perform maintenance to your website without causing all health checks to fail","title":"Calculated Health Checks"},{"location":"AWS/developer-associate/08-route-53/11-route-53-health-checks/#health-check-private-hosted-zones","text":"Route 53 Health checkers are outside the VPC They can't access private endpoints (private VPC or on-premises resources) You can create a CloudWatch metric and associate CloudWatch Alarm, then create a Health Check that checks the alarm itself.","title":"Health Check - Private Hosted Zones"},{"location":"AWS/developer-associate/08-route-53/12-health-checks-hands-on/","text":"Health Checks Hands On \u00b6 We can go to the health checks section of route 53. First we can set a name and type of the health check For the endpoint type we have following options: And some advanced configuration as well. For the calculated health checks: For the cloudwatch alarms The second step will ask us to create alarm that will notify us each time it's unhealthy.","title":"Health Checks Hands On"},{"location":"AWS/developer-associate/08-route-53/12-health-checks-hands-on/#health-checks-hands-on","text":"We can go to the health checks section of route 53. First we can set a name and type of the health check For the endpoint type we have following options: And some advanced configuration as well. For the calculated health checks: For the cloudwatch alarms The second step will ask us to create alarm that will notify us each time it's unhealthy.","title":"Health Checks Hands On"},{"location":"AWS/developer-associate/08-route-53/15-route-53-routing-policy-geoproximity/","text":"Route 53 - Routing Policies - Geoproximity \u00b6 Route traffic to your resources based on geographic location of users and resources Ability to shify more traffic to resources based on the defined bias To change the size of the geografic region, specify bias values: To expand (1 to 99) - more traffic to the resource To shrink (-1 to -99) - less traffic to resource Resources can be: AWS resources (specify AWS region) Non-AWS resources (specify Latitude and Longtitude) You must use Route 53 Traffic Flow (advanced) to use this feature","title":"Route 53 - Routing Policies - Geoproximity"},{"location":"AWS/developer-associate/08-route-53/15-route-53-routing-policy-geoproximity/#route-53-routing-policies-geoproximity","text":"Route traffic to your resources based on geographic location of users and resources Ability to shify more traffic to resources based on the defined bias To change the size of the geografic region, specify bias values: To expand (1 to 99) - more traffic to the resource To shrink (-1 to -99) - less traffic to resource Resources can be: AWS resources (specify AWS region) Non-AWS resources (specify Latitude and Longtitude) You must use Route 53 Traffic Flow (advanced) to use this feature","title":"Route 53 - Routing Policies - Geoproximity"},{"location":"AWS/developer-associate/08-route-53/16-route-53-traffic-flow-geoproximity-hands-on/","text":"Route 53 - Traffic Flow \u00b6 Simplify the process of creating and maintaining records in large and complex configurations Visual editor to manage complex routing decision trees Configurations can be saved as Traffic Flow Policy Can be applied to different route 53 hosted zones (different domain names) Supports versioning When changing biases:","title":"Route 53 - Traffic Flow"},{"location":"AWS/developer-associate/08-route-53/16-route-53-traffic-flow-geoproximity-hands-on/#route-53-traffic-flow","text":"Simplify the process of creating and maintaining records in large and complex configurations Visual editor to manage complex routing decision trees Configurations can be saved as Traffic Flow Policy Can be applied to different route 53 hosted zones (different domain names) Supports versioning When changing biases:","title":"Route 53 - Traffic Flow"},{"location":"AWS/developer-associate/08-route-53/18-route-53-3rd-party-domains/","text":"Route 53 - 3rd Party Domains \u00b6 You buy or register your domain name iwth a Domain Registrar typically by paying annual charges (e.g. GoDaddy, Amazon Registrar Inc, ...) The Domain Registrar usually provides you with a DNS service to manage your DNS records Buy you can use another DNS service to manage your DNS records Example: purchase the domain from GoDaddy and use Route 53 to manage your DNS records If you buy your domain on a 3rd Party Registrar, you can still use Route 53 as the DNS Service Provider Create a Hosted Zone in Route 53 Update NS Records on 3rd Party website to use Route 53 Name Servers Domain Registrar != DNS Service But every Domain Registrar usually comes with some DNS features","title":"Route 53 - 3rd Party Domains"},{"location":"AWS/developer-associate/08-route-53/18-route-53-3rd-party-domains/#route-53-3rd-party-domains","text":"You buy or register your domain name iwth a Domain Registrar typically by paying annual charges (e.g. GoDaddy, Amazon Registrar Inc, ...) The Domain Registrar usually provides you with a DNS service to manage your DNS records Buy you can use another DNS service to manage your DNS records Example: purchase the domain from GoDaddy and use Route 53 to manage your DNS records If you buy your domain on a 3rd Party Registrar, you can still use Route 53 as the DNS Service Provider Create a Hosted Zone in Route 53 Update NS Records on 3rd Party website to use Route 53 Name Servers Domain Registrar != DNS Service But every Domain Registrar usually comes with some DNS features","title":"Route 53 - 3rd Party Domains"},{"location":"AWS/developer-associate/09-vpc-fundamentals/01-section-introduction/","text":"VPC Fundamentals - Section Introduction \u00b6 VPC is something you should know in depth for the [[AWS Certified Solutions Architect Associate]] & [[AWS Certified SysOps Administrator]] At the [[AWS Certified Developer]] level, you should know about: VPC , Subnet s, Internet Gateway s & NAT Gateway Security Group s, Network ACL (NACL), VPC Flow Logs VPC Peering , VPC Endpoint s Site to Site VPN & Direct Connect","title":"VPC Fundamentals - Section Introduction"},{"location":"AWS/developer-associate/09-vpc-fundamentals/01-section-introduction/#vpc-fundamentals-section-introduction","text":"VPC is something you should know in depth for the [[AWS Certified Solutions Architect Associate]] & [[AWS Certified SysOps Administrator]] At the [[AWS Certified Developer]] level, you should know about: VPC , Subnet s, Internet Gateway s & NAT Gateway Security Group s, Network ACL (NACL), VPC Flow Logs VPC Peering , VPC Endpoint s Site to Site VPN & Direct Connect","title":"VPC Fundamentals - Section Introduction"},{"location":"AWS/developer-associate/09-vpc-fundamentals/02-vpc-subnets-igw-nat/","text":"VPC, Subnets, IGW and NAT \u00b6 VPC and Subnets Primer \u00b6 VPC: private network to deploy your resources (regional resource) Subnets allow you to partition your network inside your VPC (Availability Zone Resource) A public subnet is a subnet that is accessible from the internet A private subnet is a subnet that is not accessible from the internet To define access to the internet and between subnets, we use Route Tables. Internet Gateway and NAT Gateways \u00b6 Internet Gateways help our VPC instances connect with the internet Public subnets have a route to the internet gateway NAT Gateways (AWS Managed) & NAT Instances (self-managed) allow your instances in your private subnets to access the internet while remaining private.","title":"VPC, Subnets, IGW and NAT"},{"location":"AWS/developer-associate/09-vpc-fundamentals/02-vpc-subnets-igw-nat/#vpc-subnets-igw-and-nat","text":"","title":"VPC, Subnets, IGW and NAT"},{"location":"AWS/developer-associate/09-vpc-fundamentals/02-vpc-subnets-igw-nat/#vpc-and-subnets-primer","text":"VPC: private network to deploy your resources (regional resource) Subnets allow you to partition your network inside your VPC (Availability Zone Resource) A public subnet is a subnet that is accessible from the internet A private subnet is a subnet that is not accessible from the internet To define access to the internet and between subnets, we use Route Tables.","title":"VPC and Subnets Primer"},{"location":"AWS/developer-associate/09-vpc-fundamentals/02-vpc-subnets-igw-nat/#internet-gateway-and-nat-gateways","text":"Internet Gateways help our VPC instances connect with the internet Public subnets have a route to the internet gateway NAT Gateways (AWS Managed) & NAT Instances (self-managed) allow your instances in your private subnets to access the internet while remaining private.","title":"Internet Gateway and NAT Gateways"},{"location":"AWS/developer-associate/09-vpc-fundamentals/03-nacl-sg-vpc-flow-logs/","text":"Network ACL, Security Groups \u00b6 Network ACL \u00b6 A firewall which controls traffic from and to subnets Can have ALLOW and DENY rules Are attached at the Subnet level Rules only include IP addresses Security Groups \u00b6 A firewall that controls traffic to and from an ENI / an EC2 instance Can have only ALLOW rules Rules include IP addresses and other secutity groups Security Group Network ACL Operates at the instance level Operates at the subnet level Supports allow rules only Supports allow and deny rules Is stateful: return traffic is automatically allowed, regardless of any rules Is stateless: return traffic must be explicitly allowed by rules We evaluate all rules before deciding whether to allow traffic We process rules in number order when deciding whether to allow traffic Applies to an instance only if someone specifies the security group when launching the instance, or associates the security group with the instance later on Automatically applies to all instances in the subnets it's associated with (therefore, you don't have to rely on users to specify the security groups) https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Security.html#VPC_Security_Comparison VPC Flow Logs \u00b6 Captures information about IP traffic going into your interfaces VPC Flow Logs Subnet Flow Logs Elastic Network Interface Flow Logs Helps to monitor & troubleshoot connectivity issues: Subnets to internet Subnets to subnets Internet to subnets Captures network information from AWS managed interfaces too: Elastic Load Balancers, ElastiCache, RDS, Aurora, etc. VPC Flow logs data can go to S3 / CloudWatch Logs","title":"Network ACL, Security Groups"},{"location":"AWS/developer-associate/09-vpc-fundamentals/03-nacl-sg-vpc-flow-logs/#network-acl-security-groups","text":"","title":"Network ACL, Security Groups"},{"location":"AWS/developer-associate/09-vpc-fundamentals/03-nacl-sg-vpc-flow-logs/#network-acl","text":"A firewall which controls traffic from and to subnets Can have ALLOW and DENY rules Are attached at the Subnet level Rules only include IP addresses","title":"Network ACL"},{"location":"AWS/developer-associate/09-vpc-fundamentals/03-nacl-sg-vpc-flow-logs/#security-groups","text":"A firewall that controls traffic to and from an ENI / an EC2 instance Can have only ALLOW rules Rules include IP addresses and other secutity groups Security Group Network ACL Operates at the instance level Operates at the subnet level Supports allow rules only Supports allow and deny rules Is stateful: return traffic is automatically allowed, regardless of any rules Is stateless: return traffic must be explicitly allowed by rules We evaluate all rules before deciding whether to allow traffic We process rules in number order when deciding whether to allow traffic Applies to an instance only if someone specifies the security group when launching the instance, or associates the security group with the instance later on Automatically applies to all instances in the subnets it's associated with (therefore, you don't have to rely on users to specify the security groups) https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Security.html#VPC_Security_Comparison","title":"Security Groups"},{"location":"AWS/developer-associate/09-vpc-fundamentals/03-nacl-sg-vpc-flow-logs/#vpc-flow-logs","text":"Captures information about IP traffic going into your interfaces VPC Flow Logs Subnet Flow Logs Elastic Network Interface Flow Logs Helps to monitor & troubleshoot connectivity issues: Subnets to internet Subnets to subnets Internet to subnets Captures network information from AWS managed interfaces too: Elastic Load Balancers, ElastiCache, RDS, Aurora, etc. VPC Flow logs data can go to S3 / CloudWatch Logs","title":"VPC Flow Logs"},{"location":"AWS/developer-associate/09-vpc-fundamentals/04-vpc-perring-endpoints-vpn-dx/","text":"VPC Peering \u00b6 Connect two VPC, privately using AWS network Make them behave as if they were in the same network Must not have overlapping CIDR (IP address ranges) VPC Peering connection is not transitive (must be established for each VPC that need to communicate with one another) VPC Endpoints \u00b6 Endpoints allow you to connect to AWS Services using a private network instead of the public www network This gives you enhanced security and lower latency to access AWS Services VPC endpoint gateway: S3 & DynamoDB VPC endpoint interface: the rest only used within your VPC Site to Site VPN \u00b6 Connect to an on-premises VPN to AWS The connection is automatically encrypted Goes over the public internet Direct Connect (DX) \u00b6 Establish a physical connection between on-premises and AWS The connection is private, secure and fast Goes over a private network Takes at least a month to establish Note: Site-to-Site VPN and Direct Connect cannot access VPC endpoints.","title":"VPC Peering"},{"location":"AWS/developer-associate/09-vpc-fundamentals/04-vpc-perring-endpoints-vpn-dx/#vpc-peering","text":"Connect two VPC, privately using AWS network Make them behave as if they were in the same network Must not have overlapping CIDR (IP address ranges) VPC Peering connection is not transitive (must be established for each VPC that need to communicate with one another)","title":"VPC Peering"},{"location":"AWS/developer-associate/09-vpc-fundamentals/04-vpc-perring-endpoints-vpn-dx/#vpc-endpoints","text":"Endpoints allow you to connect to AWS Services using a private network instead of the public www network This gives you enhanced security and lower latency to access AWS Services VPC endpoint gateway: S3 & DynamoDB VPC endpoint interface: the rest only used within your VPC","title":"VPC Endpoints"},{"location":"AWS/developer-associate/09-vpc-fundamentals/04-vpc-perring-endpoints-vpn-dx/#site-to-site-vpn","text":"Connect to an on-premises VPN to AWS The connection is automatically encrypted Goes over the public internet","title":"Site to Site VPN"},{"location":"AWS/developer-associate/09-vpc-fundamentals/04-vpc-perring-endpoints-vpn-dx/#direct-connect-dx","text":"Establish a physical connection between on-premises and AWS The connection is private, secure and fast Goes over a private network Takes at least a month to establish Note: Site-to-Site VPN and Direct Connect cannot access VPC endpoints.","title":"Direct Connect (DX)"},{"location":"AWS/developer-associate/09-vpc-fundamentals/05-vpc-cheatsheet/","text":"VPC CheatSheet \u00b6 VPC: Virtual Private Cloud Subnets: Tied to an AZ, network parition of the VPC Internet Gateway / Instances: Give you internet access to private subnets NAT Gateway / Instances: give internet access to private subnets Network ACL : Sateless, subnet rules for inbount and outbound Security Groups: Stateful, operate at the EC2 instance level or ENI VPC Peering: Connect two VPC with non overlapping IP ranges, non transitive VPC Endpoints: Provide private access to AWS Services within VPC VPC Flow Logs: network traffic logs Site to Site VPN: VPN over public internet between on-premises DC and AWS Direct Connect: direct privat connection to AWS","title":"VPC CheatSheet"},{"location":"AWS/developer-associate/09-vpc-fundamentals/05-vpc-cheatsheet/#vpc-cheatsheet","text":"VPC: Virtual Private Cloud Subnets: Tied to an AZ, network parition of the VPC Internet Gateway / Instances: Give you internet access to private subnets NAT Gateway / Instances: give internet access to private subnets Network ACL : Sateless, subnet rules for inbount and outbound Security Groups: Stateful, operate at the EC2 instance level or ENI VPC Peering: Connect two VPC with non overlapping IP ranges, non transitive VPC Endpoints: Provide private access to AWS Services within VPC VPC Flow Logs: network traffic logs Site to Site VPN: VPN over public internet between on-premises DC and AWS Direct Connect: direct privat connection to AWS","title":"VPC CheatSheet"},{"location":"AWS/developer-associate/09-vpc-fundamentals/06-three-tier-architecture/","text":"Three Tier Architecture \u00b6 LAMP Stack on EC2 \u00b6 Linux: OS for EC2 Instances Apache: Web Server that runs on Linux (EC2) MySQL: database on RDS PHP: Application Logic (running on EC2) Can add Redis / Memcached (ElastiCache) to include caching To store local application data & software: EBS drive (root) Wordpress on AWS \u00b6 A more complicated wordpress setup \u00b6 https://aws.amazon.com/blogs/architecture/wordpress-best-practices-on-aws/","title":"Three Tier Architecture"},{"location":"AWS/developer-associate/09-vpc-fundamentals/06-three-tier-architecture/#three-tier-architecture","text":"","title":"Three Tier Architecture"},{"location":"AWS/developer-associate/09-vpc-fundamentals/06-three-tier-architecture/#lamp-stack-on-ec2","text":"Linux: OS for EC2 Instances Apache: Web Server that runs on Linux (EC2) MySQL: database on RDS PHP: Application Logic (running on EC2) Can add Redis / Memcached (ElastiCache) to include caching To store local application data & software: EBS drive (root)","title":"LAMP Stack on EC2"},{"location":"AWS/developer-associate/09-vpc-fundamentals/06-three-tier-architecture/#wordpress-on-aws","text":"","title":"Wordpress on AWS"},{"location":"AWS/developer-associate/09-vpc-fundamentals/06-three-tier-architecture/#a-more-complicated-wordpress-setup","text":"https://aws.amazon.com/blogs/architecture/wordpress-best-practices-on-aws/","title":"A more complicated wordpress setup"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/01-section-introduction/","text":"Section Introduction \u00b6 This is a very important section as it is very popular on certification exams. Amazon S3 is one of the main building blocks of AWS It's advertised as \"infinitely scaling\" storage It's widely popular and deserves its own section Many websites use Amazon S3 as a backbone Many AWS services use Amazon S3 as an integration as well","title":"Section Introduction"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/01-section-introduction/#section-introduction","text":"This is a very important section as it is very popular on certification exams. Amazon S3 is one of the main building blocks of AWS It's advertised as \"infinitely scaling\" storage It's widely popular and deserves its own section Many websites use Amazon S3 as a backbone Many AWS services use Amazon S3 as an integration as well","title":"Section Introduction"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/02-s3-buckets-and-objects/","text":"S3 Buckets and Objects \u00b6 Buckets \u00b6 Amazon S3 allows people to store objects (files) in buckets (directories) Buckets must have a globally unique name Buckets are defined at the region level Naming convention no uppercase no underscore 3-63 characters long not an IP must start with lowercase letter or number Objects \u00b6 Objects (files) have a key The key is full path: s3://my-bucket/my_file.txt s3://my-bucket/my_folder1/another_folder/my_name.txt The key is composed of prefix + object name prefix: my_folder1/another_folder name: my_name.txt There's no concept of \"directories\" within buckets. Although the UI will trick you to think otherwise. Just keys with very long names that contain slashes. Object values are content of the body: Max object size is 5 TB If uploading more than 5 GB, must use multi-part upload Metadata (list of text key / value pairs - system or user metadata) Tags (unicode key / value pair - up to 10) - useful for security / lifecycle Version ID (if versioning is enabled)","title":"S3 Buckets and Objects"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/02-s3-buckets-and-objects/#s3-buckets-and-objects","text":"","title":"S3 Buckets and Objects"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/02-s3-buckets-and-objects/#buckets","text":"Amazon S3 allows people to store objects (files) in buckets (directories) Buckets must have a globally unique name Buckets are defined at the region level Naming convention no uppercase no underscore 3-63 characters long not an IP must start with lowercase letter or number","title":"Buckets"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/02-s3-buckets-and-objects/#objects","text":"Objects (files) have a key The key is full path: s3://my-bucket/my_file.txt s3://my-bucket/my_folder1/another_folder/my_name.txt The key is composed of prefix + object name prefix: my_folder1/another_folder name: my_name.txt There's no concept of \"directories\" within buckets. Although the UI will trick you to think otherwise. Just keys with very long names that contain slashes. Object values are content of the body: Max object size is 5 TB If uploading more than 5 GB, must use multi-part upload Metadata (list of text key / value pairs - system or user metadata) Tags (unicode key / value pair - up to 10) - useful for security / lifecycle Version ID (if versioning is enabled)","title":"Objects"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/03-s3-buckets-and-objects-hands-on/","text":"S3 Buckets and Objects hands on \u00b6 We can open up the S3 in AWS Console. Here we can view and create our first bucket. Next, we have to give the bucket name. The name has to be unique across all the accounts in AWS. Then, you have to choose a region (the console is global, but the buckets are not). Then there's an option to define the ownership: We can choose the block public access settings Also, we can set the versioning, tags, encryption and object locks. Upon finishing the process, we'll see the bucket. When we open up it up, we can upload our first object. We click on upload and then add files. Upon uploading we can also see the Destination and other settings as well. When we click on upload, it should start uploading and display it's status as succeeded when finished. It will show up when viewing the bucket as well. When clicking on it - we can see the details of it. To open up the object we have 2 option on how to do that. - Click on open when viewing it Use the public object URL This by default give an error because the bucket is not public. The URLs for both look very similar, but the first one is a pre-signed URL. We can also create folders: And delete the objects:","title":"S3 Buckets and Objects hands on"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/03-s3-buckets-and-objects-hands-on/#s3-buckets-and-objects-hands-on","text":"We can open up the S3 in AWS Console. Here we can view and create our first bucket. Next, we have to give the bucket name. The name has to be unique across all the accounts in AWS. Then, you have to choose a region (the console is global, but the buckets are not). Then there's an option to define the ownership: We can choose the block public access settings Also, we can set the versioning, tags, encryption and object locks. Upon finishing the process, we'll see the bucket. When we open up it up, we can upload our first object. We click on upload and then add files. Upon uploading we can also see the Destination and other settings as well. When we click on upload, it should start uploading and display it's status as succeeded when finished. It will show up when viewing the bucket as well. When clicking on it - we can see the details of it. To open up the object we have 2 option on how to do that. - Click on open when viewing it Use the public object URL This by default give an error because the bucket is not public. The URLs for both look very similar, but the first one is a pre-signed URL. We can also create folders: And delete the objects:","title":"S3 Buckets and Objects hands on"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/04-s3-versioning/","text":"Amazon S3 - Versioning \u00b6 You can version your files in Amazon S3 It's enabled at the bucket level Same key overwrite will increment version by 1 It is best practice to version your buckets Protect against unintended deletes (ability to restore a version) Easy roll back to previous version Notes: Any file that is not versioned prior to enabling versioning will have version null Suspending versioning does not delete the previous versions","title":"Amazon S3 - Versioning"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/04-s3-versioning/#amazon-s3-versioning","text":"You can version your files in Amazon S3 It's enabled at the bucket level Same key overwrite will increment version by 1 It is best practice to version your buckets Protect against unintended deletes (ability to restore a version) Easy roll back to previous version Notes: Any file that is not versioned prior to enabling versioning will have version null Suspending versioning does not delete the previous versions","title":"Amazon S3 - Versioning"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/05-aws-versioning-hands-on/","text":"AWS S3 Versioning \u00b6 We can set up versioning when creating a bucket, but we can also do that for the existing ones. We can go to the Properties tab and the very first option will be to enable the versioning. Now in our object list there will be an option to show versions. We can see that when the object is first created, it's version is null. If we upload a file, we'll see that it will assign version ID to it. If we upload the same file again, we'll see that the second file now has 2 version IDs. Now if we toggle off the version information and delete the file: Now we can see that there is a delete marker added that tells AWS that the file was deleted. If we delete the delete marker. (this is a permanent delete) Now we can see that we have rolled back the file to a previous version: If you are to suspend the versioning, all the previously created versions will persist but all the new files will have version number of null.","title":"AWS S3 Versioning"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/05-aws-versioning-hands-on/#aws-s3-versioning","text":"We can set up versioning when creating a bucket, but we can also do that for the existing ones. We can go to the Properties tab and the very first option will be to enable the versioning. Now in our object list there will be an option to show versions. We can see that when the object is first created, it's version is null. If we upload a file, we'll see that it will assign version ID to it. If we upload the same file again, we'll see that the second file now has 2 version IDs. Now if we toggle off the version information and delete the file: Now we can see that there is a delete marker added that tells AWS that the file was deleted. If we delete the delete marker. (this is a permanent delete) Now we can see that we have rolled back the file to a previous version: If you are to suspend the versioning, all the previously created versions will persist but all the new files will have version number of null.","title":"AWS S3 Versioning"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/06-s3-encryption/","text":"Amazon S3 - Encryption \u00b6 There are 4 methods of encrypting objects in S3 SSE-S3: encrypts s3 objects using keys handled & managed by AWS SSE-KMS: leverage AWS Key Management Service to manage encryption keys SSE-C: When you want to manage your own encryption keys Client Side Encryption It's important to understand which ones are adapted to which situation for the exam. SSE-S3 \u00b6 Encryption using keys handled & managed by Amazon S3 Object is encrypted server side AES-256 encryption type Muse set header: \"x-amz-server-side-encryption\": \"AES256\" SSE-KMS \u00b6 SSE-KMS encryption using keys handled & managed by KMS KMS advantages: user control + audit trail Object is encrypted server side Must set header: \"x-amz-server-side-encryption\": \"aws:kms\" SSE-C \u00b6 Server side encryption using data keys fully managed by the customer outside of AWS Amazon S3 does not store the encryption key your provide HTTPS must be enabled Encryption key must be provided in HTTP headers, for every HTTP request made Client side encryption \u00b6 Client library such as Amazon S3 Encryption Client Clients must encrypt data themselves before sending to S3 Clients must decrypt data themselves when retrieving from S3 Customer fully manages the keys and encryption cycle Encryption in transit (SSL/TLS) \u00b6 Amazon S3 exposes: HTTP endpoint: non encrypted HTTPS endpoint: encryption in flight You're free to use the endpoint you want, but HTTPS is recomended Most clients would use the HTTPS one by default HTTPS is mandatory for SSE-C Encryption in flight is also called SSL/TLS","title":"Amazon S3 - Encryption"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/06-s3-encryption/#amazon-s3-encryption","text":"There are 4 methods of encrypting objects in S3 SSE-S3: encrypts s3 objects using keys handled & managed by AWS SSE-KMS: leverage AWS Key Management Service to manage encryption keys SSE-C: When you want to manage your own encryption keys Client Side Encryption It's important to understand which ones are adapted to which situation for the exam.","title":"Amazon S3 - Encryption"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/06-s3-encryption/#sse-s3","text":"Encryption using keys handled & managed by Amazon S3 Object is encrypted server side AES-256 encryption type Muse set header: \"x-amz-server-side-encryption\": \"AES256\"","title":"SSE-S3"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/06-s3-encryption/#sse-kms","text":"SSE-KMS encryption using keys handled & managed by KMS KMS advantages: user control + audit trail Object is encrypted server side Must set header: \"x-amz-server-side-encryption\": \"aws:kms\"","title":"SSE-KMS"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/06-s3-encryption/#sse-c","text":"Server side encryption using data keys fully managed by the customer outside of AWS Amazon S3 does not store the encryption key your provide HTTPS must be enabled Encryption key must be provided in HTTP headers, for every HTTP request made","title":"SSE-C"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/06-s3-encryption/#client-side-encryption","text":"Client library such as Amazon S3 Encryption Client Clients must encrypt data themselves before sending to S3 Clients must decrypt data themselves when retrieving from S3 Customer fully manages the keys and encryption cycle","title":"Client side encryption"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/06-s3-encryption/#encryption-in-transit-ssltls","text":"Amazon S3 exposes: HTTP endpoint: non encrypted HTTPS endpoint: encryption in flight You're free to use the endpoint you want, but HTTPS is recomended Most clients would use the HTTPS one by default HTTPS is mandatory for SSE-C Encryption in flight is also called SSL/TLS","title":"Encryption in transit (SSL/TLS)"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/07-s3-encryption-hands-on/","text":"S3 Encryption Hands On \u00b6 In the bucket properties we can see a setting for encryption: We can enable it here, but first we can upload a file and do it there: For the KMS we have multiple options: Once uploaded, we open up the files and see that they are encrypted: So, we can upload files and define the enctyption settings for each of them, but we can go to the Properties and specify the default encryption mechanism for the whole bucket. Now when we simply upload a new file to the S3: Note that the other types of encryption doesn't show up because: - SSE-C needs to be done via the CLI with a provided key - The client side - the client needs to encrypt the file himself before uploading.","title":"S3 Encryption Hands On"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/07-s3-encryption-hands-on/#s3-encryption-hands-on","text":"In the bucket properties we can see a setting for encryption: We can enable it here, but first we can upload a file and do it there: For the KMS we have multiple options: Once uploaded, we open up the files and see that they are encrypted: So, we can upload files and define the enctyption settings for each of them, but we can go to the Properties and specify the default encryption mechanism for the whole bucket. Now when we simply upload a new file to the S3: Note that the other types of encryption doesn't show up because: - SSE-C needs to be done via the CLI with a provided key - The client side - the client needs to encrypt the file himself before uploading.","title":"S3 Encryption Hands On"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/08-s3-security-bucket-policies/","text":"S3 Security & Bucket Policies \u00b6 S3 Security \u00b6 User Based IAM Roles - which API calls should be allowed for a specific user from IAM console Resource Based Bucket Policies - bucket wide rules from the S3 console - allows cross account Object Access Control List (ACL) - finer grain Bucket Access Control List (ACL) - less common Note: an IAM principal can access and S3 object if the user IAM permission allow it OR the resource policy ALLOWS it AND there's no explicit DENY S3 Bucket Policies \u00b6 Json based policies Resources: buckets and objects Actions: Set of API to Allow or Deny Effect: Allow / Deny Principal: The account or user to apply the policy to { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"PublicRead\", \"Effect\": \"Allow\", \"Principal\": \"*\", \"Action\": [ \"s3:GetObject\" ], \"Resource\": [ \"arn:aws:s3:::examplebucket/*\" ] } ] } Use S3 bucket for policy to: Grant public access to the bucket Force objects to be encrypted at upload Grant access to another account (cross account) Bucket settings for Block Public Access \u00b6 Block public access to buckets and objects granted through new access control lists (ACLs) any access control lists (ACLs) new public bucket or access point policies Block public and cross-account access to buckets and objects through any public bucket or access point policies These settings were created to prevent company data leaks If you know your bucket should never be public, leave these on Can be set at the account level S3 Security - Other \u00b6 Networking: Supports VPC Endpoints (for instances in VPC without internet) Logging and Audit: S3 Access Logs can be stored in other S3 buckets API calls can be logged in AWS CloudTrail User Security: MFA (multi-factor auth) delete can be required in versioned buckets to delete objects Pre-signed urls: URLs that are valid only for a limited time","title":"S3 Security & Bucket Policies"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/08-s3-security-bucket-policies/#s3-security-bucket-policies","text":"","title":"S3 Security &amp; Bucket Policies"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/08-s3-security-bucket-policies/#s3-security","text":"User Based IAM Roles - which API calls should be allowed for a specific user from IAM console Resource Based Bucket Policies - bucket wide rules from the S3 console - allows cross account Object Access Control List (ACL) - finer grain Bucket Access Control List (ACL) - less common Note: an IAM principal can access and S3 object if the user IAM permission allow it OR the resource policy ALLOWS it AND there's no explicit DENY","title":"S3 Security"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/08-s3-security-bucket-policies/#s3-bucket-policies","text":"Json based policies Resources: buckets and objects Actions: Set of API to Allow or Deny Effect: Allow / Deny Principal: The account or user to apply the policy to { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"PublicRead\", \"Effect\": \"Allow\", \"Principal\": \"*\", \"Action\": [ \"s3:GetObject\" ], \"Resource\": [ \"arn:aws:s3:::examplebucket/*\" ] } ] } Use S3 bucket for policy to: Grant public access to the bucket Force objects to be encrypted at upload Grant access to another account (cross account)","title":"S3 Bucket Policies"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/08-s3-security-bucket-policies/#bucket-settings-for-block-public-access","text":"Block public access to buckets and objects granted through new access control lists (ACLs) any access control lists (ACLs) new public bucket or access point policies Block public and cross-account access to buckets and objects through any public bucket or access point policies These settings were created to prevent company data leaks If you know your bucket should never be public, leave these on Can be set at the account level","title":"Bucket settings for Block Public Access"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/08-s3-security-bucket-policies/#s3-security-other","text":"Networking: Supports VPC Endpoints (for instances in VPC without internet) Logging and Audit: S3 Access Logs can be stored in other S3 buckets API calls can be logged in AWS CloudTrail User Security: MFA (multi-factor auth) delete can be required in versioned buckets to delete objects Pre-signed urls: URLs that are valid only for a limited time","title":"S3 Security - Other"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/09-bucket-policies-hands-on/","text":"S3 Bucket Policies Hands On \u00b6 We can open up the Permissions tab in the S3 bucket and see the bucket policy settings. We can create a policy that will prevent us from uploading files that are not encrypted. We can see some policy examples here: https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html We can also use the policy generator: https://awspolicygen.s3.amazonaws.com/policygen.html { \"Id\": \"Policy1644470115524\", \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Stmt1644470018424\", \"Action\": [ \"s3:PutObject\" ], \"Effect\": \"Deny\", \"Resource\": \"arn:aws:s3:::demo-dave-s3-bucket/*\", \"Principal\": \"*\" }, { \"Sid\": \"Stmt1644470113745\", \"Action\": [ \"s3:PutObject\" ], \"Effect\": \"Deny\", \"Resource\": \"arn:aws:s3:::demo-dave-s3-bucket/*\", \"Principal\": \"*\" } ] } Now we can try to upload an unencrypted file: If we upload the same file, but specify the SSE-S3 encryption: In the permissions tab we can see the block public access settings as well: If we want to set it up account wide, we can go to Block Public Access settings for this account in the sidebar. For all my objects we have something called ACL Here we can define who can access (read/write) the object","title":"S3 Bucket Policies Hands On"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/09-bucket-policies-hands-on/#s3-bucket-policies-hands-on","text":"We can open up the Permissions tab in the S3 bucket and see the bucket policy settings. We can create a policy that will prevent us from uploading files that are not encrypted. We can see some policy examples here: https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html We can also use the policy generator: https://awspolicygen.s3.amazonaws.com/policygen.html { \"Id\": \"Policy1644470115524\", \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Stmt1644470018424\", \"Action\": [ \"s3:PutObject\" ], \"Effect\": \"Deny\", \"Resource\": \"arn:aws:s3:::demo-dave-s3-bucket/*\", \"Principal\": \"*\" }, { \"Sid\": \"Stmt1644470113745\", \"Action\": [ \"s3:PutObject\" ], \"Effect\": \"Deny\", \"Resource\": \"arn:aws:s3:::demo-dave-s3-bucket/*\", \"Principal\": \"*\" } ] } Now we can try to upload an unencrypted file: If we upload the same file, but specify the SSE-S3 encryption: In the permissions tab we can see the block public access settings as well: If we want to set it up account wide, we can go to Block Public Access settings for this account in the sidebar. For all my objects we have something called ACL Here we can define who can access (read/write) the object","title":"S3 Bucket Policies Hands On"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/10-s3-websites/","text":"S3 Websites \u00b6 S3 can host static websites and have them accessible on the web The website URL will be: .s3-website- .amazonaws.com OR .s3-website. .amazonaws.com If you get a 403 (Forbidden) error, make sure the bucket policy allows public reads! We can upload a new index.html file with the following content: <html> <head> <title>My First Webpage</title> </head> <body> <h1>Hello World!</h1> </body> </html> And an error.html <h1>Oh oh, there was an error!</h1> Now we can go to the properties and enable static website hosting: In the properties we will be able to view the endpoint now: If we open it up: This is due to the fact that the bucket is not public: { \"Id\": \"Policy1644471095712\", \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Stmt1644471094609\", \"Action\": [ \"s3:GetObject\" ], \"Effect\": \"Allow\", \"Resource\": \"arn:aws:s3:::demo-dave-s3-bucket/*\", \"Principal\": \"*\" } ] } Now, if we refresh the page: If we open up a path that doesn't exist:","title":"S3 Websites"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/10-s3-websites/#s3-websites","text":"S3 can host static websites and have them accessible on the web The website URL will be: .s3-website- .amazonaws.com OR .s3-website. .amazonaws.com If you get a 403 (Forbidden) error, make sure the bucket policy allows public reads! We can upload a new index.html file with the following content: <html> <head> <title>My First Webpage</title> </head> <body> <h1>Hello World!</h1> </body> </html> And an error.html <h1>Oh oh, there was an error!</h1> Now we can go to the properties and enable static website hosting: In the properties we will be able to view the endpoint now: If we open it up: This is due to the fact that the bucket is not public: { \"Id\": \"Policy1644471095712\", \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Stmt1644471094609\", \"Action\": [ \"s3:GetObject\" ], \"Effect\": \"Allow\", \"Resource\": \"arn:aws:s3:::demo-dave-s3-bucket/*\", \"Principal\": \"*\" } ] } Now, if we refresh the page: If we open up a path that doesn't exist:","title":"S3 Websites"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/11-s3-cors/","text":"S3 CORS \u00b6 CORS Explained \u00b6 An origin is a scheme (protocol), host (domain) and port e.g. https://example.com (implied port is 443 for HTTPS, 80 for HTTP) CORS means Cross-Origin Resource Sharing Web browser based mechanism to allow requests to other origins while visiting the main origin Same origin: http://example.com/app & http://example.com/app2 Different origins: http://example.com & http://other.example.com The requests won't be fullfilled unless the other origin allows for the requests, using CORS Headers (ex: Access-Control-Allow-Origin) S3 CORS \u00b6 If a client does a cross-origin request on our S3 bucket, we need to enable the correct CORS headers It's a popular exam question You can allow for a specific origin or * (all origins)","title":"S3 CORS"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/11-s3-cors/#s3-cors","text":"","title":"S3 CORS"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/11-s3-cors/#cors-explained","text":"An origin is a scheme (protocol), host (domain) and port e.g. https://example.com (implied port is 443 for HTTPS, 80 for HTTP) CORS means Cross-Origin Resource Sharing Web browser based mechanism to allow requests to other origins while visiting the main origin Same origin: http://example.com/app & http://example.com/app2 Different origins: http://example.com & http://other.example.com The requests won't be fullfilled unless the other origin allows for the requests, using CORS Headers (ex: Access-Control-Allow-Origin)","title":"CORS Explained"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/11-s3-cors/#s3-cors_1","text":"If a client does a cross-origin request on our S3 bucket, we need to enable the correct CORS headers It's a popular exam question You can allow for a specific origin or * (all origins)","title":"S3 CORS"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/12-s3-cors-hands-on/","text":"S3 CORS Hands On \u00b6 We can modify the previously uploaded index.html file: <html> <head> <title>My First Webpage</title> </head> <body> <h1>Hello World!</h1> <div id=\"tofetch\" /> <script> var tofetch = document.getElementById('tofetch'); fetch('extra-page.html') .then((response) => response.text()) .then((html) => tofetch.innerHTML = html) </script> </body> </html> And add an extra-page.html: <p>This <strong>extra page</strong> has been sucessfully loaded!</p> If we view the index, we can see that it works: This works because we have the same origin, but if we create a new bucket now and store the extra-page there: <html> <head> <title>My First Webpage</title> </head> <body> <h1>Hello World!</h1> <div id=\"tofetch\" /> <script> var tofetch = document.getElementById('tofetch'); fetch('https://demo-dave-extra.s3.eu-west-1.amazonaws.com/extra-page.html') .then((response) => response.text()) .then((html) => tofetch.innerHTML = html) </script> </body> </html> We can go to permissions and set the CORS settings for the second bucket: [ { \"AllowedHeaders\": [\"Authorization\"], \"AllowedMethods\": [\"GET\"], \"AllowedOrigins\": [ \"http://demo-dave-s3-bucket.s3-website-eu-west-1.amazonaws.com\" ], \"ExposeHeaders\": [], \"MaxAgeSeconds\": 3000 } ] And now it works:","title":"S3 CORS Hands On"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/12-s3-cors-hands-on/#s3-cors-hands-on","text":"We can modify the previously uploaded index.html file: <html> <head> <title>My First Webpage</title> </head> <body> <h1>Hello World!</h1> <div id=\"tofetch\" /> <script> var tofetch = document.getElementById('tofetch'); fetch('extra-page.html') .then((response) => response.text()) .then((html) => tofetch.innerHTML = html) </script> </body> </html> And add an extra-page.html: <p>This <strong>extra page</strong> has been sucessfully loaded!</p> If we view the index, we can see that it works: This works because we have the same origin, but if we create a new bucket now and store the extra-page there: <html> <head> <title>My First Webpage</title> </head> <body> <h1>Hello World!</h1> <div id=\"tofetch\" /> <script> var tofetch = document.getElementById('tofetch'); fetch('https://demo-dave-extra.s3.eu-west-1.amazonaws.com/extra-page.html') .then((response) => response.text()) .then((html) => tofetch.innerHTML = html) </script> </body> </html> We can go to permissions and set the CORS settings for the second bucket: [ { \"AllowedHeaders\": [\"Authorization\"], \"AllowedMethods\": [\"GET\"], \"AllowedOrigins\": [ \"http://demo-dave-s3-bucket.s3-website-eu-west-1.amazonaws.com\" ], \"ExposeHeaders\": [], \"MaxAgeSeconds\": 3000 } ] And now it works:","title":"S3 CORS Hands On"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/13-s3-consistency-model/","text":"Amazon S3 - Consistency Model \u00b6 Strong Consistency as of December 2020: After a: Successful write of a new object (new PUT) or an overwrite or delete of an existing object (overwrite PUT or DELETE) any: subsequent read request immediately receives the latest version of the object (read after write consistency) subsequent request immediately reflects changes (list consistency) Available at no additional costs, without any performance impact","title":"Amazon S3 - Consistency Model"},{"location":"AWS/developer-associate/10-amazon-s3-introduction/13-s3-consistency-model/#amazon-s3-consistency-model","text":"Strong Consistency as of December 2020: After a: Successful write of a new object (new PUT) or an overwrite or delete of an existing object (overwrite PUT or DELETE) any: subsequent read request immediately receives the latest version of the object (read after write consistency) subsequent request immediately reflects changes (list consistency) Available at no additional costs, without any performance impact","title":"Amazon S3 - Consistency Model"},{"location":"AWS/developer-associate/11-aws-cli-sdk-iam-roles-policies/01-iam-roles-and-policies-hands-on/","text":"IAM Roles and Policies Hands On \u00b6 In IAM we can have roles that have policies attached to them. These can be: - AWS Created policies - Inline policies (use discouraged for better management) - Custom created policies Each policy is a JSON Document that specifies what user is allowed and what not. We can create our own policies by going to Policies and Clicking on Create Policy. Here we can use a visual editor or a JSON file. We can create a policy to allow getting objects from a specific bucket. Now we can give it a name Now we can view the policy: Now we can also attach the policy to roles:","title":"IAM Roles and Policies Hands On"},{"location":"AWS/developer-associate/11-aws-cli-sdk-iam-roles-policies/01-iam-roles-and-policies-hands-on/#iam-roles-and-policies-hands-on","text":"In IAM we can have roles that have policies attached to them. These can be: - AWS Created policies - Inline policies (use discouraged for better management) - Custom created policies Each policy is a JSON Document that specifies what user is allowed and what not. We can create our own policies by going to Policies and Clicking on Create Policy. Here we can use a visual editor or a JSON file. We can create a policy to allow getting objects from a specific bucket. Now we can give it a name Now we can view the policy: Now we can also attach the policy to roles:","title":"IAM Roles and Policies Hands On"},{"location":"AWS/developer-associate/11-aws-cli-sdk-iam-roles-policies/02-aws-policy-simulator/","text":"AWS Policy Simulator \u00b6 We can use a tool called IAM Policy Simulator to test policies. It will allow you to select policies and test them against API calls. The other option how to test this is to use CLI.","title":"AWS Policy Simulator"},{"location":"AWS/developer-associate/11-aws-cli-sdk-iam-roles-policies/02-aws-policy-simulator/#aws-policy-simulator","text":"We can use a tool called IAM Policy Simulator to test policies. It will allow you to select policies and test them against API calls. The other option how to test this is to use CLI.","title":"AWS Policy Simulator"},{"location":"AWS/developer-associate/11-aws-cli-sdk-iam-roles-policies/03-aws-cli-dry-run/","text":"AWS CLI Dry Runs \u00b6 Sometimes, we'd just like to make sure we have the permissions but not actually run the commands Some AWS CLI commands (such as EC2) can become expensive if they succeed, say if we wanted to try to create an EC2 Instance Some AWS CLI commands (not all) contain a --dry-run option to simulate API calls $ aws ec2 run-instances --dry-run --image-id ami-06340c8c12baa6a09 --instance-type t2.micro An error occured (UnauthorizedOperation) when calling the RunInstances operation: You are not authorized to perform this operation. Encoded authorization failure message: ... When we set the RunInstances permissions: $ aws ec2 run-instances --dry-run --image-id ami-06340c8c12baa6a09 --instance-type t2.micro An error occurred (DryRunOperation) when calling the RunInstances operation: Request would have succeeded, but DryRun flag is set.","title":"AWS CLI Dry Runs"},{"location":"AWS/developer-associate/11-aws-cli-sdk-iam-roles-policies/03-aws-cli-dry-run/#aws-cli-dry-runs","text":"Sometimes, we'd just like to make sure we have the permissions but not actually run the commands Some AWS CLI commands (such as EC2) can become expensive if they succeed, say if we wanted to try to create an EC2 Instance Some AWS CLI commands (not all) contain a --dry-run option to simulate API calls $ aws ec2 run-instances --dry-run --image-id ami-06340c8c12baa6a09 --instance-type t2.micro An error occured (UnauthorizedOperation) when calling the RunInstances operation: You are not authorized to perform this operation. Encoded authorization failure message: ... When we set the RunInstances permissions: $ aws ec2 run-instances --dry-run --image-id ami-06340c8c12baa6a09 --instance-type t2.micro An error occurred (DryRunOperation) when calling the RunInstances operation: Request would have succeeded, but DryRun flag is set.","title":"AWS CLI Dry Runs"},{"location":"AWS/developer-associate/11-aws-cli-sdk-iam-roles-policies/04-aws-cli-sts-decode-errors/","text":"AWS CLI STS Decode Errors \u00b6 When you run API calls and they fail, you can get a long error message This error message can be decoded using the STS command line sts decode-authorization-message $ aws sts decode-authorization-message --encoded-message ... An error orcurred (AccessDenied) when calling the DecodeAuthorizationMessage operation: User: arn:aws:sts:123:assumed-role/MyFirstEC2Role/i-05adcce6933809eda is not authorized to perform: sts:DecodeAuthorizationMessage If this happens, we need to add the STS DecodeAuthorizationMessage policy to the role. { \"DecodedMessage\": \"...\" }","title":"AWS CLI STS Decode Errors"},{"location":"AWS/developer-associate/11-aws-cli-sdk-iam-roles-policies/04-aws-cli-sts-decode-errors/#aws-cli-sts-decode-errors","text":"When you run API calls and they fail, you can get a long error message This error message can be decoded using the STS command line sts decode-authorization-message $ aws sts decode-authorization-message --encoded-message ... An error orcurred (AccessDenied) when calling the DecodeAuthorizationMessage operation: User: arn:aws:sts:123:assumed-role/MyFirstEC2Role/i-05adcce6933809eda is not authorized to perform: sts:DecodeAuthorizationMessage If this happens, we need to add the STS DecodeAuthorizationMessage policy to the role. { \"DecodedMessage\": \"...\" }","title":"AWS CLI STS Decode Errors"},{"location":"AWS/developer-associate/11-aws-cli-sdk-iam-roles-policies/05-aws-ec2-instance-metadata/","text":"AWS EC2 Instance Metadata \u00b6 AWS EC2 Instance Metadata is a powerful but on of the least known features to developers It allows AWS EC2 instances to learn about themselves without using an IAM Role for that purpose. The url is http://169.254.169.254/latest/meta-data You can retrieve the IAM Role name from the metadata, but you CANNOT retrieve the IAM Policy Metadata - info about the EC2 instance Userdata - launch script of the EC2 instance $ curl http://169.254.169.254/latest/meta-data/instance-id i-05adcce6933809eda $ curl http://169.254.169.254/latest/meta-data/local-ipv4 172.31.3.136 $ curl http://169.254.169.254/latest/meta-data/iam/security-credentials MyFirstEC2Role $ curl http://169.254.169.254/latest/meta-data/iam/security-credentials/MyFirstEC2Role { \"Code\" : \"Success\", \"LastUpdated\" : \"...\", \"Type\" : \"AWS-HMAC\", \"AccessKeyId\" : \"...\", \"SecretAccessKey\" : \"...\", \"Token\" : \"...\", \"Expiration\" : \"...\" }","title":"AWS EC2 Instance Metadata"},{"location":"AWS/developer-associate/11-aws-cli-sdk-iam-roles-policies/05-aws-ec2-instance-metadata/#aws-ec2-instance-metadata","text":"AWS EC2 Instance Metadata is a powerful but on of the least known features to developers It allows AWS EC2 instances to learn about themselves without using an IAM Role for that purpose. The url is http://169.254.169.254/latest/meta-data You can retrieve the IAM Role name from the metadata, but you CANNOT retrieve the IAM Policy Metadata - info about the EC2 instance Userdata - launch script of the EC2 instance $ curl http://169.254.169.254/latest/meta-data/instance-id i-05adcce6933809eda $ curl http://169.254.169.254/latest/meta-data/local-ipv4 172.31.3.136 $ curl http://169.254.169.254/latest/meta-data/iam/security-credentials MyFirstEC2Role $ curl http://169.254.169.254/latest/meta-data/iam/security-credentials/MyFirstEC2Role { \"Code\" : \"Success\", \"LastUpdated\" : \"...\", \"Type\" : \"AWS-HMAC\", \"AccessKeyId\" : \"...\", \"SecretAccessKey\" : \"...\", \"Token\" : \"...\", \"Expiration\" : \"...\" }","title":"AWS EC2 Instance Metadata"},{"location":"AWS/developer-associate/11-aws-cli-sdk-iam-roles-policies/06-aws-cli-profiles/","text":"AWS CLI Profiles \u00b6 When we configure our CLI it creates a default profile, that is stored in ~/.aws/credentials , but we can also set other profiles as well. $ aws configure --profile my-other-aws-account $ cat ~/.aws/credentials [default] aws_access_key_id = ... aws_secret_access_key = ... [my-other-aws-account] aws_access_key_id = ... aws_secret_access_key = ... $ cat ~/.aws/config [default] region = eu-west3 [profile my-other-aws-account] region = us-west-2 Now you can use this profile for CLI calls: $ aws s3 ls --profile my-other-aws-account","title":"AWS CLI Profiles"},{"location":"AWS/developer-associate/11-aws-cli-sdk-iam-roles-policies/06-aws-cli-profiles/#aws-cli-profiles","text":"When we configure our CLI it creates a default profile, that is stored in ~/.aws/credentials , but we can also set other profiles as well. $ aws configure --profile my-other-aws-account $ cat ~/.aws/credentials [default] aws_access_key_id = ... aws_secret_access_key = ... [my-other-aws-account] aws_access_key_id = ... aws_secret_access_key = ... $ cat ~/.aws/config [default] region = eu-west3 [profile my-other-aws-account] region = us-west-2 Now you can use this profile for CLI calls: $ aws s3 ls --profile my-other-aws-account","title":"AWS CLI Profiles"},{"location":"AWS/developer-associate/11-aws-cli-sdk-iam-roles-policies/07-aws-cli-with-mfa/","text":"AWS CLI with MFA \u00b6 To use MFA with the CLI, you must create a temporary session To do so, you must run the STS GetSessionToken API aws sts get-session-token --serial-number arn-of-the-mfa-device --token-code code-from-token --duration 3600 { \"Credentials\": { \"SecretAccessKey\": \"secret-access-key\", \"SessionToken\": \"temporary-session-token\", \"Expiration\": \"expiration-date-time\", \"AccessKeyId\": \"access-key-id\" } } In order to do this, you need to go to your IAM settings and assign an MFA device under the Security Credentials tab: This Assigned MFA device is the $arn-of-the-mfa-device, the $code-from-token is the code on your MFA device. You can add the SessionToken to your ~/.aws/credentials aws_session_token = ...","title":"AWS CLI with MFA"},{"location":"AWS/developer-associate/11-aws-cli-sdk-iam-roles-policies/07-aws-cli-with-mfa/#aws-cli-with-mfa","text":"To use MFA with the CLI, you must create a temporary session To do so, you must run the STS GetSessionToken API aws sts get-session-token --serial-number arn-of-the-mfa-device --token-code code-from-token --duration 3600 { \"Credentials\": { \"SecretAccessKey\": \"secret-access-key\", \"SessionToken\": \"temporary-session-token\", \"Expiration\": \"expiration-date-time\", \"AccessKeyId\": \"access-key-id\" } } In order to do this, you need to go to your IAM settings and assign an MFA device under the Security Credentials tab: This Assigned MFA device is the $arn-of-the-mfa-device, the $code-from-token is the code on your MFA device. You can add the SessionToken to your ~/.aws/credentials aws_session_token = ...","title":"AWS CLI with MFA"},{"location":"AWS/developer-associate/11-aws-cli-sdk-iam-roles-policies/08-aws-sdk-overview/","text":"AWS SDK Overview \u00b6 What if you want to perform actions on AWS directly from your applications code? (without using the CLI) You can use an SDK (software development kit) Official SDKs are Java .NET Node.js PHP Python (named boto3 / botocore) Go Ruby C++ We have to use the AWS SDK when using AWS Services like DynamoDB The AWS CLI uses the python SDK (boto3) The exam expects you to know when youu should use an SDK Good to know: if you don't specify or configure a default region, then us-east-1 will be chosen by default","title":"AWS SDK Overview"},{"location":"AWS/developer-associate/11-aws-cli-sdk-iam-roles-policies/08-aws-sdk-overview/#aws-sdk-overview","text":"What if you want to perform actions on AWS directly from your applications code? (without using the CLI) You can use an SDK (software development kit) Official SDKs are Java .NET Node.js PHP Python (named boto3 / botocore) Go Ruby C++ We have to use the AWS SDK when using AWS Services like DynamoDB The AWS CLI uses the python SDK (boto3) The exam expects you to know when youu should use an SDK Good to know: if you don't specify or configure a default region, then us-east-1 will be chosen by default","title":"AWS SDK Overview"},{"location":"AWS/developer-associate/11-aws-cli-sdk-iam-roles-policies/09-exponential-backoff-service-limit-increase/","text":"Exponential Backoff & Service Limit Increase \u00b6 AWS Limits (Quotas) \u00b6 API Rate Limits DescribeInstances API for EC2 has a limit of 100 calls per second GetObject on S3 has a limit of 5500 GET per second per prefix For Intermittent Errors: implement Exponential backoff For Consistent Errors: request an API throttling limit increase Service Quotas (Service Limits) Running On-Demand Standard Instances: 1152 vCPU You can request a service limit increase by opening a ticket You can request quote increase by using Service Quotas API Exponential Backoff (any AWS Service) \u00b6 If you get ThrottlingException intermittently, use exponential backoff Retry mechanism already included in AWS SDK API calls Must implement yourself if using the AWS API as-is or in specific cases Must only implement retries on 5xx server errors and throttling Do not implement on 4xx client errors","title":"Exponential Backoff & Service Limit Increase"},{"location":"AWS/developer-associate/11-aws-cli-sdk-iam-roles-policies/09-exponential-backoff-service-limit-increase/#exponential-backoff-service-limit-increase","text":"","title":"Exponential Backoff &amp; Service Limit Increase"},{"location":"AWS/developer-associate/11-aws-cli-sdk-iam-roles-policies/09-exponential-backoff-service-limit-increase/#aws-limits-quotas","text":"API Rate Limits DescribeInstances API for EC2 has a limit of 100 calls per second GetObject on S3 has a limit of 5500 GET per second per prefix For Intermittent Errors: implement Exponential backoff For Consistent Errors: request an API throttling limit increase Service Quotas (Service Limits) Running On-Demand Standard Instances: 1152 vCPU You can request a service limit increase by opening a ticket You can request quote increase by using Service Quotas API","title":"AWS Limits (Quotas)"},{"location":"AWS/developer-associate/11-aws-cli-sdk-iam-roles-policies/09-exponential-backoff-service-limit-increase/#exponential-backoff-any-aws-service","text":"If you get ThrottlingException intermittently, use exponential backoff Retry mechanism already included in AWS SDK API calls Must implement yourself if using the AWS API as-is or in specific cases Must only implement retries on 5xx server errors and throttling Do not implement on 4xx client errors","title":"Exponential Backoff (any AWS Service)"},{"location":"AWS/developer-associate/11-aws-cli-sdk-iam-roles-policies/10-aws-credentials-provider-chain/","text":"AWS Credentials Provider & Chain \u00b6 The CLI will look for credentials in this order: Command line options --region, --output, --profile Environment variables AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_SESSION_TOKEN CLI credentials file - ~/.aws/credentials CLI configuration file - ~/.aws/config Container credentials - for ECS tasks Instance profile credentials - for EC2 Instance Profiles AWS SDK Default Credentials Provider Chain \u00b6 The Java SDK (example) will look for credentials in this order: Java system properties - aws.accessKeyId, aws.secretKey Environment variables AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY The default credential profiles file at ~/.aws/credentials Amazon ECS container credentials - for ECS containers Instance profile credentials - used on EC2 instances AWS Credentials Scenario \u00b6 An application deployed on an EC2 instance is using environment variables with credentials from an IAM user to call the Amazon S3 API. The IAM user has S3FullAccess permissions The application only uses one S3 bucket, so according to best practices: An IAM Role & EC2 instance profile was created for the EC2 instance The Role was assigned the minimum permissions to access that one S3 bucket The IAM Instance Profile was assigned to the EC2 instance, but it still had access to all S3 buckets. Why? The credentials chain is still giving priorities to the environment variables unset the environment variables AWS Credentials Best Practices \u00b6 Overall, NEVER STORE AWS CREDENTIALS IN YOUR CODE Best practice is for credentials to be inherited from the credentials chain If working within AWS, use IAM Roles EC2 instance roles for EC2 instances ECS Roles for ECS tasks Lambda roles for lambda functions If working outside of AWS, use environment variables / named profiles","title":"AWS Credentials Provider & Chain"},{"location":"AWS/developer-associate/11-aws-cli-sdk-iam-roles-policies/10-aws-credentials-provider-chain/#aws-credentials-provider-chain","text":"The CLI will look for credentials in this order: Command line options --region, --output, --profile Environment variables AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_SESSION_TOKEN CLI credentials file - ~/.aws/credentials CLI configuration file - ~/.aws/config Container credentials - for ECS tasks Instance profile credentials - for EC2 Instance Profiles","title":"AWS Credentials Provider &amp; Chain"},{"location":"AWS/developer-associate/11-aws-cli-sdk-iam-roles-policies/10-aws-credentials-provider-chain/#aws-sdk-default-credentials-provider-chain","text":"The Java SDK (example) will look for credentials in this order: Java system properties - aws.accessKeyId, aws.secretKey Environment variables AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY The default credential profiles file at ~/.aws/credentials Amazon ECS container credentials - for ECS containers Instance profile credentials - used on EC2 instances","title":"AWS SDK Default Credentials Provider Chain"},{"location":"AWS/developer-associate/11-aws-cli-sdk-iam-roles-policies/10-aws-credentials-provider-chain/#aws-credentials-scenario","text":"An application deployed on an EC2 instance is using environment variables with credentials from an IAM user to call the Amazon S3 API. The IAM user has S3FullAccess permissions The application only uses one S3 bucket, so according to best practices: An IAM Role & EC2 instance profile was created for the EC2 instance The Role was assigned the minimum permissions to access that one S3 bucket The IAM Instance Profile was assigned to the EC2 instance, but it still had access to all S3 buckets. Why? The credentials chain is still giving priorities to the environment variables unset the environment variables","title":"AWS Credentials Scenario"},{"location":"AWS/developer-associate/11-aws-cli-sdk-iam-roles-policies/10-aws-credentials-provider-chain/#aws-credentials-best-practices","text":"Overall, NEVER STORE AWS CREDENTIALS IN YOUR CODE Best practice is for credentials to be inherited from the credentials chain If working within AWS, use IAM Roles EC2 instance roles for EC2 instances ECS Roles for ECS tasks Lambda roles for lambda functions If working outside of AWS, use environment variables / named profiles","title":"AWS Credentials Best Practices"},{"location":"AWS/developer-associate/11-aws-cli-sdk-iam-roles-policies/11-aws-signature-v4-signing/","text":"Signing AWS API requests \u00b6 When you call the AWS HTTP API, you sign the request so that AWS can identify you, using your AWS credentials (access key & secret key) Note: some requests to Amazon S3 don't need to be signed If you use the SDK or CLI, the HTTP requests are signed for you You should sign an AWS HTTP request using Signature v4 (SigV4) SigV4 Request examples \u00b6 HTTP HEADER Option GET https://iam.amazonaws.com/?Action=ListUsers&Version=2010-05-08 HTTP/1.1 Authorization: AWS4-HMAC-SHA256 Credential=..., SignedHeaders=content-type;host;x-amz-date, Signature-... Query String Option (ex: S3 pre-signed URLs) GET https://iam.amazonaws.com/?Action=ListUsers&Version=2010-05-08&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=...&X-Amz-Date=...&X-Amz-Signature=...","title":"Signing AWS API requests"},{"location":"AWS/developer-associate/11-aws-cli-sdk-iam-roles-policies/11-aws-signature-v4-signing/#signing-aws-api-requests","text":"When you call the AWS HTTP API, you sign the request so that AWS can identify you, using your AWS credentials (access key & secret key) Note: some requests to Amazon S3 don't need to be signed If you use the SDK or CLI, the HTTP requests are signed for you You should sign an AWS HTTP request using Signature v4 (SigV4)","title":"Signing AWS API requests"},{"location":"AWS/developer-associate/11-aws-cli-sdk-iam-roles-policies/11-aws-signature-v4-signing/#sigv4-request-examples","text":"HTTP HEADER Option GET https://iam.amazonaws.com/?Action=ListUsers&Version=2010-05-08 HTTP/1.1 Authorization: AWS4-HMAC-SHA256 Credential=..., SignedHeaders=content-type;host;x-amz-date, Signature-... Query String Option (ex: S3 pre-signed URLs) GET https://iam.amazonaws.com/?Action=ListUsers&Version=2010-05-08&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=...&X-Amz-Date=...&X-Amz-Signature=...","title":"SigV4 Request examples"},{"location":"AWS/developer-associate/12-advanced-s3-athena/01-s3-mfa-delete/","text":"S3 MFA Delete \u00b6 MFA (Multi Factor Authentication) forces user to generate a code on a device (usually a mobile phone or hardware) before doing important operations on S3 To use MFA-Delete, enable versioning on the S3 bucket You will need MFA to: permanently delete an object version suspend versioning on bucket You won't need MFA for enabling versioning listing deleted versions Only the bucket owner (root account) can enable/disable MFA-delete MFA-Delete currently can only be enabled using the CLI","title":"S3 MFA Delete"},{"location":"AWS/developer-associate/12-advanced-s3-athena/01-s3-mfa-delete/#s3-mfa-delete","text":"MFA (Multi Factor Authentication) forces user to generate a code on a device (usually a mobile phone or hardware) before doing important operations on S3 To use MFA-Delete, enable versioning on the S3 bucket You will need MFA to: permanently delete an object version suspend versioning on bucket You won't need MFA for enabling versioning listing deleted versions Only the bucket owner (root account) can enable/disable MFA-delete MFA-Delete currently can only be enabled using the CLI","title":"S3 MFA Delete"},{"location":"AWS/developer-associate/12-advanced-s3-athena/02-s3-mfa-delete-hands-on/","text":"S3 MFA Delete Hands On \u00b6 We can create a new S3 bucket, make sure it's versioning is turned on. Then we'll see that under the Versioning there is an option for multi-factor authentication delete, but we cannot activate it through the console. We can go into our root user, make sure that MFA is enabled for it. Make a access key for the root user. We can then enable the MFA delete: $ aws s3api put-bucket-versioning --bucket BUCKETNAME --versioning-configuration Status=Enabled,MFADelete=Enabled --mfa \"MFA:ARN CODE\" --profile root-profile Disabling MFA delete: $ aws s3api put-bucket-versioning --bucket BUCKETNAME --versioning-configuration Status=Enabled,MFADelete=Disabled --mfa \"MFA:ARN CODE\" --profile root-profile","title":"S3 MFA Delete Hands On"},{"location":"AWS/developer-associate/12-advanced-s3-athena/02-s3-mfa-delete-hands-on/#s3-mfa-delete-hands-on","text":"We can create a new S3 bucket, make sure it's versioning is turned on. Then we'll see that under the Versioning there is an option for multi-factor authentication delete, but we cannot activate it through the console. We can go into our root user, make sure that MFA is enabled for it. Make a access key for the root user. We can then enable the MFA delete: $ aws s3api put-bucket-versioning --bucket BUCKETNAME --versioning-configuration Status=Enabled,MFADelete=Enabled --mfa \"MFA:ARN CODE\" --profile root-profile Disabling MFA delete: $ aws s3api put-bucket-versioning --bucket BUCKETNAME --versioning-configuration Status=Enabled,MFADelete=Disabled --mfa \"MFA:ARN CODE\" --profile root-profile","title":"S3 MFA Delete Hands On"},{"location":"AWS/developer-associate/12-advanced-s3-athena/03-s3-default-encryption/","text":"S3 Default Encryption vs Bucket Policies \u00b6 One way to \"enforce encryption\" is to use a bucket policy and refuse any API call to PUT and S3 object without encryption headers. Another way is to use \"default encryption\" option in S3 Note: bucket policies are evaluated before default enctyption. We can manage this in the S3 bucket properties:","title":"S3 Default Encryption vs Bucket Policies"},{"location":"AWS/developer-associate/12-advanced-s3-athena/03-s3-default-encryption/#s3-default-encryption-vs-bucket-policies","text":"One way to \"enforce encryption\" is to use a bucket policy and refuse any API call to PUT and S3 object without encryption headers. Another way is to use \"default encryption\" option in S3 Note: bucket policies are evaluated before default enctyption. We can manage this in the S3 bucket properties:","title":"S3 Default Encryption vs Bucket Policies"},{"location":"AWS/developer-associate/12-advanced-s3-athena/04-s3-access-logs/","text":"S3 Access Logs \u00b6 For audit purpose, you may want to log all access to S3 buckets Any request made to S3, from any account, authorized or denied, will be logged into another S3 bucket That data can be analyzed using data analysis tools Or Amazon Athena as we'll see that later in the section. The log format is: https://docs.aws.amazon.com/AmazonS3/latest/dev/LogFormat.html Warning \u00b6 Do not set your logging bucket to be the monitored bucket It will create a logging loop and your bucket will grow in size exponentially","title":"S3 Access Logs"},{"location":"AWS/developer-associate/12-advanced-s3-athena/04-s3-access-logs/#s3-access-logs","text":"For audit purpose, you may want to log all access to S3 buckets Any request made to S3, from any account, authorized or denied, will be logged into another S3 bucket That data can be analyzed using data analysis tools Or Amazon Athena as we'll see that later in the section. The log format is: https://docs.aws.amazon.com/AmazonS3/latest/dev/LogFormat.html","title":"S3 Access Logs"},{"location":"AWS/developer-associate/12-advanced-s3-athena/04-s3-access-logs/#warning","text":"Do not set your logging bucket to be the monitored bucket It will create a logging loop and your bucket will grow in size exponentially","title":"Warning"},{"location":"AWS/developer-associate/12-advanced-s3-athena/05-s3-access-logs-hands-on/","text":"S3 Access Logs Hands On \u00b6 We can enable the access logging in the s3 bucket properties. You should start seeing the logs within an hour or so.","title":"S3 Access Logs Hands On"},{"location":"AWS/developer-associate/12-advanced-s3-athena/05-s3-access-logs-hands-on/#s3-access-logs-hands-on","text":"We can enable the access logging in the s3 bucket properties. You should start seeing the logs within an hour or so.","title":"S3 Access Logs Hands On"},{"location":"AWS/developer-associate/12-advanced-s3-athena/06-s3-replication-cross-region-same-region/","text":"S3 Replication (CRR & SRR) \u00b6 Must enable versioning in source and destination Cross Region Replication (CRR) Same Region Replication (SRR) Buckets can be in different accounts Copying is asynchronous Must give proper IAM permissions to S3 CRR - Use cases: compliance, lower latency access, replication across accounts. SRR - Use cases: log aggregation, live replication between production and test accounts Notes \u00b6 After activating, only new objects are replicated (not rectroactive) For DELETE operations: Can replicate delete markers from source to target (optional setting) Deletions with a version ID are not replicated (to avoid malicious deletes) There is no \"chaining\" of replication If bucket 1 has replication into bucket 2, which has replication into bucket 3 Then objects created in bucket 1 are not replicated into bucket 3","title":"S3 Replication (CRR & SRR)"},{"location":"AWS/developer-associate/12-advanced-s3-athena/06-s3-replication-cross-region-same-region/#s3-replication-crr-srr","text":"Must enable versioning in source and destination Cross Region Replication (CRR) Same Region Replication (SRR) Buckets can be in different accounts Copying is asynchronous Must give proper IAM permissions to S3 CRR - Use cases: compliance, lower latency access, replication across accounts. SRR - Use cases: log aggregation, live replication between production and test accounts","title":"S3 Replication (CRR &amp; SRR)"},{"location":"AWS/developer-associate/12-advanced-s3-athena/06-s3-replication-cross-region-same-region/#notes","text":"After activating, only new objects are replicated (not rectroactive) For DELETE operations: Can replicate delete markers from source to target (optional setting) Deletions with a version ID are not replicated (to avoid malicious deletes) There is no \"chaining\" of replication If bucket 1 has replication into bucket 2, which has replication into bucket 3 Then objects created in bucket 1 are not replicated into bucket 3","title":"Notes"},{"location":"AWS/developer-associate/12-advanced-s3-athena/07-s3-replication-hands-on/","text":"S3 Replication Hands On \u00b6 Make sure the versioning is turned on Create a new bucket, preferably into a different region, enable the versioning for it as well. Under the original bucket we can go to Management -> Replication Rules and add one. Here we can either enable the scope to be some specific files or all objects in the bucket. We can select an S3 bucket in this or another account. Finally we need an IAM role to perform this action, we can ask AWS to create one for us. Other options include encryption, destination storage class, and replication settings. When creating, there is an option to replicate existing objects as well. Now, if we upload a file into the primary bucket: After a few seconds it should show up in the replica as well: Note: that the deletes are not replicated across the buckets.","title":"S3 Replication Hands On"},{"location":"AWS/developer-associate/12-advanced-s3-athena/07-s3-replication-hands-on/#s3-replication-hands-on","text":"Make sure the versioning is turned on Create a new bucket, preferably into a different region, enable the versioning for it as well. Under the original bucket we can go to Management -> Replication Rules and add one. Here we can either enable the scope to be some specific files or all objects in the bucket. We can select an S3 bucket in this or another account. Finally we need an IAM role to perform this action, we can ask AWS to create one for us. Other options include encryption, destination storage class, and replication settings. When creating, there is an option to replicate existing objects as well. Now, if we upload a file into the primary bucket: After a few seconds it should show up in the replica as well: Note: that the deletes are not replicated across the buckets.","title":"S3 Replication Hands On"},{"location":"AWS/developer-associate/12-advanced-s3-athena/08-s3-presigned-urls/","text":"S3 Pre-signed URLs \u00b6 Can generate pre-signed URLs using SDK or CLI For downloads (seasy, can use the CLI) For uploads (harder, must use the SDK) Valid for a default of 3600 seconds, can change timeout with --expires-in argument Users given a pre-signed URL inherit the permissions of the person who generated the URL for GET / PUT Examples: - Allow only logged-in users to download a premium video on your S3 bucket - Allow an ever changing list of users to download files by gnerating URLs dynamically - Allow temporarily a user to upload a file to a precise location in our bucket","title":"S3 Pre-signed URLs"},{"location":"AWS/developer-associate/12-advanced-s3-athena/08-s3-presigned-urls/#s3-pre-signed-urls","text":"Can generate pre-signed URLs using SDK or CLI For downloads (seasy, can use the CLI) For uploads (harder, must use the SDK) Valid for a default of 3600 seconds, can change timeout with --expires-in argument Users given a pre-signed URL inherit the permissions of the person who generated the URL for GET / PUT Examples: - Allow only logged-in users to download a premium video on your S3 bucket - Allow an ever changing list of users to download files by gnerating URLs dynamically - Allow temporarily a user to upload a file to a precise location in our bucket","title":"S3 Pre-signed URLs"},{"location":"AWS/developer-associate/12-advanced-s3-athena/09-s3-presigned-urls-hands-on/","text":"S3 Pre-signed URLs hands on \u00b6 We can pre-sign objects with following CLI commands: $ aws s3 presign s3://mybucket/myobject --region my-region We can add a custom expiration time $ aws s3 presign s3://mybucket/myobject --expires-in 300 --region my-region If you are getting issues - set the proper signature version in order not to get issues when generating URLs for encrypted files $ aws configure set default.s3.signature_version s3v4","title":"S3 Pre-signed URLs hands on"},{"location":"AWS/developer-associate/12-advanced-s3-athena/09-s3-presigned-urls-hands-on/#s3-pre-signed-urls-hands-on","text":"We can pre-sign objects with following CLI commands: $ aws s3 presign s3://mybucket/myobject --region my-region We can add a custom expiration time $ aws s3 presign s3://mybucket/myobject --expires-in 300 --region my-region If you are getting issues - set the proper signature version in order not to get issues when generating URLs for encrypted files $ aws configure set default.s3.signature_version s3v4","title":"S3 Pre-signed URLs hands on"},{"location":"AWS/developer-associate/12-advanced-s3-athena/10-s3-storage-classes/","text":"S3 Storage Classess \u00b6 Amazon S3 Standard - General Purpose Amazon S3 Standard-Infrequent Access (IA) Amazon S3 One Zone-Infrequent Access Amazon S3 Intelligent Tiering Amazon Glacier Amazon Glacier Deep Archive Amazon S3 Reduced Redundancy Storage Amazon S3 Standard - General Purpose \u00b6 High durability (99.99999999999%) of objects across multiple AZ If you store 10,000,000 objects with Amazon S3, you can on average expect to incur a loss of a single object once every 10,000 years 99.99% availability over a given year Sustain 2 concurrent facility failures Use Cases: Big Data analytics, mobile & haming applications, content distribution... Amazon S3 Standard - Infrequent Access (IA) \u00b6 Suitable for data that is less frequently accessed, but requires rapid access when needed High durability (99.99999999999%) of objects across multiple AZs 99.99% availability Low cost compared to Amazon S3 Standard Sustain 2 concurrent facility failures Use cases: As a data store for disaster recovery, backups Amazon S3 One Zone - Infrequent Access (IA) \u00b6 Same as IA but data is stored in a single AZ High durability (99.99999999999%) of objects ub a single AZ, data lost when AZ is destroyed 99.5% Availability Low latency and high throughput performance Supports SLL for data at transit and encryption at rest Low cost compared to IA (by 20%) Use Cases: Storing secondary beckup copies of on-premise data, or storing data you can recreate. Amazon S3 Intelligent Tiering \u00b6 Same low latency and high throughput performance of S3 Standard Small monthly monitoring and auto-tiering fee Automatically moves objects between two access tiers based on changing access patterns Designed for durability of 99.99999999999% of objects across multiple availability zones Reslilient against events that impact an entire Availability Zone Designed for 99.9% availability over a given year Amazon S3 Glacier \u00b6 Low cost object storage meant for archiving / backups Data is retained for the longer term (10s of years) Alternative to on-premise magnetic tape storage Average anual durability is 99.99999999999% Cost per storage per month ($0.004 / GB) + retrieval cost Each item in Glacier is called \"Archive\" (up to 40TB) Archives are stored in \"Vaults\" Amazon Glacier & Glacier Deep Archive \u00b6 Amazon Glacier - 3 retrieval options: Expedited (1 to 5 minutes) Standard (3 to 5 hours) Bulk (5 to 12 hours) Minimum storage duration of 90 days Amazon Glacier Deep Archive - for long term storage - cheaper Standard (12 hours) Bulk (48 hours) Minimum storage duration of 180 days S3 Storage Classes Comparison \u00b6 We can see the comparison of the classes here: https://aws.amazon.com/s3/storage-classes/","title":"S3 Storage Classess"},{"location":"AWS/developer-associate/12-advanced-s3-athena/10-s3-storage-classes/#s3-storage-classess","text":"Amazon S3 Standard - General Purpose Amazon S3 Standard-Infrequent Access (IA) Amazon S3 One Zone-Infrequent Access Amazon S3 Intelligent Tiering Amazon Glacier Amazon Glacier Deep Archive Amazon S3 Reduced Redundancy Storage","title":"S3 Storage Classess"},{"location":"AWS/developer-associate/12-advanced-s3-athena/10-s3-storage-classes/#amazon-s3-standard-general-purpose","text":"High durability (99.99999999999%) of objects across multiple AZ If you store 10,000,000 objects with Amazon S3, you can on average expect to incur a loss of a single object once every 10,000 years 99.99% availability over a given year Sustain 2 concurrent facility failures Use Cases: Big Data analytics, mobile & haming applications, content distribution...","title":"Amazon S3 Standard - General Purpose"},{"location":"AWS/developer-associate/12-advanced-s3-athena/10-s3-storage-classes/#amazon-s3-standard-infrequent-access-ia","text":"Suitable for data that is less frequently accessed, but requires rapid access when needed High durability (99.99999999999%) of objects across multiple AZs 99.99% availability Low cost compared to Amazon S3 Standard Sustain 2 concurrent facility failures Use cases: As a data store for disaster recovery, backups","title":"Amazon S3 Standard - Infrequent Access (IA)"},{"location":"AWS/developer-associate/12-advanced-s3-athena/10-s3-storage-classes/#amazon-s3-one-zone-infrequent-access-ia","text":"Same as IA but data is stored in a single AZ High durability (99.99999999999%) of objects ub a single AZ, data lost when AZ is destroyed 99.5% Availability Low latency and high throughput performance Supports SLL for data at transit and encryption at rest Low cost compared to IA (by 20%) Use Cases: Storing secondary beckup copies of on-premise data, or storing data you can recreate.","title":"Amazon S3 One Zone - Infrequent Access (IA)"},{"location":"AWS/developer-associate/12-advanced-s3-athena/10-s3-storage-classes/#amazon-s3-intelligent-tiering","text":"Same low latency and high throughput performance of S3 Standard Small monthly monitoring and auto-tiering fee Automatically moves objects between two access tiers based on changing access patterns Designed for durability of 99.99999999999% of objects across multiple availability zones Reslilient against events that impact an entire Availability Zone Designed for 99.9% availability over a given year","title":"Amazon S3 Intelligent Tiering"},{"location":"AWS/developer-associate/12-advanced-s3-athena/10-s3-storage-classes/#amazon-s3-glacier","text":"Low cost object storage meant for archiving / backups Data is retained for the longer term (10s of years) Alternative to on-premise magnetic tape storage Average anual durability is 99.99999999999% Cost per storage per month ($0.004 / GB) + retrieval cost Each item in Glacier is called \"Archive\" (up to 40TB) Archives are stored in \"Vaults\"","title":"Amazon S3 Glacier"},{"location":"AWS/developer-associate/12-advanced-s3-athena/10-s3-storage-classes/#amazon-glacier-glacier-deep-archive","text":"Amazon Glacier - 3 retrieval options: Expedited (1 to 5 minutes) Standard (3 to 5 hours) Bulk (5 to 12 hours) Minimum storage duration of 90 days Amazon Glacier Deep Archive - for long term storage - cheaper Standard (12 hours) Bulk (48 hours) Minimum storage duration of 180 days","title":"Amazon Glacier &amp; Glacier Deep Archive"},{"location":"AWS/developer-associate/12-advanced-s3-athena/10-s3-storage-classes/#s3-storage-classes-comparison","text":"We can see the comparison of the classes here: https://aws.amazon.com/s3/storage-classes/","title":"S3 Storage Classes Comparison"},{"location":"AWS/developer-associate/12-advanced-s3-athena/11-s3-storage-classes-hands-on/","text":"S3 Storage Classes Hands On \u00b6 When uploading a file into a bucket, we can choose the storage class for that object. If we choose the glacier and we want to open the file, we can't - we have to restore it first. We can initiate the restore, set for how many days the object will be kept. And choose a retrieval tier which will impact the cost.","title":"S3 Storage Classes Hands On"},{"location":"AWS/developer-associate/12-advanced-s3-athena/11-s3-storage-classes-hands-on/#s3-storage-classes-hands-on","text":"When uploading a file into a bucket, we can choose the storage class for that object. If we choose the glacier and we want to open the file, we can't - we have to restore it first. We can initiate the restore, set for how many days the object will be kept. And choose a retrieval tier which will impact the cost.","title":"S3 Storage Classes Hands On"},{"location":"AWS/developer-associate/12-advanced-s3-athena/12-s3-lifecycle-rules/","text":"S3 Lifecycle Rules \u00b6 Moving between storage classes \u00b6 You can transition objects between storage classes For infrequently accessed object, move them to Standard IA For archive objects you don't need in real-time, glacier or deep archive Moving objects can be automated using a lifecycle configuration. S3 Lifecycle Rules \u00b6 Transition actions - it defines when objects are transitioned to another storage class. Move objects to Standard IA class 60 days after creation Move to Glacier for archiving after 6 months Expiration actions: configure objects to expire (delete) after some time Access log files can be set to delete after 365 days Can be used to delete old versions of files (if versionining is enabled) Can be used to delete incomplete multi-part uploads Rules can be created for a certain prefix (ex - s3://mybucket/mp3/*) Rules can be created for certain object tags (ex - Department: Finance) S3 Lifecycle Rules - Scenario 1 \u00b6 Your application on EC2 creates image thumbnails after profile photos are uploaded to Amazon S3. These thumbnails can be easily recreated, and only need to be kept for 45 days. The source images should be able to immediately retrieved for these 45 days, and afterwards, the user can wait up to 6 hours. How would you design this? S3 source images can be on STANDARD, with a lifecycle configuration to transition them to GLACIER after 45 days. S3 thumbnails can be on ONEZONE_IA, with a lifecycle configuration to expire them (delete) after 45 days. S3 Lifecycle Rules - Scenario 2 \u00b6 A rule in your company states that you should be able to recover your deleted S3 objects immediately for 15 days, although this may happen rarely. After this time, and for up to 365 days, deleted objects should be recoverable within 48 hours. You need to enable S3 versioning in order to have object versions, so that \"deleted objects\" are in fact hidden with a \"delete marker\" and can be recovered. You can transition these \"noncurrent versions\" of the object to S3_IA You can transition afterwards these \"noncurrent versions\" to deep archive.","title":"S3 Lifecycle Rules"},{"location":"AWS/developer-associate/12-advanced-s3-athena/12-s3-lifecycle-rules/#s3-lifecycle-rules","text":"","title":"S3 Lifecycle Rules"},{"location":"AWS/developer-associate/12-advanced-s3-athena/12-s3-lifecycle-rules/#moving-between-storage-classes","text":"You can transition objects between storage classes For infrequently accessed object, move them to Standard IA For archive objects you don't need in real-time, glacier or deep archive Moving objects can be automated using a lifecycle configuration.","title":"Moving between storage classes"},{"location":"AWS/developer-associate/12-advanced-s3-athena/12-s3-lifecycle-rules/#s3-lifecycle-rules_1","text":"Transition actions - it defines when objects are transitioned to another storage class. Move objects to Standard IA class 60 days after creation Move to Glacier for archiving after 6 months Expiration actions: configure objects to expire (delete) after some time Access log files can be set to delete after 365 days Can be used to delete old versions of files (if versionining is enabled) Can be used to delete incomplete multi-part uploads Rules can be created for a certain prefix (ex - s3://mybucket/mp3/*) Rules can be created for certain object tags (ex - Department: Finance)","title":"S3 Lifecycle Rules"},{"location":"AWS/developer-associate/12-advanced-s3-athena/12-s3-lifecycle-rules/#s3-lifecycle-rules-scenario-1","text":"Your application on EC2 creates image thumbnails after profile photos are uploaded to Amazon S3. These thumbnails can be easily recreated, and only need to be kept for 45 days. The source images should be able to immediately retrieved for these 45 days, and afterwards, the user can wait up to 6 hours. How would you design this? S3 source images can be on STANDARD, with a lifecycle configuration to transition them to GLACIER after 45 days. S3 thumbnails can be on ONEZONE_IA, with a lifecycle configuration to expire them (delete) after 45 days.","title":"S3 Lifecycle Rules - Scenario 1"},{"location":"AWS/developer-associate/12-advanced-s3-athena/12-s3-lifecycle-rules/#s3-lifecycle-rules-scenario-2","text":"A rule in your company states that you should be able to recover your deleted S3 objects immediately for 15 days, although this may happen rarely. After this time, and for up to 365 days, deleted objects should be recoverable within 48 hours. You need to enable S3 versioning in order to have object versions, so that \"deleted objects\" are in fact hidden with a \"delete marker\" and can be recovered. You can transition these \"noncurrent versions\" of the object to S3_IA You can transition afterwards these \"noncurrent versions\" to deep archive.","title":"S3 Lifecycle Rules - Scenario 2"},{"location":"AWS/developer-associate/12-advanced-s3-athena/13-s3-lifecycle-rules-hands-on/","text":"S3 Lifecycle Rules Hands On \u00b6 We can set up the lifecycle rules in Management -> Lifecycle Rules . When creating a rule, we can target specific files or all the objects. Then we can select an rule action we want, it will display options for that action. Here are the other options as well:","title":"S3 Lifecycle Rules Hands On"},{"location":"AWS/developer-associate/12-advanced-s3-athena/13-s3-lifecycle-rules-hands-on/#s3-lifecycle-rules-hands-on","text":"We can set up the lifecycle rules in Management -> Lifecycle Rules . When creating a rule, we can target specific files or all the objects. Then we can select an rule action we want, it will display options for that action. Here are the other options as well:","title":"S3 Lifecycle Rules Hands On"},{"location":"AWS/developer-associate/12-advanced-s3-athena/14-s3-performance/","text":"S3 Performance \u00b6 Baseline performance \u00b6 Amazon S3 automatically scales to high request rates, latency 100-200ms Your application can achieve at least 3,500 PUT/COPY/POST/DELETE and 5,5,500 GET/HEAD requests per second per prefix in a bucket. There are no limites to the number of prefixes in a bucket. Example (object => prefix) bucket/folder1/sub1/file => /folder1/sub1 bucket/folder1/sub2/file => /folder1/sub2 bucket/1/file => /1/ bucket/2/file => /2/ If you spread reads across all four prefixes evenly, you can achieve 22,000 requests per second for GET and HEAD. KMS Limitation \u00b6 If you use SSE-KMS, you may be impacted by the KMS limits When you upload, it calls the GenerateDataKey KMS API When you download, it calls the Decrypt KMS API Count towards the KMS quote per second (5500, 10000, 30000 req/s based on region) You can request a quote increase using Service Quotas Console S3 Performance \u00b6 Multi-Part upload: Recommended for files > 100MB, must use for files > 5GB Can help parallelize uploads (speed up transfers) S3 Transfer Acceleration Increase transfer speed by transferring file to an AWS Edge Location which will forward the data to the S3 bucket in the target region Compatible with multi-part upload S3 Performance - S3 Byte-Range Fetches \u00b6 Parallelize GETs by requesting specific byte ranges Better resilience in case of failures Can be used to speed up downloads Can be used to retrieve only partial data (for example the head of a file)","title":"S3 Performance"},{"location":"AWS/developer-associate/12-advanced-s3-athena/14-s3-performance/#s3-performance","text":"","title":"S3 Performance"},{"location":"AWS/developer-associate/12-advanced-s3-athena/14-s3-performance/#baseline-performance","text":"Amazon S3 automatically scales to high request rates, latency 100-200ms Your application can achieve at least 3,500 PUT/COPY/POST/DELETE and 5,5,500 GET/HEAD requests per second per prefix in a bucket. There are no limites to the number of prefixes in a bucket. Example (object => prefix) bucket/folder1/sub1/file => /folder1/sub1 bucket/folder1/sub2/file => /folder1/sub2 bucket/1/file => /1/ bucket/2/file => /2/ If you spread reads across all four prefixes evenly, you can achieve 22,000 requests per second for GET and HEAD.","title":"Baseline performance"},{"location":"AWS/developer-associate/12-advanced-s3-athena/14-s3-performance/#kms-limitation","text":"If you use SSE-KMS, you may be impacted by the KMS limits When you upload, it calls the GenerateDataKey KMS API When you download, it calls the Decrypt KMS API Count towards the KMS quote per second (5500, 10000, 30000 req/s based on region) You can request a quote increase using Service Quotas Console","title":"KMS Limitation"},{"location":"AWS/developer-associate/12-advanced-s3-athena/14-s3-performance/#s3-performance_1","text":"Multi-Part upload: Recommended for files > 100MB, must use for files > 5GB Can help parallelize uploads (speed up transfers) S3 Transfer Acceleration Increase transfer speed by transferring file to an AWS Edge Location which will forward the data to the S3 bucket in the target region Compatible with multi-part upload","title":"S3 Performance"},{"location":"AWS/developer-associate/12-advanced-s3-athena/14-s3-performance/#s3-performance-s3-byte-range-fetches","text":"Parallelize GETs by requesting specific byte ranges Better resilience in case of failures Can be used to speed up downloads Can be used to retrieve only partial data (for example the head of a file)","title":"S3 Performance - S3 Byte-Range Fetches"},{"location":"AWS/developer-associate/12-advanced-s3-athena/15-s3-glacier-select/","text":"S3 & Glacier Select \u00b6 Retrieve less data using SQL by performing server side filtering Can filter by rows & columns (simple SQL statements) Less network transfer, less CPU cost client-side https://aws.amazon.com/blogs/aws/s3-glacier-select/","title":"S3 & Glacier Select"},{"location":"AWS/developer-associate/12-advanced-s3-athena/15-s3-glacier-select/#s3-glacier-select","text":"Retrieve less data using SQL by performing server side filtering Can filter by rows & columns (simple SQL statements) Less network transfer, less CPU cost client-side https://aws.amazon.com/blogs/aws/s3-glacier-select/","title":"S3 &amp; Glacier Select"},{"location":"AWS/developer-associate/12-advanced-s3-athena/16-s3-event-notifications/","text":"S3 Event Notifications \u00b6 S3:ObjectCreated, S3:ObjectRemoved, S3:ObjectRestore, S3:Replication Object name filtering possible (*.jpg) Use case: generate thumbnails of images uploaded to S3 Can create as many \"S3 events\" as desired S3 event notifications typically deliver events in seconds but can sometimes take a minute or longer If two writes are made to a single non-versioned object at the same time, it is possible that only a single event notification will be sent. If you want to ensure that an event notification is sent for every sucessful write, you can enable versioning in your bucket.","title":"S3 Event Notifications"},{"location":"AWS/developer-associate/12-advanced-s3-athena/16-s3-event-notifications/#s3-event-notifications","text":"S3:ObjectCreated, S3:ObjectRemoved, S3:ObjectRestore, S3:Replication Object name filtering possible (*.jpg) Use case: generate thumbnails of images uploaded to S3 Can create as many \"S3 events\" as desired S3 event notifications typically deliver events in seconds but can sometimes take a minute or longer If two writes are made to a single non-versioned object at the same time, it is possible that only a single event notification will be sent. If you want to ensure that an event notification is sent for every sucessful write, you can enable versioning in your bucket.","title":"S3 Event Notifications"},{"location":"AWS/developer-associate/12-advanced-s3-athena/17-s3-event-notifications-hands-on/","text":"S3 Event Notificaitons Hands On \u00b6 We can configure the notifications at Properties -> Event Notifications . First, we create an event name and the prefix/suffix. Then we can select all the event types which we want to use: Then we can select a destination where the notifications will be sent into. When using an SQS queue, make sure that the access policy is set up to allow the SendMessage from the bucket.","title":"S3 Event Notificaitons Hands On"},{"location":"AWS/developer-associate/12-advanced-s3-athena/17-s3-event-notifications-hands-on/#s3-event-notificaitons-hands-on","text":"We can configure the notifications at Properties -> Event Notifications . First, we create an event name and the prefix/suffix. Then we can select all the event types which we want to use: Then we can select a destination where the notifications will be sent into. When using an SQS queue, make sure that the access policy is set up to allow the SendMessage from the bucket.","title":"S3 Event Notificaitons Hands On"},{"location":"AWS/developer-associate/12-advanced-s3-athena/18-athena-overview/","text":"Athena Overview \u00b6 Serverless query service to perform analytics against S3 objects Uses standard SQL language to query the files Supports CSV, JSON, ORC, Avro and Parquet Pricing: $5.00 per TB of data scanned Use compressed or columnar data for cost-savings (less scan) Use cases: Business Intelligence / analytics / reporting, analyze & query FPC Flow logs, ELB logs, CloudTrail trails etc... Exam Tip: Analyze data in S3 using serverless SQL, use athena. Users -- load data --> S3 Bucket -- query & analyze --> Athena -- Reporting & Dashboards --> Amazon QuickSight","title":"Athena Overview"},{"location":"AWS/developer-associate/12-advanced-s3-athena/18-athena-overview/#athena-overview","text":"Serverless query service to perform analytics against S3 objects Uses standard SQL language to query the files Supports CSV, JSON, ORC, Avro and Parquet Pricing: $5.00 per TB of data scanned Use compressed or columnar data for cost-savings (less scan) Use cases: Business Intelligence / analytics / reporting, analyze & query FPC Flow logs, ELB logs, CloudTrail trails etc... Exam Tip: Analyze data in S3 using serverless SQL, use athena. Users -- load data --> S3 Bucket -- query & analyze --> Athena -- Reporting & Dashboards --> Amazon QuickSight","title":"Athena Overview"},{"location":"AWS/developer-associate/12-advanced-s3-athena/19-athena-hands-on/","text":"Athena Hands On \u00b6 We can open up the Athena in AWS Console and open up the query editor. If this is the first time using it, we need to set up an S3 bucket for results. On the left side of the query editor we can see data sources and databases we can use. In the s3_access_logs_db we can run following queries from https://aws.amazon.com/premiumsupport/knowledge-center/analyze-logs-athena/ CREATE EXTERNAL TABLE `s3_access_logs_db.demo-dave-s3-bucket-access`( `bucketowner` STRING, `bucket_name` STRING, `requestdatetime` STRING, `remoteip` STRING, `requester` STRING, `requestid` STRING, `operation` STRING, `key` STRING, `request_uri` STRING, `httpstatus` STRING, `errorcode` STRING, `bytessent` BIGINT, `objectsize` BIGINT, `totaltime` STRING, `turnaroundtime` STRING, `referrer` STRING, `useragent` STRING, `versionid` STRING, `hostid` STRING, `sigv` STRING, `ciphersuite` STRING, `authtype` STRING, `endpoint` STRING, `tlsversion` STRING) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe' WITH SERDEPROPERTIES ( 'input.regex'='([^ ]*) ([^ ]*) \\\\[(.*?)\\\\] ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) (\\\"[^\\\"]*\\\"|-) (-|[0-9]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) (\\\"[^\\\"]*\\\"|-) ([^ ]*)(?: ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*))?.*$') STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION 's3://demo-dave-s3-bucket-access/s3-logs/' Now we'll be able to see the table on the left side and preview it: SELECT * FROM \"s3_access_logs_db\".\"demo-dave-s3-bucket-access\" limit 10;","title":"Athena Hands On"},{"location":"AWS/developer-associate/12-advanced-s3-athena/19-athena-hands-on/#athena-hands-on","text":"We can open up the Athena in AWS Console and open up the query editor. If this is the first time using it, we need to set up an S3 bucket for results. On the left side of the query editor we can see data sources and databases we can use. In the s3_access_logs_db we can run following queries from https://aws.amazon.com/premiumsupport/knowledge-center/analyze-logs-athena/ CREATE EXTERNAL TABLE `s3_access_logs_db.demo-dave-s3-bucket-access`( `bucketowner` STRING, `bucket_name` STRING, `requestdatetime` STRING, `remoteip` STRING, `requester` STRING, `requestid` STRING, `operation` STRING, `key` STRING, `request_uri` STRING, `httpstatus` STRING, `errorcode` STRING, `bytessent` BIGINT, `objectsize` BIGINT, `totaltime` STRING, `turnaroundtime` STRING, `referrer` STRING, `useragent` STRING, `versionid` STRING, `hostid` STRING, `sigv` STRING, `ciphersuite` STRING, `authtype` STRING, `endpoint` STRING, `tlsversion` STRING) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe' WITH SERDEPROPERTIES ( 'input.regex'='([^ ]*) ([^ ]*) \\\\[(.*?)\\\\] ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) (\\\"[^\\\"]*\\\"|-) (-|[0-9]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) (\\\"[^\\\"]*\\\"|-) ([^ ]*)(?: ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*))?.*$') STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION 's3://demo-dave-s3-bucket-access/s3-logs/' Now we'll be able to see the table on the left side and preview it: SELECT * FROM \"s3_access_logs_db\".\"demo-dave-s3-bucket-access\" limit 10;","title":"Athena Hands On"},{"location":"AWS/developer-associate/13-cloudfront/02-cloudfront-hands-on/","text":"CloudFront Hands On \u00b6 Before working with CloudFront we need to create an S3 Bucket. There we can upload an index.html file and some pictures. Then navigate to cloudfront on AWS Console, create a new distribution. We are going to set up the origin as the S3 bucket we created, and set an origin access identity that the cloudfront will use to access the s3 bucket. Then, we'll set up the default root object as index.html Once we have created the cloudfront application, it will take about 5 minutes to deploy. We can also see that the distribution changed the bucket policy. Once the deployment has been done - we can open it up.","title":"CloudFront Hands On"},{"location":"AWS/developer-associate/13-cloudfront/02-cloudfront-hands-on/#cloudfront-hands-on","text":"Before working with CloudFront we need to create an S3 Bucket. There we can upload an index.html file and some pictures. Then navigate to cloudfront on AWS Console, create a new distribution. We are going to set up the origin as the S3 bucket we created, and set an origin access identity that the cloudfront will use to access the s3 bucket. Then, we'll set up the default root object as index.html Once we have created the cloudfront application, it will take about 5 minutes to deploy. We can also see that the distribution changed the bucket policy. Once the deployment has been done - we can open it up.","title":"CloudFront Hands On"},{"location":"AWS/developer-associate/13-cloudfront/03-cloudfront-caching-and-invalidations/","text":"CloudFront Caching & Caching Invalidations \u00b6 CloudFront Caching \u00b6 Cache based on Headers Session Cookies Query String Parameters The cache lives at each CloudFront Edge Location You want to maximize the cache hit rate to minimize requests on the origin Control the TTL (0 seconds to 1 year), can be set by the origin using the Cache-Control header, Expires header You can invalidate part of the cache using the CreateInvalidation API","title":"CloudFront Caching & Caching Invalidations"},{"location":"AWS/developer-associate/13-cloudfront/03-cloudfront-caching-and-invalidations/#cloudfront-caching-caching-invalidations","text":"","title":"CloudFront Caching &amp; Caching Invalidations"},{"location":"AWS/developer-associate/13-cloudfront/03-cloudfront-caching-and-invalidations/#cloudfront-caching","text":"Cache based on Headers Session Cookies Query String Parameters The cache lives at each CloudFront Edge Location You want to maximize the cache hit rate to minimize requests on the origin Control the TTL (0 seconds to 1 year), can be set by the origin using the Cache-Control header, Expires header You can invalidate part of the cache using the CreateInvalidation API","title":"CloudFront Caching"},{"location":"AWS/developer-associate/13-cloudfront/04-cloudfront-caching-and-invalidations-hands-on/","text":"CloudFront Caching And Invalidations Hands On \u00b6 We can open up the cloudfront distribution, open the behaviours tab, select the first result and edit it. There will be settings for cache. We can choose a policy there or create a new one. As for the invalidations. We can open up the Invalidations tab and create a new invalidation.","title":"CloudFront Caching And Invalidations Hands On"},{"location":"AWS/developer-associate/13-cloudfront/04-cloudfront-caching-and-invalidations-hands-on/#cloudfront-caching-and-invalidations-hands-on","text":"We can open up the cloudfront distribution, open the behaviours tab, select the first result and edit it. There will be settings for cache. We can choose a policy there or create a new one. As for the invalidations. We can open up the Invalidations tab and create a new invalidation.","title":"CloudFront Caching And Invalidations Hands On"},{"location":"AWS/developer-associate/13-cloudfront/05-cloudfront-security/","text":"CloudFront Security \u00b6 CloudFront Geo Restriction \u00b6 You can restrict who can access your distribution Whitelist: allow users from specific countries Blacklist: Deny users from specific countries Country determined using a 3rd party geo-ip database Use case: copyright laws to control access to content CloudFront and HTTPS \u00b6 Viewer Protocol Policy: Redirect HTTP to HTTPS Or use HTTPS only Origin Protocol Policy (HTTP to S3) HTTPS only Or Match viewer (HTTP => HTTP, HTTPS => HTTPS) Note: s3 bucket websites dont support https This can be configured under behaviour. For the geographic restrictions we can choose the Geographic restrictions tab.","title":"CloudFront Security"},{"location":"AWS/developer-associate/13-cloudfront/05-cloudfront-security/#cloudfront-security","text":"","title":"CloudFront Security"},{"location":"AWS/developer-associate/13-cloudfront/05-cloudfront-security/#cloudfront-geo-restriction","text":"You can restrict who can access your distribution Whitelist: allow users from specific countries Blacklist: Deny users from specific countries Country determined using a 3rd party geo-ip database Use case: copyright laws to control access to content","title":"CloudFront Geo Restriction"},{"location":"AWS/developer-associate/13-cloudfront/05-cloudfront-security/#cloudfront-and-https","text":"Viewer Protocol Policy: Redirect HTTP to HTTPS Or use HTTPS only Origin Protocol Policy (HTTP to S3) HTTPS only Or Match viewer (HTTP => HTTP, HTTPS => HTTPS) Note: s3 bucket websites dont support https This can be configured under behaviour. For the geographic restrictions we can choose the Geographic restrictions tab.","title":"CloudFront and HTTPS"},{"location":"AWS/developer-associate/13-cloudfront/06-cloudfront-signed-url-cookies/","text":"CloudFront Signed URL / Cookies \u00b6 You want to distribute paid shared content to premium users over the world We can use CloudFront Signed URL / Cookie. We attach a policy with: Includes URL expiration Includes IP ranges to access the data from Trusted signers (which AWS accounts can create signed URLs) How long should the URL be valid for? shared content (movie, music): make it short (a few minutes) private content (private to the user): you can make it last for years Signed URL = access to individual files (one signed URL per file) Signed Cookies = access to multiple files (one signed cookie for many files) CloudFront Signed URL vs S3 Pre-Signed URL \u00b6 CloudFront Signed URL Allow access to a path, no matter the origin Account wide key-pair, only the root can manage it Can filter by IP, path,date, expiration Can leverage caching features S3 Pre-Signed URL: Issue a request as the person who pre-signed the URL Uses IAM key of the signing IAM principal Limited Lifetime","title":"CloudFront Signed URL / Cookies"},{"location":"AWS/developer-associate/13-cloudfront/06-cloudfront-signed-url-cookies/#cloudfront-signed-url-cookies","text":"You want to distribute paid shared content to premium users over the world We can use CloudFront Signed URL / Cookie. We attach a policy with: Includes URL expiration Includes IP ranges to access the data from Trusted signers (which AWS accounts can create signed URLs) How long should the URL be valid for? shared content (movie, music): make it short (a few minutes) private content (private to the user): you can make it last for years Signed URL = access to individual files (one signed URL per file) Signed Cookies = access to multiple files (one signed cookie for many files)","title":"CloudFront Signed URL / Cookies"},{"location":"AWS/developer-associate/13-cloudfront/06-cloudfront-signed-url-cookies/#cloudfront-signed-url-vs-s3-pre-signed-url","text":"CloudFront Signed URL Allow access to a path, no matter the origin Account wide key-pair, only the root can manage it Can filter by IP, path,date, expiration Can leverage caching features S3 Pre-Signed URL: Issue a request as the person who pre-signed the URL Uses IAM key of the signing IAM principal Limited Lifetime","title":"CloudFront Signed URL vs S3 Pre-Signed URL"},{"location":"AWS/developer-associate/13-cloudfront/07-cloudfront-signed-url-key-groups-hands-on/","text":"CloudFront Signed URL - Key Groups + Hands On \u00b6 CloudFront Signed URL Process \u00b6 Two types of signers: Either a trusted key group (recommended) Can leverage APIs to create and rotate keys (and IAM for API Security) An AWS account that contains a CloudFront Key Pair Need to manage keys using the root account and the AWS Console Not recommended because you shouldn't use the root account for this. In your CloudFront distribution, create one or more trusted key groups You generate your own public / private key The private key is used by your applications (e.g. EC2) to sign URLs The public key (uploaded) is used by CloudFront to verify URLs To add the key first we generate a RSA 2048 bit key, the private one we keep to ourselves, the public one we add to the CloudFront -> Public Keys . Then we can create a key group. You can also create the keypairs using the unrecommended way - log into the root account and go to security credentials.","title":"CloudFront Signed URL - Key Groups + Hands On"},{"location":"AWS/developer-associate/13-cloudfront/07-cloudfront-signed-url-key-groups-hands-on/#cloudfront-signed-url-key-groups-hands-on","text":"","title":"CloudFront Signed URL - Key Groups + Hands On"},{"location":"AWS/developer-associate/13-cloudfront/07-cloudfront-signed-url-key-groups-hands-on/#cloudfront-signed-url-process","text":"Two types of signers: Either a trusted key group (recommended) Can leverage APIs to create and rotate keys (and IAM for API Security) An AWS account that contains a CloudFront Key Pair Need to manage keys using the root account and the AWS Console Not recommended because you shouldn't use the root account for this. In your CloudFront distribution, create one or more trusted key groups You generate your own public / private key The private key is used by your applications (e.g. EC2) to sign URLs The public key (uploaded) is used by CloudFront to verify URLs To add the key first we generate a RSA 2048 bit key, the private one we keep to ourselves, the public one we add to the CloudFront -> Public Keys . Then we can create a key group. You can also create the keypairs using the unrecommended way - log into the root account and go to security credentials.","title":"CloudFront Signed URL Process"},{"location":"AWS/developer-associate/13-cloudfront/08-cloudfront-advanced-concepts/","text":"CloudFront Advanced Concepts \u00b6 Pricing \u00b6 CloudFront Edge Locations are all around the world The cost of date per edge location varies https://aws.amazon.com/cloudfront/pricing/ Price Classes \u00b6 You can reduce the number of edge locations for cost reduction Three price classes: Price Class All: all regions - best performance Price Class 200: most regions, but excludes the most expensive regions Price Class 100: only the least expensive regions CloudFront - Multiple Origin \u00b6 To route to different kind of origins based on the content type Based on path pattern: /images/* /api/* /* CloudFront - Origin Groups \u00b6 To increase high-availability and do failover Origin Group: one primary and one secondary origin If the primary origin fails, the second one is used CloudFront - Field Level Encryption \u00b6 Protect user sensitive information through application stack Adds an additional layer of security along with HTTPS Sensitive information encrypted at the edge close to user Uses asymmetric encryption Usage: Specify set of fields in POST requests that you want to be encrypted (up to 10 fields) Specify thee public key to encrypt them","title":"CloudFront Advanced Concepts"},{"location":"AWS/developer-associate/13-cloudfront/08-cloudfront-advanced-concepts/#cloudfront-advanced-concepts","text":"","title":"CloudFront Advanced Concepts"},{"location":"AWS/developer-associate/13-cloudfront/08-cloudfront-advanced-concepts/#pricing","text":"CloudFront Edge Locations are all around the world The cost of date per edge location varies https://aws.amazon.com/cloudfront/pricing/","title":"Pricing"},{"location":"AWS/developer-associate/13-cloudfront/08-cloudfront-advanced-concepts/#price-classes","text":"You can reduce the number of edge locations for cost reduction Three price classes: Price Class All: all regions - best performance Price Class 200: most regions, but excludes the most expensive regions Price Class 100: only the least expensive regions","title":"Price Classes"},{"location":"AWS/developer-associate/13-cloudfront/08-cloudfront-advanced-concepts/#cloudfront-multiple-origin","text":"To route to different kind of origins based on the content type Based on path pattern: /images/* /api/* /*","title":"CloudFront - Multiple Origin"},{"location":"AWS/developer-associate/13-cloudfront/08-cloudfront-advanced-concepts/#cloudfront-origin-groups","text":"To increase high-availability and do failover Origin Group: one primary and one secondary origin If the primary origin fails, the second one is used","title":"CloudFront - Origin Groups"},{"location":"AWS/developer-associate/13-cloudfront/08-cloudfront-advanced-concepts/#cloudfront-field-level-encryption","text":"Protect user sensitive information through application stack Adds an additional layer of security along with HTTPS Sensitive information encrypted at the edge close to user Uses asymmetric encryption Usage: Specify set of fields in POST requests that you want to be encrypted (up to 10 fields) Specify thee public key to encrypt them","title":"CloudFront - Field Level Encryption"},{"location":"AWS/developer-associate/13-cloudfront/AWS%20CloudFront/","text":"AWS CloudFront \u00b6 Content Delivery Network (CDN) Improves read performance, content is cached at the edge 216 Point of Presence globally (edge locations) DDoS protection, integration with Shield, AWS Web Application Firewall Can expose external HTTPS and can talk to internal HTTPS backends Origins \u00b6 S3 bucket For distributing files and caching them at the edge Enhanced security with cloudfront Origin Access Identity (OAI) CloudFront can be used as an ingress (to upload files to S3) Custom Origin (HTTP) Application Load Balancer EC2 instance S3 website (must first enable the bucket as static S3 website) Any HTTP backend you want CloudFront Geo Restriction \u00b6 You can restrict who can access your distribution Whitelist: Allow your users to access your content only if they're in one of the countries on a list of approved countries Blacklist: Prevent your users from accessing your content if they're in one of the countries on a blacklist of banned countries. The \"country\" is determined using a 3rd party geo-ip database Use case: Copyright laws to control access to content CloudFront vs S3 Cross Region Replication \u00b6 ClouFront Global Edge network Files are cached for a TLL (maybe a day) Great for static content that must be available everywhere S3 Cross Region Replication: Must be setup for each region you want the replacation to happen Files are updated in near real-time Read only Great for dynamic content that needs to be available at low-latency in few regions","title":"AWS CloudFront"},{"location":"AWS/developer-associate/13-cloudfront/AWS%20CloudFront/#aws-cloudfront","text":"Content Delivery Network (CDN) Improves read performance, content is cached at the edge 216 Point of Presence globally (edge locations) DDoS protection, integration with Shield, AWS Web Application Firewall Can expose external HTTPS and can talk to internal HTTPS backends","title":"AWS CloudFront"},{"location":"AWS/developer-associate/13-cloudfront/AWS%20CloudFront/#origins","text":"S3 bucket For distributing files and caching them at the edge Enhanced security with cloudfront Origin Access Identity (OAI) CloudFront can be used as an ingress (to upload files to S3) Custom Origin (HTTP) Application Load Balancer EC2 instance S3 website (must first enable the bucket as static S3 website) Any HTTP backend you want","title":"Origins"},{"location":"AWS/developer-associate/13-cloudfront/AWS%20CloudFront/#cloudfront-geo-restriction","text":"You can restrict who can access your distribution Whitelist: Allow your users to access your content only if they're in one of the countries on a list of approved countries Blacklist: Prevent your users from accessing your content if they're in one of the countries on a blacklist of banned countries. The \"country\" is determined using a 3rd party geo-ip database Use case: Copyright laws to control access to content","title":"CloudFront Geo Restriction"},{"location":"AWS/developer-associate/13-cloudfront/AWS%20CloudFront/#cloudfront-vs-s3-cross-region-replication","text":"ClouFront Global Edge network Files are cached for a TLL (maybe a day) Great for static content that must be available everywhere S3 Cross Region Replication: Must be setup for each region you want the replacation to happen Files are updated in near real-time Read only Great for dynamic content that needs to be available at low-latency in few regions","title":"CloudFront vs S3 Cross Region Replication"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/01-docker-introduction/","text":"Dock Introduction \u00b6 What is docker \u00b6 Docker is a software development platform to deploy apps Apps are packaged in containers that can be run on any OS Apps run the same, regardless of where they're run Any machine No compatibility issues Predictable behavior Less work Easier to maintain and deploy Works with any language, any OS, any technology Use cases: microservices architecture, lift-and-shift apps from on-premises to AWS cloud, ... Where are Docker images stored? \u00b6 Docker images are stored in repositories Docker Hub (https://hub.docker.com) public repository find base images for many technologies or OS (e.g. ubuntu, mysql) Amazon ECR (Amazon Elastic Container Registry) private repository public repository (Amazon ECR Public Gallery https://gallery.ecr.aws) Docker vs Virtual Machines \u00b6 Docker is \"sort of\" a virtualization technology, but not exactly Resources are shared with the host to many containers on one server Docker Containers Management on AWS \u00b6 Amazon Elastic Container Service (Amazon ECS) Amazon's own container platform Amazon Elastic Kubernetes Service (Amazon EKS) Amazon's managed Kubernetes (Open Source) AWS Fargate Amazon's own serverless container platform Works with ECS and with EKS Amazon ECR: Store container images","title":"Dock Introduction"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/01-docker-introduction/#dock-introduction","text":"","title":"Dock Introduction"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/01-docker-introduction/#what-is-docker","text":"Docker is a software development platform to deploy apps Apps are packaged in containers that can be run on any OS Apps run the same, regardless of where they're run Any machine No compatibility issues Predictable behavior Less work Easier to maintain and deploy Works with any language, any OS, any technology Use cases: microservices architecture, lift-and-shift apps from on-premises to AWS cloud, ...","title":"What is docker"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/01-docker-introduction/#where-are-docker-images-stored","text":"Docker images are stored in repositories Docker Hub (https://hub.docker.com) public repository find base images for many technologies or OS (e.g. ubuntu, mysql) Amazon ECR (Amazon Elastic Container Registry) private repository public repository (Amazon ECR Public Gallery https://gallery.ecr.aws)","title":"Where are Docker images stored?"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/01-docker-introduction/#docker-vs-virtual-machines","text":"Docker is \"sort of\" a virtualization technology, but not exactly Resources are shared with the host to many containers on one server","title":"Docker vs Virtual Machines"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/01-docker-introduction/#docker-containers-management-on-aws","text":"Amazon Elastic Container Service (Amazon ECS) Amazon's own container platform Amazon Elastic Kubernetes Service (Amazon EKS) Amazon's managed Kubernetes (Open Source) AWS Fargate Amazon's own serverless container platform Works with ECS and with EKS Amazon ECR: Store container images","title":"Docker Containers Management on AWS"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/02-amazon-ecs/","text":"Amazon ECS \u00b6 EC2 Launch Type \u00b6 ECS stands for Elastic Container Service When you launch an docker containers on AWS - launch ECS Tasks on ECS Clusters EC2 Launch Type: you must provision & maintain the infrastructure (the EC2 instances) Each EC2 instance must run the ECS agent to register in the ECS cluster AWS takes care of starting / stopping the containers Fargate Launch Type \u00b6 Launch Docker containers on AWS You do not provision the infrastructure (no EC2 instances to manage) It's all serverless You just creeate task definitions AWS just runs ECS tasks for you based on the CPU/RAM you need IAM Roles for ECS \u00b6 EC2 Instance Profile (EC2 Launch Profile Only) Used by the ECS agent Makes API calls to ECS service Send container logs to CloudWatch Logs Pull Docker image from ECR Reference sensitive data in Secrets Manager or SSM Parameter Store ECS Task Role Allows each task to have a specific role Use different roles for the different ECS Services you run Task role is defined in the task definition Load Balancer Integrations \u00b6 Application Load Balancer - supported and works for most use cases. Network Load Balancer - recommended only for high throughput / high performance use cases, or to pair it with AWS Private Link Elastic Load balancer - supported but not recommended (no advanced features - no Fargate). Data Volumes (EFS) \u00b6 Mount EFS file systems onto ECS tasks Works for both EC2 and Fargate launch types Tasks running in any AZ will share the same data in the EFS file system Fargate + EFS = serverless Use cases: persistent multi-AZ shared storeage for containers Note: FSx for Lustre & Windows not supported Amazon S3 cannot be mounted as a file system","title":"Amazon ECS"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/02-amazon-ecs/#amazon-ecs","text":"","title":"Amazon ECS"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/02-amazon-ecs/#ec2-launch-type","text":"ECS stands for Elastic Container Service When you launch an docker containers on AWS - launch ECS Tasks on ECS Clusters EC2 Launch Type: you must provision & maintain the infrastructure (the EC2 instances) Each EC2 instance must run the ECS agent to register in the ECS cluster AWS takes care of starting / stopping the containers","title":"EC2 Launch Type"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/02-amazon-ecs/#fargate-launch-type","text":"Launch Docker containers on AWS You do not provision the infrastructure (no EC2 instances to manage) It's all serverless You just creeate task definitions AWS just runs ECS tasks for you based on the CPU/RAM you need","title":"Fargate Launch Type"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/02-amazon-ecs/#iam-roles-for-ecs","text":"EC2 Instance Profile (EC2 Launch Profile Only) Used by the ECS agent Makes API calls to ECS service Send container logs to CloudWatch Logs Pull Docker image from ECR Reference sensitive data in Secrets Manager or SSM Parameter Store ECS Task Role Allows each task to have a specific role Use different roles for the different ECS Services you run Task role is defined in the task definition","title":"IAM Roles for ECS"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/02-amazon-ecs/#load-balancer-integrations","text":"Application Load Balancer - supported and works for most use cases. Network Load Balancer - recommended only for high throughput / high performance use cases, or to pair it with AWS Private Link Elastic Load balancer - supported but not recommended (no advanced features - no Fargate).","title":"Load Balancer Integrations"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/02-amazon-ecs/#data-volumes-efs","text":"Mount EFS file systems onto ECS tasks Works for both EC2 and Fargate launch types Tasks running in any AZ will share the same data in the EFS file system Fargate + EFS = serverless Use cases: persistent multi-AZ shared storeage for containers Note: FSx for Lustre & Windows not supported Amazon S3 cannot be mounted as a file system","title":"Data Volumes (EFS)"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/03-creating-ecs-cluster-hands-on/","text":"ECS Cluster - Hands On \u00b6 We can go into the ECS Container Service to start working on it. We are going to create our first cluster. We are going to fill out the creating form. We are going to choose a name for it and the default VPC, subnets. For infrastructure the default option is the Fargate, but we can select to use the EC2 instances as well. There is also an external instances option where you can configure external instances like on premises. There is also an option to configure monitoring and tags, but we are going to leave it as is. Since we selected the ECS cluster with EC2 ec2, we can go to Auto Scaling Groups and see it. In the ECS we will see a cluster created with basically nothing deployed to it. In the infrastructure tab we'll see ithat there are 3 providers. We have FARGATE, FARGATE_SPOT and our ASGProvider. If we were to increase the minimum capacity of the ASG instances to 1, it will automatically register it in the container instances. Now we can see that it is also registered on ECS.","title":"ECS Cluster - Hands On"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/03-creating-ecs-cluster-hands-on/#ecs-cluster-hands-on","text":"We can go into the ECS Container Service to start working on it. We are going to create our first cluster. We are going to fill out the creating form. We are going to choose a name for it and the default VPC, subnets. For infrastructure the default option is the Fargate, but we can select to use the EC2 instances as well. There is also an external instances option where you can configure external instances like on premises. There is also an option to configure monitoring and tags, but we are going to leave it as is. Since we selected the ECS cluster with EC2 ec2, we can go to Auto Scaling Groups and see it. In the ECS we will see a cluster created with basically nothing deployed to it. In the infrastructure tab we'll see ithat there are 3 providers. We have FARGATE, FARGATE_SPOT and our ASGProvider. If we were to increase the minimum capacity of the ASG instances to 1, it will automatically register it in the container instances. Now we can see that it is also registered on ECS.","title":"ECS Cluster - Hands On"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/04-creating-ecs-service-hands-on/","text":"Creating ECS Service - Hands On \u00b6 In order to launch a service, first we need to create an ECS task definition. We are going to use nginxdemos/hello image for the container. For the environment, we can choose whether to run it on Fargate or EC2 instances. We can also choose the OS, CPU, Memory, Role, Network. We can configure storage - how much storage do we need or adding volumes. We can also configure monitoring and logging. Next we want to run this task definition as a service behind a load balancer. For that to work, we need to define 2 security groups. One that allows the load balancer to connect to the port 80. And one for the ECS task. Now we can open up the ECS cluster, open up our demo-cluster and click on deploy. We are also going to use a Load Balancer. For the networking - we are going to use a nginx-demo-sg security group. We are going to modify the ALB, and set the alb-ecs-sg security group for it. Now we see that the task is running: If we open up the ALB url: Now we can open up our service, click on edit and scale it up: We'll see them provisioning. And if we refresh the ALB url, we'll see different IDs.","title":"Creating ECS Service - Hands On"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/04-creating-ecs-service-hands-on/#creating-ecs-service-hands-on","text":"In order to launch a service, first we need to create an ECS task definition. We are going to use nginxdemos/hello image for the container. For the environment, we can choose whether to run it on Fargate or EC2 instances. We can also choose the OS, CPU, Memory, Role, Network. We can configure storage - how much storage do we need or adding volumes. We can also configure monitoring and logging. Next we want to run this task definition as a service behind a load balancer. For that to work, we need to define 2 security groups. One that allows the load balancer to connect to the port 80. And one for the ECS task. Now we can open up the ECS cluster, open up our demo-cluster and click on deploy. We are also going to use a Load Balancer. For the networking - we are going to use a nginx-demo-sg security group. We are going to modify the ALB, and set the alb-ecs-sg security group for it. Now we see that the task is running: If we open up the ALB url: Now we can open up our service, click on edit and scale it up: We'll see them provisioning. And if we refresh the ALB url, we'll see different IDs.","title":"Creating ECS Service - Hands On"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/05-ecs-auto-scaling/","text":"ECS Service Auto Scaling \u00b6 Automaticall increase/descrease the desired number of ECS tasks Amazon ECS Auto Scaling uses AWS Application Auto Scaling ECS Service Average CPU Utilization ECS Service Average Memory Utilization - Scale on RAM ALB Request Count Per Target - metric coming from the ALB Target Tracking - scale based on target value for a specific CloudWatch metric Step Scaling - scale based on a specific CloudWatch Alarm Scheduled Scaling - scale based on a specific date/time (predictable changes) ECS Service Auto Scaling (task level) != EC2 Auto Scaling (EC2 instance level) Fargate Auto Scaling is much easier to setup (because it's serverless) Auto Scaling EC2 Instances \u00b6 Accomodate ECS Service Scaling by adding underlying EC2 instances Auto Scaling Group Scaling Scale your ASG based on CPU utilization Add EC2 instances over time ECS Cluster Capacity Provider Used to automatically provision and scale the infrastructure for your ECS tasks Capacity Provider paired with an Auto Scaling Group Add EC2 Instances when you're missing capacity (CPU,RAM)","title":"ECS Service Auto Scaling"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/05-ecs-auto-scaling/#ecs-service-auto-scaling","text":"Automaticall increase/descrease the desired number of ECS tasks Amazon ECS Auto Scaling uses AWS Application Auto Scaling ECS Service Average CPU Utilization ECS Service Average Memory Utilization - Scale on RAM ALB Request Count Per Target - metric coming from the ALB Target Tracking - scale based on target value for a specific CloudWatch metric Step Scaling - scale based on a specific CloudWatch Alarm Scheduled Scaling - scale based on a specific date/time (predictable changes) ECS Service Auto Scaling (task level) != EC2 Auto Scaling (EC2 instance level) Fargate Auto Scaling is much easier to setup (because it's serverless)","title":"ECS Service Auto Scaling"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/05-ecs-auto-scaling/#auto-scaling-ec2-instances","text":"Accomodate ECS Service Scaling by adding underlying EC2 instances Auto Scaling Group Scaling Scale your ASG based on CPU utilization Add EC2 instances over time ECS Cluster Capacity Provider Used to automatically provision and scale the infrastructure for your ECS tasks Capacity Provider paired with an Auto Scaling Group Add EC2 Instances when you're missing capacity (CPU,RAM)","title":"Auto Scaling EC2 Instances"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/06-ecs-rolling-updates/","text":"ECS Rolling Updates \u00b6 When updating your tasks, we can control how many tasks can be started and stopped in which order. (minimum healthy percent, maximum percent) If we have a min 50% and max 100% If we have min 100% and max 150%","title":"ECS Rolling Updates"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/06-ecs-rolling-updates/#ecs-rolling-updates","text":"When updating your tasks, we can control how many tasks can be started and stopped in which order. (minimum healthy percent, maximum percent) If we have a min 50% and max 100% If we have min 100% and max 150%","title":"ECS Rolling Updates"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/07-ecs-solutions-architectures/","text":"ECS Solutions Architectures \u00b6 ECS Tasks invoked by Event Bridge \u00b6 We can have a scenario where an EventBridge event is fired that spins up an AWS Fargate task, which essentially is serverless. ECS Tasks invoked by Event Bridge Schedule \u00b6 We can also schedule tasks ECS - SQS Queue \u00b6 We can have tasks that are polling SQS queue and uses autoscaling so that the more messages we have, the more tasks we will deploy.","title":"ECS Solutions Architectures"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/07-ecs-solutions-architectures/#ecs-solutions-architectures","text":"","title":"ECS Solutions Architectures"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/07-ecs-solutions-architectures/#ecs-tasks-invoked-by-event-bridge","text":"We can have a scenario where an EventBridge event is fired that spins up an AWS Fargate task, which essentially is serverless.","title":"ECS Tasks invoked by Event Bridge"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/07-ecs-solutions-architectures/#ecs-tasks-invoked-by-event-bridge-schedule","text":"We can also schedule tasks","title":"ECS Tasks invoked by Event Bridge Schedule"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/07-ecs-solutions-architectures/#ecs-sqs-queue","text":"We can have tasks that are polling SQS queue and uses autoscaling so that the more messages we have, the more tasks we will deploy.","title":"ECS - SQS Queue"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/08-ecs-task-definitions-deep-dive/","text":"Amazon ECS - Task Definitions \u00b6 Task definitions are metadata in JSON format to tell ECS how to run a Docker container It contains crucial information, such as: Image name Port binding for Container and Host Memory and CPU required Environment Variables Networking information IAM Role Can define up to 10 containers in a task definition Load Balancing (EC2 Launch Type) \u00b6 We get a Dynamic Host Port Mapping if you define only the container port in the task definition The ALB finds the right port on your EC2 instances. You must allow on the EC2 instance's Security group any port from the ALB's Security Group Load Balancing (Fargate) \u00b6 Each task has a unique private IP Only define the container port (host port is not applicable) Example ECS ENI Security Group Allow port 0 from the ALB ALB Security Group Allow port 80/443 from web One IAM Role per Task Definition \u00b6 IAM roles are assigned per task definition. Each ECS task will asume the provided role. Environment Variables \u00b6 Environment Variable Hardcoded - e.g. URLs SSM Parameter Store - sensitive variables (e.g. API keys, shared configs) Secrets Manager - sensitive variables (e.g. DB passwords) Environment Files (bulk) - Amazon S3 Data Volumes (Bind Mounts) \u00b6 Share data between multiple containers in the same task definition Works for both EC2 and Fargate tasks EC2 tasks - using EC2 instance storage data are tied to the lifecycle of the EC2 instance Fargate tasks - using ephemeral storage Data are tied to the contain(s) using them 20 GiB - 200 GiB (default 20 GiB) Use cases: Share ephemeral data between multiple containers \"Sidecar\" container pattern, where the \"sidecar\" container used to send metrics/logs to other destinations (seperation of concerns)","title":"Amazon ECS - Task Definitions"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/08-ecs-task-definitions-deep-dive/#amazon-ecs-task-definitions","text":"Task definitions are metadata in JSON format to tell ECS how to run a Docker container It contains crucial information, such as: Image name Port binding for Container and Host Memory and CPU required Environment Variables Networking information IAM Role Can define up to 10 containers in a task definition","title":"Amazon ECS - Task Definitions"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/08-ecs-task-definitions-deep-dive/#load-balancing-ec2-launch-type","text":"We get a Dynamic Host Port Mapping if you define only the container port in the task definition The ALB finds the right port on your EC2 instances. You must allow on the EC2 instance's Security group any port from the ALB's Security Group","title":"Load Balancing (EC2 Launch Type)"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/08-ecs-task-definitions-deep-dive/#load-balancing-fargate","text":"Each task has a unique private IP Only define the container port (host port is not applicable) Example ECS ENI Security Group Allow port 0 from the ALB ALB Security Group Allow port 80/443 from web","title":"Load Balancing (Fargate)"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/08-ecs-task-definitions-deep-dive/#one-iam-role-per-task-definition","text":"IAM roles are assigned per task definition. Each ECS task will asume the provided role.","title":"One IAM Role per Task Definition"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/08-ecs-task-definitions-deep-dive/#environment-variables","text":"Environment Variable Hardcoded - e.g. URLs SSM Parameter Store - sensitive variables (e.g. API keys, shared configs) Secrets Manager - sensitive variables (e.g. DB passwords) Environment Files (bulk) - Amazon S3","title":"Environment Variables"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/08-ecs-task-definitions-deep-dive/#data-volumes-bind-mounts","text":"Share data between multiple containers in the same task definition Works for both EC2 and Fargate tasks EC2 tasks - using EC2 instance storage data are tied to the lifecycle of the EC2 instance Fargate tasks - using ephemeral storage Data are tied to the contain(s) using them 20 GiB - 200 GiB (default 20 GiB) Use cases: Share ephemeral data between multiple containers \"Sidecar\" container pattern, where the \"sidecar\" container used to send metrics/logs to other destinations (seperation of concerns)","title":"Data Volumes (Bind Mounts)"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/09-ecs-task-definitions-hands-on/","text":"Task Definitions - Hands On \u00b6 In the task definition we can define multiple containers. We can also define whether it's essential or not. It means whether the task can continue running with the container being stopped. For each container we can provide environment variables by defining them individually or by providing a file. Currently via the console you cannot specify to use Parameter Store or Secrets Manager, but you can do it via the older version of the console, JSON format or CloudFormation.","title":"Task Definitions - Hands On"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/09-ecs-task-definitions-hands-on/#task-definitions-hands-on","text":"In the task definition we can define multiple containers. We can also define whether it's essential or not. It means whether the task can continue running with the container being stopped. For each container we can provide environment variables by defining them individually or by providing a file. Currently via the console you cannot specify to use Parameter Store or Secrets Manager, but you can do it via the older version of the console, JSON format or CloudFormation.","title":"Task Definitions - Hands On"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/10-ecs-task-placements/","text":"ECS Task Placements \u00b6 When a task of thpe EC2 is launched, ECS must determine where to place it, with the constraints of CPU, memory and available port. Similarly, when a service scales in, ECS needs to determine which task to terminate. To assist with this, you can define a task placement strategy and task placement constraints. Note: this is only for ECS with EC2 not Fargate ECS Task Placement Process \u00b6 Task placement strategies are best effort When Amazon ECS places tasks, it uses the following process to select container instances: Identify the instances that satisfy the CPU, memory and port requirements in the task definition. Identify the instances that satisfy the task placement constraints. Identify the instances that satisfy the task placement strategies. Select the instance for task placement. ECS Task Placement Strategies \u00b6 Binpack \u00b6 Place tasks based on the least available amount of CPU or memory This minimizes the number of instances in use (cost savings) \"placementStrategy\": [ {\"field\": \"memory\", \"type\": \"binpack\"} ] Random \u00b6 Place the task randomly \"placementStrategy\": [ {\"type\": \"random\"} ] Spread \u00b6 Place the task evenly based on the specified value Example: instanceId, attribute:ecs.availability-zone \"placementStrategy\": [ {\"field\": \"attribute:ecs.availability-zone\", \"type\": \"spread\"} ] Mix \u00b6 You can mix the strategies together: spread by AZ and spread by instanceId spread by AZ, binpack on memory \"placementStrategy\": [ {\"field\": \"attribute:ecs.availability-zone\", \"type\": \"spread\"}, {\"field\": \"instanceId\", \"type\": \"spread\"} ] \"placementStrategy\": [ {\"field\": \"attribute:ecs.availability-zone\", \"type\": \"spread\"}, {\"field\": \"memory\", \"type\": \"binpack\"} ] Task Placement Constraints \u00b6 distinctInstance: place each task on a different container instance \"placementConstraints\": [ {\"type\": \"distinctInstance\"} ] memberOf: place task on instances that satisfy an expression uses cluster Query Language (advanced) \"placementConstraints\": [ { \"expression\": \"attribute.ecs.instance-type =~ t2.*\", \"type\": \"memberOf\" } ]","title":"ECS Task Placements"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/10-ecs-task-placements/#ecs-task-placements","text":"When a task of thpe EC2 is launched, ECS must determine where to place it, with the constraints of CPU, memory and available port. Similarly, when a service scales in, ECS needs to determine which task to terminate. To assist with this, you can define a task placement strategy and task placement constraints. Note: this is only for ECS with EC2 not Fargate","title":"ECS Task Placements"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/10-ecs-task-placements/#ecs-task-placement-process","text":"Task placement strategies are best effort When Amazon ECS places tasks, it uses the following process to select container instances: Identify the instances that satisfy the CPU, memory and port requirements in the task definition. Identify the instances that satisfy the task placement constraints. Identify the instances that satisfy the task placement strategies. Select the instance for task placement.","title":"ECS Task Placement Process"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/10-ecs-task-placements/#ecs-task-placement-strategies","text":"","title":"ECS Task Placement Strategies"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/10-ecs-task-placements/#binpack","text":"Place tasks based on the least available amount of CPU or memory This minimizes the number of instances in use (cost savings) \"placementStrategy\": [ {\"field\": \"memory\", \"type\": \"binpack\"} ]","title":"Binpack"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/10-ecs-task-placements/#random","text":"Place the task randomly \"placementStrategy\": [ {\"type\": \"random\"} ]","title":"Random"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/10-ecs-task-placements/#spread","text":"Place the task evenly based on the specified value Example: instanceId, attribute:ecs.availability-zone \"placementStrategy\": [ {\"field\": \"attribute:ecs.availability-zone\", \"type\": \"spread\"} ]","title":"Spread"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/10-ecs-task-placements/#mix","text":"You can mix the strategies together: spread by AZ and spread by instanceId spread by AZ, binpack on memory \"placementStrategy\": [ {\"field\": \"attribute:ecs.availability-zone\", \"type\": \"spread\"}, {\"field\": \"instanceId\", \"type\": \"spread\"} ] \"placementStrategy\": [ {\"field\": \"attribute:ecs.availability-zone\", \"type\": \"spread\"}, {\"field\": \"memory\", \"type\": \"binpack\"} ]","title":"Mix"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/10-ecs-task-placements/#task-placement-constraints","text":"distinctInstance: place each task on a different container instance \"placementConstraints\": [ {\"type\": \"distinctInstance\"} ] memberOf: place task on instances that satisfy an expression uses cluster Query Language (advanced) \"placementConstraints\": [ { \"expression\": \"attribute.ecs.instance-type =~ t2.*\", \"type\": \"memberOf\" } ]","title":"Task Placement Constraints"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/12-amazon-ecr-hands-on/","text":"Amazon ECR \u00b6 Using AWS CLI \u00b6 Login Command via AWS CLI v2 aws ecr get-login-password --region region | docker login --username AWS --pasword-stdin aws_account_id.dkr.ecr.region.amazonaws.com Push docker push aws_account_id.dkr.ecr.region.amazonaws.com/demo:latest Pull docker pull aws_account_id.dkr.ecr.region.amazonaws.com/demo:latest In case an EC2 instance (or you) can't pull a docker image, check IAM permissions. To create a repository we can go to Amazon ECR in the console and click on create a repository. There we have multilple options like having the repository private or public, it's name. We can also have features like: - Tag immutability (prevents image tags from being overwritten) - Automatic image scans for vulnerabilities - KMS encryption Once it's created we we can do the following things $ aws ecr get-login-password --region eu-west-1 | docker login --username AWS --pasword-stdin {account_id}.dkr.ecr.eu-west-1.amazonaws.com $ docker build -t {reponame} . $ docker tag {reponame}:latest {account_id}.dkr.ecr.eu-west-1.amazonaws.com/{reponame}:latest $ docker push {account_id}.dkr.ecr.eu-west-1.amazonaws.com/{reponame}:latest","title":"Amazon ECR"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/12-amazon-ecr-hands-on/#amazon-ecr","text":"","title":"Amazon ECR"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/12-amazon-ecr-hands-on/#using-aws-cli","text":"Login Command via AWS CLI v2 aws ecr get-login-password --region region | docker login --username AWS --pasword-stdin aws_account_id.dkr.ecr.region.amazonaws.com Push docker push aws_account_id.dkr.ecr.region.amazonaws.com/demo:latest Pull docker pull aws_account_id.dkr.ecr.region.amazonaws.com/demo:latest In case an EC2 instance (or you) can't pull a docker image, check IAM permissions. To create a repository we can go to Amazon ECR in the console and click on create a repository. There we have multilple options like having the repository private or public, it's name. We can also have features like: - Tag immutability (prevents image tags from being overwritten) - Automatic image scans for vulnerabilities - KMS encryption Once it's created we we can do the following things $ aws ecr get-login-password --region eu-west-1 | docker login --username AWS --pasword-stdin {account_id}.dkr.ecr.eu-west-1.amazonaws.com $ docker build -t {reponame} . $ docker tag {reponame}:latest {account_id}.dkr.ecr.eu-west-1.amazonaws.com/{reponame}:latest $ docker push {account_id}.dkr.ecr.eu-west-1.amazonaws.com/{reponame}:latest","title":"Using AWS CLI"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/AWS%20ECR%20%28Elastic%20Container%20Registry%29/","text":"Amazon ECR \u00b6 Stands for Elastic Container Registry Store and manage Docker images on AWS Private and Public repository (Amazon ECR Public Gallery) Fully integrated with ECS, backed by Amazon S3 Access is controlled through IAM (if there are any permission errors, take a look into policies) Supports image vulnerability scanning, versioning, image tags, image lifecycles,...","title":"Amazon ECR"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/AWS%20ECR%20%28Elastic%20Container%20Registry%29/#amazon-ecr","text":"Stands for Elastic Container Registry Store and manage Docker images on AWS Private and Public repository (Amazon ECR Public Gallery) Fully integrated with ECS, backed by Amazon S3 Access is controlled through IAM (if there are any permission errors, take a look into policies) Supports image vulnerability scanning, versioning, image tags, image lifecycles,...","title":"Amazon ECR"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/AWS%20EKS/","text":"Amazon EKS Overview \u00b6 Amazon EKS - Amazon Elastic Kubernetes Service It is a way to launch managed Kubernetes clusters on AWS Kubernetes is an open-source system for automatic deployment, scaling and management of containerized (usually Docker) applications. It's an alternative to ECS, similar goal but different API EKS supports EC2 if you want to deploy worker nodes or Fargate to deploy serverless containers Use case: if your company already is using Kubernetes on-premises or in another cloud, and wants to migrate to AWS using Kubernetes Kubernetes is cloud-agnostic (can be used in any cloud - Azure, GCP...)","title":"Amazon EKS Overview"},{"location":"AWS/developer-associate/14-ecs-ecr-fargate-docker-in-aws/AWS%20EKS/#amazon-eks-overview","text":"Amazon EKS - Amazon Elastic Kubernetes Service It is a way to launch managed Kubernetes clusters on AWS Kubernetes is an open-source system for automatic deployment, scaling and management of containerized (usually Docker) applications. It's an alternative to ECS, similar goal but different API EKS supports EC2 if you want to deploy worker nodes or Fargate to deploy serverless containers Use case: if your company already is using Kubernetes on-premises or in another cloud, and wants to migrate to AWS using Kubernetes Kubernetes is cloud-agnostic (can be used in any cloud - Azure, GCP...)","title":"Amazon EKS Overview"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/01-elastic-beanstalk-overview/","text":"Elastic Beanstalk Overview \u00b6 Developer problems on AWS Managing infrastructure Deploying Code Configuring all the databases, load balancers etc Scaling concerns Most web apps have the same architecture (ALB+ASG) All the developers want is for their code to run! Possibly, consistently across different applications and environments Elastic Beanstalk \u00b6 Elastic Beanstalk is a developer centric view of deploying an application on AWS It uses all the components we've seen before : EC2, ASG, ELB, RDS, ... Managed service Automatically handles capacity provisioning, load balancing, scaling, application health monitoring, instance configuration,... Just the application code is the responsibility of the developer We still have full control over the configuration Beanstalk is free but you pay for the underlying instances Components \u00b6 Application: collection of Elastic Beanstalk components (environments, versions, configurations ...) Application Version: an iteration of your application code Environment Collection of AWS resources running an application version (only one application version at a time) Tiers: Web Server Environment Tier & Worker Environment Tier You can create multiple environments (dev, test, prod) Supported platforms \u00b6 Go Java SE Java with Tomcat .Net Core on Linux .Net on Windows Server Node.js PHP Python Ruby Packer Builder Single Container Docker Multi-container Docker Preconfigured Docker If not supported, you can write your custom platform (advanced) Web Server Tier vs Worker Tier \u00b6 Scale based on the number of SQS messages Can push messages to SQS queue from another Web Server Tier","title":"Elastic Beanstalk Overview"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/01-elastic-beanstalk-overview/#elastic-beanstalk-overview","text":"Developer problems on AWS Managing infrastructure Deploying Code Configuring all the databases, load balancers etc Scaling concerns Most web apps have the same architecture (ALB+ASG) All the developers want is for their code to run! Possibly, consistently across different applications and environments","title":"Elastic Beanstalk Overview"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/01-elastic-beanstalk-overview/#elastic-beanstalk","text":"Elastic Beanstalk is a developer centric view of deploying an application on AWS It uses all the components we've seen before : EC2, ASG, ELB, RDS, ... Managed service Automatically handles capacity provisioning, load balancing, scaling, application health monitoring, instance configuration,... Just the application code is the responsibility of the developer We still have full control over the configuration Beanstalk is free but you pay for the underlying instances","title":"Elastic Beanstalk"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/01-elastic-beanstalk-overview/#components","text":"Application: collection of Elastic Beanstalk components (environments, versions, configurations ...) Application Version: an iteration of your application code Environment Collection of AWS resources running an application version (only one application version at a time) Tiers: Web Server Environment Tier & Worker Environment Tier You can create multiple environments (dev, test, prod)","title":"Components"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/01-elastic-beanstalk-overview/#supported-platforms","text":"Go Java SE Java with Tomcat .Net Core on Linux .Net on Windows Server Node.js PHP Python Ruby Packer Builder Single Container Docker Multi-container Docker Preconfigured Docker If not supported, you can write your custom platform (advanced)","title":"Supported platforms"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/01-elastic-beanstalk-overview/#web-server-tier-vs-worker-tier","text":"Scale based on the number of SQS messages Can push messages to SQS queue from another Web Server Tier","title":"Web Server Tier vs Worker Tier"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/02-beanstalk-first-environment/","text":"Beanstalk First Environment Hands On \u00b6 We are going to create our first elastic beanstalk environment. The creation process will take a bit of time, but once it's ready. You can open it up and there will be following options: - Events - This will list all of the events that have happened in your environment. - Configuration - We will see it in-depth in the upcoming lessons - Logs - You can pull log files from all the instances - Health - Shows the health of your environment - Monitoring - Shows metrics for the environment - Alarms - Managed Updates - Tags We have a concept of applications and environments. An application has multiple environments.","title":"Beanstalk First Environment Hands On"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/02-beanstalk-first-environment/#beanstalk-first-environment-hands-on","text":"We are going to create our first elastic beanstalk environment. The creation process will take a bit of time, but once it's ready. You can open it up and there will be following options: - Events - This will list all of the events that have happened in your environment. - Configuration - We will see it in-depth in the upcoming lessons - Logs - You can pull log files from all the instances - Health - Shows the health of your environment - Monitoring - Shows metrics for the environment - Alarms - Managed Updates - Tags We have a concept of applications and environments. An application has multiple environments.","title":"Beanstalk First Environment Hands On"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/03-beanstalk-second-environment/","text":"Creating a second environment \u00b6 If we were to create new environment, it will ask us to choose the tier we want to use: Web Server environment Worker environment While creating, we are are going to use the Configure more options . Here, we'll have multiple presets available: Also, there will be an option to configure everything related to the beanstalk We can configure: - Software - X-Ray daemon - S3 log storage - Instance log streaming to CloudWatch logs - Environment Properties - Instances: - Root volume type - Security groups - Capacity - Single / Load balanced environment type - Auto Scaling Group configuration - instance Type - AZs - Placement - Scaling triggers - Load Balancer - Load balancer type (Application LB, Classic LB, network LB) - Listeners - Rules - Access Log file - You cannot change the load balancer once the environment is created - Rolling Updates & Deployments - Security - Service role - Keypairs - Monitoring - Managed updates - Notifications - Networking - Databases - if you create an RDS through this, when deleting the environment it will delete it as well. - Tags","title":"Creating a second environment"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/03-beanstalk-second-environment/#creating-a-second-environment","text":"If we were to create new environment, it will ask us to choose the tier we want to use: Web Server environment Worker environment While creating, we are are going to use the Configure more options . Here, we'll have multiple presets available: Also, there will be an option to configure everything related to the beanstalk We can configure: - Software - X-Ray daemon - S3 log storage - Instance log streaming to CloudWatch logs - Environment Properties - Instances: - Root volume type - Security groups - Capacity - Single / Load balanced environment type - Auto Scaling Group configuration - instance Type - AZs - Placement - Scaling triggers - Load Balancer - Load balancer type (Application LB, Classic LB, network LB) - Listeners - Rules - Access Log file - You cannot change the load balancer once the environment is created - Rolling Updates & Deployments - Security - Service role - Keypairs - Monitoring - Managed updates - Notifications - Networking - Databases - if you create an RDS through this, when deleting the environment it will delete it as well. - Tags","title":"Creating a second environment"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/04-beanstalk-deployment-modes/","text":"Elastic Beanstalk Deployment Modes \u00b6 Single Instance Great for development High Availability with Load Balancer Great for production Deployment Options for Updates \u00b6 All at once (deploying all in one go) - fastest, but instances aren't available to serve traffic for a bit (downtime) Rolling: update a few instances at a time (bucket), and then move onto the next bucket once the first bucket is healthy Rolling with additional batches: like rolling, but spins up new instances to move the batch (so that the old application is still available) Immutable: spins up new instances in a new ASG, deploys version to these instances, and then swaps all the instances when everything is healthy Blue / Green \u00b6 Not a direct feature of Elastic Beanstalk Zero downtime and realease facility Create a new stage environment and deploy v2 there The new environment (green) can be validated independently and roll back if there are issues Route 53 can be setup using weighted policies to redirect a little bit of traffic to the stage environment Using Beanstalk, \"swap URLs\" when done with the environment test Canary testing \u00b6 New application version is deployed to a temporary ASG with the same capacity A small % of the traffic is sent to the temporary ASG for configurable amount of time Deployment health is monitored If there's a deployment failure, this triggers and automated rollback. No application downtime New instances are migrated from the temporary to the original ASG Old application version is teminated Deployment Summary \u00b6 https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html","title":"Elastic Beanstalk Deployment Modes"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/04-beanstalk-deployment-modes/#elastic-beanstalk-deployment-modes","text":"Single Instance Great for development High Availability with Load Balancer Great for production","title":"Elastic Beanstalk Deployment Modes"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/04-beanstalk-deployment-modes/#deployment-options-for-updates","text":"All at once (deploying all in one go) - fastest, but instances aren't available to serve traffic for a bit (downtime) Rolling: update a few instances at a time (bucket), and then move onto the next bucket once the first bucket is healthy Rolling with additional batches: like rolling, but spins up new instances to move the batch (so that the old application is still available) Immutable: spins up new instances in a new ASG, deploys version to these instances, and then swaps all the instances when everything is healthy","title":"Deployment Options for Updates"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/04-beanstalk-deployment-modes/#blue-green","text":"Not a direct feature of Elastic Beanstalk Zero downtime and realease facility Create a new stage environment and deploy v2 there The new environment (green) can be validated independently and roll back if there are issues Route 53 can be setup using weighted policies to redirect a little bit of traffic to the stage environment Using Beanstalk, \"swap URLs\" when done with the environment test","title":"Blue / Green"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/04-beanstalk-deployment-modes/#canary-testing","text":"New application version is deployed to a temporary ASG with the same capacity A small % of the traffic is sent to the temporary ASG for configurable amount of time Deployment health is monitored If there's a deployment failure, this triggers and automated rollback. No application downtime New instances are migrated from the temporary to the original ASG Old application version is teminated","title":"Canary testing"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/04-beanstalk-deployment-modes/#deployment-summary","text":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html","title":"Deployment Summary"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/05-beanstalk-cli-and-deployment-process/","text":"Elastic Beanstalk CLI \u00b6 We can install an additional CLI called the \"EB cli\" which makes working with Beanstalk from the CLI easier. Basic commands are: eb create eb status eb health eb events eb logs eb open eb deploy eb config eb terminate It's helpful for your automated deployment pipelines! Elastic Beanstalk Deployment Process \u00b6 Describe dependencies (requirements.txt for python, package.json for Node.js) Package code as zip, and describe dependencies Python: requirements.txt Node.js: package.json Console: upload zip file (creates new app version), and then deploy CLI: create new app version using CLI (uploads zip), then deploy Elastic Beanstalk will deploy the zip on each EC2 instance, resolve dependencies and start the application","title":"Elastic Beanstalk CLI"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/05-beanstalk-cli-and-deployment-process/#elastic-beanstalk-cli","text":"We can install an additional CLI called the \"EB cli\" which makes working with Beanstalk from the CLI easier. Basic commands are: eb create eb status eb health eb events eb logs eb open eb deploy eb config eb terminate It's helpful for your automated deployment pipelines!","title":"Elastic Beanstalk CLI"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/05-beanstalk-cli-and-deployment-process/#elastic-beanstalk-deployment-process","text":"Describe dependencies (requirements.txt for python, package.json for Node.js) Package code as zip, and describe dependencies Python: requirements.txt Node.js: package.json Console: upload zip file (creates new app version), and then deploy CLI: create new app version using CLI (uploads zip), then deploy Elastic Beanstalk will deploy the zip on each EC2 instance, resolve dependencies and start the application","title":"Elastic Beanstalk Deployment Process"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/06-beanstalk-lifecycle-policy/","text":"Beanstalk lifecycle policy \u00b6 Elastic Beanstalk can store at most 1000 application versions If you don't remove old versions you won't be able to deploy anymore To phase out old application versions, use a lifecycle policy Based on time (old versions are removed) Based on space (when you have too many versions) Versions that are currently used won't be deleted Option not to delete the source bundle in S3 to prevent data loss","title":"Beanstalk lifecycle policy"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/06-beanstalk-lifecycle-policy/#beanstalk-lifecycle-policy","text":"Elastic Beanstalk can store at most 1000 application versions If you don't remove old versions you won't be able to deploy anymore To phase out old application versions, use a lifecycle policy Based on time (old versions are removed) Based on space (when you have too many versions) Versions that are currently used won't be deleted Option not to delete the source bundle in S3 to prevent data loss","title":"Beanstalk lifecycle policy"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/07-beanstalk-extensions/","text":"Elastic Beanstalk Extensions \u00b6 A zip file containing our code must be deployed to Elastic Beanstalk All parameters set in the UI can be configured with code using files Requirements: in the .ebextensions/ directory in the root of source code YAML / JSON format .config extensions (example logging.config) Able to modify some default settings using option_settings Ability to add resources such as RDS, ElastiCache, DynamoDB, etc Resources managed by .ebextensions get deleted if the environment goes away. We can add an .ebextensions/environment-variables.config file: # You must place this file in .ebextensions # And must have a .config file name # So the file is at .ebextensions/environment-variables.config # Note: Even though the markup language is YAML, you must use .config extension option_settings: # see: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/command-options-general.html#command-options-general-elasticbeanstalkapplicationenvironment aws:elasticbeanstalk:application:environment: DB_URL: \"jdbc:postgresql://rds-url-here.com/db\" DB_USER: username # This format works too: # option_settings: # - namespace: namespace # option_name: option_name # value: option value # See: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions-optionsettings.html","title":"Elastic Beanstalk Extensions"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/07-beanstalk-extensions/#elastic-beanstalk-extensions","text":"A zip file containing our code must be deployed to Elastic Beanstalk All parameters set in the UI can be configured with code using files Requirements: in the .ebextensions/ directory in the root of source code YAML / JSON format .config extensions (example logging.config) Able to modify some default settings using option_settings Ability to add resources such as RDS, ElastiCache, DynamoDB, etc Resources managed by .ebextensions get deleted if the environment goes away. We can add an .ebextensions/environment-variables.config file: # You must place this file in .ebextensions # And must have a .config file name # So the file is at .ebextensions/environment-variables.config # Note: Even though the markup language is YAML, you must use .config extension option_settings: # see: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/command-options-general.html#command-options-general-elasticbeanstalkapplicationenvironment aws:elasticbeanstalk:application:environment: DB_URL: \"jdbc:postgresql://rds-url-here.com/db\" DB_USER: username # This format works too: # option_settings: # - namespace: namespace # option_name: option_name # value: option value # See: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions-optionsettings.html","title":"Elastic Beanstalk Extensions"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/08-beanstalk-cloudformation/","text":"Beanstalk & CloudFormation \u00b6 Under the hood, Elastic Beanstalk relies on CloudFormation CloudFormation is used to provision other AWS Services Use Case: you can define CloudFormation resources in your .ebextensions to provision ElastiCache, an S3 Bucket, anything you want!","title":"Beanstalk & CloudFormation"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/08-beanstalk-cloudformation/#beanstalk-cloudformation","text":"Under the hood, Elastic Beanstalk relies on CloudFormation CloudFormation is used to provision other AWS Services Use Case: you can define CloudFormation resources in your .ebextensions to provision ElastiCache, an S3 Bucket, anything you want!","title":"Beanstalk &amp; CloudFormation"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/09-beanstalk-cloning/","text":"Elastic Beanstalk Cloning \u00b6 Clone an environment with the exact same configuration Useful for deploying a \"test\" version of your application All resources and configuration are preserved: Load Balancer type and configuration RDS database type (but the data is not preserved) Environment variables After cloning an environment, you can change settings","title":"Elastic Beanstalk Cloning"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/09-beanstalk-cloning/#elastic-beanstalk-cloning","text":"Clone an environment with the exact same configuration Useful for deploying a \"test\" version of your application All resources and configuration are preserved: Load Balancer type and configuration RDS database type (but the data is not preserved) Environment variables After cloning an environment, you can change settings","title":"Elastic Beanstalk Cloning"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/10-beanstalk-migrations/","text":"Beanstalk Migrations \u00b6 After creating an Elastic Beanstalk environment, you cannot change the Elastic Load Balancer type (only the configuration). To migrate: Create a new environment with the same configuration except LB (can't clone) Deploy your application onto the new environment Perform a CNAME swap or Route 53 update RDS with Elastic Beanstalk \u00b6 RDS can be provisioned with Beanstalk which is great for dev / test This is not great for production as the database lifecycle is tied to the beanstalk environment lifecycle The best for prod is to separately create an RDS database and provide our EB application with the connection string Decouple RDS \u00b6 Create a snapshot of RDS DB (as a safeguard) Go to the RDS console and protect the RDS database from deletion Create a new Elastic Beanstalk environment, without RDS, point your application to existing RDS Perform a CNAME swap (blue/green) or route 53 update, confirm working Terminate the old environment (RDS won't be deleted) Delete CloudFormation stack in DELETE_FAILED state","title":"Beanstalk Migrations"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/10-beanstalk-migrations/#beanstalk-migrations","text":"After creating an Elastic Beanstalk environment, you cannot change the Elastic Load Balancer type (only the configuration). To migrate: Create a new environment with the same configuration except LB (can't clone) Deploy your application onto the new environment Perform a CNAME swap or Route 53 update","title":"Beanstalk Migrations"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/10-beanstalk-migrations/#rds-with-elastic-beanstalk","text":"RDS can be provisioned with Beanstalk which is great for dev / test This is not great for production as the database lifecycle is tied to the beanstalk environment lifecycle The best for prod is to separately create an RDS database and provide our EB application with the connection string","title":"RDS with Elastic Beanstalk"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/10-beanstalk-migrations/#decouple-rds","text":"Create a snapshot of RDS DB (as a safeguard) Go to the RDS console and protect the RDS database from deletion Create a new Elastic Beanstalk environment, without RDS, point your application to existing RDS Perform a CNAME swap (blue/green) or route 53 update, confirm working Terminate the old environment (RDS won't be deleted) Delete CloudFormation stack in DELETE_FAILED state","title":"Decouple RDS"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/11-beanstalk-with-docker/","text":"Beanstalk with Docker \u00b6 Single DOcker \u00b6 Run your application as a single docker container Either provide: Dockerfile: Elastic Beanstalk will build and run the Docker container Dockerrun.aws.json (v1): Describe where already built docker image is: Image Ports Volumes Logging Etc Beanstalk in Single Docker Container does not use ECS Multi Docker Container \u00b6 Multi Docker helps run multiple containers per EC2 instance in EB This will create for you: ECS CLuster EC2 Instance, configured to use the ECS Cluster Load Balancer (in high availability mode) Task definitions and execution Requires a config Dockerrun.aws.json (v2) at the root of source code Dockerrun.aws.json is used to generate the ECS task definition Your Docker images must be pre-built and stored in ECR for example We can find sample configurations at: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/tutorials.html","title":"Beanstalk with Docker"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/11-beanstalk-with-docker/#beanstalk-with-docker","text":"","title":"Beanstalk with Docker"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/11-beanstalk-with-docker/#single-docker","text":"Run your application as a single docker container Either provide: Dockerfile: Elastic Beanstalk will build and run the Docker container Dockerrun.aws.json (v1): Describe where already built docker image is: Image Ports Volumes Logging Etc Beanstalk in Single Docker Container does not use ECS","title":"Single DOcker"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/11-beanstalk-with-docker/#multi-docker-container","text":"Multi Docker helps run multiple containers per EC2 instance in EB This will create for you: ECS CLuster EC2 Instance, configured to use the ECS Cluster Load Balancer (in high availability mode) Task definitions and execution Requires a config Dockerrun.aws.json (v2) at the root of source code Dockerrun.aws.json is used to generate the ECS task definition Your Docker images must be pre-built and stored in ECR for example We can find sample configurations at: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/tutorials.html","title":"Multi Docker Container"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/12-beanstalk-advanced-concepts/","text":"Beanstalk Advanced Concepts \u00b6 Beanstalk with HTTPS \u00b6 Idea: Load the SSL certificate onto the Load Balancer Can be done from the Console (EB Console, load balancer configuration) Can be done from the code: .ebextensions/securelistener-alb.config SSL Certificate can be provisioned using ACM (AWS Certificate Manager) or CLI Must configure a security group rule to allow incoming port 443 (HTTPS port) Beanstalk redirect HTTP to HTTPS Configure your instances to redirect HTTP to HTTPS: https://github.com/awsdocs/elastic-beanstalk-samples/tree/master/configuration-files/aws-provided/security-configuration/https-redirect OR configure the Application Load Balancer (ALB only) with a rule Make sure health checks are not redirected (so they keep giving 200 OK) Web Server vs Worker Environment \u00b6 If your application performs tasks that are long to complete, offload these tasks to a dedicated worker environment Decoupling your application into two tiers is common Example: processing a video, generating a zip file, etc You can define periodic tasks in a file cron.yaml Custom Platform (Advanced) \u00b6 Custom Platforms are very advanced, they allow to define from scratch: The Operating System (OS) Additional Software Scripts that Beanstalk runs on these platforms Use case: app language is incompatible with Beanstalk & doesn't use Docker To create your own platform: Define an AMI using Platform.yaml file Build that platform using the Packer software (open source tool to create AMIs) Custom Platform vs Custom Image (AMI) Custom Image is to tweak an existing Beanstalk Platform (python, node.js, java) Custom platform is to create an entirely new Beanstalk Platform","title":"Beanstalk Advanced Concepts"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/12-beanstalk-advanced-concepts/#beanstalk-advanced-concepts","text":"","title":"Beanstalk Advanced Concepts"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/12-beanstalk-advanced-concepts/#beanstalk-with-https","text":"Idea: Load the SSL certificate onto the Load Balancer Can be done from the Console (EB Console, load balancer configuration) Can be done from the code: .ebextensions/securelistener-alb.config SSL Certificate can be provisioned using ACM (AWS Certificate Manager) or CLI Must configure a security group rule to allow incoming port 443 (HTTPS port) Beanstalk redirect HTTP to HTTPS Configure your instances to redirect HTTP to HTTPS: https://github.com/awsdocs/elastic-beanstalk-samples/tree/master/configuration-files/aws-provided/security-configuration/https-redirect OR configure the Application Load Balancer (ALB only) with a rule Make sure health checks are not redirected (so they keep giving 200 OK)","title":"Beanstalk with HTTPS"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/12-beanstalk-advanced-concepts/#web-server-vs-worker-environment","text":"If your application performs tasks that are long to complete, offload these tasks to a dedicated worker environment Decoupling your application into two tiers is common Example: processing a video, generating a zip file, etc You can define periodic tasks in a file cron.yaml","title":"Web Server vs Worker Environment"},{"location":"AWS/developer-associate/15-aws-elastic-beanstalk/12-beanstalk-advanced-concepts/#custom-platform-advanced","text":"Custom Platforms are very advanced, they allow to define from scratch: The Operating System (OS) Additional Software Scripts that Beanstalk runs on these platforms Use case: app language is incompatible with Beanstalk & doesn't use Docker To create your own platform: Define an AMI using Platform.yaml file Build that platform using the Packer software (open source tool to create AMIs) Custom Platform vs Custom Image (AMI) Custom Image is to tweak an existing Beanstalk Platform (python, node.js, java) Custom platform is to create an entirely new Beanstalk Platform","title":"Custom Platform (Advanced)"},{"location":"AWS/developer-associate/16-aws-cicd/01-introduction-to-cicd/","text":"CICD - Introduction \u00b6 We have learned how to: Create AWS resources, manually (fundamentals) Interact with AWS programmatically (AWS CLI) Deploy code to AWS using Elastic Beanstalk All these manual steps make it very likely for us to do mistakes. We would like our code \"in a repository\" and have it deployed onto AWS Automatically The right way Making sure it's tested before deployed With possibility to go into different stages (dev, test, staging, prod) With manual approval where needed To be a proper AWS developer - we need to learn AWS CICD This section is all about automating the deployment we've done so far while adding increased safety We'll learn about: AWS CodeCommit - storing our code AWS CodePipeline - automating our pipeline from code to Elastic Beanstalk AWS CodeBuild - building and testing our code AWS CodeDeploy - deploying the code to EC2 instances (not ElasticBeanstalk) AWS CodeStar - manage software development activities in one place AWS CodeArtifact - store, publish and share software packages AWS CodeGuru - automated code reviews using Machine Learning Continuous Integration (CI) \u00b6 Developers push the code to a code repository often (e.g. GitHub, CodeCommit, BitBucket...) A testing / build server checks the code as soon as it's published (CodeBuild, Jenkins CI, ...) The developer gets feedback about the tests and checks that have passed / failed Find bugs early, then fix bugs Deliver faster as the code is tested Deploy often Happier developers, as they're unblocked Continous Deliver (CD) \u00b6 Ensures that the software can be released reliably whenever needed Ensures deployments happen often and are quick Shift away from \"one release every 3 months\" to \"5 releases a day\" That usually means automated deployment (e.g. CodeDeploy, Jenkins CD, Spinnaker, ...) Technology Stack for CICD \u00b6","title":"CICD - Introduction"},{"location":"AWS/developer-associate/16-aws-cicd/01-introduction-to-cicd/#cicd-introduction","text":"We have learned how to: Create AWS resources, manually (fundamentals) Interact with AWS programmatically (AWS CLI) Deploy code to AWS using Elastic Beanstalk All these manual steps make it very likely for us to do mistakes. We would like our code \"in a repository\" and have it deployed onto AWS Automatically The right way Making sure it's tested before deployed With possibility to go into different stages (dev, test, staging, prod) With manual approval where needed To be a proper AWS developer - we need to learn AWS CICD This section is all about automating the deployment we've done so far while adding increased safety We'll learn about: AWS CodeCommit - storing our code AWS CodePipeline - automating our pipeline from code to Elastic Beanstalk AWS CodeBuild - building and testing our code AWS CodeDeploy - deploying the code to EC2 instances (not ElasticBeanstalk) AWS CodeStar - manage software development activities in one place AWS CodeArtifact - store, publish and share software packages AWS CodeGuru - automated code reviews using Machine Learning","title":"CICD - Introduction"},{"location":"AWS/developer-associate/16-aws-cicd/01-introduction-to-cicd/#continuous-integration-ci","text":"Developers push the code to a code repository often (e.g. GitHub, CodeCommit, BitBucket...) A testing / build server checks the code as soon as it's published (CodeBuild, Jenkins CI, ...) The developer gets feedback about the tests and checks that have passed / failed Find bugs early, then fix bugs Deliver faster as the code is tested Deploy often Happier developers, as they're unblocked","title":"Continuous Integration (CI)"},{"location":"AWS/developer-associate/16-aws-cicd/01-introduction-to-cicd/#continous-deliver-cd","text":"Ensures that the software can be released reliably whenever needed Ensures deployments happen often and are quick Shift away from \"one release every 3 months\" to \"5 releases a day\" That usually means automated deployment (e.g. CodeDeploy, Jenkins CD, Spinnaker, ...)","title":"Continous Deliver (CD)"},{"location":"AWS/developer-associate/16-aws-cicd/01-introduction-to-cicd/#technology-stack-for-cicd","text":"","title":"Technology Stack for CICD"},{"location":"AWS/developer-associate/16-aws-cicd/03-codecommit-hands-on-part-1/","text":"CodeCommit Hands On Part 1 \u00b6 We can access the CodeCommit in our Console When opening, we can access the CodeCommit, CodeArtifact, CodeBuild, CodeDeploy and CodePipeline from the sidebar. We can create a repository When created, you'll see the commands for cloning the repository. If you don't see the SSH option - you're using the root user. In it you can do all the standard git things, commit files into branches, create merge requests etc. In the settings you can create notifications on specific events that will trigger either SNS (email etc) or AWS Chatbot (slack).","title":"CodeCommit Hands On Part 1"},{"location":"AWS/developer-associate/16-aws-cicd/03-codecommit-hands-on-part-1/#codecommit-hands-on-part-1","text":"We can access the CodeCommit in our Console When opening, we can access the CodeCommit, CodeArtifact, CodeBuild, CodeDeploy and CodePipeline from the sidebar. We can create a repository When created, you'll see the commands for cloning the repository. If you don't see the SSH option - you're using the root user. In it you can do all the standard git things, commit files into branches, create merge requests etc. In the settings you can create notifications on specific events that will trigger either SNS (email etc) or AWS Chatbot (slack).","title":"CodeCommit Hands On Part 1"},{"location":"AWS/developer-associate/16-aws-cicd/04-codecommit-hands-on-part-2/","text":"CodeCommit Hands On Part 2 \u00b6 In order to push files from our computer we need to set up the credentials. That can be done by going into IAM -> Security Credentials in your AWS Console. There is a tab for creating SSH keys and HTTPS credentials for the IAM account.","title":"CodeCommit Hands On Part 2"},{"location":"AWS/developer-associate/16-aws-cicd/04-codecommit-hands-on-part-2/#codecommit-hands-on-part-2","text":"In order to push files from our computer we need to set up the credentials. That can be done by going into IAM -> Security Credentials in your AWS Console. There is a tab for creating SSH keys and HTTPS credentials for the IAM account.","title":"CodeCommit Hands On Part 2"},{"location":"AWS/developer-associate/16-aws-cicd/06-codepipeline-hands-on/","text":"CodePipeline Hands On \u00b6 We are going to create a new CodePipeline Then selecting the change detection options the AWS CodePipeline option isn't really recommended as it will periodically check for changes and it can take time until it's picked up. When the pipeline is created you can create your own stages and in them you can have multiple action groups. The actions in an action group will be done in parallel, the next action group will be sequential.","title":"CodePipeline Hands On"},{"location":"AWS/developer-associate/16-aws-cicd/06-codepipeline-hands-on/#codepipeline-hands-on","text":"We are going to create a new CodePipeline Then selecting the change detection options the AWS CodePipeline option isn't really recommended as it will periodically check for changes and it can take time until it's picked up. When the pipeline is created you can create your own stages and in them you can have multiple action groups. The actions in an action group will be done in parallel, the next action group will be sequential.","title":"CodePipeline Hands On"},{"location":"AWS/developer-associate/16-aws-cicd/08-codebuild-hands-on-part-1/","text":"CodeBuild Hands On Part 1 \u00b6 When creating a CodeBuild project we have a ton of things to configure. Here we can enable badges, restrict concurrency as wel ass add tags. Next, we can set one or multiple sources. Then we can choose whether to use a AWS managed docker image for the CodeBuild or we can create our own. We can configure timeouts, certificates, VPC, compute scale, environment variables, file systems. Choose how to define the build commands. Allow batching Define artifacts Configure logging Once created, if the buildspec.yml isn't in the root directory, it should fail.","title":"CodeBuild Hands On Part 1"},{"location":"AWS/developer-associate/16-aws-cicd/08-codebuild-hands-on-part-1/#codebuild-hands-on-part-1","text":"When creating a CodeBuild project we have a ton of things to configure. Here we can enable badges, restrict concurrency as wel ass add tags. Next, we can set one or multiple sources. Then we can choose whether to use a AWS managed docker image for the CodeBuild or we can create our own. We can configure timeouts, certificates, VPC, compute scale, environment variables, file systems. Choose how to define the build commands. Allow batching Define artifacts Configure logging Once created, if the buildspec.yml isn't in the root directory, it should fail.","title":"CodeBuild Hands On Part 1"},{"location":"AWS/developer-associate/16-aws-cicd/09-codebuild-hands-on-part-2/","text":"CodeBuild Hands On Part 2 \u00b6 We can create the buildspec.yml version: 0.2 phases: install: runtime-versions: nodejs: 10 commands: - echo \"Installing something\" pre_build: commands: - echo \"We are in the pre build phase\" build: commands: - echo \"we are in the build block\" - echo \"we will run some tests\" - echo grep -Fq \"Congratulations\" index.html post_build: commands: - echo \"we are in the post build phase\" Once added, the build should succeed. Then, we can add it to our CodePipeline:","title":"CodeBuild Hands On Part 2"},{"location":"AWS/developer-associate/16-aws-cicd/09-codebuild-hands-on-part-2/#codebuild-hands-on-part-2","text":"We can create the buildspec.yml version: 0.2 phases: install: runtime-versions: nodejs: 10 commands: - echo \"Installing something\" pre_build: commands: - echo \"We are in the pre build phase\" build: commands: - echo \"we are in the build block\" - echo \"we will run some tests\" - echo grep -Fq \"Congratulations\" index.html post_build: commands: - echo \"we are in the post build phase\" Once added, the build should succeed. Then, we can add it to our CodePipeline:","title":"CodeBuild Hands On Part 2"},{"location":"AWS/developer-associate/16-aws-cicd/11-codedeploy-hands-on/","text":"CodeDeploy Hands On \u00b6 Before we start using CodeDeploy we need to create 2 new IAM roles: A service role When creating a role, we can choose an AWS Service as CodeDeploy and it will allow us to choose common use cases: EC2 Service Role Our EC2 instance must be able to pull the data from S3, so we need to add a role for that as well. AmazonS3ReadOnlyAccess permission is sufficient for that role. Now we can create an CodeDeploy application. Then we can create an EC2 instance we can deploy to. Make sure it has the EC2 Service role. We'll also allow access from port 80. Now we can ssh into the instance and run following commands: $ sudo yum update $ sudo yum install ruby $ wget https://aws-codedeploy-eu-west-3.s3.eu-west-3.amazonaws.com/latest/install $ chmod +x ./install $ sudo ./install auto $ sudo service codedeploy-agent status Once the EC2 is set up we need to create a deployment group in the CodeDeploy application. Their basically a set of EC2 instances, for example, that you are going to deploy to. Like dev instances, production instances etc. We can attach tags to our instances: Now we fill in the deployment group form: Now we can create a new deployment. We can create a new S3 bucket and modify the buildspec.yml to upload the files to that S3 bucket.","title":"CodeDeploy Hands On"},{"location":"AWS/developer-associate/16-aws-cicd/11-codedeploy-hands-on/#codedeploy-hands-on","text":"Before we start using CodeDeploy we need to create 2 new IAM roles: A service role When creating a role, we can choose an AWS Service as CodeDeploy and it will allow us to choose common use cases: EC2 Service Role Our EC2 instance must be able to pull the data from S3, so we need to add a role for that as well. AmazonS3ReadOnlyAccess permission is sufficient for that role. Now we can create an CodeDeploy application. Then we can create an EC2 instance we can deploy to. Make sure it has the EC2 Service role. We'll also allow access from port 80. Now we can ssh into the instance and run following commands: $ sudo yum update $ sudo yum install ruby $ wget https://aws-codedeploy-eu-west-3.s3.eu-west-3.amazonaws.com/latest/install $ chmod +x ./install $ sudo ./install auto $ sudo service codedeploy-agent status Once the EC2 is set up we need to create a deployment group in the CodeDeploy application. Their basically a set of EC2 instances, for example, that you are going to deploy to. Like dev instances, production instances etc. We can attach tags to our instances: Now we fill in the deployment group form: Now we can create a new deployment. We can create a new S3 bucket and modify the buildspec.yml to upload the files to that S3 bucket.","title":"CodeDeploy Hands On"},{"location":"AWS/developer-associate/16-aws-cicd/12-codedeploy-for-ec2-and-asg/","text":"CodeDeploy for EC2 and ASG \u00b6 Define how to deploy the application using appspec.yml + Deployment Strategy Will do In-place update to your fleet of EC2 instances Can use hooks to verify the deployment after each deployment phase Deploy to an ASG \u00b6 In-place Deployment Updates existing EC2 instances Newly created EC2 instances by an ASG will also get automated deployments Blue / Green Deployment A new Auto-Scaling Group is created (settings are copied) Choose how long to keep the old EC2 instances (old ASG) Must be using an ELB Redeploy & Rollbacks \u00b6 Rollback = redeploy a previously deployed revision of your application Deployments can be rolled back: Automatically - rollback when a deployment fails or rollback when a CloudWatch Alarm thresholds are met Manually Disable Rollbacks - do not perform rollbacks for this deployment If a roll back happens, CodeDeploy redeploys the last known good revision as a new deployment (not a restored version)","title":"CodeDeploy for EC2 and ASG"},{"location":"AWS/developer-associate/16-aws-cicd/12-codedeploy-for-ec2-and-asg/#codedeploy-for-ec2-and-asg","text":"Define how to deploy the application using appspec.yml + Deployment Strategy Will do In-place update to your fleet of EC2 instances Can use hooks to verify the deployment after each deployment phase","title":"CodeDeploy for EC2 and ASG"},{"location":"AWS/developer-associate/16-aws-cicd/12-codedeploy-for-ec2-and-asg/#deploy-to-an-asg","text":"In-place Deployment Updates existing EC2 instances Newly created EC2 instances by an ASG will also get automated deployments Blue / Green Deployment A new Auto-Scaling Group is created (settings are copied) Choose how long to keep the old EC2 instances (old ASG) Must be using an ELB","title":"Deploy to an ASG"},{"location":"AWS/developer-associate/16-aws-cicd/12-codedeploy-for-ec2-and-asg/#redeploy-rollbacks","text":"Rollback = redeploy a previously deployed revision of your application Deployments can be rolled back: Automatically - rollback when a deployment fails or rollback when a CloudWatch Alarm thresholds are met Manually Disable Rollbacks - do not perform rollbacks for this deployment If a roll back happens, CodeDeploy redeploys the last known good revision as a new deployment (not a restored version)","title":"Redeploy &amp; Rollbacks"},{"location":"AWS/developer-associate/16-aws-cicd/13-codestar-overview/","text":"CodeStar Overview \u00b6 An integrated solution that groups: Github, CodeCommit, CodeBuild, CodeDeploy, CloudFormation, CodePipeline, CloudWatch, ... Quickly create \"CICD-ready\" projects for EC2, Lambda, Elastic Beanstalk Supported languages: C#, Go, HTML5, Java, Node.js, PHP, Python, Ruby Issue tracking integration with JIRA / GitHub issues Ability to integrate with Cloud9 to obtain a web IDE (not all regions) One dashboard to view all your components Free service, pay only for the underlying usage of other services Limited Customization","title":"CodeStar Overview"},{"location":"AWS/developer-associate/16-aws-cicd/13-codestar-overview/#codestar-overview","text":"An integrated solution that groups: Github, CodeCommit, CodeBuild, CodeDeploy, CloudFormation, CodePipeline, CloudWatch, ... Quickly create \"CICD-ready\" projects for EC2, Lambda, Elastic Beanstalk Supported languages: C#, Go, HTML5, Java, Node.js, PHP, Python, Ruby Issue tracking integration with JIRA / GitHub issues Ability to integrate with Cloud9 to obtain a web IDE (not all regions) One dashboard to view all your components Free service, pay only for the underlying usage of other services Limited Customization","title":"CodeStar Overview"},{"location":"AWS/developer-associate/16-aws-cicd/14-codestar-hands-on/","text":"CodeStar Hands On \u00b6 We can create a new CodeStar project. In order to use it, we'll need to create a service role: Then we can choose from various templates. In the next step we can choose to name it, where to place there repository. Also, additional options, like beanstalk for example: Then we can set up resources like AWS Cloud9 or other IDEs. Link issue tracking and add team members. It will automatically add a CloudFormation stack and set everything up as well as make the CodePipeline.","title":"CodeStar Hands On"},{"location":"AWS/developer-associate/16-aws-cicd/14-codestar-hands-on/#codestar-hands-on","text":"We can create a new CodeStar project. In order to use it, we'll need to create a service role: Then we can choose from various templates. In the next step we can choose to name it, where to place there repository. Also, additional options, like beanstalk for example: Then we can set up resources like AWS Cloud9 or other IDEs. Link issue tracking and add team members. It will automatically add a CloudFormation stack and set everything up as well as make the CodePipeline.","title":"CodeStar Hands On"},{"location":"AWS/developer-associate/16-aws-cicd/15-codeartifact-overview/","text":"CodeArtifact Overview \u00b6 Software packages depend on each other to be built (also called code dependencies) and new ones are created Storing and retrieving these dependencies is called artifact management Traditionally you need to setup your own artifact management system CodeArtifact is a secure, scalable, and cost-effective artifact management for software development Works with common dependency management tools such as Maven, Gradle, npm, yarn, twine, pip and NuGet Developers and CodeBuild can then retrieve dependencies from CodeArtifact","title":"CodeArtifact Overview"},{"location":"AWS/developer-associate/16-aws-cicd/15-codeartifact-overview/#codeartifact-overview","text":"Software packages depend on each other to be built (also called code dependencies) and new ones are created Storing and retrieving these dependencies is called artifact management Traditionally you need to setup your own artifact management system CodeArtifact is a secure, scalable, and cost-effective artifact management for software development Works with common dependency management tools such as Maven, Gradle, npm, yarn, twine, pip and NuGet Developers and CodeBuild can then retrieve dependencies from CodeArtifact","title":"CodeArtifact Overview"},{"location":"AWS/developer-associate/16-aws-cicd/16-codeguru-overview/","text":"CodeGuru Overview \u00b6 An ML-powered service for automated code reviews and application performance recommendations Provides two functionalities CodeGuru Reviewer: automated code reviews for static code analysis (development) CodeGuru Profiler: visibility/recommendations about application performance during runtime (production) CodeGuru Reviewer \u00b6 Identify critical issues, security vulnerabilities, and hard-to-find bugs Example: common coding best practices, resource leaks, security detection, input validation Uses machine learning and automated reasoning Hard-learned lessons accross millions of code reviews on 1000s of open-source and Amazon repositories Supports Java and Python Integrates with GitHub, BitBucke and AWS CodeCommit CodeGuru Profiler \u00b6 Helps understand the runtime behavior of your application Example: identify if your application is consuming excessive CPU capacity on a logging routine Features: Identify and remove code inefficiencies Improve application performance (e.g. reduce CPU utilization) Decrese compute costs Provides heap summary (identify which objects using up memory) Anomaly Detection Support applications running on AWS or on-premise Minimal overhead on application","title":"CodeGuru Overview"},{"location":"AWS/developer-associate/16-aws-cicd/16-codeguru-overview/#codeguru-overview","text":"An ML-powered service for automated code reviews and application performance recommendations Provides two functionalities CodeGuru Reviewer: automated code reviews for static code analysis (development) CodeGuru Profiler: visibility/recommendations about application performance during runtime (production)","title":"CodeGuru Overview"},{"location":"AWS/developer-associate/16-aws-cicd/16-codeguru-overview/#codeguru-reviewer","text":"Identify critical issues, security vulnerabilities, and hard-to-find bugs Example: common coding best practices, resource leaks, security detection, input validation Uses machine learning and automated reasoning Hard-learned lessons accross millions of code reviews on 1000s of open-source and Amazon repositories Supports Java and Python Integrates with GitHub, BitBucke and AWS CodeCommit","title":"CodeGuru Reviewer"},{"location":"AWS/developer-associate/16-aws-cicd/16-codeguru-overview/#codeguru-profiler","text":"Helps understand the runtime behavior of your application Example: identify if your application is consuming excessive CPU capacity on a logging routine Features: Identify and remove code inefficiencies Improve application performance (e.g. reduce CPU utilization) Decrese compute costs Provides heap summary (identify which objects using up memory) Anomaly Detection Support applications running on AWS or on-premise Minimal overhead on application","title":"CodeGuru Profiler"},{"location":"AWS/developer-associate/16-aws-cicd/AWS%20CodeBuild/","text":"CodeBuild Overview \u00b6 Source - CodeCommit, S3, BitBucket, GitHub Build Instructions: code file buildspec.yml or insert manually in Console Output logs can be stored in Amazon S3 & CloudWatch Logs Use CloudWatch Metrics to monitor build statistics Use CloudWatch Events to detect failed builds and trigger notifications Use CloudWatch Amarms to notify if you need \"thresholds\" for failures Build Projects can be defined within CodePipeline or CodeBuild Supported Environment \u00b6 Java Ruby Python Go Node.js Android .NET Core PHP Docker - extend any environment you like How It works \u00b6 buildspec.yml \u00b6 buildspec.yml file must be at the root of your code env - define environment variables variables - plain text variables parameter-store - variables stored in SSM Parameter Store secrets-manager - variables stored in AWS Secret Manager phases - specify commands to run install - install dependencies you may need for your build pre_build - final commands to execute before build build - actual build commands post_build - finishing touches (e.g. zip output) artifacts - what to upload to S3 (encrypted with KMS) cache - files to cache (usually dependencies) to S3 for future build speedup version: 0.2 env: variables: JAVA_HOME: \"/usr/lib/jvm/java-8-openjdk-amd64\" parameter-store: LOGIN_PASSWORD: /CodeBuild/dockerLoginPassword phases: install: commands: - echo \"Entered the install phase...\" - apt-get update -y - apt-get install -y maven pre_build: commands: - echo \"Entered the pre_build phase...\" - docker login -u User -p $LOGIN_PASSWORD build: commands: - echo \"Entered the build phase...\" - echo Build started on `date`\" - mvn install post_build: commands: - echo \"entered the post_build phase...\" - echo \"Build completed on `date`\" artifacts: files: - target/messageUtil-1.0.jar cache: paths: - \"/root/.m2/**/*\" Local Build \u00b6 In case of need of deep troubleshooting beyon logs.. You can run CodeBuild locally on your desktop (after installing Docker) For this, leverage CodeBuild Agent https://docs.aws.amazon.com/codebuild/latest/userguide/use-codebuild-agent.html Inside VPC \u00b6 By default, you CodeBuild containers are launched outside your VPC it cannot access resources in a VPC You can specify a VPC configuration: VPC ID Subnet IDs Security Group IDs Then your build can access resources in your VPC (e.g. RDS, ElastiCache, EC2, ALB, ...) Use cases: integration tests, data query, internal load balancers,...","title":"CodeBuild Overview"},{"location":"AWS/developer-associate/16-aws-cicd/AWS%20CodeBuild/#codebuild-overview","text":"Source - CodeCommit, S3, BitBucket, GitHub Build Instructions: code file buildspec.yml or insert manually in Console Output logs can be stored in Amazon S3 & CloudWatch Logs Use CloudWatch Metrics to monitor build statistics Use CloudWatch Events to detect failed builds and trigger notifications Use CloudWatch Amarms to notify if you need \"thresholds\" for failures Build Projects can be defined within CodePipeline or CodeBuild","title":"CodeBuild Overview"},{"location":"AWS/developer-associate/16-aws-cicd/AWS%20CodeBuild/#supported-environment","text":"Java Ruby Python Go Node.js Android .NET Core PHP Docker - extend any environment you like","title":"Supported Environment"},{"location":"AWS/developer-associate/16-aws-cicd/AWS%20CodeBuild/#how-it-works","text":"","title":"How It works"},{"location":"AWS/developer-associate/16-aws-cicd/AWS%20CodeBuild/#buildspecyml","text":"buildspec.yml file must be at the root of your code env - define environment variables variables - plain text variables parameter-store - variables stored in SSM Parameter Store secrets-manager - variables stored in AWS Secret Manager phases - specify commands to run install - install dependencies you may need for your build pre_build - final commands to execute before build build - actual build commands post_build - finishing touches (e.g. zip output) artifacts - what to upload to S3 (encrypted with KMS) cache - files to cache (usually dependencies) to S3 for future build speedup version: 0.2 env: variables: JAVA_HOME: \"/usr/lib/jvm/java-8-openjdk-amd64\" parameter-store: LOGIN_PASSWORD: /CodeBuild/dockerLoginPassword phases: install: commands: - echo \"Entered the install phase...\" - apt-get update -y - apt-get install -y maven pre_build: commands: - echo \"Entered the pre_build phase...\" - docker login -u User -p $LOGIN_PASSWORD build: commands: - echo \"Entered the build phase...\" - echo Build started on `date`\" - mvn install post_build: commands: - echo \"entered the post_build phase...\" - echo \"Build completed on `date`\" artifacts: files: - target/messageUtil-1.0.jar cache: paths: - \"/root/.m2/**/*\"","title":"buildspec.yml"},{"location":"AWS/developer-associate/16-aws-cicd/AWS%20CodeBuild/#local-build","text":"In case of need of deep troubleshooting beyon logs.. You can run CodeBuild locally on your desktop (after installing Docker) For this, leverage CodeBuild Agent https://docs.aws.amazon.com/codebuild/latest/userguide/use-codebuild-agent.html","title":"Local Build"},{"location":"AWS/developer-associate/16-aws-cicd/AWS%20CodeBuild/#inside-vpc","text":"By default, you CodeBuild containers are launched outside your VPC it cannot access resources in a VPC You can specify a VPC configuration: VPC ID Subnet IDs Security Group IDs Then your build can access resources in your VPC (e.g. RDS, ElastiCache, EC2, ALB, ...) Use cases: integration tests, data query, internal load balancers,...","title":"Inside VPC"},{"location":"AWS/developer-associate/16-aws-cicd/AWS%20CodeCommit/","text":"CodeCommit Overview \u00b6 Version control is the ability to understand the various changes that happened to the code over time (and possibly roll back) All these are enabled by using a version control system such as Git A Git repository can be synchronized on your computer but it usually is uploaded on a central online repository Benefits are: Collaborate with other developers Make sure the code is backed-up somewhere Make sure it's fully viewable and auditable CodeCommit \u00b6 Git repositories can be expensive The industry includes GitHub, GitLab, BitBucket, ... And AWs CodeCommit: Private Git repositories No size limit on repositories (scale seamlessly) Fully managed, highly available Code only in AWS Cloud account => increased security and compliance Security (encrypted, access control, ...) Integrated with Jenkins, AWS CodeBuild and other CI tools Security \u00b6 Interactions are done using Git (standard) Authentication SSH keys - aws users can configure SSH keys in their IAM Console HTTPS - with AWS CLI Credential helper or Git Credentials for IAM user Authorization IAM policies to manage users/roles permissions to repositories Encryption Repositories are automatically encrypted at rest using AWS KMS Encrypted in transit (can only use HTTPS or SSH - both secure) Cross-account Access DO NOT share your SSH keys or your AWS credentials Use an IAM Role in your AWS account and use AWS STS (AssumeRole API) CodeCommit vs GitHub \u00b6 CodeCommit GitHub Support Code Review (Pull Requests) y y Integration with AWS CodeBuild y y Authentication (SSH & HTTPS) y y Security IAM Users & Roles Github users Hosting Managed & hosted by AWS Hosted by Github Github enterprise: self hosted on your servers UI minimal fully featured","title":"CodeCommit Overview"},{"location":"AWS/developer-associate/16-aws-cicd/AWS%20CodeCommit/#codecommit-overview","text":"Version control is the ability to understand the various changes that happened to the code over time (and possibly roll back) All these are enabled by using a version control system such as Git A Git repository can be synchronized on your computer but it usually is uploaded on a central online repository Benefits are: Collaborate with other developers Make sure the code is backed-up somewhere Make sure it's fully viewable and auditable","title":"CodeCommit Overview"},{"location":"AWS/developer-associate/16-aws-cicd/AWS%20CodeCommit/#codecommit","text":"Git repositories can be expensive The industry includes GitHub, GitLab, BitBucket, ... And AWs CodeCommit: Private Git repositories No size limit on repositories (scale seamlessly) Fully managed, highly available Code only in AWS Cloud account => increased security and compliance Security (encrypted, access control, ...) Integrated with Jenkins, AWS CodeBuild and other CI tools","title":"CodeCommit"},{"location":"AWS/developer-associate/16-aws-cicd/AWS%20CodeCommit/#security","text":"Interactions are done using Git (standard) Authentication SSH keys - aws users can configure SSH keys in their IAM Console HTTPS - with AWS CLI Credential helper or Git Credentials for IAM user Authorization IAM policies to manage users/roles permissions to repositories Encryption Repositories are automatically encrypted at rest using AWS KMS Encrypted in transit (can only use HTTPS or SSH - both secure) Cross-account Access DO NOT share your SSH keys or your AWS credentials Use an IAM Role in your AWS account and use AWS STS (AssumeRole API)","title":"Security"},{"location":"AWS/developer-associate/16-aws-cicd/AWS%20CodeCommit/#codecommit-vs-github","text":"CodeCommit GitHub Support Code Review (Pull Requests) y y Integration with AWS CodeBuild y y Authentication (SSH & HTTPS) y y Security IAM Users & Roles Github users Hosting Managed & hosted by AWS Hosted by Github Github enterprise: self hosted on your servers UI minimal fully featured","title":"CodeCommit vs GitHub"},{"location":"AWS/developer-associate/16-aws-cicd/AWS%20CodeDeploy/","text":"CodeDeploy Overview \u00b6 We want to deploy our application automatically to many EC2 instances These EC2 instances are not managed by ElasticBeanstalk There are several ways to handle deployments using open-source tools (Ansible, Terraform, Chef, Puppet,...) We can use the managed service AWS CodeDeploy Steps to make it work \u00b6 Each EC2 instance/on-premises server must be running CodeDeploy Agent The agent is continously polling AWS CodeDeploy for work to do Application + appspec.yml is pulled from GitHub or S3 EC2 instances will run the deployment instructions in appspec.yml CodeDeploy Agent will report of success/failure of the deployment Primary Components \u00b6 Application - a unique name functions as a container (revision, deployment, configuration) Compute Platform - EC2/On-Premises, AWS Lambda, or Amazon ECS Deployment Configuration - a set of deployment rules for success/failure EC2/On-premises - specify the minimum number of healthy instances for the deployment AWS Lambda or Amazon ECS - specify how traffic is routed to your updated versions Deployment Group - group of tagged EC2 instances (allows to deploy gradually, or dev, test, prod...) Deployment Type - method used to deploy the application to a Deployment Group In-place Deployment - supports EC2/On-Premises Blue/Green Deployment - supports EC2 instances only, AWS Lambda and Amazon ECS IAM Instance Profile - give EC2 instances the permissions to access both S3 / GitHub Application Revision - application code + appspec.yml file Service Role - an IAM Role for CodeDeploy to perform operations on EC2 instances, ASGs, ELBs... Target Revision - the most recent revision that you want to deploy to a Deployment Group appspec.yml \u00b6 files - how to source and copy from S3 / GitHub to filesystem source destination hooks - set of instructions to do to deploy the new version (hooks can have timeouts), the order is: ApplicationStop DownloadBundle BeforeInstall Install AfterInstall ApplicationStart ValidateService (important) version: 0.0 os: linnux files: - source: Config/config.txt destination: /webapps/Config - source: source destination: /webapps/myApp hooks: BeforeInstall: - location: Scripts/UnzipResourceBundle.zip - location: Scripts/UnzipDataBundle.zip AfterInstall: - location: Scripts/RunResourceTests.sh timeout: 180 ApplicationStart: - location: Scripts/RunFunctionalTests.sh timeout: 3600 ValidateService: - location: Scripts/MonitorService.sh timeout: 3600 runas: codedeployuser Deployment Configurations \u00b6 Configurations: On at A Time - one EC2 instance at a time, of one instance fails, then deployment stops Half At A Time - 50% All At Once - quick but no healthy host, downtime. Good for dev Custom - min, healthy host = 75% Failures: EC2 instances stay in failed stated New deployments will first be deployed to failed instances To rollback, redeploy old deployment or enable automated rollback for failures Deployment Groups: A set of tagged EC2 instances Directly to an ASG Mix of ASG / Tags so you can build deployment segments Customization in scripts with DEPLOYTMENT_GROUP_NAME environment variables","title":"CodeDeploy Overview"},{"location":"AWS/developer-associate/16-aws-cicd/AWS%20CodeDeploy/#codedeploy-overview","text":"We want to deploy our application automatically to many EC2 instances These EC2 instances are not managed by ElasticBeanstalk There are several ways to handle deployments using open-source tools (Ansible, Terraform, Chef, Puppet,...) We can use the managed service AWS CodeDeploy","title":"CodeDeploy Overview"},{"location":"AWS/developer-associate/16-aws-cicd/AWS%20CodeDeploy/#steps-to-make-it-work","text":"Each EC2 instance/on-premises server must be running CodeDeploy Agent The agent is continously polling AWS CodeDeploy for work to do Application + appspec.yml is pulled from GitHub or S3 EC2 instances will run the deployment instructions in appspec.yml CodeDeploy Agent will report of success/failure of the deployment","title":"Steps to make it work"},{"location":"AWS/developer-associate/16-aws-cicd/AWS%20CodeDeploy/#primary-components","text":"Application - a unique name functions as a container (revision, deployment, configuration) Compute Platform - EC2/On-Premises, AWS Lambda, or Amazon ECS Deployment Configuration - a set of deployment rules for success/failure EC2/On-premises - specify the minimum number of healthy instances for the deployment AWS Lambda or Amazon ECS - specify how traffic is routed to your updated versions Deployment Group - group of tagged EC2 instances (allows to deploy gradually, or dev, test, prod...) Deployment Type - method used to deploy the application to a Deployment Group In-place Deployment - supports EC2/On-Premises Blue/Green Deployment - supports EC2 instances only, AWS Lambda and Amazon ECS IAM Instance Profile - give EC2 instances the permissions to access both S3 / GitHub Application Revision - application code + appspec.yml file Service Role - an IAM Role for CodeDeploy to perform operations on EC2 instances, ASGs, ELBs... Target Revision - the most recent revision that you want to deploy to a Deployment Group","title":"Primary Components"},{"location":"AWS/developer-associate/16-aws-cicd/AWS%20CodeDeploy/#appspecyml","text":"files - how to source and copy from S3 / GitHub to filesystem source destination hooks - set of instructions to do to deploy the new version (hooks can have timeouts), the order is: ApplicationStop DownloadBundle BeforeInstall Install AfterInstall ApplicationStart ValidateService (important) version: 0.0 os: linnux files: - source: Config/config.txt destination: /webapps/Config - source: source destination: /webapps/myApp hooks: BeforeInstall: - location: Scripts/UnzipResourceBundle.zip - location: Scripts/UnzipDataBundle.zip AfterInstall: - location: Scripts/RunResourceTests.sh timeout: 180 ApplicationStart: - location: Scripts/RunFunctionalTests.sh timeout: 3600 ValidateService: - location: Scripts/MonitorService.sh timeout: 3600 runas: codedeployuser","title":"appspec.yml"},{"location":"AWS/developer-associate/16-aws-cicd/AWS%20CodeDeploy/#deployment-configurations","text":"Configurations: On at A Time - one EC2 instance at a time, of one instance fails, then deployment stops Half At A Time - 50% All At Once - quick but no healthy host, downtime. Good for dev Custom - min, healthy host = 75% Failures: EC2 instances stay in failed stated New deployments will first be deployed to failed instances To rollback, redeploy old deployment or enable automated rollback for failures Deployment Groups: A set of tagged EC2 instances Directly to an ASG Mix of ASG / Tags so you can build deployment segments Customization in scripts with DEPLOYTMENT_GROUP_NAME environment variables","title":"Deployment Configurations"},{"location":"AWS/developer-associate/16-aws-cicd/AWS%20CodePipeline/","text":"CodePipeline Overview \u00b6 Visual Workflow to orchestrate your CICD Source - CodeCommit, ECR, S3, Bitbucket, Github Build - CodeBuild, Jenkins, CloudBees, TeamCity Test - CodeBuild, AWS Device Farm, 3rd party tools Deploy - CodeDeploy, ElasticBeanstalk, CloudFormation, ECS, S3, ... Consists of stages: Each stage can have sequential actions and/or parallel actions Example: Build->Test->Deploy->Load Testing->... Manual approval can be defined at any stage Artifacts \u00b6 Each pipeline stage can create artifacts Artifacts stored in an S3 bucket and passed on to the next stage Troubleshooting \u00b6 For CodePipeline Pipeline/Action/Stage Execution State Changes Use CloudWatch Events (Amazon EventBridge). Example: You can create events for failed pipelines You can create events for cancelled stages If CodePipeline fails a stage, your pipeline stops, and you can get information in the console If pipeline can't perform an action, make sure the \"IAM Service Role\" attached does have enough IAM permissions (IAM Policy) AWS CloudTrail can be used to audit AWS API calls","title":"CodePipeline Overview"},{"location":"AWS/developer-associate/16-aws-cicd/AWS%20CodePipeline/#codepipeline-overview","text":"Visual Workflow to orchestrate your CICD Source - CodeCommit, ECR, S3, Bitbucket, Github Build - CodeBuild, Jenkins, CloudBees, TeamCity Test - CodeBuild, AWS Device Farm, 3rd party tools Deploy - CodeDeploy, ElasticBeanstalk, CloudFormation, ECS, S3, ... Consists of stages: Each stage can have sequential actions and/or parallel actions Example: Build->Test->Deploy->Load Testing->... Manual approval can be defined at any stage","title":"CodePipeline Overview"},{"location":"AWS/developer-associate/16-aws-cicd/AWS%20CodePipeline/#artifacts","text":"Each pipeline stage can create artifacts Artifacts stored in an S3 bucket and passed on to the next stage","title":"Artifacts"},{"location":"AWS/developer-associate/16-aws-cicd/AWS%20CodePipeline/#troubleshooting","text":"For CodePipeline Pipeline/Action/Stage Execution State Changes Use CloudWatch Events (Amazon EventBridge). Example: You can create events for failed pipelines You can create events for cancelled stages If CodePipeline fails a stage, your pipeline stops, and you can get information in the console If pipeline can't perform an action, make sure the \"IAM Service Role\" attached does have enough IAM permissions (IAM Policy) AWS CloudTrail can be used to audit AWS API calls","title":"Troubleshooting"},{"location":"AWS/developer-associate/17-cloudformation/01-cloudformation-overview/","text":"CloudFormation Overview \u00b6 Infrastructure as Code \u00b6 Currently, we have been doing a lot of manual work All this manual work will be very tough to reproduce: In another region In another AWS account Within the same region if everything was deleted Wouldn't it be great, if all our infrastructure was... code? That code would be deployed and create / update / delete our infrastructure. CloudFormation \u00b6 CloudFormation is a declarative way of outlining our AWS Infrastructure, for any resources (most of them are supported). For example, within a CloudFormation template, you say: I want a security group I want two EC2 machines using this security group I want two Elastic IPs for these EC2 machines I want an S3 bucket I want a load balancer (ELB) if front of these machines Then CloudFormation creates those for you, in the right order, with the exact configuration you specify. Benefits of AWS CloudFormation \u00b6 Infrastructure as code No resources are manually creasted, which is excellent for control The code can be version controlled for example using git Changes to the infrastructure are reviewed through code Cost Each resource within the stack is tagged with an identifier so you can easily see how much a stack costs you You can estimate the costs of your resources using CloudFormation template Savings strategy: In Dev, you could automate deletion of templates at 5pm and recreate at 8am safely. Productivity Ability to destroy and re-create an infrastructure on the cloud on the fly Automated generation of Diagram for your templates Declarative programming (no need to figure out ordering and orchestration) Separation of concern: create many stacks for many pps, and many layers. Ex: VPC stacks, network stacks, App stacks. Don't re-invent the wheel Leverage existing templates on the web! Leverage the documentation! How CloudFormation Works \u00b6 Templates have to be uploaded in S3 and then referenced in CloudFormation To update a template, we can't edit previous ones. We have to re-upload a new version of the template to AWS Stacks are identified by a name Deleting a stack deletes every single artifact that was created by CloudFormation Deploying CloudFormation templates \u00b6 Manual way: Editing templates in the cloudformation designer using the console to input parameters Automated way: Editing templates in a YAML file Using the AWS CLI to deploy the templates Recommended way when you fully want to automate your flow ClouFormation Building Blocks \u00b6 Template components \u00b6 Resources: your AWS resources declared in the template (mandatory) Parameters: the dynamic inputs for your template Mappings: the static variables for your template Outputs: references to what has been created Conditionals: list of conditions to perform resource creation Metadata Template helpers: \u00b6 References Functions","title":"CloudFormation Overview"},{"location":"AWS/developer-associate/17-cloudformation/01-cloudformation-overview/#cloudformation-overview","text":"","title":"CloudFormation Overview"},{"location":"AWS/developer-associate/17-cloudformation/01-cloudformation-overview/#infrastructure-as-code","text":"Currently, we have been doing a lot of manual work All this manual work will be very tough to reproduce: In another region In another AWS account Within the same region if everything was deleted Wouldn't it be great, if all our infrastructure was... code? That code would be deployed and create / update / delete our infrastructure.","title":"Infrastructure as Code"},{"location":"AWS/developer-associate/17-cloudformation/01-cloudformation-overview/#cloudformation","text":"CloudFormation is a declarative way of outlining our AWS Infrastructure, for any resources (most of them are supported). For example, within a CloudFormation template, you say: I want a security group I want two EC2 machines using this security group I want two Elastic IPs for these EC2 machines I want an S3 bucket I want a load balancer (ELB) if front of these machines Then CloudFormation creates those for you, in the right order, with the exact configuration you specify.","title":"CloudFormation"},{"location":"AWS/developer-associate/17-cloudformation/01-cloudformation-overview/#benefits-of-aws-cloudformation","text":"Infrastructure as code No resources are manually creasted, which is excellent for control The code can be version controlled for example using git Changes to the infrastructure are reviewed through code Cost Each resource within the stack is tagged with an identifier so you can easily see how much a stack costs you You can estimate the costs of your resources using CloudFormation template Savings strategy: In Dev, you could automate deletion of templates at 5pm and recreate at 8am safely. Productivity Ability to destroy and re-create an infrastructure on the cloud on the fly Automated generation of Diagram for your templates Declarative programming (no need to figure out ordering and orchestration) Separation of concern: create many stacks for many pps, and many layers. Ex: VPC stacks, network stacks, App stacks. Don't re-invent the wheel Leverage existing templates on the web! Leverage the documentation!","title":"Benefits of AWS CloudFormation"},{"location":"AWS/developer-associate/17-cloudformation/01-cloudformation-overview/#how-cloudformation-works","text":"Templates have to be uploaded in S3 and then referenced in CloudFormation To update a template, we can't edit previous ones. We have to re-upload a new version of the template to AWS Stacks are identified by a name Deleting a stack deletes every single artifact that was created by CloudFormation","title":"How CloudFormation Works"},{"location":"AWS/developer-associate/17-cloudformation/01-cloudformation-overview/#deploying-cloudformation-templates","text":"Manual way: Editing templates in the cloudformation designer using the console to input parameters Automated way: Editing templates in a YAML file Using the AWS CLI to deploy the templates Recommended way when you fully want to automate your flow","title":"Deploying CloudFormation templates"},{"location":"AWS/developer-associate/17-cloudformation/01-cloudformation-overview/#clouformation-building-blocks","text":"","title":"ClouFormation Building Blocks"},{"location":"AWS/developer-associate/17-cloudformation/01-cloudformation-overview/#template-components","text":"Resources: your AWS resources declared in the template (mandatory) Parameters: the dynamic inputs for your template Mappings: the static variables for your template Outputs: references to what has been created Conditionals: list of conditions to perform resource creation Metadata","title":"Template components"},{"location":"AWS/developer-associate/17-cloudformation/01-cloudformation-overview/#template-helpers","text":"References Functions","title":"Template helpers:"},{"location":"AWS/developer-associate/17-cloudformation/02-cloudformation-create-stack-hands-on/","text":"CloudFormation Create Stack Hands On \u00b6 We are going to create a simple EC2 instance. Then we're going to create an Elastic IP, 2 security groups and attach it to it. When creating a stack, we can choose from following options. In this case we can upload a template file or point to one in S3 bucket. --- Resources: MyInstance: Type: AWS::EC2::Instance Properties: AvailabilityZone: us-east-1a ImageId: ami-a4c7edb2 InstanceType: t2.micro Once we have created a stack, it will create all the resources specified. In the events tab we'll see what events have been currently triggered. The resources tab will show all the resources that has been created by the CloudFormation Stack.","title":"CloudFormation Create Stack Hands On"},{"location":"AWS/developer-associate/17-cloudformation/02-cloudformation-create-stack-hands-on/#cloudformation-create-stack-hands-on","text":"We are going to create a simple EC2 instance. Then we're going to create an Elastic IP, 2 security groups and attach it to it. When creating a stack, we can choose from following options. In this case we can upload a template file or point to one in S3 bucket. --- Resources: MyInstance: Type: AWS::EC2::Instance Properties: AvailabilityZone: us-east-1a ImageId: ami-a4c7edb2 InstanceType: t2.micro Once we have created a stack, it will create all the resources specified. In the events tab we'll see what events have been currently triggered. The resources tab will show all the resources that has been created by the CloudFormation Stack.","title":"CloudFormation Create Stack Hands On"},{"location":"AWS/developer-associate/17-cloudformation/03-cloudformation-update-and-delete-stack-hands-on/","text":"CloudFormation Update and Delete Stack Hands On \u00b6 We can select our stack and click on update . Then you can choose how you want to update it. We are going to upload a new file. --- Parameters: SecurityGroupDescription: Description: Security Group Description Type: String Resources: MyInstance: Type: AWS::EC2::Instance Properties: AvailabilityZone: us-east-1a ImageId: ami-a4c7edb2 InstanceType: t2.micro SecurityGroups: - !Ref SSHSecurityGroup - !Ref ServerSecurityGroup MyEIP: Type: AWS::EC2::EIP Properties: InstanceId: !Ref MyInstance SSHSecurityGroup: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: Enable SSH access via port 22 SecurityGroupIngress: - CidrIp: 0.0.0.0/0 FromPort: 22 IpProtocol: tcp ToPort: 22 ServerSecurityGroup: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: !Ref SecurityGroupDescription SecurityGroupIngress: - CidrIp: 0.0.0.0/0 FromPort: 80 IpProtocol: tcp ToPort: 80 - CidrIp: 192.168.1.1.1/32 FromPort: 22 IpProtocol: tcp ToPort: 22 Now we'll get prompted to input a parameter. In the review we'll see that we'll add a bunch of stuff, but will modify the already existing instance. We can also select the cloudformation stack and click on delete. It will delete all the resources created.","title":"CloudFormation Update and Delete Stack Hands On"},{"location":"AWS/developer-associate/17-cloudformation/03-cloudformation-update-and-delete-stack-hands-on/#cloudformation-update-and-delete-stack-hands-on","text":"We can select our stack and click on update . Then you can choose how you want to update it. We are going to upload a new file. --- Parameters: SecurityGroupDescription: Description: Security Group Description Type: String Resources: MyInstance: Type: AWS::EC2::Instance Properties: AvailabilityZone: us-east-1a ImageId: ami-a4c7edb2 InstanceType: t2.micro SecurityGroups: - !Ref SSHSecurityGroup - !Ref ServerSecurityGroup MyEIP: Type: AWS::EC2::EIP Properties: InstanceId: !Ref MyInstance SSHSecurityGroup: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: Enable SSH access via port 22 SecurityGroupIngress: - CidrIp: 0.0.0.0/0 FromPort: 22 IpProtocol: tcp ToPort: 22 ServerSecurityGroup: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: !Ref SecurityGroupDescription SecurityGroupIngress: - CidrIp: 0.0.0.0/0 FromPort: 80 IpProtocol: tcp ToPort: 80 - CidrIp: 192.168.1.1.1/32 FromPort: 22 IpProtocol: tcp ToPort: 22 Now we'll get prompted to input a parameter. In the review we'll see that we'll add a bunch of stuff, but will modify the already existing instance. We can also select the cloudformation stack and click on delete. It will delete all the resources created.","title":"CloudFormation Update and Delete Stack Hands On"},{"location":"AWS/developer-associate/17-cloudformation/04-cloudformation-resources/","text":"CloudFormation Resources \u00b6 What are resources? \u00b6 Resources are the core of your CloudFormation template (mandatory) They represent the different AWS Components that will be created and configured Resources are declared and can reference each other AWS figures out creation, updates and deletes of resources for us There are over 224 types of resources Resource types identifiers are of the form: AWS::aws-product-name::data-type-name How do I find resources documentation? \u00b6 All the resources can be found in https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-template-resource-type-ref.html An EC2 instance example is here: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ec2-instance.html FAQ for resources \u00b6 Can I create dynamic amount of resources? No, you can't. Everything in CloudFormation template has to be declared. You can't perform code generation there. Is every AWS Service supported? Almost. Only a few select few niches are not there yet. You can work around that usin AWS Lambda Custom Resources","title":"CloudFormation Resources"},{"location":"AWS/developer-associate/17-cloudformation/04-cloudformation-resources/#cloudformation-resources","text":"","title":"CloudFormation Resources"},{"location":"AWS/developer-associate/17-cloudformation/04-cloudformation-resources/#what-are-resources","text":"Resources are the core of your CloudFormation template (mandatory) They represent the different AWS Components that will be created and configured Resources are declared and can reference each other AWS figures out creation, updates and deletes of resources for us There are over 224 types of resources Resource types identifiers are of the form: AWS::aws-product-name::data-type-name","title":"What are resources?"},{"location":"AWS/developer-associate/17-cloudformation/04-cloudformation-resources/#how-do-i-find-resources-documentation","text":"All the resources can be found in https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-template-resource-type-ref.html An EC2 instance example is here: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ec2-instance.html","title":"How do I find resources documentation?"},{"location":"AWS/developer-associate/17-cloudformation/04-cloudformation-resources/#faq-for-resources","text":"Can I create dynamic amount of resources? No, you can't. Everything in CloudFormation template has to be declared. You can't perform code generation there. Is every AWS Service supported? Almost. Only a few select few niches are not there yet. You can work around that usin AWS Lambda Custom Resources","title":"FAQ for resources"},{"location":"AWS/developer-associate/17-cloudformation/05-cloudformation-parameters/","text":"CloudFormation Parameters \u00b6 Parameters are a way to provide inputs to your AWS CloudFormation template They're important to know about if: You want to reuse your templates across the company Some inputs can not be determined ahead of time Parameters are extremely powerful, controlled, and can prevent errors from happening in your template thanks to types. When should you use a parameter? \u00b6 Ask yourself this: Is this cloudformation resource configuration likely to change in the future? If so, make it a parameter You won't have to re-upload a template to change it's content Parameters: SecurityGroupDescription: Description: Security Group Description Type: String Parameter Settings \u00b6 Parameters can be controlled by all these settings. Type String Number CommaDelimitedList List AWS Parameter (to help catch invalid values - match against existing values in the AWS Account) Description Constraints ConstraintDescription (String) Min/MaxLength Min/MaxValue Defaults AllowedValues (array) AllowedPattern (regexp) NoEcho (boolean) How to Reference a Parameter \u00b6 The Fn::Ref function can be leveraged to reference parameters Parameters can be used anywhere in a template The shorthand for this in YAML is !Ref The function can also reference other elements within the template DbSubnet1: Type:: AWS::EC2::Subnet Properties: VpcId: !Ref MyVPC Pseudo Parameters \u00b6 AWS offers us pseudo parameters in any CloudFormation template These can be used at any time and are enabled by default","title":"CloudFormation Parameters"},{"location":"AWS/developer-associate/17-cloudformation/05-cloudformation-parameters/#cloudformation-parameters","text":"Parameters are a way to provide inputs to your AWS CloudFormation template They're important to know about if: You want to reuse your templates across the company Some inputs can not be determined ahead of time Parameters are extremely powerful, controlled, and can prevent errors from happening in your template thanks to types.","title":"CloudFormation Parameters"},{"location":"AWS/developer-associate/17-cloudformation/05-cloudformation-parameters/#when-should-you-use-a-parameter","text":"Ask yourself this: Is this cloudformation resource configuration likely to change in the future? If so, make it a parameter You won't have to re-upload a template to change it's content Parameters: SecurityGroupDescription: Description: Security Group Description Type: String","title":"When should you use a parameter?"},{"location":"AWS/developer-associate/17-cloudformation/05-cloudformation-parameters/#parameter-settings","text":"Parameters can be controlled by all these settings. Type String Number CommaDelimitedList List AWS Parameter (to help catch invalid values - match against existing values in the AWS Account) Description Constraints ConstraintDescription (String) Min/MaxLength Min/MaxValue Defaults AllowedValues (array) AllowedPattern (regexp) NoEcho (boolean)","title":"Parameter Settings"},{"location":"AWS/developer-associate/17-cloudformation/05-cloudformation-parameters/#how-to-reference-a-parameter","text":"The Fn::Ref function can be leveraged to reference parameters Parameters can be used anywhere in a template The shorthand for this in YAML is !Ref The function can also reference other elements within the template DbSubnet1: Type:: AWS::EC2::Subnet Properties: VpcId: !Ref MyVPC","title":"How to Reference a Parameter"},{"location":"AWS/developer-associate/17-cloudformation/05-cloudformation-parameters/#pseudo-parameters","text":"AWS offers us pseudo parameters in any CloudFormation template These can be used at any time and are enabled by default","title":"Pseudo Parameters"},{"location":"AWS/developer-associate/17-cloudformation/06-cloudformation-mappings/","text":"CloudFormation Mappings \u00b6 Mappings are fixed variables within your CloudFormation Template. They're very handy to differentiate between different environments (dev vs prod), regions (AWS regions), AMI types etc All the Values are hardcoded within the template: RegionMap: us-east-1: \"32\": \"ami-6411e20d\" \"64\": \"ami-7a11e213\" us-west-1: \"32\": \"ami-c9c7978c\" \"64\": \"ami-cfc7978a\" eu-west-1: \"32\": \"ami-37c2f643\" \"64\": \"ami-31c2f645\" Resources: myEc2Instance: Type: AWS::EC2::Instance Properties: ImageId: !FindInMap [RegionMap, !Ref AWS::Region, 32] InstanceType: m1.small You can reference the map using !FindInMap [ MapName, TopLevelKey, SecondLevelKey ] When would you use mappings vs parameters? \u00b6 Mappings are great when you know in advance all the values that can be taken and that they can be deducted from variables such as: Region Availability Zone AWS Account Environment (dev vs prod) etc They allow safer control over the template Use parameters when the values are really user specific","title":"CloudFormation Mappings"},{"location":"AWS/developer-associate/17-cloudformation/06-cloudformation-mappings/#cloudformation-mappings","text":"Mappings are fixed variables within your CloudFormation Template. They're very handy to differentiate between different environments (dev vs prod), regions (AWS regions), AMI types etc All the Values are hardcoded within the template: RegionMap: us-east-1: \"32\": \"ami-6411e20d\" \"64\": \"ami-7a11e213\" us-west-1: \"32\": \"ami-c9c7978c\" \"64\": \"ami-cfc7978a\" eu-west-1: \"32\": \"ami-37c2f643\" \"64\": \"ami-31c2f645\" Resources: myEc2Instance: Type: AWS::EC2::Instance Properties: ImageId: !FindInMap [RegionMap, !Ref AWS::Region, 32] InstanceType: m1.small You can reference the map using !FindInMap [ MapName, TopLevelKey, SecondLevelKey ]","title":"CloudFormation Mappings"},{"location":"AWS/developer-associate/17-cloudformation/06-cloudformation-mappings/#when-would-you-use-mappings-vs-parameters","text":"Mappings are great when you know in advance all the values that can be taken and that they can be deducted from variables such as: Region Availability Zone AWS Account Environment (dev vs prod) etc They allow safer control over the template Use parameters when the values are really user specific","title":"When would you use mappings vs parameters?"},{"location":"AWS/developer-associate/17-cloudformation/07-cloudformation-outputs/","text":"CloudFormation Outputs \u00b6 The outputs section declares optional outputs values that we can import into other stacks (if you export them first) You can also view the outputs in the AWS Console or in using the AWS CLI They're very useful for example if you define a network CloudFormation, and output the variables such as VPC ID and your Subnet ID It's the best way to perform some collaboration cross stack, as you let exper handle their own part of the stack You can't delete a CloudFormation Stack if its output are being referenced by another CloudFormation stack Outputs Example \u00b6 Creating an SSH Security Group as part of one template We create an output that references that security group Outputs: StackSSHSecurityGroup: Description: The SSH Security Group for our company Value: !Ref MyCompanyWideSSHSecurityGroup Export: Name: SSHSecurityGroup Cross Stack Reference \u00b6 We then create a second template that leverages that security group For this, we use the Fn::ImportValue function You can't delete the underlying stack until all the references are deleted too. Resources: MySecureInstance: Type: AWS::EC2::Instance Properties: AvailabilityZone: us-east-1a ImageId: ami-a4c7edb2 InstanceType: t2.micro SecurityGroups: - !ImportValue SSHSecurityGroup","title":"CloudFormation Outputs"},{"location":"AWS/developer-associate/17-cloudformation/07-cloudformation-outputs/#cloudformation-outputs","text":"The outputs section declares optional outputs values that we can import into other stacks (if you export them first) You can also view the outputs in the AWS Console or in using the AWS CLI They're very useful for example if you define a network CloudFormation, and output the variables such as VPC ID and your Subnet ID It's the best way to perform some collaboration cross stack, as you let exper handle their own part of the stack You can't delete a CloudFormation Stack if its output are being referenced by another CloudFormation stack","title":"CloudFormation Outputs"},{"location":"AWS/developer-associate/17-cloudformation/07-cloudformation-outputs/#outputs-example","text":"Creating an SSH Security Group as part of one template We create an output that references that security group Outputs: StackSSHSecurityGroup: Description: The SSH Security Group for our company Value: !Ref MyCompanyWideSSHSecurityGroup Export: Name: SSHSecurityGroup","title":"Outputs Example"},{"location":"AWS/developer-associate/17-cloudformation/07-cloudformation-outputs/#cross-stack-reference","text":"We then create a second template that leverages that security group For this, we use the Fn::ImportValue function You can't delete the underlying stack until all the references are deleted too. Resources: MySecureInstance: Type: AWS::EC2::Instance Properties: AvailabilityZone: us-east-1a ImageId: ami-a4c7edb2 InstanceType: t2.micro SecurityGroups: - !ImportValue SSHSecurityGroup","title":"Cross Stack Reference"},{"location":"AWS/developer-associate/17-cloudformation/08-cloudformation-conditions/","text":"CloudFormation Conditions \u00b6 Conditions are used to control the creation of resources or outputs based on a condition. Conditions can be whatever you want them to be, but common ones are: Environment (dev / test / prod) AWS Region Any parameter value Each condition can reference another condition, parameter value or mapping Conditions: CreateProdResources: !Equal [ !Ref EnvType, prod ] The logical ID is for you yo choose. It's how you name condition The intrinsic function (logical) can be any of the following: Fn::And Fn::Equals Fn::If Fn::Not Fn::Or These can be applied to resources / outputs etc Resources: MountPoint: Type: AWS::EC2::VolumeAttachment Condition: CreateProdResources","title":"CloudFormation Conditions"},{"location":"AWS/developer-associate/17-cloudformation/08-cloudformation-conditions/#cloudformation-conditions","text":"Conditions are used to control the creation of resources or outputs based on a condition. Conditions can be whatever you want them to be, but common ones are: Environment (dev / test / prod) AWS Region Any parameter value Each condition can reference another condition, parameter value or mapping Conditions: CreateProdResources: !Equal [ !Ref EnvType, prod ] The logical ID is for you yo choose. It's how you name condition The intrinsic function (logical) can be any of the following: Fn::And Fn::Equals Fn::If Fn::Not Fn::Or These can be applied to resources / outputs etc Resources: MountPoint: Type: AWS::EC2::VolumeAttachment Condition: CreateProdResources","title":"CloudFormation Conditions"},{"location":"AWS/developer-associate/17-cloudformation/09-cloudformation-intrinsic-functions/","text":"CloudFormation Intrinsic Functions \u00b6 Some of the intrinsic functions: - Ref - Fn::GetAtt - Fn::FindInMap - Fn::ImportValue - Fn::Join - Fn::Sub - Condition Functions (Fn::If, Fn::Not, Fn::Equals, etc) Fn::Ref \u00b6 The Fn::Ref function can be leveraged to reference parameters -> returns the value of the parameter resources -> returns the physical ID of the underlying resource The shorthand for this in YAML is !Ref DbSubnet1: Type: AWS::EC2::Subnet Properties: VpcId: !Ref MyVPC Fn::GetAtt \u00b6 Attributes are attached to any resource you create To know the attributes of your resources, the best place to look at is the documentation Resources: EC2Instance: Type: AWS::EC2::Instance Properties: ImageId: ami-1234567 InstanceType: t2.micro NewVolume: Type: AWS::EC2::Volume Condition: CreateProdResources Properties: Size: 100 AvailabilityZone: !GetAtt EC2Instance.AvailabilityZone Fn::FindInMap \u00b6 We use Fn::FindInMap to return a named value from a specific key RegionMap: us-east-1: \"32\": \"ami-6411e20d\" \"64\": \"ami-7a11e213\" us-west-1: \"32\": \"ami-c9c7978c\" \"64\": \"ami-cfc7978a\" eu-west-1: \"32\": \"ami-37c2f643\" \"64\": \"ami-31c2f645\" Resources: myEc2Instance: Type: AWS::EC2::Instance Properties: ImageId: !FindInMap [RegionMap, !Ref AWS::Region, 32] InstanceType: m1.small You can reference the map using !FindInMap [ MapName, TopLevelKey, SecondLevelKey ] Fn::ImportValue \u00b6 Import values that are exported in other templates Resources: MySecureInstance: Type: AWS::EC2::Instance Properties: AvailabilityZone: us-east-1a ImageId: ami-a4c7edb2 InstanceType: t2.micro SecurityGroups: - !ImportValue SSHSecurityGroup Fn::join \u00b6 Join values with a delimiter !Join [ delimiter, [ comma-delimited list of values ] ] This creates \"a:b:c\" !Join [ \":\", [ a, b, c ] ] Fn::Sub \u00b6 Sn::Sub or !Sub as a shorthand is used to substitute variables from a text. It's a very handy function that will allow you to fully customize your templates. For example you can combine Fn::Sub with references or AWS Pseudo variables. String must contain ${variableName} and will substitute them !Sub - String - {Var1Name: Var1Value, Var2Name: Var2Value } !Sub String Condition Functions \u00b6 Conditions: CreateProdResources: !Equal [ !Ref EnvType, prod ] The logical ID is for you yo choose. It's how you name condition The intrinsic function (logical) can be any of the following: Fn::And Fn::Equals Fn::If Fn::Not Fn::Or","title":"CloudFormation Intrinsic Functions"},{"location":"AWS/developer-associate/17-cloudformation/09-cloudformation-intrinsic-functions/#cloudformation-intrinsic-functions","text":"Some of the intrinsic functions: - Ref - Fn::GetAtt - Fn::FindInMap - Fn::ImportValue - Fn::Join - Fn::Sub - Condition Functions (Fn::If, Fn::Not, Fn::Equals, etc)","title":"CloudFormation Intrinsic Functions"},{"location":"AWS/developer-associate/17-cloudformation/09-cloudformation-intrinsic-functions/#fnref","text":"The Fn::Ref function can be leveraged to reference parameters -> returns the value of the parameter resources -> returns the physical ID of the underlying resource The shorthand for this in YAML is !Ref DbSubnet1: Type: AWS::EC2::Subnet Properties: VpcId: !Ref MyVPC","title":"Fn::Ref"},{"location":"AWS/developer-associate/17-cloudformation/09-cloudformation-intrinsic-functions/#fngetatt","text":"Attributes are attached to any resource you create To know the attributes of your resources, the best place to look at is the documentation Resources: EC2Instance: Type: AWS::EC2::Instance Properties: ImageId: ami-1234567 InstanceType: t2.micro NewVolume: Type: AWS::EC2::Volume Condition: CreateProdResources Properties: Size: 100 AvailabilityZone: !GetAtt EC2Instance.AvailabilityZone","title":"Fn::GetAtt"},{"location":"AWS/developer-associate/17-cloudformation/09-cloudformation-intrinsic-functions/#fnfindinmap","text":"We use Fn::FindInMap to return a named value from a specific key RegionMap: us-east-1: \"32\": \"ami-6411e20d\" \"64\": \"ami-7a11e213\" us-west-1: \"32\": \"ami-c9c7978c\" \"64\": \"ami-cfc7978a\" eu-west-1: \"32\": \"ami-37c2f643\" \"64\": \"ami-31c2f645\" Resources: myEc2Instance: Type: AWS::EC2::Instance Properties: ImageId: !FindInMap [RegionMap, !Ref AWS::Region, 32] InstanceType: m1.small You can reference the map using !FindInMap [ MapName, TopLevelKey, SecondLevelKey ]","title":"Fn::FindInMap"},{"location":"AWS/developer-associate/17-cloudformation/09-cloudformation-intrinsic-functions/#fnimportvalue","text":"Import values that are exported in other templates Resources: MySecureInstance: Type: AWS::EC2::Instance Properties: AvailabilityZone: us-east-1a ImageId: ami-a4c7edb2 InstanceType: t2.micro SecurityGroups: - !ImportValue SSHSecurityGroup","title":"Fn::ImportValue"},{"location":"AWS/developer-associate/17-cloudformation/09-cloudformation-intrinsic-functions/#fnjoin","text":"Join values with a delimiter !Join [ delimiter, [ comma-delimited list of values ] ] This creates \"a:b:c\" !Join [ \":\", [ a, b, c ] ]","title":"Fn::join"},{"location":"AWS/developer-associate/17-cloudformation/09-cloudformation-intrinsic-functions/#fnsub","text":"Sn::Sub or !Sub as a shorthand is used to substitute variables from a text. It's a very handy function that will allow you to fully customize your templates. For example you can combine Fn::Sub with references or AWS Pseudo variables. String must contain ${variableName} and will substitute them !Sub - String - {Var1Name: Var1Value, Var2Name: Var2Value } !Sub String","title":"Fn::Sub"},{"location":"AWS/developer-associate/17-cloudformation/09-cloudformation-intrinsic-functions/#condition-functions","text":"Conditions: CreateProdResources: !Equal [ !Ref EnvType, prod ] The logical ID is for you yo choose. It's how you name condition The intrinsic function (logical) can be any of the following: Fn::And Fn::Equals Fn::If Fn::Not Fn::Or","title":"Condition Functions"},{"location":"AWS/developer-associate/17-cloudformation/10-cloudformation-rollbacks/","text":"CloudFormation Rollbacks \u00b6 Stack Creation Fails: Default: everything rolls back (gets deleted). We can look at the log Option to disable rollback and troubleshoot what happened Stack Update fails: The stack automatically rolls back to the previous known working state Ability to see in the log what happened and error message To prevent this, you can change the options when importing the template:","title":"CloudFormation Rollbacks"},{"location":"AWS/developer-associate/17-cloudformation/10-cloudformation-rollbacks/#cloudformation-rollbacks","text":"Stack Creation Fails: Default: everything rolls back (gets deleted). We can look at the log Option to disable rollback and troubleshoot what happened Stack Update fails: The stack automatically rolls back to the previous known working state Ability to see in the log what happened and error message To prevent this, you can change the options when importing the template:","title":"CloudFormation Rollbacks"},{"location":"AWS/developer-associate/17-cloudformation/11-cloudformation-changesets-nested-stacks-stackset/","text":"CloudFormation ChangeSets, Nested Stacks, StackSet \u00b6 ChangeSets \u00b6 When you update a stack, you need to know what changes before it happens for greater condidence ChangeSets won't say if the update will be successful Nested Stacks \u00b6 Nested stacks are stacks as part of other stacks They allow you to isolate repeated patterns / common components in separate stacks and call them from other stacks Example: Load Balancer configuration that is re-used Security Group that is re-used Nested stacks are considered best practice To update a nested stack, always update the parent (root stack) Cross vs Nested Stacks \u00b6 Cross Stacks Helpful when stacks have different lifecycles Use Outputs Export and Fn::ImportValue When you need to pass export values to many stacks (VPC Id, etc) Nested Stacks Helpful when components must be re-used Ex: re-use how to properly configure an Application Load Balancer The nested stack only is important to the higher level stack (it's not shared) StackSets \u00b6 Create, update or delete stacks across multiple accounts and regions with a single operation Administrator account to create StackSets Trusted accounts to create, update, delete stack instances from StackSets When you update a stack set, all associated stack instances are updated throughout all accounts and regions.","title":"CloudFormation ChangeSets, Nested Stacks, StackSet"},{"location":"AWS/developer-associate/17-cloudformation/11-cloudformation-changesets-nested-stacks-stackset/#cloudformation-changesets-nested-stacks-stackset","text":"","title":"CloudFormation ChangeSets, Nested Stacks, StackSet"},{"location":"AWS/developer-associate/17-cloudformation/11-cloudformation-changesets-nested-stacks-stackset/#changesets","text":"When you update a stack, you need to know what changes before it happens for greater condidence ChangeSets won't say if the update will be successful","title":"ChangeSets"},{"location":"AWS/developer-associate/17-cloudformation/11-cloudformation-changesets-nested-stacks-stackset/#nested-stacks","text":"Nested stacks are stacks as part of other stacks They allow you to isolate repeated patterns / common components in separate stacks and call them from other stacks Example: Load Balancer configuration that is re-used Security Group that is re-used Nested stacks are considered best practice To update a nested stack, always update the parent (root stack)","title":"Nested Stacks"},{"location":"AWS/developer-associate/17-cloudformation/11-cloudformation-changesets-nested-stacks-stackset/#cross-vs-nested-stacks","text":"Cross Stacks Helpful when stacks have different lifecycles Use Outputs Export and Fn::ImportValue When you need to pass export values to many stacks (VPC Id, etc) Nested Stacks Helpful when components must be re-used Ex: re-use how to properly configure an Application Load Balancer The nested stack only is important to the higher level stack (it's not shared)","title":"Cross vs Nested Stacks"},{"location":"AWS/developer-associate/17-cloudformation/11-cloudformation-changesets-nested-stacks-stackset/#stacksets","text":"Create, update or delete stacks across multiple accounts and regions with a single operation Administrator account to create StackSets Trusted accounts to create, update, delete stack instances from StackSets When you update a stack set, all associated stack instances are updated throughout all accounts and regions.","title":"StackSets"},{"location":"AWS/developer-associate/17-cloudformation/12-cloudformation-drift/","text":"CloudFormation Drift \u00b6 CloudFormation allows you to create infrastructure But it doesn't protect you against manual configuration changes How do we know if our resources have drifted? We can use CloudFormation drift Not all resources are supported yet: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-drift-resource-list.html In order to detect the drift, we can choose it under stack actions.","title":"CloudFormation Drift"},{"location":"AWS/developer-associate/17-cloudformation/12-cloudformation-drift/#cloudformation-drift","text":"CloudFormation allows you to create infrastructure But it doesn't protect you against manual configuration changes How do we know if our resources have drifted? We can use CloudFormation drift Not all resources are supported yet: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-drift-resource-list.html In order to detect the drift, we can choose it under stack actions.","title":"CloudFormation Drift"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/01-monitoring-overview-in-aws/","text":"Monitoring Overview in AWS \u00b6 Why is monitoring important \u00b6 We know how to deploy applications Safely Automatically Using IaC Leveraging best AWS components Our applications are deployed, and our users don't care how we did it. Our users only care that the application is working Application latency: will it increase over time? Application outages: customer experience should not be degraded Users contacting the IT department or complaining is not a good outcome Troubleshooting and remediation Internal monitoring Can we prevent issues before they happen? Performance and cost Trends (scalling patterns) Learning and improvement Monitoring in AWS \u00b6 CloudWatch Metrics: Collect and track key metrics Logs: Collect, monitor, analyize and store log files Events: Send notifications when certain events happen in your AWS Alarms: React in real-time to metrics / events AWS X-Ray: Troubleshooting application performance and errors Distributed tracing of microservices AWS CloudTrail Internal monitoring of API calls being made Audit changes to AWS Resources by your users","title":"Monitoring Overview in AWS"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/01-monitoring-overview-in-aws/#monitoring-overview-in-aws","text":"","title":"Monitoring Overview in AWS"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/01-monitoring-overview-in-aws/#why-is-monitoring-important","text":"We know how to deploy applications Safely Automatically Using IaC Leveraging best AWS components Our applications are deployed, and our users don't care how we did it. Our users only care that the application is working Application latency: will it increase over time? Application outages: customer experience should not be degraded Users contacting the IT department or complaining is not a good outcome Troubleshooting and remediation Internal monitoring Can we prevent issues before they happen? Performance and cost Trends (scalling patterns) Learning and improvement","title":"Why is monitoring important"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/01-monitoring-overview-in-aws/#monitoring-in-aws","text":"CloudWatch Metrics: Collect and track key metrics Logs: Collect, monitor, analyize and store log files Events: Send notifications when certain events happen in your AWS Alarms: React in real-time to metrics / events AWS X-Ray: Troubleshooting application performance and errors Distributed tracing of microservices AWS CloudTrail Internal monitoring of API calls being made Audit changes to AWS Resources by your users","title":"Monitoring in AWS"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/02-cloudwatch-metrics/","text":"CloudWatch Metrics \u00b6 CloudWatch provides metrics for every service in AWS Metric is a variable to monitor (CPUUtilization, NetworkIn...) Metrics belong to namespaces Dimension is an attribute of a metric (instance id, environment etc) Up to 10 dimensions per metric Metrics have timestamps Can create CloudWatch dashboards of metrics EC2 Detailed Monitoring \u00b6 EC2 instance metrics have metrics every 5 minutes With detailed monitoring (for a cost), you get data every 1 minute Use detailed monitoring if you want to scale faster your ASG The AWS Free Tier allows us to have 10 detailed monitoring metrics Note: EC2 Memory usage is by default not pushed (must be pushed from inside the instance as custom metric).","title":"CloudWatch Metrics"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/02-cloudwatch-metrics/#cloudwatch-metrics","text":"CloudWatch provides metrics for every service in AWS Metric is a variable to monitor (CPUUtilization, NetworkIn...) Metrics belong to namespaces Dimension is an attribute of a metric (instance id, environment etc) Up to 10 dimensions per metric Metrics have timestamps Can create CloudWatch dashboards of metrics","title":"CloudWatch Metrics"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/02-cloudwatch-metrics/#ec2-detailed-monitoring","text":"EC2 instance metrics have metrics every 5 minutes With detailed monitoring (for a cost), you get data every 1 minute Use detailed monitoring if you want to scale faster your ASG The AWS Free Tier allows us to have 10 detailed monitoring metrics Note: EC2 Memory usage is by default not pushed (must be pushed from inside the instance as custom metric).","title":"EC2 Detailed Monitoring"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/03-cloudwatch-custom-metrics/","text":"CloudWatch Custom Metrics \u00b6 Possibility to define and send your own custom metrics to CloudWatch Example: memory (RAM) usage, disk space, number of logged in users Use API Call PutMetricData Ability to use dimensions (attributes) to segment metrics Instance.id Environment.name Metric resolution (StorageResolution API parameter - two possible values) Standard: 1 minute (60 seconds) High Resolution: 1/5/10/30 second(s) - Higher cost Important: Accepts metric data points two weeks in the past and two hours in the future (make sure to configure your EC2 instance time correctly) $ aws cloudwatch put-metric-data --namespace \"Usage Metrics\" --metric data file://metric.json [ { \"MetricName\": \"New Posts\", \"TimeStamp\": \"Wednesday, June 12, 2013 8:28:20 PM\", \"Value\": 0.50, \"Unit\": \"Count\" } ] Specicify multiple dimensions $ aws cloudwatch put-metric-data --metric-name Buffers --name MyNameSpace --unit Bytes --value 231434333 --dimensions InstanceId=1-23456789,InstanceType=m1.small","title":"CloudWatch Custom Metrics"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/03-cloudwatch-custom-metrics/#cloudwatch-custom-metrics","text":"Possibility to define and send your own custom metrics to CloudWatch Example: memory (RAM) usage, disk space, number of logged in users Use API Call PutMetricData Ability to use dimensions (attributes) to segment metrics Instance.id Environment.name Metric resolution (StorageResolution API parameter - two possible values) Standard: 1 minute (60 seconds) High Resolution: 1/5/10/30 second(s) - Higher cost Important: Accepts metric data points two weeks in the past and two hours in the future (make sure to configure your EC2 instance time correctly) $ aws cloudwatch put-metric-data --namespace \"Usage Metrics\" --metric data file://metric.json [ { \"MetricName\": \"New Posts\", \"TimeStamp\": \"Wednesday, June 12, 2013 8:28:20 PM\", \"Value\": 0.50, \"Unit\": \"Count\" } ] Specicify multiple dimensions $ aws cloudwatch put-metric-data --metric-name Buffers --name MyNameSpace --unit Bytes --value 231434333 --dimensions InstanceId=1-23456789,InstanceType=m1.small","title":"CloudWatch Custom Metrics"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/04-cloudwatch-logs/","text":"CloudWatch Logs \u00b6 Log groups - arbitrary name, usually representing an application Log stream - instances with application / log files / containers Can define lof expiration policies (never expire, 30 days, etc) CloudWatch Logs can send logs to Amazon S3 (exports) Kinesis Data Streams Kinesis Data Firehose AWS Lambda ElasticSearch Sources \u00b6 SDK, CloudWatch Logs Agent, CloudWatch Unified Agent Elastic Beanstalk: collection of logs from application ECS: collection from containers AWS Lambda: collection from function logs VPC FLow Logs: VPC specific logs API Gateway CloudTrail based on filter Route53: Log DNS queries CloudWatch Logs Metric Filter & Insights \u00b6 CloudWatch Logs can use filter expressions For example, find a specific IP inside of a log Or count occurences of ERROR in your logs Metric filters can be used to trigger CloudWatch alarms CloudWatch Logs Insights can be used to query logs and add queries to CloudWatch Dashboards S3 Export \u00b6 Log data can take up to 12 hours to become available for export The API call is CreateExportTask Not near-real time or real-time... use Logs Subscriptions instead Logs Subscriptions \u00b6 Logs Aggregation Multi-Account & Multi Region \u00b6","title":"CloudWatch Logs"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/04-cloudwatch-logs/#cloudwatch-logs","text":"Log groups - arbitrary name, usually representing an application Log stream - instances with application / log files / containers Can define lof expiration policies (never expire, 30 days, etc) CloudWatch Logs can send logs to Amazon S3 (exports) Kinesis Data Streams Kinesis Data Firehose AWS Lambda ElasticSearch","title":"CloudWatch Logs"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/04-cloudwatch-logs/#sources","text":"SDK, CloudWatch Logs Agent, CloudWatch Unified Agent Elastic Beanstalk: collection of logs from application ECS: collection from containers AWS Lambda: collection from function logs VPC FLow Logs: VPC specific logs API Gateway CloudTrail based on filter Route53: Log DNS queries","title":"Sources"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/04-cloudwatch-logs/#cloudwatch-logs-metric-filter-insights","text":"CloudWatch Logs can use filter expressions For example, find a specific IP inside of a log Or count occurences of ERROR in your logs Metric filters can be used to trigger CloudWatch alarms CloudWatch Logs Insights can be used to query logs and add queries to CloudWatch Dashboards","title":"CloudWatch Logs Metric Filter &amp; Insights"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/04-cloudwatch-logs/#s3-export","text":"Log data can take up to 12 hours to become available for export The API call is CreateExportTask Not near-real time or real-time... use Logs Subscriptions instead","title":"S3 Export"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/04-cloudwatch-logs/#logs-subscriptions","text":"","title":"Logs Subscriptions"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/04-cloudwatch-logs/#logs-aggregation-multi-account-multi-region","text":"","title":"Logs Aggregation Multi-Account &amp; Multi Region"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/05-cloudwatch-logs-hands-on/","text":"CloudWatch Logs Hands On \u00b6 If we open up CloudWatch Logs, we can see multiple Log groups. If we open one up, we can see all the log streams related to it. And, when opening up a log stream, we can see all the events. We can use the search bar to filter the log lines, e.g. containing HTTP. Creating Metric Filters \u00b6 Creating Filter pattern \u00b6 Test the filter pattern on a dataset. Assign metric \u00b6 Review & Create \u00b6 Creating Subscription Filters \u00b6 Edit retention settings \u00b6 Export to Amazon S3 \u00b6 Create Log Group \u00b6 You can setup the retention settings and the KMS ARN if you want the logs to be encrypted. Log insights \u00b6 You can also use Logs Insights to query the logs with a query languange and visusualize it. You can also export the results. And save the queries on the right side or look up some queries.","title":"CloudWatch Logs Hands On"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/05-cloudwatch-logs-hands-on/#cloudwatch-logs-hands-on","text":"If we open up CloudWatch Logs, we can see multiple Log groups. If we open one up, we can see all the log streams related to it. And, when opening up a log stream, we can see all the events. We can use the search bar to filter the log lines, e.g. containing HTTP.","title":"CloudWatch Logs Hands On"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/05-cloudwatch-logs-hands-on/#creating-metric-filters","text":"","title":"Creating Metric Filters"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/05-cloudwatch-logs-hands-on/#creating-filter-pattern","text":"Test the filter pattern on a dataset.","title":"Creating Filter pattern"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/05-cloudwatch-logs-hands-on/#assign-metric","text":"","title":"Assign metric"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/05-cloudwatch-logs-hands-on/#review-create","text":"","title":"Review &amp; Create"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/05-cloudwatch-logs-hands-on/#creating-subscription-filters","text":"","title":"Creating Subscription Filters"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/05-cloudwatch-logs-hands-on/#edit-retention-settings","text":"","title":"Edit retention settings"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/05-cloudwatch-logs-hands-on/#export-to-amazon-s3","text":"","title":"Export to Amazon S3"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/05-cloudwatch-logs-hands-on/#create-log-group","text":"You can setup the retention settings and the KMS ARN if you want the logs to be encrypted.","title":"Create Log Group"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/05-cloudwatch-logs-hands-on/#log-insights","text":"You can also use Logs Insights to query the logs with a query languange and visusualize it. You can also export the results. And save the queries on the right side or look up some queries.","title":"Log insights"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/06-cloudwatch-agent-cloudwatch-logs-agent/","text":"CloudWatch Agent & CloudWatch Logs Agent \u00b6 CloudWatch Logs for EC2 \u00b6 By default, no logs from your EC2 machine will go to CloudWatch You need to run a CloudWatch agent on EC2 to push the log files you want Make sure IAM permissions are correct The CloudWatch log agent can be setup on-premises too CloudWatch Logs Agent & Unified Agent \u00b6 For virtual servers (EC2 instances, on-premise servers) CloudWatch Logs Agent Old version of the agent Can only send to CloudWatch Logs CloudWatch Unified Agent Collect additional system-level metrics such as RAM, processes, etc Collect logs to send to CloudWatch Logs Centralized configuration using SSM Parameter Store CloudWatch Unified Agent - Metrics \u00b6 Collected directly on your Linux Server / EC2 instance CPU (active, guest, idle, system, user, steal) Disk metrics (free, used, total), Disk IO (writes, reads, bytes, iops) RAM (free, inactive, used, total, cached) Netstat (number of TCP and UDP connections, net packets, bytes) Processes (total, dead, bloqued, idle, running, sleep) Swap Space (free, used, used %) Reminder: out-of-the-box metrics for EC2 - disk, CPU, network (high level)","title":"CloudWatch Agent & CloudWatch Logs Agent"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/06-cloudwatch-agent-cloudwatch-logs-agent/#cloudwatch-agent-cloudwatch-logs-agent","text":"","title":"CloudWatch Agent &amp; CloudWatch Logs Agent"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/06-cloudwatch-agent-cloudwatch-logs-agent/#cloudwatch-logs-for-ec2","text":"By default, no logs from your EC2 machine will go to CloudWatch You need to run a CloudWatch agent on EC2 to push the log files you want Make sure IAM permissions are correct The CloudWatch log agent can be setup on-premises too","title":"CloudWatch Logs for EC2"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/06-cloudwatch-agent-cloudwatch-logs-agent/#cloudwatch-logs-agent-unified-agent","text":"For virtual servers (EC2 instances, on-premise servers) CloudWatch Logs Agent Old version of the agent Can only send to CloudWatch Logs CloudWatch Unified Agent Collect additional system-level metrics such as RAM, processes, etc Collect logs to send to CloudWatch Logs Centralized configuration using SSM Parameter Store","title":"CloudWatch Logs Agent &amp; Unified Agent"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/06-cloudwatch-agent-cloudwatch-logs-agent/#cloudwatch-unified-agent-metrics","text":"Collected directly on your Linux Server / EC2 instance CPU (active, guest, idle, system, user, steal) Disk metrics (free, used, total), Disk IO (writes, reads, bytes, iops) RAM (free, inactive, used, total, cached) Netstat (number of TCP and UDP connections, net packets, bytes) Processes (total, dead, bloqued, idle, running, sleep) Swap Space (free, used, used %) Reminder: out-of-the-box metrics for EC2 - disk, CPU, network (high level)","title":"CloudWatch Unified Agent - Metrics"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/07-cloudwatch-logs-metric-filters/","text":"CloudWatch Logs Metric Filters \u00b6 CloudWatch Logs can use filter expressions For example, find a specific IP inside of a log Or count occurrences of \"ERROR\" in your logs Metric filters can be used to trigger alarms Filters do not retroactively filter data. Filters only publish the metric data points for events that happen after the filter was created. Creating Metric Filters \u00b6 Creating Filter pattern \u00b6 Test the filter pattern on a dataset. Assign metric \u00b6 Review & Create \u00b6 When that's done, after some time, when new logs have came in with the specific criteria - a new Custom Namespace should show up under metrics. We can also use this metric to create a CloudWatch Alarm Once it's created we'll see it under alarms. Under metric filters we'll also see that it's linked to an alarm","title":"CloudWatch Logs Metric Filters"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/07-cloudwatch-logs-metric-filters/#cloudwatch-logs-metric-filters","text":"CloudWatch Logs can use filter expressions For example, find a specific IP inside of a log Or count occurrences of \"ERROR\" in your logs Metric filters can be used to trigger alarms Filters do not retroactively filter data. Filters only publish the metric data points for events that happen after the filter was created.","title":"CloudWatch Logs Metric Filters"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/07-cloudwatch-logs-metric-filters/#creating-metric-filters","text":"","title":"Creating Metric Filters"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/07-cloudwatch-logs-metric-filters/#creating-filter-pattern","text":"Test the filter pattern on a dataset.","title":"Creating Filter pattern"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/07-cloudwatch-logs-metric-filters/#assign-metric","text":"","title":"Assign metric"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/07-cloudwatch-logs-metric-filters/#review-create","text":"When that's done, after some time, when new logs have came in with the specific criteria - a new Custom Namespace should show up under metrics. We can also use this metric to create a CloudWatch Alarm Once it's created we'll see it under alarms. Under metric filters we'll also see that it's linked to an alarm","title":"Review &amp; Create"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/08-cloudwatch-alarms/","text":"CloudWatch Alarms \u00b6 Alarms are used to trigger notifications for any metric Various options (sampling, %, max, min, etc...) Alarm states: OK INSUFFICIENT_DATA ALARM Period: Length of time in seconds to evaluate the metric High resolution custom metrics: 10 sec, 30 sec or multiples of 60 sec Alarm Targets \u00b6 Stop, Terminate, Reboot or Recover an EC2 instance (EC2) Trigger Auto Scaling Action (EC2 AutoScaling) Send notification to SNS (from which you can do pretty much anything) Instance Recovery \u00b6 Status Check: Instance status = check the EC2 VM System status = check the underlying hardware Recovery: Same private, public, elastic IP, metadata, placement group Good to Know \u00b6 Alarms can be created based on CloudWatch Logs Metrics Filters To test alarms and notifications, set the alarm state to Alarm using CLI $ aws cloudwatch set-alarm-state --alarm-name \"myalarm\" --state-value ALARM --state-reason \"testing purposes\"","title":"CloudWatch Alarms"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/08-cloudwatch-alarms/#cloudwatch-alarms","text":"Alarms are used to trigger notifications for any metric Various options (sampling, %, max, min, etc...) Alarm states: OK INSUFFICIENT_DATA ALARM Period: Length of time in seconds to evaluate the metric High resolution custom metrics: 10 sec, 30 sec or multiples of 60 sec","title":"CloudWatch Alarms"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/08-cloudwatch-alarms/#alarm-targets","text":"Stop, Terminate, Reboot or Recover an EC2 instance (EC2) Trigger Auto Scaling Action (EC2 AutoScaling) Send notification to SNS (from which you can do pretty much anything)","title":"Alarm Targets"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/08-cloudwatch-alarms/#instance-recovery","text":"Status Check: Instance status = check the EC2 VM System status = check the underlying hardware Recovery: Same private, public, elastic IP, metadata, placement group","title":"Instance Recovery"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/08-cloudwatch-alarms/#good-to-know","text":"Alarms can be created based on CloudWatch Logs Metrics Filters To test alarms and notifications, set the alarm state to Alarm using CLI $ aws cloudwatch set-alarm-state --alarm-name \"myalarm\" --state-value ALARM --state-reason \"testing purposes\"","title":"Good to Know"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/09-cloudwatch-alarms-hands-on/","text":"CloudWatch Alarms Hands On \u00b6 We are going to create an EC2 instance. We are going terminate the instance if the CPU goes to 100%. We are going to create the alarm. So, we set an alarm to terminate the EC2 instance if it's CPU usage is over 95% for 15minutes. Because it will take a long time and we need to simulate the CPU workload for that, we are just going to use a CLI command to trigger the alarm. $ aws cloudwatch set-alarm-state --alarm-name TerminateEC2OnHighCPU --state-value ALARM --state-reason \"Testing\"","title":"CloudWatch Alarms Hands On"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/09-cloudwatch-alarms-hands-on/#cloudwatch-alarms-hands-on","text":"We are going to create an EC2 instance. We are going terminate the instance if the CPU goes to 100%. We are going to create the alarm. So, we set an alarm to terminate the EC2 instance if it's CPU usage is over 95% for 15minutes. Because it will take a long time and we need to simulate the CPU workload for that, we are just going to use a CLI command to trigger the alarm. $ aws cloudwatch set-alarm-state --alarm-name TerminateEC2OnHighCPU --state-value ALARM --state-reason \"Testing\"","title":"CloudWatch Alarms Hands On"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/10-cloudwatch-events/","text":"CloudWatch Events \u00b6 Event Pattern: Intercept events from AWS services (Sources) Example sources: EC2 Instance Start, CodeBuild Failure, S3, Trusted Advisor Can intercept any API call with CloudTrail integration Schedule or Cron (example: create an event every 4 hours) A JSON payload is created from the event and passed to a target Compute: Lambda, Batch, ECS task Integration: SQS, SNS, Kinesis Data Streams, Kinsesis Data Firehose Orchestration: Step Functions, CodePipeline, CodeBuild Maintenance: SSM, EC2 Actions In CloudWatch, on the left sidebar there is a section Rules We can create a new rule Every time an EC2 instance goes into a pending state, we create an event. We are going to send an email from SNS with the entire event JSON The rules that you create in CloudWatch events will also appear on EventBridge.","title":"CloudWatch Events"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/10-cloudwatch-events/#cloudwatch-events","text":"Event Pattern: Intercept events from AWS services (Sources) Example sources: EC2 Instance Start, CodeBuild Failure, S3, Trusted Advisor Can intercept any API call with CloudTrail integration Schedule or Cron (example: create an event every 4 hours) A JSON payload is created from the event and passed to a target Compute: Lambda, Batch, ECS task Integration: SQS, SNS, Kinesis Data Streams, Kinsesis Data Firehose Orchestration: Step Functions, CodePipeline, CodeBuild Maintenance: SSM, EC2 Actions In CloudWatch, on the left sidebar there is a section Rules We can create a new rule Every time an EC2 instance goes into a pending state, we create an event. We are going to send an email from SNS with the entire event JSON The rules that you create in CloudWatch events will also appear on EventBridge.","title":"CloudWatch Events"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/11-amazon-eventbridge/","text":"Amazon EventBridge \u00b6 EventBridge is the next evolution of CloudWatch Events Default Event Bus - generated by AWS services (CloudWatch Events) Partner Event Bus - receive events from Saas service or applications (Zendesk, DataDog, Segment, Auth0...) Custom Event Buses - for your own applications Event buses can be accessed byh other AWS accounts You can archive events (all/filter) sent to an event bus (indefinitely or set period) Ability to replay archived events Rules: how to process the events (like CloudWatch Events) Schema Registry \u00b6 EventBridge can analyze the events in your bus and infer the schema The Schema Registry allows you to generate code for your application, that will know in advance how data is structured in the event bus Schema can be versioned. Resource-based Policy \u00b6 Manage permissions for a specific Event Bus Example: allow/deny events from another AWS account or AWS region Use case: aggregate all events from your AWS Organization in a single AWS account or AWS region Amazon EventBridge vs CloudWatch Events \u00b6 Amazon EventBridge builds upon and extends CloudWatch Events It uses the same service API and endpoint, and the same underlying service infrastructure EventBridge allows extension to add event buses for your custom applications and your third-party SaaS apps. Event Bridge has the Schema Registry capability EventBridge has a different name to mark the new capabilities Over time, the CloudWatch Events name will be replaced with EventBridge","title":"Amazon EventBridge"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/11-amazon-eventbridge/#amazon-eventbridge","text":"EventBridge is the next evolution of CloudWatch Events Default Event Bus - generated by AWS services (CloudWatch Events) Partner Event Bus - receive events from Saas service or applications (Zendesk, DataDog, Segment, Auth0...) Custom Event Buses - for your own applications Event buses can be accessed byh other AWS accounts You can archive events (all/filter) sent to an event bus (indefinitely or set period) Ability to replay archived events Rules: how to process the events (like CloudWatch Events)","title":"Amazon EventBridge"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/11-amazon-eventbridge/#schema-registry","text":"EventBridge can analyze the events in your bus and infer the schema The Schema Registry allows you to generate code for your application, that will know in advance how data is structured in the event bus Schema can be versioned.","title":"Schema Registry"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/11-amazon-eventbridge/#resource-based-policy","text":"Manage permissions for a specific Event Bus Example: allow/deny events from another AWS account or AWS region Use case: aggregate all events from your AWS Organization in a single AWS account or AWS region","title":"Resource-based Policy"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/11-amazon-eventbridge/#amazon-eventbridge-vs-cloudwatch-events","text":"Amazon EventBridge builds upon and extends CloudWatch Events It uses the same service API and endpoint, and the same underlying service infrastructure EventBridge allows extension to add event buses for your custom applications and your third-party SaaS apps. Event Bridge has the Schema Registry capability EventBridge has a different name to mark the new capabilities Over time, the CloudWatch Events name will be replaced with EventBridge","title":"Amazon EventBridge vs CloudWatch Events"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/12-amazon-eventbridge-hands-on/","text":"Amazon EventBridge Hands On \u00b6 In EventBridge you have EventBuses. There is a Default event bus that Amazon Creates for you and you can use it to define rules for Amazon Services. You can also create custom event buses for your applications. If you need to enable cross account options, you can set up the Resource-based policy as well. Events \u00b6 We have event sources that can come from partners You can follow the steps on the setup page and you'll have events coming in from the service. Once we have events, we can set up some rules. We can select the bus and type (whenever a specific event happens or schedule the rule) We can have multiple targets for the rule that can be either: - EventBridge bus - EventBridge API destination - Other AWS service. This will send an email each time an instance is stopped or terminated. We can also access the event archives. We can also replay events. We can also look up event schemas. You can also download the code bindings for them.","title":"Amazon EventBridge Hands On"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/12-amazon-eventbridge-hands-on/#amazon-eventbridge-hands-on","text":"In EventBridge you have EventBuses. There is a Default event bus that Amazon Creates for you and you can use it to define rules for Amazon Services. You can also create custom event buses for your applications. If you need to enable cross account options, you can set up the Resource-based policy as well.","title":"Amazon EventBridge Hands On"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/12-amazon-eventbridge-hands-on/#events","text":"We have event sources that can come from partners You can follow the steps on the setup page and you'll have events coming in from the service. Once we have events, we can set up some rules. We can select the bus and type (whenever a specific event happens or schedule the rule) We can have multiple targets for the rule that can be either: - EventBridge bus - EventBridge API destination - Other AWS service. This will send an email each time an instance is stopped or terminated. We can also access the event archives. We can also replay events. We can also look up event schemas. You can also download the code bindings for them.","title":"Events"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/14-x-ray-hands-on/","text":"X-Ray Hands On \u00b6 We can open up the AWS X-Ray console in AWS. If we click on the start button, it's going to generate signups.","title":"X-Ray Hands On"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/14-x-ray-hands-on/#x-ray-hands-on","text":"We can open up the AWS X-Ray console in AWS. If we click on the start button, it's going to generate signups.","title":"X-Ray Hands On"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/15-x-ray-instrumentation-and-concepts/","text":"X-Ray Instrumentation and Concepts \u00b6 Instrumentation means the measure of product's performance, diagnose errors, and to write trace information. To instrument your application code, you use the X-Ray SDK Many SDK require only configuration changes YHou can modify your application code to customize and annotate the data that the SDK sends to X-Ray, using interceptors, filters, handlers, middleware. Concepts \u00b6 Segments: each application / service will send them Subsegments: if you need more details in your segment Trace: segments collected together to form an end-to-end trace Sampling: decrease the amount of requests sent to X-Ray, reduce cost Annotations: Key Value pairs used to index traces and use with filters Metadata: Key Value pairs, not indexed, not used for searching The X-Ray deaemon / agent has a config to send traces cross account: make sure the IAM permissions are correct - the agent will assume the role. This allows to have a central account for all your application tracing. Sampling Rules \u00b6 With sampling rules, you control the amount of data that you record You can modify sampling rules without changing your code By default, the X-Ray SDK records the first request each second, and five percent of any additional requests One request per second is the reservoir, which ensures that at least one trace is reacorded each second as long the service is serving requests. Five percent is the rate which additional requests beyond the reservoir size are sampled. You can create your own rules with the reservoir and rate.","title":"X-Ray Instrumentation and Concepts"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/15-x-ray-instrumentation-and-concepts/#x-ray-instrumentation-and-concepts","text":"Instrumentation means the measure of product's performance, diagnose errors, and to write trace information. To instrument your application code, you use the X-Ray SDK Many SDK require only configuration changes YHou can modify your application code to customize and annotate the data that the SDK sends to X-Ray, using interceptors, filters, handlers, middleware.","title":"X-Ray Instrumentation and Concepts"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/15-x-ray-instrumentation-and-concepts/#concepts","text":"Segments: each application / service will send them Subsegments: if you need more details in your segment Trace: segments collected together to form an end-to-end trace Sampling: decrease the amount of requests sent to X-Ray, reduce cost Annotations: Key Value pairs used to index traces and use with filters Metadata: Key Value pairs, not indexed, not used for searching The X-Ray deaemon / agent has a config to send traces cross account: make sure the IAM permissions are correct - the agent will assume the role. This allows to have a central account for all your application tracing.","title":"Concepts"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/15-x-ray-instrumentation-and-concepts/#sampling-rules","text":"With sampling rules, you control the amount of data that you record You can modify sampling rules without changing your code By default, the X-Ray SDK records the first request each second, and five percent of any additional requests One request per second is the reservoir, which ensures that at least one trace is reacorded each second as long the service is serving requests. Five percent is the rate which additional requests beyond the reservoir size are sampled. You can create your own rules with the reservoir and rate.","title":"Sampling Rules"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/16-x-ray-sampling-rules/","text":"X-Ray Sampling Rules \u00b6 In X-Ray we can create multiple sampling rules with different priorities.","title":"X-Ray Sampling Rules"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/16-x-ray-sampling-rules/#x-ray-sampling-rules","text":"In X-Ray we can create multiple sampling rules with different priorities.","title":"X-Ray Sampling Rules"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/17-x-ray-apis/","text":"X-Ray APIs \u00b6 X-Ray Write APIs (used by the X-Ray daemon) PutTraceSegments: Uploads segment documents to AWS X-Ray PutTelemetryRecords: Used by the AWS X-Ray daemon to upload telemetry. SegmentsReceivedCount SegmentsRejectedCount BackendConnectionErrors ... GetSamplingRules: Retrieve all sampling rules (to know what/when to send) GetSamplingTargets & GetSamplingStatisticSummaries: advanced The X-Ray deaemon needs to have an IAM policy authorizing the correct API calls to function correctly. \"Effect\": \"Allow\", \"Action\": [ \"xray:PutTraceSegments\", \"xray:PutTelemetryRecords\", \"xray:GetSamplingRules\", \"xray:GetSamplingTargets\", \"xray:GetSamplingStatisticSummaries\" ], \"Resource\": [ \"*\" ] arn::aws::iam::aws::policy/AWSXrayWriteOnlyAccess Read APIs GetServiceGraph: main graph BetchGetTraces: Retrieves a list of traces specified by ID. Each trace is a collection of segment documents that originates from a single request. GetTraceSummaries: Retrieves IDs and annotations for traces available for a specified time frame using an optional filter. To get the full traces, pass the trace IDs to BatchGetTraces. GetTraceGraph: Retrieves a service graph for one or more specific trace IDs. \"Effect\": \"Allow\", \"Action\": [ \"xray:GetSamplingRules\", \"xray:GetSamplingTargets\", \"xray:GetSamplingStatisticSummaries\", \"xray:BatchGetTraces\", \"xray:GetServiceGraph\", \"xray:GetTraceGraph\", \"xray:GetTraceSummaries\", \"xray:GetGroups\", \"xray:GetGroup\", \"xray:GetTimeSeriesServiceStatistics\" ], \"Resource\": [ \"*\" ]","title":"X-Ray APIs"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/17-x-ray-apis/#x-ray-apis","text":"X-Ray Write APIs (used by the X-Ray daemon) PutTraceSegments: Uploads segment documents to AWS X-Ray PutTelemetryRecords: Used by the AWS X-Ray daemon to upload telemetry. SegmentsReceivedCount SegmentsRejectedCount BackendConnectionErrors ... GetSamplingRules: Retrieve all sampling rules (to know what/when to send) GetSamplingTargets & GetSamplingStatisticSummaries: advanced The X-Ray deaemon needs to have an IAM policy authorizing the correct API calls to function correctly. \"Effect\": \"Allow\", \"Action\": [ \"xray:PutTraceSegments\", \"xray:PutTelemetryRecords\", \"xray:GetSamplingRules\", \"xray:GetSamplingTargets\", \"xray:GetSamplingStatisticSummaries\" ], \"Resource\": [ \"*\" ] arn::aws::iam::aws::policy/AWSXrayWriteOnlyAccess Read APIs GetServiceGraph: main graph BetchGetTraces: Retrieves a list of traces specified by ID. Each trace is a collection of segment documents that originates from a single request. GetTraceSummaries: Retrieves IDs and annotations for traces available for a specified time frame using an optional filter. To get the full traces, pass the trace IDs to BatchGetTraces. GetTraceGraph: Retrieves a service graph for one or more specific trace IDs. \"Effect\": \"Allow\", \"Action\": [ \"xray:GetSamplingRules\", \"xray:GetSamplingTargets\", \"xray:GetSamplingStatisticSummaries\", \"xray:BatchGetTraces\", \"xray:GetServiceGraph\", \"xray:GetTraceGraph\", \"xray:GetTraceSummaries\", \"xray:GetGroups\", \"xray:GetGroup\", \"xray:GetTimeSeriesServiceStatistics\" ], \"Resource\": [ \"*\" ]","title":"X-Ray APIs"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/18-x-ray-with-beanstalk/","text":"X-Ray with Beanstalk \u00b6 AWS Elastic Beanstalk platforms include the X-Ray daemon You can run the daemon by setting an option in the Elastic Beanstalk console or with a configuration file (in .ebextensions/xray-daemon.config) option_settings: aws:elasticbeanstalk:xray: XRayEnabled: true Make sure to give your instance profile the correct IAM permissions so that the X-Ray daemon can function correctly. Then make sure your application code is instrumented with the X-Ray SDK Note: The X-Ray daemon is not provided for Multicontainer Docker","title":"X-Ray with Beanstalk"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/18-x-ray-with-beanstalk/#x-ray-with-beanstalk","text":"AWS Elastic Beanstalk platforms include the X-Ray daemon You can run the daemon by setting an option in the Elastic Beanstalk console or with a configuration file (in .ebextensions/xray-daemon.config) option_settings: aws:elasticbeanstalk:xray: XRayEnabled: true Make sure to give your instance profile the correct IAM permissions so that the X-Ray daemon can function correctly. Then make sure your application code is instrumented with the X-Ray SDK Note: The X-Ray daemon is not provided for Multicontainer Docker","title":"X-Ray with Beanstalk"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/19-x-ray-and-ecs/","text":"X-Ray and ECS \u00b6 Example task definition \u00b6 { \"name\": \"xray-daemon\", \"image\": \"123456789012.dkr.ecr.us-east-2.amazonaws.com/xray-daemon\", \"cpu\": 32, \"memoryReservation\": 256, \"portMappings\": [ { \"hostPort\": 0, \"containerPort\": 2000, \"protocol\": \"udp\" } ] }, { \"name\": \"scorekeep-api\", \"image\": \"123456789012.dkr.ecr.us-east-2.amazonaws.com/scorekeep-api\", \"cpu\": 192, \"memoryReservation\": 512, \"environment\": [ { \"name\": \"AWS_REGION\", \"value\" : \"us-east-2\" }, { \"name\": \"NOTIFICATION_TOPIC\", \"value\" : \"arn:aws:sns...\" }, { \"name\": \"AWS_XRAY_DAEMON_ADDRESS\", \"value\" : \"xray-daemon:2000\" } ], \"portMappings\": [ { \"hostPort\": 5000, \"containerPort\": 5000, } ], \"links\": [ \"xray-daemon\" ] } https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ecs.html#xray-daemon-ecs-build","title":"X-Ray and ECS"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/19-x-ray-and-ecs/#x-ray-and-ecs","text":"","title":"X-Ray and ECS"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/19-x-ray-and-ecs/#example-task-definition","text":"{ \"name\": \"xray-daemon\", \"image\": \"123456789012.dkr.ecr.us-east-2.amazonaws.com/xray-daemon\", \"cpu\": 32, \"memoryReservation\": 256, \"portMappings\": [ { \"hostPort\": 0, \"containerPort\": 2000, \"protocol\": \"udp\" } ] }, { \"name\": \"scorekeep-api\", \"image\": \"123456789012.dkr.ecr.us-east-2.amazonaws.com/scorekeep-api\", \"cpu\": 192, \"memoryReservation\": 512, \"environment\": [ { \"name\": \"AWS_REGION\", \"value\" : \"us-east-2\" }, { \"name\": \"NOTIFICATION_TOPIC\", \"value\" : \"arn:aws:sns...\" }, { \"name\": \"AWS_XRAY_DAEMON_ADDRESS\", \"value\" : \"xray-daemon:2000\" } ], \"portMappings\": [ { \"hostPort\": 5000, \"containerPort\": 5000, } ], \"links\": [ \"xray-daemon\" ] } https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ecs.html#xray-daemon-ecs-build","title":"Example task definition"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/21-cloudtrail-hands-on/","text":"CloudTrail Hands On \u00b6 We can open up CloudTrail and use filters to see when an instance was terminated. Here we can see that 2 instances were terminated by root, one by Cloud9 and one by AutoScaling. We can also check for Read-only events etc. Trails \u00b6 You can create a trail to capture more events. Event source options are S3 and Lambda. We can see that the CloudTrail is also created in CloudWatch now. You can also view the events in S3 bucket or create an Athena table from the s3 bucket and query them.","title":"CloudTrail Hands On"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/21-cloudtrail-hands-on/#cloudtrail-hands-on","text":"We can open up CloudTrail and use filters to see when an instance was terminated. Here we can see that 2 instances were terminated by root, one by Cloud9 and one by AutoScaling. We can also check for Read-only events etc.","title":"CloudTrail Hands On"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/21-cloudtrail-hands-on/#trails","text":"You can create a trail to capture more events. Event source options are S3 and Lambda. We can see that the CloudTrail is also created in CloudWatch now. You can also view the events in S3 bucket or create an Athena table from the s3 bucket and query them.","title":"Trails"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/22-cloudtrail-vs-cloudwatch-vs-x-ray/","text":"CloudTrail vs CloudWatch vs X-Ray \u00b6 CloudTrail Audit API calls made by users / services / AWS console Useful to detect unauthorized calls or root cause of changes CloudWatch CloudWatch Metrics over time for monitoring CloudWatch Logs for storing application logs CloudWatch Alarms to send notifications in case of unexpected metrics X-Ray Automated Trace Analysis & Central Service Map Visualization Latency, Errors and Fault analysis Request tracking across distributed systems","title":"CloudTrail vs CloudWatch vs X-Ray"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/22-cloudtrail-vs-cloudwatch-vs-x-ray/#cloudtrail-vs-cloudwatch-vs-x-ray","text":"CloudTrail Audit API calls made by users / services / AWS console Useful to detect unauthorized calls or root cause of changes CloudWatch CloudWatch Metrics over time for monitoring CloudWatch Logs for storing application logs CloudWatch Alarms to send notifications in case of unexpected metrics X-Ray Automated Trace Analysis & Central Service Map Visualization Latency, Errors and Fault analysis Request tracking across distributed systems","title":"CloudTrail vs CloudWatch vs X-Ray"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/AWS%20X-Ray/","text":"X-Ray Overview \u00b6 Debugging in Production, the good old way: Test locally Add log statements everywhere Re-deploy in production Log formats differ across applications using CloudWatch and analytics is hard Debugging: monolith \"easy\", distributed services \"hard\" No common views of your entire architecture Enter... AWS X-Ray! Visual analysis of our applications \u00b6 X-Ray advantages \u00b6 Troubleshooting performance (bottlenecks) Understand dependencies in a microservice architecture Pinpoint service issues Review request behavior Find errors and exceptions Are we meeting time SLA? Where I am throttled? Identify users that are impacted X-Ray compatibility \u00b6 AWS Lambda Elastic Beanstalk ECS ELB Api Gateway EC2 Instances or any application server (even on premise) AWS X-Ray Leverages Tracing \u00b6 Tracing is an end to end way to following a request Each component dealing with the request adds its own trace Tracing is made of segments (+ sub segments) Annotations can be added to traces to provide extra-information Ability to trace: Every request Sample request (as % for example or a rate per minute) X-Ray Security IAM for authorization KSM for encryption at rest How to enable it? \u00b6 Your code (Java, Python, Go, Node.js, .NET) must import the AWS X-Ray SDK Very little code modification needed The applications SDK will then capture Calls to AWS services HTTP / HTTPS requests Database calls (MySQL, PostgreSQL, DynamoDB) Queue calls (SQS) Install the X-Ray daemon or enable X-Ray AWS integration X-Ray daemon works as a low level UDP packet interceptor (Linux / Window / Mac...) AWS Lambda / other AWS services alreeady run the X-Ray daemon for you Each application must have the IAM rights to write data to X-Ray The X-Ray magic \u00b6 X-Ray service collects data from all the different services Service mapo is computed from all the segments and traces X-Ray is graphical, so even non technical people can troubleshoot Troubleshooting \u00b6 If X-Ray is not working on EC2 Ensure the EC2 IAM Role has the proper permissions Ensure the EC2 instance is running the X-Ray Daemon To enable on AWS Lambda: Ensure it has an IAM execution role with proper policy (AWSX-RayWriteOnlyAccess) Ensure that X-Ray is imported in the code","title":"X-Ray Overview"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/AWS%20X-Ray/#x-ray-overview","text":"Debugging in Production, the good old way: Test locally Add log statements everywhere Re-deploy in production Log formats differ across applications using CloudWatch and analytics is hard Debugging: monolith \"easy\", distributed services \"hard\" No common views of your entire architecture Enter... AWS X-Ray!","title":"X-Ray Overview"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/AWS%20X-Ray/#visual-analysis-of-our-applications","text":"","title":"Visual analysis of our applications"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/AWS%20X-Ray/#x-ray-advantages","text":"Troubleshooting performance (bottlenecks) Understand dependencies in a microservice architecture Pinpoint service issues Review request behavior Find errors and exceptions Are we meeting time SLA? Where I am throttled? Identify users that are impacted","title":"X-Ray advantages"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/AWS%20X-Ray/#x-ray-compatibility","text":"AWS Lambda Elastic Beanstalk ECS ELB Api Gateway EC2 Instances or any application server (even on premise)","title":"X-Ray compatibility"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/AWS%20X-Ray/#aws-x-ray-leverages-tracing","text":"Tracing is an end to end way to following a request Each component dealing with the request adds its own trace Tracing is made of segments (+ sub segments) Annotations can be added to traces to provide extra-information Ability to trace: Every request Sample request (as % for example or a rate per minute) X-Ray Security IAM for authorization KSM for encryption at rest","title":"AWS X-Ray Leverages Tracing"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/AWS%20X-Ray/#how-to-enable-it","text":"Your code (Java, Python, Go, Node.js, .NET) must import the AWS X-Ray SDK Very little code modification needed The applications SDK will then capture Calls to AWS services HTTP / HTTPS requests Database calls (MySQL, PostgreSQL, DynamoDB) Queue calls (SQS) Install the X-Ray daemon or enable X-Ray AWS integration X-Ray daemon works as a low level UDP packet interceptor (Linux / Window / Mac...) AWS Lambda / other AWS services alreeady run the X-Ray daemon for you Each application must have the IAM rights to write data to X-Ray","title":"How to enable it?"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/AWS%20X-Ray/#the-x-ray-magic","text":"X-Ray service collects data from all the different services Service mapo is computed from all the segments and traces X-Ray is graphical, so even non technical people can troubleshoot","title":"The X-Ray magic"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/AWS%20X-Ray/#troubleshooting","text":"If X-Ray is not working on EC2 Ensure the EC2 IAM Role has the proper permissions Ensure the EC2 instance is running the X-Ray Daemon To enable on AWS Lambda: Ensure it has an IAM execution role with proper policy (AWSX-RayWriteOnlyAccess) Ensure that X-Ray is imported in the code","title":"Troubleshooting"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/CloudTrail/","text":"CloudTrail \u00b6 Provides governance, compliance and audit for your AWS Account CloudTrail is enabled by default. Get a history of events / API calls made withing your AWS account by: Console SDK CLI AWS Services Can put logs from CloudTrail into CloudWatch Logs or S3 A trail can be applied to All Regions (default) or a single Region If a resource is deleted in AWS, investigate CloudTrail first! The CloudWatch logs and S3 bucket is for the logs we want to keep more than 90 days. Events \u00b6 Management Events: Operations that are performed on resources in your AWS account Configuring security (IAM AttachRolePolicy) Configuring rules for routing data (AmazonEC2CreateSubnet) Settung up logging (AwsCloudTrailCreateTrail) By default, trails are configured to log management events Can separate ReadEvents (that don't modify resources) from WriteEvents (that may modify resources) Data Events By default, data events are not logged (because high volume operations) Amazon S3 object-level activity (ex: GetObject, DeleteObject, PutObject): can separate Read and Write Events AWS Lambda function execution activity (the Invoke API) CloudTrail Insights Events CloudTrail Insights \u00b6 Enable CloudTrail Insights to detect unusual activity in your account: Inaccurate resource provisioning Hitting service limits Bursts of AWS IAM actions Gaps in periodic maintenance activity CloudTrail Insights analyzes normal management events to create a baseline And then continuously analyzes write events to detect unusual patterns Anomalies appear in the CloudTrail console Event is sent to Amazon S3 An EventBridge event is generated for (automation needs) Events Retention \u00b6 Events are stored for 90 days in CloudTrail To keep events beyond this period, log them to S3 and use Athena","title":"CloudTrail"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/CloudTrail/#cloudtrail","text":"Provides governance, compliance and audit for your AWS Account CloudTrail is enabled by default. Get a history of events / API calls made withing your AWS account by: Console SDK CLI AWS Services Can put logs from CloudTrail into CloudWatch Logs or S3 A trail can be applied to All Regions (default) or a single Region If a resource is deleted in AWS, investigate CloudTrail first! The CloudWatch logs and S3 bucket is for the logs we want to keep more than 90 days.","title":"CloudTrail"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/CloudTrail/#events","text":"Management Events: Operations that are performed on resources in your AWS account Configuring security (IAM AttachRolePolicy) Configuring rules for routing data (AmazonEC2CreateSubnet) Settung up logging (AwsCloudTrailCreateTrail) By default, trails are configured to log management events Can separate ReadEvents (that don't modify resources) from WriteEvents (that may modify resources) Data Events By default, data events are not logged (because high volume operations) Amazon S3 object-level activity (ex: GetObject, DeleteObject, PutObject): can separate Read and Write Events AWS Lambda function execution activity (the Invoke API) CloudTrail Insights Events","title":"Events"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/CloudTrail/#cloudtrail-insights","text":"Enable CloudTrail Insights to detect unusual activity in your account: Inaccurate resource provisioning Hitting service limits Bursts of AWS IAM actions Gaps in periodic maintenance activity CloudTrail Insights analyzes normal management events to create a baseline And then continuously analyzes write events to detect unusual patterns Anomalies appear in the CloudTrail console Event is sent to Amazon S3 An EventBridge event is generated for (automation needs)","title":"CloudTrail Insights"},{"location":"AWS/developer-associate/18-monitoring-audit-cloudwatch-xray-cloudtrail/CloudTrail/#events-retention","text":"Events are stored for 90 days in CloudTrail To keep events beyond this period, log them to S3 and use Athena","title":"Events Retention"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/01-introduction-to-messaging/","text":"Introduction to Messaging \u00b6 When we start deploying multiple applications, we will inevitably need to communicate with one another There are two patterns of application communication - synchronous and asynchronous (event based). Synchronous communication between applications can be problematic if there are sudden spikes of traffic What if you need to suddenly encode 1000 videos but usually it's 10? In that case, it's better to decouple your applications using SQS, queue model using SNS: pub/sub model using Kinesis: real-time streaming model These services can scale independently from our application","title":"Introduction to Messaging"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/01-introduction-to-messaging/#introduction-to-messaging","text":"When we start deploying multiple applications, we will inevitably need to communicate with one another There are two patterns of application communication - synchronous and asynchronous (event based). Synchronous communication between applications can be problematic if there are sudden spikes of traffic What if you need to suddenly encode 1000 videos but usually it's 10? In that case, it's better to decouple your applications using SQS, queue model using SNS: pub/sub model using Kinesis: real-time streaming model These services can scale independently from our application","title":"Introduction to Messaging"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/02-amazon-sqs-standard-queues-overview/","text":"Amazon SQS Standard Queues Overview \u00b6 Amazon SQS - Standard Queue \u00b6 Oldest offering (over 10 years old) Fully managed service, used to decouple applications Attributes: Unlimited throughput, unlimited number of messages in queue Default retention of messages: 4 days, maximum 14 days Low latency (<10 ms on publish and receive) Limitation of 256KB per message sent Can have duplicate messages (at least once delivery, occasionally) Can have out of order messages (best effort ordering) Producing Messages \u00b6 Produced to SQS using the SDK (SendMessage API) The message is persisted in SQS until a consumer deletes it Message retention: 4 days, maximum 14 days Example: send an order to be processed Order ID Customer ID Any attributes you want SQS standard: unlimited throughput Message up to 256KB Consuming Messages \u00b6 Consumers (running on EC2 instances, servers or AWS Lambda) Poll SQS for messages (receive up to 10 messages at a time) Process the messages (example: insert the message into an RDS database) Delete the messages using the DeleteMessage API Multiple EC2 Instances Consumers \u00b6 Consumers receive and process messages in parallel At least once delivery Best-effort message ordering Consumers delete messages after processing them We can scale consumers horizontally to improve throughput of processing SQS with Auto Scaling Group \u00b6 Declouple between application tiers \u00b6 Security \u00b6 Encryption In-flight encryption using HTTPS API At-rest encryption using KMS Keys Client-side encryption if the client wants to perform encryption/decryption itself Access Controls: IAM policies to regulate access to the SQS API SQS Access Policies (similar to S3 bucket policies) Useful for cross-account access to SQS queues Useful for allowing other services (SNS, S3...) to write to an SQS queue","title":"Amazon SQS Standard Queues Overview"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/02-amazon-sqs-standard-queues-overview/#amazon-sqs-standard-queues-overview","text":"","title":"Amazon SQS Standard Queues Overview"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/02-amazon-sqs-standard-queues-overview/#amazon-sqs-standard-queue","text":"Oldest offering (over 10 years old) Fully managed service, used to decouple applications Attributes: Unlimited throughput, unlimited number of messages in queue Default retention of messages: 4 days, maximum 14 days Low latency (<10 ms on publish and receive) Limitation of 256KB per message sent Can have duplicate messages (at least once delivery, occasionally) Can have out of order messages (best effort ordering)","title":"Amazon SQS - Standard Queue"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/02-amazon-sqs-standard-queues-overview/#producing-messages","text":"Produced to SQS using the SDK (SendMessage API) The message is persisted in SQS until a consumer deletes it Message retention: 4 days, maximum 14 days Example: send an order to be processed Order ID Customer ID Any attributes you want SQS standard: unlimited throughput Message up to 256KB","title":"Producing Messages"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/02-amazon-sqs-standard-queues-overview/#consuming-messages","text":"Consumers (running on EC2 instances, servers or AWS Lambda) Poll SQS for messages (receive up to 10 messages at a time) Process the messages (example: insert the message into an RDS database) Delete the messages using the DeleteMessage API","title":"Consuming Messages"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/02-amazon-sqs-standard-queues-overview/#multiple-ec2-instances-consumers","text":"Consumers receive and process messages in parallel At least once delivery Best-effort message ordering Consumers delete messages after processing them We can scale consumers horizontally to improve throughput of processing","title":"Multiple EC2 Instances Consumers"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/02-amazon-sqs-standard-queues-overview/#sqs-with-auto-scaling-group","text":"","title":"SQS with Auto Scaling Group"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/02-amazon-sqs-standard-queues-overview/#declouple-between-application-tiers","text":"","title":"Declouple between application tiers"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/02-amazon-sqs-standard-queues-overview/#security","text":"Encryption In-flight encryption using HTTPS API At-rest encryption using KMS Keys Client-side encryption if the client wants to perform encryption/decryption itself Access Controls: IAM policies to regulate access to the SQS API SQS Access Policies (similar to S3 bucket policies) Useful for cross-account access to SQS queues Useful for allowing other services (SNS, S3...) to write to an SQS queue","title":"Security"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/03-sqs-standard-queue-hands-on/","text":"SQS Standard Queue Hands On \u00b6 We are going to create a new queue Now the queue is created, we can send and receive messages from it. Click on Send and Receive messages when opening up the queue. Each time you poll, the receive count for the message will increase, but it will still persist in the queue. When you are done with the message - delete it. You can also purge the queue - it will remove all the messages in the queue. But that might not be a good idea in production.","title":"SQS Standard Queue Hands On"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/03-sqs-standard-queue-hands-on/#sqs-standard-queue-hands-on","text":"We are going to create a new queue Now the queue is created, we can send and receive messages from it. Click on Send and Receive messages when opening up the queue. Each time you poll, the receive count for the message will increase, but it will still persist in the queue. When you are done with the message - delete it. You can also purge the queue - it will remove all the messages in the queue. But that might not be a good idea in production.","title":"SQS Standard Queue Hands On"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/04-sqs-queue-access-policy/","text":"SQS Queue Access Policy \u00b6 Cross Account \u00b6 { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Allow\", \"Principal\": {\"AWS\": [111122223333]}, \"Action\": [\"sqs:ReceiveMessage\"], \"Resource\": \"arn:aws:sqs:us-east-1:4444555666:queue1\" }] } Publish S3 Event Notifications to SQS Queue \u00b6 { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Allow\", \"Principal\": {\"AWS\": \"*\"}, \"Action\": [\"sqs:SendMessage\"], \"Resource\": \"arn:aws:sqs:us-east-1:4444555666:queue1\", \"Condition\": { \"ArnLike\": {\"aws:SourceArn\": \"arn:aws:s3:*:*:bucket1\"}, \"StringEquals\": {\"aws:SourceAccount\":\"<bucket1_owner_account_id\"} } }] } When creating an S3 bucket: In SQS:","title":"SQS Queue Access Policy"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/04-sqs-queue-access-policy/#sqs-queue-access-policy","text":"","title":"SQS Queue Access Policy"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/04-sqs-queue-access-policy/#cross-account","text":"{ \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Allow\", \"Principal\": {\"AWS\": [111122223333]}, \"Action\": [\"sqs:ReceiveMessage\"], \"Resource\": \"arn:aws:sqs:us-east-1:4444555666:queue1\" }] }","title":"Cross Account"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/04-sqs-queue-access-policy/#publish-s3-event-notifications-to-sqs-queue","text":"{ \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Allow\", \"Principal\": {\"AWS\": \"*\"}, \"Action\": [\"sqs:SendMessage\"], \"Resource\": \"arn:aws:sqs:us-east-1:4444555666:queue1\", \"Condition\": { \"ArnLike\": {\"aws:SourceArn\": \"arn:aws:s3:*:*:bucket1\"}, \"StringEquals\": {\"aws:SourceAccount\":\"<bucket1_owner_account_id\"} } }] } When creating an S3 bucket: In SQS:","title":"Publish S3 Event Notifications to SQS Queue"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/05-sqs-message-visibility-timeout/","text":"SQS Message Visibility Timeout \u00b6 After a message is polled by a consumer, it becomes invisible to other consumers By default, the \"message visibility timeout\" is 30 seconds That means the message has 30 seconds to be processed After the message visibility timeout is over, the message is visible in SQS If a message is not processed within the visibility timeout, it will be processed twice A consumer call the ChangeMessageVisibility API to get more time If visibility timeout is high (hours), and consumer crashes, re-processing will take time If visibility timeout is too low (seconds), we may get duplicates","title":"SQS Message Visibility Timeout"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/05-sqs-message-visibility-timeout/#sqs-message-visibility-timeout","text":"After a message is polled by a consumer, it becomes invisible to other consumers By default, the \"message visibility timeout\" is 30 seconds That means the message has 30 seconds to be processed After the message visibility timeout is over, the message is visible in SQS If a message is not processed within the visibility timeout, it will be processed twice A consumer call the ChangeMessageVisibility API to get more time If visibility timeout is high (hours), and consumer crashes, re-processing will take time If visibility timeout is too low (seconds), we may get duplicates","title":"SQS Message Visibility Timeout"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/06-sqs-dead-letter-queues/","text":"SQS - Dead Letter Queues \u00b6 If a consumer failts to process a message within the Visibility Timeout - the message goes back to the queue. We can set a threshold of how many times a message can go back to the queue. After the MaximumReceives threshold is exceeded, the message goes into a dead letter queue (DLQ) Useful for debugging Make sure to process the messages in the DLQ before they expire Good to set a retention to 14 days in the DLQ Redrive to Source \u00b6 Feature to help consume messages in the DLQ to understand what is wrong with them When our code is fixed, we can redrive the messages from the DLQ back into the source queue (or any other queue) in batches wihout writing custom code.","title":"SQS - Dead Letter Queues"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/06-sqs-dead-letter-queues/#sqs-dead-letter-queues","text":"If a consumer failts to process a message within the Visibility Timeout - the message goes back to the queue. We can set a threshold of how many times a message can go back to the queue. After the MaximumReceives threshold is exceeded, the message goes into a dead letter queue (DLQ) Useful for debugging Make sure to process the messages in the DLQ before they expire Good to set a retention to 14 days in the DLQ","title":"SQS - Dead Letter Queues"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/06-sqs-dead-letter-queues/#redrive-to-source","text":"Feature to help consume messages in the DLQ to understand what is wrong with them When our code is fixed, we can redrive the messages from the DLQ back into the source queue (or any other queue) in batches wihout writing custom code.","title":"Redrive to Source"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/07-sqs-delay-queues/","text":"SQS - Delay Queue \u00b6 Delay a message (consumers don't see it immediately) up to 15 minutes Default is 0 seconds (message is available right away) Can set a default at queue level Can override the default on send using the DelaySeconds parameter","title":"SQS - Delay Queue"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/07-sqs-delay-queues/#sqs-delay-queue","text":"Delay a message (consumers don't see it immediately) up to 15 minutes Default is 0 seconds (message is available right away) Can set a default at queue level Can override the default on send using the DelaySeconds parameter","title":"SQS - Delay Queue"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/08-sqs-certified-developer-concepts/","text":"SQS - Certified Developer Concepts \u00b6 Long Polling \u00b6 When a consumer requests messages from the queue, it can optionally \"wait\" for messages to arrive if there are none in the queue This is called Long Polling LongPolling decreases the number of API calls made to SQS while increasing the efficiency and latency of your application. The wait time can be between 1 sec to 20 sec (20 sec preferable). Long polling is preferable to Short Polling Long polling can be enabled at the queue level or at the API level using WaitTimeSeconds . Extended Client \u00b6 Message size is 256KB, how to send large messages, e.g. 1GB? Using the SQS Extended Client (Java Library) Must know API \u00b6 CreateQueue (MessageRetentionPeriod), DeleteQueue PurgeQueue: delete all the messages in the queue SendMessage (DelaySeconds), ReceiveMessage, DeleteMessage MaxNumberOfMessages: default 1, max 10 (for ReceiveMessage API) ReceiveMessageWaitTimeSeconds: Long Polling ChangeVisibility: change the message timeout Batch APIs for SendMessage, DeleteMessage, ChangeMessageVisibility helps decrease your costs.","title":"SQS - Certified Developer Concepts"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/08-sqs-certified-developer-concepts/#sqs-certified-developer-concepts","text":"","title":"SQS - Certified Developer Concepts"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/08-sqs-certified-developer-concepts/#long-polling","text":"When a consumer requests messages from the queue, it can optionally \"wait\" for messages to arrive if there are none in the queue This is called Long Polling LongPolling decreases the number of API calls made to SQS while increasing the efficiency and latency of your application. The wait time can be between 1 sec to 20 sec (20 sec preferable). Long polling is preferable to Short Polling Long polling can be enabled at the queue level or at the API level using WaitTimeSeconds .","title":"Long Polling"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/08-sqs-certified-developer-concepts/#extended-client","text":"Message size is 256KB, how to send large messages, e.g. 1GB? Using the SQS Extended Client (Java Library)","title":"Extended Client"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/08-sqs-certified-developer-concepts/#must-know-api","text":"CreateQueue (MessageRetentionPeriod), DeleteQueue PurgeQueue: delete all the messages in the queue SendMessage (DelaySeconds), ReceiveMessage, DeleteMessage MaxNumberOfMessages: default 1, max 10 (for ReceiveMessage API) ReceiveMessageWaitTimeSeconds: Long Polling ChangeVisibility: change the message timeout Batch APIs for SendMessage, DeleteMessage, ChangeMessageVisibility helps decrease your costs.","title":"Must know API"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/09-sqs-fifo-queues/","text":"Amazon SQS - FIFO Queue \u00b6 FIFO - First In First Out (ordering messages in the queue) Limited throughput: 300msg/s without matching, 3000 msg/s with batching Exactly-once send capability (by removing duplicates) Messages are processed in order by the consumer You have to end the queue name with .fifo .","title":"Amazon SQS - FIFO Queue"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/09-sqs-fifo-queues/#amazon-sqs-fifo-queue","text":"FIFO - First In First Out (ordering messages in the queue) Limited throughput: 300msg/s without matching, 3000 msg/s with batching Exactly-once send capability (by removing duplicates) Messages are processed in order by the consumer You have to end the queue name with .fifo .","title":"Amazon SQS - FIFO Queue"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/10-sqs-fifo-queues-advanced/","text":"SQS FIFO Queues Advanced \u00b6 Deduplication \u00b6 De-duplication interval is 5 minutes Two de-duplication methods: Content-based deduplication: will do a SHA-256 hash of the message body Explicitly provide a Message Deduplication ID Message Grouping \u00b6 If you specify the same value of MessageGroupID in an SQS FIFO queue, you can only have one consumer, and all the messages are in order To get ordering at the level of a subset of messages, specify different values for MessageGroupID Messages that share a common MessageGroupID will be in order within the group Each Group ID can have different consumer (parallel processing) Ordering across groups is not guaranteed","title":"SQS FIFO Queues Advanced"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/10-sqs-fifo-queues-advanced/#sqs-fifo-queues-advanced","text":"","title":"SQS FIFO Queues Advanced"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/10-sqs-fifo-queues-advanced/#deduplication","text":"De-duplication interval is 5 minutes Two de-duplication methods: Content-based deduplication: will do a SHA-256 hash of the message body Explicitly provide a Message Deduplication ID","title":"Deduplication"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/10-sqs-fifo-queues-advanced/#message-grouping","text":"If you specify the same value of MessageGroupID in an SQS FIFO queue, you can only have one consumer, and all the messages are in order To get ordering at the level of a subset of messages, specify different values for MessageGroupID Messages that share a common MessageGroupID will be in order within the group Each Group ID can have different consumer (parallel processing) Ordering across groups is not guaranteed","title":"Message Grouping"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/11-amazon-sns/","text":"Amazon SNS \u00b6 What if you want to send one message to many receivers? The \"event producer\" only sends message to SNS topic As many \"event receivers\" (subscriptions) as we want to listen to the SNS topic notifications Each subscriber to the topic will get all the messages (note: new feature to filter messages) Up to 12,500,000 subscriptions per topic 100,000 topics limit Many AWS services can send data directly to SNS for notifications How to publish \u00b6 Topic Publish (using the SDK) Create a topic Create a subscription (or many) Publish to the topic Direct publish (for mobile apps SDK) Create a platform application Create a platform endpoint Publish to the platform endpoint Works with Google GCM, Apple APNS, Amazon ADM Security \u00b6 Encryption: In-flight encryption using HTTPS API At-rest encryption using KMS Client-side encryption if the client wants to perform encryption/decryption itself Access Controls: IAM policies to regulate access to the SNS API SNS Access Policies (similar to S3 bucket policies) useful for cross-account access to SNS topics Useful for allowing other services (s3...) to write to an SNS topic","title":"Amazon SNS"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/11-amazon-sns/#amazon-sns","text":"What if you want to send one message to many receivers? The \"event producer\" only sends message to SNS topic As many \"event receivers\" (subscriptions) as we want to listen to the SNS topic notifications Each subscriber to the topic will get all the messages (note: new feature to filter messages) Up to 12,500,000 subscriptions per topic 100,000 topics limit Many AWS services can send data directly to SNS for notifications","title":"Amazon SNS"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/11-amazon-sns/#how-to-publish","text":"Topic Publish (using the SDK) Create a topic Create a subscription (or many) Publish to the topic Direct publish (for mobile apps SDK) Create a platform application Create a platform endpoint Publish to the platform endpoint Works with Google GCM, Apple APNS, Amazon ADM","title":"How to publish"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/11-amazon-sns/#security","text":"Encryption: In-flight encryption using HTTPS API At-rest encryption using KMS Client-side encryption if the client wants to perform encryption/decryption itself Access Controls: IAM policies to regulate access to the SNS API SNS Access Policies (similar to S3 bucket policies) useful for cross-account access to SNS topics Useful for allowing other services (s3...) to write to an SNS topic","title":"Security"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/12-amazon-sns-and-sqs-fan-out-pattern/","text":"SNS + SQS: Fan Out \u00b6 Push once in SNS, receive all in all SQS queues that are subscribers Fully decoupled, no data loss SQS allows for: data persistence, delayed processing and retries of work Ability to add more SQS subscribers over time Make sure your SQS queue access policy allows for SNS to write Application: S3 Events to multiple queues \u00b6 For the same combination of: event type (e.g. object create) and prefix (e.g.images/) you can only have one S3 Event Rule. If you want to send the same S3 event to many SQS queues, use fan-out Application: SNS to Amazon S3 through Kinesis Data Firehose \u00b6 SNS can send to Kinesis and therefore we can have the following solutions architecture: FIFO Topic \u00b6 FIFO - First In First Out (ordering of messages in the topic) Similar features as SQS FIFO Ordering by Message Group ID (all messages in the same group are ordered) Deduplication using a Deduplication ID or Content Based Deduplication Can only have SQS FIFO queus as subscribers Limited throughput (same as thoughput as SQS FIFO) SNS FIFO + SQS FIFO: Fan Out \u00b6 SNS - Message Filtering \u00b6 JSON policy used to filter messages sent to SNS topic's subscriptions If a subscription doesn't have a filter policy, it receives every message","title":"SNS + SQS: Fan Out"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/12-amazon-sns-and-sqs-fan-out-pattern/#sns-sqs-fan-out","text":"Push once in SNS, receive all in all SQS queues that are subscribers Fully decoupled, no data loss SQS allows for: data persistence, delayed processing and retries of work Ability to add more SQS subscribers over time Make sure your SQS queue access policy allows for SNS to write","title":"SNS + SQS: Fan Out"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/12-amazon-sns-and-sqs-fan-out-pattern/#application-s3-events-to-multiple-queues","text":"For the same combination of: event type (e.g. object create) and prefix (e.g.images/) you can only have one S3 Event Rule. If you want to send the same S3 event to many SQS queues, use fan-out","title":"Application: S3 Events to multiple queues"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/12-amazon-sns-and-sqs-fan-out-pattern/#application-sns-to-amazon-s3-through-kinesis-data-firehose","text":"SNS can send to Kinesis and therefore we can have the following solutions architecture:","title":"Application: SNS to Amazon S3 through Kinesis Data Firehose"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/12-amazon-sns-and-sqs-fan-out-pattern/#fifo-topic","text":"FIFO - First In First Out (ordering of messages in the topic) Similar features as SQS FIFO Ordering by Message Group ID (all messages in the same group are ordered) Deduplication using a Deduplication ID or Content Based Deduplication Can only have SQS FIFO queus as subscribers Limited throughput (same as thoughput as SQS FIFO)","title":"FIFO Topic"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/12-amazon-sns-and-sqs-fan-out-pattern/#sns-fifo-sqs-fifo-fan-out","text":"","title":"SNS FIFO + SQS FIFO: Fan Out"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/12-amazon-sns-and-sqs-fan-out-pattern/#sns-message-filtering","text":"JSON policy used to filter messages sent to SNS topic's subscriptions If a subscription doesn't have a filter policy, it receives every message","title":"SNS - Message Filtering"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/13-sns-hands-on/","text":"SNS Hands On \u00b6 We are going to create a new topic. When the topic is created, we are going to create a new subscription. We can also set the Subscription Filter Policy Now we can publish a message","title":"SNS Hands On"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/13-sns-hands-on/#sns-hands-on","text":"We are going to create a new topic. When the topic is created, we are going to create a new subscription. We can also set the Subscription Filter Policy Now we can publish a message","title":"SNS Hands On"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/14-kinesis-overview/","text":"Kinesis Overview \u00b6 Makes it easy to collect, process and analyize streaming data in real-time Ingest real-time data such as: Application logs, Metrics, Website clickstreams, IoT telemetry data... Kinesis Data Streams: capture, process and store data streams Kinesis Data Firehose: load data streams into AWS data stores Kinesis Data Analytics: analyze data streams with SQL or Apache Flink Kinesis Video Streams: capture, process and store video streams","title":"Kinesis Overview"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/14-kinesis-overview/#kinesis-overview","text":"Makes it easy to collect, process and analyize streaming data in real-time Ingest real-time data such as: Application logs, Metrics, Website clickstreams, IoT telemetry data... Kinesis Data Streams: capture, process and store data streams Kinesis Data Firehose: load data streams into AWS data stores Kinesis Data Analytics: analyze data streams with SQL or Apache Flink Kinesis Video Streams: capture, process and store video streams","title":"Kinesis Overview"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/15-kinesis-data-streams-overview/","text":"Kinesis Data Streams \u00b6 Retention between 1 day to 365 days Ability to reprocess (replay) data Once data is inserted in Kinesis, it can't be deleted (immutability) Data that shares the same partition goes to the same shard (ordering) Producers: AWS SDK, Kinesis Producer Library (KPL), Kinesis Agent Consumers: Write your own: Kinesis Client Library (KCL), AWS SDK Managed: AWS Lambda, Kinesis Data Firehose, Kinesis Data Analytics Capacity Modes \u00b6 Provisioned mode: You choose the number of shards provisioned, scale manually or using API Each shard gets 1MB/s in (or 1000 records per second) Each shard gets 2MB/s out (classic or enhanced fan-out consumer) You pay per shard provisioned per hour On-demand mode: No need to provision or manage the capacity Default capacity provisioned (4MB/s in our 4000 records per second) Scales automatically based on observed throughput peak during the last 30 days Pay per stream per hour & data in/out per GB Kinesis Data Streams Security \u00b6 Control access / authorizatioin using IAM policies Encryption in flight using HTTP endpoints Encryption at rest using KMS You can implement encryption / decryption of data on client side (harder) VPC Endpoints available for Kinesis to access within VPC Monitor API calls using CloudTrail","title":"Kinesis Data Streams"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/15-kinesis-data-streams-overview/#kinesis-data-streams","text":"Retention between 1 day to 365 days Ability to reprocess (replay) data Once data is inserted in Kinesis, it can't be deleted (immutability) Data that shares the same partition goes to the same shard (ordering) Producers: AWS SDK, Kinesis Producer Library (KPL), Kinesis Agent Consumers: Write your own: Kinesis Client Library (KCL), AWS SDK Managed: AWS Lambda, Kinesis Data Firehose, Kinesis Data Analytics","title":"Kinesis Data Streams"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/15-kinesis-data-streams-overview/#capacity-modes","text":"Provisioned mode: You choose the number of shards provisioned, scale manually or using API Each shard gets 1MB/s in (or 1000 records per second) Each shard gets 2MB/s out (classic or enhanced fan-out consumer) You pay per shard provisioned per hour On-demand mode: No need to provision or manage the capacity Default capacity provisioned (4MB/s in our 4000 records per second) Scales automatically based on observed throughput peak during the last 30 days Pay per stream per hour & data in/out per GB","title":"Capacity Modes"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/15-kinesis-data-streams-overview/#kinesis-data-streams-security","text":"Control access / authorizatioin using IAM policies Encryption in flight using HTTP endpoints Encryption at rest using KMS You can implement encryption / decryption of data on client side (harder) VPC Endpoints available for Kinesis to access within VPC Monitor API calls using CloudTrail","title":"Kinesis Data Streams Security"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/16-kinesis-producers/","text":"Kinesis Producers \u00b6 Puts data records into data streams Data record consists of: Sequence number (unique per partition-key within shard) Partition key (must specify while put records into stream) Data blob (up to 1 MB) Producers: AWS SDK: simple producer Kinesis Producer Library (KPL): C++, Java, batch, compression, retries Kinesis agent: monitor log files Write throughput: 1MB/s or 1000 records/s per shard PutRecord API Use batching with PutRecords API to reduce costs & increase throughput ProvisionedThroughputExceeded \u00b6 Use highly distributed partition key Retries with exponential backoff Increase shards (scaling)","title":"Kinesis Producers"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/16-kinesis-producers/#kinesis-producers","text":"Puts data records into data streams Data record consists of: Sequence number (unique per partition-key within shard) Partition key (must specify while put records into stream) Data blob (up to 1 MB) Producers: AWS SDK: simple producer Kinesis Producer Library (KPL): C++, Java, batch, compression, retries Kinesis agent: monitor log files Write throughput: 1MB/s or 1000 records/s per shard PutRecord API Use batching with PutRecords API to reduce costs & increase throughput","title":"Kinesis Producers"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/16-kinesis-producers/#provisionedthroughputexceeded","text":"Use highly distributed partition key Retries with exponential backoff Increase shards (scaling)","title":"ProvisionedThroughputExceeded"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/17-kinesis-consumers/","text":"Kinesis Data Streams Consumers \u00b6 Get data records from data streams and process them AWS Lambda Kinesis Data Analytics Kinesis Data Firehose Custom Consumer (AWS SDK) - Classic or Enhanced Fan-Out Kinesis Client Library (KCL): library to simplify reading from data stream Custom Consumer Shared (Classic) Fan-out Consumer Low number of consuming applications Read throughput: 2MB/s per shard across all consumers Max: 5 GetRecords API calls/sec Latency ~200ms Minimize cost Consumer poll data from Kinesis using GetRecords API call Returns up to 10 MB (then throttle for 5 seconds) or up to 10000 records Enhanced Fan-out Consumer Multiple consuming applications for the same stream 2MB/sec per consumer per shard Latency ~70ms Higher costs Kinesis pushes data to consumers over HTTP/2 (SubscribeToShard API) Soft limit of 5 consumer applications (KCL) per data stream (default) AWS Lambda Supports Classic & Enhanced Fan-out consumers Read records in batches Can configure batch size and batch window If error occurs, Lambda retries until succeeds or data expired Can process up to 10 batches per shard simultaneously","title":"Kinesis Data Streams Consumers"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/17-kinesis-consumers/#kinesis-data-streams-consumers","text":"Get data records from data streams and process them AWS Lambda Kinesis Data Analytics Kinesis Data Firehose Custom Consumer (AWS SDK) - Classic or Enhanced Fan-Out Kinesis Client Library (KCL): library to simplify reading from data stream Custom Consumer Shared (Classic) Fan-out Consumer Low number of consuming applications Read throughput: 2MB/s per shard across all consumers Max: 5 GetRecords API calls/sec Latency ~200ms Minimize cost Consumer poll data from Kinesis using GetRecords API call Returns up to 10 MB (then throttle for 5 seconds) or up to 10000 records Enhanced Fan-out Consumer Multiple consuming applications for the same stream 2MB/sec per consumer per shard Latency ~70ms Higher costs Kinesis pushes data to consumers over HTTP/2 (SubscribeToShard API) Soft limit of 5 consumer applications (KCL) per data stream (default) AWS Lambda Supports Classic & Enhanced Fan-out consumers Read records in batches Can configure batch size and batch window If error occurs, Lambda retries until succeeds or data expired Can process up to 10 batches per shard simultaneously","title":"Kinesis Data Streams Consumers"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/18-kinesis-data-streams-hands-on/","text":"Kinesis Data Streams Hands On \u00b6 $ aws --version # Producer for CLI v2 $ aws kinesis put-record --stream-name test --partition-key user1 --data \"user signup\" --cli-binary-format raw-in-base64-out # Producer for CLI v1 $ aws kinesis put-record --stream-name test --partition-key user1 --data \"user signup\" # Consumer $ aws kinesis describe-stream --stream-name test # We get the shard id from the describe and get a shard iterator next $ aws kinesis get-shard-iterator --stream-name test --shard-id shardId-000000000 --shard-iterator-type TRIM_HORIZON # next, we use the shard iterator to get the records { \"ShardIterator\": \"AAAAA...==\" } $ aws kinesis get records --shard-iterator <iterator> { \"Records\": [ { \"SequenceNumber\": \"123456\", \"ApproximateArrivalTimestamp\": \"...\", \"Data\": \"dXNlciBzaWdudXA=\", # base64 encoded \"PartitionKey\": \"user1\" }, ... ], \"NextSharditerator\": \"AAAA...==\" }","title":"Kinesis Data Streams Hands On"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/18-kinesis-data-streams-hands-on/#kinesis-data-streams-hands-on","text":"$ aws --version # Producer for CLI v2 $ aws kinesis put-record --stream-name test --partition-key user1 --data \"user signup\" --cli-binary-format raw-in-base64-out # Producer for CLI v1 $ aws kinesis put-record --stream-name test --partition-key user1 --data \"user signup\" # Consumer $ aws kinesis describe-stream --stream-name test # We get the shard id from the describe and get a shard iterator next $ aws kinesis get-shard-iterator --stream-name test --shard-id shardId-000000000 --shard-iterator-type TRIM_HORIZON # next, we use the shard iterator to get the records { \"ShardIterator\": \"AAAAA...==\" } $ aws kinesis get records --shard-iterator <iterator> { \"Records\": [ { \"SequenceNumber\": \"123456\", \"ApproximateArrivalTimestamp\": \"...\", \"Data\": \"dXNlciBzaWdudXA=\", # base64 encoded \"PartitionKey\": \"user1\" }, ... ], \"NextSharditerator\": \"AAAA...==\" }","title":"Kinesis Data Streams Hands On"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/19-kinesis-client-library/","text":"Kinesis Client Library (KCL) \u00b6 A java library that helps read record from a Kinesis Data Stream with distributed applications sharing the read workload Each shard is to be read by only one KCL instance 4 shards = max 4 KCL instances 6 shards = max 6 KCL instances Progress is checkpointed into DynamoDB (needs IAM access) Track other workers and share the work amongst shards using DynamoDB KCL can run on EC2, ElasticBeanstalk, and on premises Records are read in order at the shard level Versions: KCL 1.x (supports shared consumer) KCL 2.x (supports shared consumer & enhanced fan-out consumer)","title":"Kinesis Client Library (KCL)"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/19-kinesis-client-library/#kinesis-client-library-kcl","text":"A java library that helps read record from a Kinesis Data Stream with distributed applications sharing the read workload Each shard is to be read by only one KCL instance 4 shards = max 4 KCL instances 6 shards = max 6 KCL instances Progress is checkpointed into DynamoDB (needs IAM access) Track other workers and share the work amongst shards using DynamoDB KCL can run on EC2, ElasticBeanstalk, and on premises Records are read in order at the shard level Versions: KCL 1.x (supports shared consumer) KCL 2.x (supports shared consumer & enhanced fan-out consumer)","title":"Kinesis Client Library (KCL)"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/20-kinesis-operations/","text":"Kinesis Operations \u00b6 Shard Splitting \u00b6 Used to increase the Stream capacity (1 MB/s data in per shard) Used to divide a \"hot shard\" The old shard is closed and will be deleted once the data is expired No automatic scaling (manually increase/decrease capacity) Can't split into more than two shards in a single operation Merging Shards \u00b6 Decrease the Stream Capacity and save costs Can be used to group two shards with low traffic (cold shards) Old shards are closed and will be deleted once the data is expired Can't merge more than two shards in a single operation","title":"Kinesis Operations"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/20-kinesis-operations/#kinesis-operations","text":"","title":"Kinesis Operations"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/20-kinesis-operations/#shard-splitting","text":"Used to increase the Stream capacity (1 MB/s data in per shard) Used to divide a \"hot shard\" The old shard is closed and will be deleted once the data is expired No automatic scaling (manually increase/decrease capacity) Can't split into more than two shards in a single operation","title":"Shard Splitting"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/20-kinesis-operations/#merging-shards","text":"Decrease the Stream Capacity and save costs Can be used to group two shards with low traffic (cold shards) Old shards are closed and will be deleted once the data is expired Can't merge more than two shards in a single operation","title":"Merging Shards"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/21-kinesis-data-firehose-overview/","text":"Kinesis Data Firehose \u00b6 Fully Managed Service, no administration, automatic scaling, serverless AWS: Redshift / Amazon S3 / ElasticSearch 3rd part partner: Splunk / MongoDB / DataDog / NewRelic / ... Custom: send to any HTTP endpoint Pay for data going through Firehose Near Real Time 60 seconds latency minumum for non full batches or minumum 32 MB of data at a time Supports many data formats, conversions, transformations, compression Supports custom data transformations using AWS Lambda Can send failed or all data to backup S3 bucket Kinesis Data Streams vs Firehose \u00b6 Kinesis Data Streams Streaming service for ingest at scale Write custom code (producer / consumer) Real-time (~200ms) Manage scaling (shard splitting / merging) Data storage for 1 to 365 days Supports replay capability Kinesis Data Firehose Load streaming data into S3 / Redshift / ES / 3rd party / Custom HTTP Fully managed Near real-time (buffer time min 60 sec) Automatic scaling No data storage Doesn't support replay capability","title":"Kinesis Data Firehose"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/21-kinesis-data-firehose-overview/#kinesis-data-firehose","text":"Fully Managed Service, no administration, automatic scaling, serverless AWS: Redshift / Amazon S3 / ElasticSearch 3rd part partner: Splunk / MongoDB / DataDog / NewRelic / ... Custom: send to any HTTP endpoint Pay for data going through Firehose Near Real Time 60 seconds latency minumum for non full batches or minumum 32 MB of data at a time Supports many data formats, conversions, transformations, compression Supports custom data transformations using AWS Lambda Can send failed or all data to backup S3 bucket","title":"Kinesis Data Firehose"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/21-kinesis-data-firehose-overview/#kinesis-data-streams-vs-firehose","text":"Kinesis Data Streams Streaming service for ingest at scale Write custom code (producer / consumer) Real-time (~200ms) Manage scaling (shard splitting / merging) Data storage for 1 to 365 days Supports replay capability Kinesis Data Firehose Load streaming data into S3 / Redshift / ES / 3rd party / Custom HTTP Fully managed Near real-time (buffer time min 60 sec) Automatic scaling No data storage Doesn't support replay capability","title":"Kinesis Data Streams vs Firehose"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/22-kinesis-data-firehose-hands-on/","text":"Kinesis Data Firehose Hands On \u00b6 We are going to create a new data stream. We have an option to trasform the records with AWS Lambda before they are being delivered by Firehose. Also, we have an option to transform the record format into Apache Parquet or Apache ORC. Next, we are setting the destination Under the additional settings we can configure the buffer size it will wait to fill as well as the interval. For example, it will wait until it fills 1MB buffer or times out at 60 seconds and then writes it into S3.","title":"Kinesis Data Firehose Hands On"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/22-kinesis-data-firehose-hands-on/#kinesis-data-firehose-hands-on","text":"We are going to create a new data stream. We have an option to trasform the records with AWS Lambda before they are being delivered by Firehose. Also, we have an option to transform the record format into Apache Parquet or Apache ORC. Next, we are setting the destination Under the additional settings we can configure the buffer size it will wait to fill as well as the interval. For example, it will wait until it fills 1MB buffer or times out at 60 seconds and then writes it into S3.","title":"Kinesis Data Firehose Hands On"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/23-kinesis-data-analytics/","text":"Kinesis Data Analytics \u00b6 Perform real-time analytics on Kinesis Streams using SQL Fully managed, no servers to provision Automatic scaling Real-time analytics Pay for actual consumption rate Can create streams out of real-time queries Use cases: Time-series analytics Real-time dashboards Real-time metrics SQS \u00b6 Consumer \"pull data\" Data is deleted after being consumed Can have as many workers (consumers) as we want No need to provision throughput Ordering guarantees only on FIFO queues Individual message delay capability SNS \u00b6 Push data to many subscribers Up to 12,500,000 subscribers Data is not persisted (list if delivered) Pub/Sub Up to 100,000 topics No need to provision throughput Integrates with SQS for fan-out architecture pattern FIFO capability for SQS FIFO Kinesis \u00b6 Standard: pull data (2MB per shard) Enhanced fan-out: push data(2MB per shard per consumer) Possibility to replay data Meant for real-time big data, analytics and ETL Ordering at the shard level Data expires after X days Provisioned mode or on-demand capacity mode","title":"Kinesis Data Analytics"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/23-kinesis-data-analytics/#kinesis-data-analytics","text":"Perform real-time analytics on Kinesis Streams using SQL Fully managed, no servers to provision Automatic scaling Real-time analytics Pay for actual consumption rate Can create streams out of real-time queries Use cases: Time-series analytics Real-time dashboards Real-time metrics","title":"Kinesis Data Analytics"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/23-kinesis-data-analytics/#sqs","text":"Consumer \"pull data\" Data is deleted after being consumed Can have as many workers (consumers) as we want No need to provision throughput Ordering guarantees only on FIFO queues Individual message delay capability","title":"SQS"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/23-kinesis-data-analytics/#sns","text":"Push data to many subscribers Up to 12,500,000 subscribers Data is not persisted (list if delivered) Pub/Sub Up to 100,000 topics No need to provision throughput Integrates with SQS for fan-out architecture pattern FIFO capability for SQS FIFO","title":"SNS"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/23-kinesis-data-analytics/#kinesis","text":"Standard: pull data (2MB per shard) Enhanced fan-out: push data(2MB per shard per consumer) Possibility to replay data Meant for real-time big data, analytics and ETL Ordering at the shard level Data expires after X days Provisioned mode or on-demand capacity mode","title":"Kinesis"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/24-data-ordering-into-kinesis/","text":"Ordering data into Kinesis \u00b6 Imagine you have 100 trucks (truck_1, truck_2, ... truck_100)on the road sending their GPS positions regularly into AWS. You want to consume data in order for each truck, so that you can track their movement accurately. How should you send that data into Kinesis? Answer: send using a \"Partition Key\" value of the \"truck_id\" The same key will always go to the same shard. Ordering data into SQS \u00b6 For SQS standard, there is no ordering For SQS FIFO, if you don't use a Group ID, messages are consumed in the order they are sent, with only one consumer You want to scale the number of consumers, but you want messages to be \"grouped\" when they are related to each other Then you use a Group ID (similar to Partition Key in Kinesis). https://aws.amazon.com/blogs/compute/solving-complex-ordering-challenges-with-amazon-sqs-fifo-queues/ Kinesis vs SQS ordering \u00b6 Let's assume 100 trucks, 5 kinesis shards, 1 SQS FIFO Kinesis Data Streams On average you'll have 20 trucks per shard Trucks will have their data ordered within each shard The maximum amount of consumers in parallel we can have is 5 Can receive up to 5MB/s of data SQS FIFO You only have one SQS FIFO queue You will have 100 Group ID You can have up to 100 Consumers (due to the 100 Group ID) You have up to 300 messages per second (or 3000 if using batching)","title":"Ordering data into Kinesis"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/24-data-ordering-into-kinesis/#ordering-data-into-kinesis","text":"Imagine you have 100 trucks (truck_1, truck_2, ... truck_100)on the road sending their GPS positions regularly into AWS. You want to consume data in order for each truck, so that you can track their movement accurately. How should you send that data into Kinesis? Answer: send using a \"Partition Key\" value of the \"truck_id\" The same key will always go to the same shard.","title":"Ordering data into Kinesis"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/24-data-ordering-into-kinesis/#ordering-data-into-sqs","text":"For SQS standard, there is no ordering For SQS FIFO, if you don't use a Group ID, messages are consumed in the order they are sent, with only one consumer You want to scale the number of consumers, but you want messages to be \"grouped\" when they are related to each other Then you use a Group ID (similar to Partition Key in Kinesis). https://aws.amazon.com/blogs/compute/solving-complex-ordering-challenges-with-amazon-sqs-fifo-queues/","title":"Ordering data into SQS"},{"location":"AWS/developer-associate/19-aws-integration-messaging-sqs-sns-kinesis/24-data-ordering-into-kinesis/#kinesis-vs-sqs-ordering","text":"Let's assume 100 trucks, 5 kinesis shards, 1 SQS FIFO Kinesis Data Streams On average you'll have 20 trucks per shard Trucks will have their data ordered within each shard The maximum amount of consumers in parallel we can have is 5 Can receive up to 5MB/s of data SQS FIFO You only have one SQS FIFO queue You will have 100 Group ID You can have up to 100 Consumers (due to the 100 Group ID) You have up to 300 messages per second (or 3000 if using batching)","title":"Kinesis vs SQS ordering"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/01-serverless-introduction/","text":"Serverless introduction \u00b6 Serverless is a new paradigm in which the developers don't have to manage servers anymore They just deploy code They just deploy... functions! Initially Serverless == FaaS (Function as a Service) Serverless was pioneered by AWS Lambda but not also includes anything that's managed: databases, messaging, storage etc Serverless does not mean there are no servers - it means you just don't manage / provision / see them.","title":"Serverless introduction"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/01-serverless-introduction/#serverless-introduction","text":"Serverless is a new paradigm in which the developers don't have to manage servers anymore They just deploy code They just deploy... functions! Initially Serverless == FaaS (Function as a Service) Serverless was pioneered by AWS Lambda but not also includes anything that's managed: databases, messaging, storage etc Serverless does not mean there are no servers - it means you just don't manage / provision / see them.","title":"Serverless introduction"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/02-aws-lambda-overview/","text":"AWS Lambda Overview \u00b6 Why AWS Lambda \u00b6 Amazon EC2 Virtual Servers in the Cloud Limited by RAM and CPU Continously running Scaling means intervention to add / remove servers Amazon Lambda Virtual functions - no servers to manage Limited by time - short executions Run on-demand Scaling is automated Benefits of AWS Lambda \u00b6 Easy pricing Pay per request and compute time Free tier of 1,000,000 AWS Lambda requests and 400,000 GBs of compute time Integrated with the whole AWS suite of services Integrated with many programming languages Easy monitoring through AWS CloudWatch Easy to get more resources per functions (up to 10GB of RAM) Increasing RAM will also improve CPU and network AWS Lambda language support \u00b6 Node.js (JavaScript) Python Java (Java 8 compatible) C# (.NET Core) Golang C# / Powershell Ruby Custom Runtime API (community supported, example Rust) Lambda Container Image The container image must implement the Lambda Runtime API ECS / Fargate is preferred for running arbitrary Docker images Important: Docker is not for AWS LAmbda, it's for ECS / Fargate AWS Lambda Integrations \u00b6 The main ones are: - API Gateway - Kinesis - DynamoDB - S3 - CloudFront - CloudWatch Events / EventBridge - CloudWatch Logs - SNS - SQS - Cognito but there are way more Example - thumbnail creation with serverless \u00b6 Example - serverless CRON job \u00b6 AWS Lambda Pricing: example \u00b6 You can find overall pricing information here: https://aws.amazon.com/lambda/pricing/ Pay per calls: First 1,000,000 requests are free $0.20 per 1 million requests thereafter (0.0000002 per request) Pay per duration (in iincrement of 1ms) 400,000 GB-seconds of compute time per month is FREE 400,000 seconds if function is 1GB RAM 3,200,000 seconds if function is 128MB RAM After that $1.00 for 600,000 GB-seconds It is usually very cheap to run AWS Lambda so it's very popular","title":"AWS Lambda Overview"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/02-aws-lambda-overview/#aws-lambda-overview","text":"","title":"AWS Lambda Overview"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/02-aws-lambda-overview/#why-aws-lambda","text":"Amazon EC2 Virtual Servers in the Cloud Limited by RAM and CPU Continously running Scaling means intervention to add / remove servers Amazon Lambda Virtual functions - no servers to manage Limited by time - short executions Run on-demand Scaling is automated","title":"Why AWS Lambda"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/02-aws-lambda-overview/#benefits-of-aws-lambda","text":"Easy pricing Pay per request and compute time Free tier of 1,000,000 AWS Lambda requests and 400,000 GBs of compute time Integrated with the whole AWS suite of services Integrated with many programming languages Easy monitoring through AWS CloudWatch Easy to get more resources per functions (up to 10GB of RAM) Increasing RAM will also improve CPU and network","title":"Benefits of AWS Lambda"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/02-aws-lambda-overview/#aws-lambda-language-support","text":"Node.js (JavaScript) Python Java (Java 8 compatible) C# (.NET Core) Golang C# / Powershell Ruby Custom Runtime API (community supported, example Rust) Lambda Container Image The container image must implement the Lambda Runtime API ECS / Fargate is preferred for running arbitrary Docker images Important: Docker is not for AWS LAmbda, it's for ECS / Fargate","title":"AWS Lambda language support"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/02-aws-lambda-overview/#aws-lambda-integrations","text":"The main ones are: - API Gateway - Kinesis - DynamoDB - S3 - CloudFront - CloudWatch Events / EventBridge - CloudWatch Logs - SNS - SQS - Cognito but there are way more","title":"AWS Lambda Integrations"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/02-aws-lambda-overview/#example-thumbnail-creation-with-serverless","text":"","title":"Example - thumbnail creation with serverless"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/02-aws-lambda-overview/#example-serverless-cron-job","text":"","title":"Example - serverless CRON job"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/02-aws-lambda-overview/#aws-lambda-pricing-example","text":"You can find overall pricing information here: https://aws.amazon.com/lambda/pricing/ Pay per calls: First 1,000,000 requests are free $0.20 per 1 million requests thereafter (0.0000002 per request) Pay per duration (in iincrement of 1ms) 400,000 GB-seconds of compute time per month is FREE 400,000 seconds if function is 1GB RAM 3,200,000 seconds if function is 128MB RAM After that $1.00 for 600,000 GB-seconds It is usually very cheap to run AWS Lambda so it's very popular","title":"AWS Lambda Pricing: example"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/03-aws-lambda-first-hands-on/","text":"AWS Lambda - First Hands On \u00b6 On the begin screen of the AWS Lambda we can play around with it and run functions. We can also have lambdas respond to events. We are going to click on the Create function button. There we can choose between multiple options for our function. We are going to use a Bluprint of hello-world-python . Then we are going to configure the blueprint. Give it a name and create basic permissions. The code will remain unchanged and we will create the function. Once it's created, we will see a code editor available to change our function. We can also test it, but first we need to configure a test event. And we can now test the function. We can also configure various settings like, memory, timeouts, execution role. We can also monitor the lambda for how many times it was invoked, for how long, errors vs successes etc. We can also see logs of the invocations.","title":"AWS Lambda - First Hands On"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/03-aws-lambda-first-hands-on/#aws-lambda-first-hands-on","text":"On the begin screen of the AWS Lambda we can play around with it and run functions. We can also have lambdas respond to events. We are going to click on the Create function button. There we can choose between multiple options for our function. We are going to use a Bluprint of hello-world-python . Then we are going to configure the blueprint. Give it a name and create basic permissions. The code will remain unchanged and we will create the function. Once it's created, we will see a code editor available to change our function. We can also test it, but first we need to configure a test event. And we can now test the function. We can also configure various settings like, memory, timeouts, execution role. We can also monitor the lambda for how many times it was invoked, for how long, errors vs successes etc. We can also see logs of the invocations.","title":"AWS Lambda - First Hands On"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/04-lambda-synchronous-invocations/","text":"Lambda - Synchronous Invocations \u00b6 Synchronous - CLI, SDK, API Gateway, Application Load Balancer Results are returned right away Error handling must happen client side (retries, expenential backoff, etc) User Invoked Elastic Load Balancing Amazon API Gateway Amazon CloudFront (Lambda@Edge) Amazon S3 Batch Service Invoked Amazon Cognito AWS Step Functions Other Services Amazon Lex Amazon Alexa Amazon Kinesis Data Firehose","title":"Lambda - Synchronous Invocations"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/04-lambda-synchronous-invocations/#lambda-synchronous-invocations","text":"Synchronous - CLI, SDK, API Gateway, Application Load Balancer Results are returned right away Error handling must happen client side (retries, expenential backoff, etc) User Invoked Elastic Load Balancing Amazon API Gateway Amazon CloudFront (Lambda@Edge) Amazon S3 Batch Service Invoked Amazon Cognito AWS Step Functions Other Services Amazon Lex Amazon Alexa Amazon Kinesis Data Firehose","title":"Lambda - Synchronous Invocations"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/05-lambda-synchronous-invocations-hands-on/","text":"Lambda Synchronous Invocations Hands On \u00b6 Previously we tested the Lambda using the test button, which was a syncronous invocation meaning that we waited for the lambda to be finished. We can also invoke it via shell // Linux / Mac $ aws lambda invoke --function-name hello-world --cli-binary-format raw-in-base64-out --payload '{\"key1\": \"value1\", \"key2\": \"value2\", \"key3\": \"value3\"}' --region eu-west-1 response.json // Windows Powershell $ aws lambda invoke --function-name hello-world --cli-binary-format raw-in-base64-out --payload '{\\\"key1\\\": \\\"value1\\\", \\\"key2\\\": \\\"value2\\\", \\\"key3\\\": \\\"value3\\\"}' --region eu-west-1 response.json // Windows CMD $ aws lambda invoke --function-name hello-world --cli-binary-format raw-in-base64-out --payload '{\"\"key1\"\": \"\"value1\"\", \"\"key2\"\": \"\"value2\"\", \"\"key3\"\": \"\"value3\"\"}' --region eu-west-1 response.json $ cat response.json","title":"Lambda Synchronous Invocations Hands On"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/05-lambda-synchronous-invocations-hands-on/#lambda-synchronous-invocations-hands-on","text":"Previously we tested the Lambda using the test button, which was a syncronous invocation meaning that we waited for the lambda to be finished. We can also invoke it via shell // Linux / Mac $ aws lambda invoke --function-name hello-world --cli-binary-format raw-in-base64-out --payload '{\"key1\": \"value1\", \"key2\": \"value2\", \"key3\": \"value3\"}' --region eu-west-1 response.json // Windows Powershell $ aws lambda invoke --function-name hello-world --cli-binary-format raw-in-base64-out --payload '{\\\"key1\\\": \\\"value1\\\", \\\"key2\\\": \\\"value2\\\", \\\"key3\\\": \\\"value3\\\"}' --region eu-west-1 response.json // Windows CMD $ aws lambda invoke --function-name hello-world --cli-binary-format raw-in-base64-out --payload '{\"\"key1\"\": \"\"value1\"\", \"\"key2\"\": \"\"value2\"\", \"\"key3\"\": \"\"value3\"\"}' --region eu-west-1 response.json $ cat response.json","title":"Lambda Synchronous Invocations Hands On"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/06-lambda-and-application-load-balancer/","text":"Lambda Integration with ALB \u00b6 To expose a Lambda function as an HTTP(s) endpoint You can use the Application Load Balancer (or an API Gateway) The Lambda function must be registered in a target group The HTTPS payload will be provided to Lambda in JSON format { \"requestContext\": { \"elb\": { \"targetGroupArn\": \"arn:aws:elasticloadbalancing:us-east-2:123...\" } }, \"httpMethod\": \"GET\", \"path\": \"/lambda\", \"queryStringParameters\": { \"query\": \"123ABCD\" }, \"headers\": { \"connection\": \"keep-alive\", \"host\": \"lambda-alb...\", \"user-agent\": \"Mozilla/5.0 ...\", \"x-amzn-trace-id\": \"Root=1-5c...\", \"x-forwarded-for\": \"123.123.123.123\", \"x-forwarded-port\": \"80\", \"x-forwarded-proto\": \"http\" }, \"body\": \"\", \"isBase64Encoded\": false } The lambda should respond with Json as well ```json { \"statusCode\": 200, \"statusDescription\": \"200 OK\", \"headers\": { \"Content-Type\": \"text/html; charset=utf-8\" }, \"body\": \"<h1>Hello world!</h1>\", \"isBaseEncoded\": false } ALB Multi-Header Values \u00b6 ALB can support multi header values (ALB setting) When you enable multi-value headers, HTTP headers and query string parameters that are sent with multiple values are shown as arrays within the AWS Lambda event and response objects. HTTP: http://example.com/path?name=foor&name=bar JSON: \"queryStringParameters\": { \"name\": [\"foo\", \"bar\"] }","title":"Lambda Integration with ALB"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/06-lambda-and-application-load-balancer/#lambda-integration-with-alb","text":"To expose a Lambda function as an HTTP(s) endpoint You can use the Application Load Balancer (or an API Gateway) The Lambda function must be registered in a target group The HTTPS payload will be provided to Lambda in JSON format { \"requestContext\": { \"elb\": { \"targetGroupArn\": \"arn:aws:elasticloadbalancing:us-east-2:123...\" } }, \"httpMethod\": \"GET\", \"path\": \"/lambda\", \"queryStringParameters\": { \"query\": \"123ABCD\" }, \"headers\": { \"connection\": \"keep-alive\", \"host\": \"lambda-alb...\", \"user-agent\": \"Mozilla/5.0 ...\", \"x-amzn-trace-id\": \"Root=1-5c...\", \"x-forwarded-for\": \"123.123.123.123\", \"x-forwarded-port\": \"80\", \"x-forwarded-proto\": \"http\" }, \"body\": \"\", \"isBase64Encoded\": false } The lambda should respond with Json as well ```json { \"statusCode\": 200, \"statusDescription\": \"200 OK\", \"headers\": { \"Content-Type\": \"text/html; charset=utf-8\" }, \"body\": \"<h1>Hello world!</h1>\", \"isBaseEncoded\": false }","title":"Lambda Integration with ALB"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/06-lambda-and-application-load-balancer/#alb-multi-header-values","text":"ALB can support multi header values (ALB setting) When you enable multi-value headers, HTTP headers and query string parameters that are sent with multiple values are shown as arrays within the AWS Lambda event and response objects. HTTP: http://example.com/path?name=foor&name=bar JSON: \"queryStringParameters\": { \"name\": [\"foo\", \"bar\"] }","title":"ALB Multi-Header Values"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/07-lambda-application-load-balancer-hands-on/","text":"Lambda & Application Load Balancer Hands On \u00b6 We are going to create a new function, this time we are choosing Author from scratch with a Python 3.8 runtime. Once the function is created, we are going to EC2 - Load Balancers and creating a new Application Load Balancer. We are enabling it across 3 availability zones. We are going to create a new security group. Next, in routing we are choosing to create a new target group and target lambda functions. Next, we are registering the target If we are to open up the ALB url now, it will download the response JSON file. In order for it to properly work, we need to change the response format: Once deployed, it should work properly now:","title":"Lambda & Application Load Balancer Hands On"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/07-lambda-application-load-balancer-hands-on/#lambda-application-load-balancer-hands-on","text":"We are going to create a new function, this time we are choosing Author from scratch with a Python 3.8 runtime. Once the function is created, we are going to EC2 - Load Balancers and creating a new Application Load Balancer. We are enabling it across 3 availability zones. We are going to create a new security group. Next, in routing we are choosing to create a new target group and target lambda functions. Next, we are registering the target If we are to open up the ALB url now, it will download the response JSON file. In order for it to properly work, we need to change the response format: Once deployed, it should work properly now:","title":"Lambda &amp; Application Load Balancer Hands On"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/08-aws-lambda-at-edge-overview/","text":"Lambda@Edge Overview \u00b6 You have deployed a CDN using CloudFront What if you wanted to run a global AWS Lambda alongside? Or how to implement request filtering before reaching your application? For this, you can use Lambda@Edge Deploy lambda functions alongside your CloudFront CDN Build more responsive applications You don't manage servers, Lambda is deployed globally Customize CDN content Pay only for what you use Lambda@Edge \u00b6 You can use Lambda to change CloudFront requests and responses After CloudFront receives a request from a viewer (viewer request) Before CloudFront forwwards the request to the origin (origin request) After CloudFront receives the response from the origin (origin response) Before CloudFront forwards the response to the viewer (viewer response) You can also generate responses to viewers without ever sending the request to the origin. Use Cases \u00b6 Website Security and Privacy Dynamic Web Application at the Edge Search Enging Optimization (SEO) Intelligently Route Across Origins and Data Centers Bot Mitigation at the Edge Real-time Image Transformation A/B Testing User Authentication and Authorization User Prioritization User Tracking and Analytics","title":"Lambda@Edge Overview"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/08-aws-lambda-at-edge-overview/#lambdaedge-overview","text":"You have deployed a CDN using CloudFront What if you wanted to run a global AWS Lambda alongside? Or how to implement request filtering before reaching your application? For this, you can use Lambda@Edge Deploy lambda functions alongside your CloudFront CDN Build more responsive applications You don't manage servers, Lambda is deployed globally Customize CDN content Pay only for what you use","title":"Lambda@Edge Overview"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/08-aws-lambda-at-edge-overview/#lambdaedge","text":"You can use Lambda to change CloudFront requests and responses After CloudFront receives a request from a viewer (viewer request) Before CloudFront forwwards the request to the origin (origin request) After CloudFront receives the response from the origin (origin response) Before CloudFront forwards the response to the viewer (viewer response) You can also generate responses to viewers without ever sending the request to the origin.","title":"Lambda@Edge"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/08-aws-lambda-at-edge-overview/#use-cases","text":"Website Security and Privacy Dynamic Web Application at the Edge Search Enging Optimization (SEO) Intelligently Route Across Origins and Data Centers Bot Mitigation at the Edge Real-time Image Transformation A/B Testing User Authentication and Authorization User Prioritization User Tracking and Analytics","title":"Use Cases"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/09-lambda-asynchronous-invocations-dlq/","text":"Lambda Asynchronous Invocations & DLQ \u00b6 S3, SNS, CloudWatch Events The events are placed in an Event Queue Lambda attempts to retry on errors 3 tries in total 1 minute wait after 1st, then 2 minutes wait Make sure the processing is idempotent (in case of retries) If the function is retried, you will see duplicate logs entries in CloudWatch Logs Can define a DLQ (dead letter queue) - SNS or SQS - for failed processing (need correct IAM permissions) Asynchronous invocations allow you to speed up the processing if you don't need to wait for the result (ex: you need 1000 files processed) Services invocated asynchronously \u00b6 Amazon Simple Storage Service (S3) Amazon Simple Notification Service (SNS) Amazon CloudWatch Events / EventBridge AWS CodeCommit (CodeCommit Trigger: new branch, new tag, new push) AWs CodePipeline (invoke a Lambda function during the pipeline, Lambda must callback) other Amazon CloudWatch Logs (log processing) Amazon Simple Email Service AWS CloudFormation AWS Config AWS IoT AWS IoT Events","title":"Lambda Asynchronous Invocations & DLQ"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/09-lambda-asynchronous-invocations-dlq/#lambda-asynchronous-invocations-dlq","text":"S3, SNS, CloudWatch Events The events are placed in an Event Queue Lambda attempts to retry on errors 3 tries in total 1 minute wait after 1st, then 2 minutes wait Make sure the processing is idempotent (in case of retries) If the function is retried, you will see duplicate logs entries in CloudWatch Logs Can define a DLQ (dead letter queue) - SNS or SQS - for failed processing (need correct IAM permissions) Asynchronous invocations allow you to speed up the processing if you don't need to wait for the result (ex: you need 1000 files processed)","title":"Lambda Asynchronous Invocations &amp; DLQ"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/09-lambda-asynchronous-invocations-dlq/#services-invocated-asynchronously","text":"Amazon Simple Storage Service (S3) Amazon Simple Notification Service (SNS) Amazon CloudWatch Events / EventBridge AWS CodeCommit (CodeCommit Trigger: new branch, new tag, new push) AWs CodePipeline (invoke a Lambda function during the pipeline, Lambda must callback) other Amazon CloudWatch Logs (log processing) Amazon Simple Email Service AWS CloudFormation AWS Config AWS IoT AWS IoT Events","title":"Services invocated asynchronously"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/10-lambda-asynchronous-invocations-hands-on/","text":"Lambda Asynchronous Invocations Hands On \u00b6 We can invoke a function asynchronously via CLI // Linux / Mac $ aws lambda invoke --function-name hello-world --cli-binary-format raw-in-base64-out --payload '{\"key1\": \"value1\", \"key2\": \"value2\", \"key3\": \"value3\"}' --invocation-type Event --region eu-west-1 response.json // Windows Powershell $ aws lambda invoke --function-name hello-world --cli-binary-format raw-in-base64-out --payload '{\\\"key1\\\": \\\"value1\\\", \\\"key2\\\": \\\"value2\\\", \\\"key3\\\": \\\"value3\\\"}' --invocation-type Event --region eu-west-1 response.json // Windows CMD $ aws lambda invoke --function-name hello-world --cli-binary-format raw-in-base64-out --payload '{\"\"key1\"\": \"\"value1\"\", \"\"key2\"\": \"\"value2\"\", \"\"key3\"\": \"\"value3\"\"}' --invocation-type Event --region eu-west-1 response.json For us to know the result of the invocation we will need to look at the logs. We can also configure the asynchronous invocation by opening up the lambda Configuration - Asynchronous invocation . There we can configure maximum age of the event, retry attempts and a DLQ. When configuring a DLQ you might see a message about insufficient permissions. To fix that, we navigate to Configuration - Permissions . Then click on the Execution Role and attach a policy to it related to Amazon SQS.","title":"Lambda Asynchronous Invocations Hands On"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/10-lambda-asynchronous-invocations-hands-on/#lambda-asynchronous-invocations-hands-on","text":"We can invoke a function asynchronously via CLI // Linux / Mac $ aws lambda invoke --function-name hello-world --cli-binary-format raw-in-base64-out --payload '{\"key1\": \"value1\", \"key2\": \"value2\", \"key3\": \"value3\"}' --invocation-type Event --region eu-west-1 response.json // Windows Powershell $ aws lambda invoke --function-name hello-world --cli-binary-format raw-in-base64-out --payload '{\\\"key1\\\": \\\"value1\\\", \\\"key2\\\": \\\"value2\\\", \\\"key3\\\": \\\"value3\\\"}' --invocation-type Event --region eu-west-1 response.json // Windows CMD $ aws lambda invoke --function-name hello-world --cli-binary-format raw-in-base64-out --payload '{\"\"key1\"\": \"\"value1\"\", \"\"key2\"\": \"\"value2\"\", \"\"key3\"\": \"\"value3\"\"}' --invocation-type Event --region eu-west-1 response.json For us to know the result of the invocation we will need to look at the logs. We can also configure the asynchronous invocation by opening up the lambda Configuration - Asynchronous invocation . There we can configure maximum age of the event, retry attempts and a DLQ. When configuring a DLQ you might see a message about insufficient permissions. To fix that, we navigate to Configuration - Permissions . Then click on the Execution Role and attach a policy to it related to Amazon SQS.","title":"Lambda Asynchronous Invocations Hands On"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/11-lambda-cloudwatch-events-eventbridge/","text":"Lambda & CloudWatch Events / EventBridge \u00b6","title":"Lambda & CloudWatch Events / EventBridge"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/11-lambda-cloudwatch-events-eventbridge/#lambda-cloudwatch-events-eventbridge","text":"","title":"Lambda &amp; CloudWatch Events / EventBridge"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/12-lambda-cloudwatch-events-eventbridge-hands-on/","text":"Lambda & CloudWatch Events / EventBridge Hands On \u00b6 We are going to create a new rule on EventBridge. It is going to be invoked once every minute. It will be the default event bus And the target lambda function.","title":"Lambda & CloudWatch Events / EventBridge Hands On"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/12-lambda-cloudwatch-events-eventbridge-hands-on/#lambda-cloudwatch-events-eventbridge-hands-on","text":"We are going to create a new rule on EventBridge. It is going to be invoked once every minute. It will be the default event bus And the target lambda function.","title":"Lambda &amp; CloudWatch Events / EventBridge Hands On"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/13-lambda-s3-event-notifications/","text":"Lambda & S3 Event Notifications \u00b6 S3:ObjectCreated, S3:ObjectRemoved, S3:ObjectRestore, S3:Replication... Object name filtering possible (*.jpg) Use case: generate thumbnails of images uploaded to S3 S3 event notifications typically deliver events in seconds but can sometimes take a minute or longer If two writes are made to a single non-versioned object at the same time, it is possible that only a single event notification will be sent If you want to ensure that an event notification is sent for every successful write, you can enable versioning on your bucket. Simple Pattern - Metadata Sync \u00b6","title":"Lambda & S3 Event Notifications"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/13-lambda-s3-event-notifications/#lambda-s3-event-notifications","text":"S3:ObjectCreated, S3:ObjectRemoved, S3:ObjectRestore, S3:Replication... Object name filtering possible (*.jpg) Use case: generate thumbnails of images uploaded to S3 S3 event notifications typically deliver events in seconds but can sometimes take a minute or longer If two writes are made to a single non-versioned object at the same time, it is possible that only a single event notification will be sent If you want to ensure that an event notification is sent for every successful write, you can enable versioning on your bucket.","title":"Lambda &amp; S3 Event Notifications"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/13-lambda-s3-event-notifications/#simple-pattern-metadata-sync","text":"","title":"Simple Pattern - Metadata Sync"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/14-lambda-s3-event-notifications-hands-on/","text":"Lambda & S3 Event Notifications - Hands On \u00b6 We are going to create a new S3 bucket. Once it's created we are going to properties and finding the Event notifications panel. Create a new event in it. When viewing the Lambda, we can see that the S3 is now invoking it: Now we can upload a file to the S3 Bucket and we'll see that the lambda will be invoked.","title":"Lambda & S3 Event Notifications - Hands On"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/14-lambda-s3-event-notifications-hands-on/#lambda-s3-event-notifications-hands-on","text":"We are going to create a new S3 bucket. Once it's created we are going to properties and finding the Event notifications panel. Create a new event in it. When viewing the Lambda, we can see that the S3 is now invoking it: Now we can upload a file to the S3 Bucket and we'll see that the lambda will be invoked.","title":"Lambda &amp; S3 Event Notifications - Hands On"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/15-lambda-event-source-mapping/","text":"Lambda Event Source Mapping \u00b6 Kinesis Data Streams SQS & SQS FIFO queue DynamoDB Streams Common denominator: records need to be polled from the source You Lambda function is invoked synchonously Streams & Lambda (Kinesis & DynamoDB) \u00b6 An event source mapping creates an iterator for each shard, processes items in order Start with new items, from the beginning or from timestamp Processed items aren't removed from the stream (other consumers can read them) Low traffic: use batch window to accumulate records before processing You can process multiple batches in parallel Up to 10 batches per shard in-order processing is still guaranteed for each partition key https://aws.amazon.com/blogs/compute/new-aws-lambda-scaling-controls-for-kinesis-and-dynamodb-event-sources/ Streams & Lambda - Error Handling \u00b6 By default, if your function returns an error, the entire batch is reprocessed until the function suceeds, or the items in the batch expire. To ensure in-order processing, processing for the affected shard is paused until error is resolved. You can configure the event source mapping to: discard old events restrict the number of retries split the batch on error (to work around Lambda timeout issues) Discarded events can go to a Destination Lambda - Event Soutce Mapping \u00b6 SQS & SQS FIFO \u00b6 Event Source Mapping will poll SQS (Long Polling) Specify batch size (1-10 messages) Recommended: Set the queue visibility timeout to 6x the timeout of your Lambda function To use a DLQ Set up the SQS queue, not Lambda (DLQ for Lambda is only for async invocations) Queues & Lambda \u00b6 Lambda also supports in-order processing for FIFO (first-in, first-out) queues, scaling up to the number of active message groups . For standard queues, items aren't necessarily processed in order. Lambda scales up to process a standard queue as quickly as possible. When an error occurs, batches are returned to a queue as individual items and might be processed in a different grouping than the original batch. Ocassionaly, the event source mapping might receive the same item from the queue twice, even if no function error ocurred. Lambda deletes items from the queue after they're processed successfully. You can configure the source queue to send items to a dead-letter queue if they can't be processed. Lambda Event Mapper Scaling \u00b6 Kinesis Data Streams & DynamoDB Streams One Lambda invocation per stream shard If you use parallelization, up to 10 batches processed per shard simultaneously SQS Standard Lambda adds 60 more instances per minute to scale up Up to 1000 batches of messages processed simultaneously SQS FIFO Messages with the same GroupID will be processed in order The Lambda function scales to the number of active message groups","title":"Lambda Event Source Mapping"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/15-lambda-event-source-mapping/#lambda-event-source-mapping","text":"Kinesis Data Streams SQS & SQS FIFO queue DynamoDB Streams Common denominator: records need to be polled from the source You Lambda function is invoked synchonously","title":"Lambda Event Source Mapping"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/15-lambda-event-source-mapping/#streams-lambda-kinesis-dynamodb","text":"An event source mapping creates an iterator for each shard, processes items in order Start with new items, from the beginning or from timestamp Processed items aren't removed from the stream (other consumers can read them) Low traffic: use batch window to accumulate records before processing You can process multiple batches in parallel Up to 10 batches per shard in-order processing is still guaranteed for each partition key https://aws.amazon.com/blogs/compute/new-aws-lambda-scaling-controls-for-kinesis-and-dynamodb-event-sources/","title":"Streams &amp; Lambda (Kinesis &amp; DynamoDB)"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/15-lambda-event-source-mapping/#streams-lambda-error-handling","text":"By default, if your function returns an error, the entire batch is reprocessed until the function suceeds, or the items in the batch expire. To ensure in-order processing, processing for the affected shard is paused until error is resolved. You can configure the event source mapping to: discard old events restrict the number of retries split the batch on error (to work around Lambda timeout issues) Discarded events can go to a Destination","title":"Streams &amp; Lambda - Error Handling"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/15-lambda-event-source-mapping/#lambda-event-soutce-mapping","text":"","title":"Lambda - Event Soutce Mapping"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/15-lambda-event-source-mapping/#sqs-sqs-fifo","text":"Event Source Mapping will poll SQS (Long Polling) Specify batch size (1-10 messages) Recommended: Set the queue visibility timeout to 6x the timeout of your Lambda function To use a DLQ Set up the SQS queue, not Lambda (DLQ for Lambda is only for async invocations)","title":"SQS &amp; SQS FIFO"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/15-lambda-event-source-mapping/#queues-lambda","text":"Lambda also supports in-order processing for FIFO (first-in, first-out) queues, scaling up to the number of active message groups . For standard queues, items aren't necessarily processed in order. Lambda scales up to process a standard queue as quickly as possible. When an error occurs, batches are returned to a queue as individual items and might be processed in a different grouping than the original batch. Ocassionaly, the event source mapping might receive the same item from the queue twice, even if no function error ocurred. Lambda deletes items from the queue after they're processed successfully. You can configure the source queue to send items to a dead-letter queue if they can't be processed.","title":"Queues &amp; Lambda"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/15-lambda-event-source-mapping/#lambda-event-mapper-scaling","text":"Kinesis Data Streams & DynamoDB Streams One Lambda invocation per stream shard If you use parallelization, up to 10 batches processed per shard simultaneously SQS Standard Lambda adds 60 more instances per minute to scale up Up to 1000 batches of messages processed simultaneously SQS FIFO Messages with the same GroupID will be processed in order The Lambda function scales to the number of active message groups","title":"Lambda Event Mapper Scaling"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/16-lambda-event-source-mapping-hands-on/","text":"Lambda Event Source Mapping Hands On \u00b6 We are going to create a new lambda function and a new SQS queue. Then in a lambda function, we are going to add the SQS trigger. You might also need to setup permissions for this as well. In order to set it up, we need to attach the AWSLambdaSQSQueueExecutionRole permissions to the lambda execution role.","title":"Lambda Event Source Mapping Hands On"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/16-lambda-event-source-mapping-hands-on/#lambda-event-source-mapping-hands-on","text":"We are going to create a new lambda function and a new SQS queue. Then in a lambda function, we are going to add the SQS trigger. You might also need to setup permissions for this as well. In order to set it up, we need to attach the AWSLambdaSQSQueueExecutionRole permissions to the lambda execution role.","title":"Lambda Event Source Mapping Hands On"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/17-lambda-destinations/","text":"Lambda - Destinations \u00b6 Nov 2019: Can configure to send result to a destination Asynchronous invocations - can define destinations for successful and failed event: Amazon SNS Amazon SQS AWS Lambda Amazon EventBridge bus Note: AWS recommends you use destinations instead of DLQ now (but both can be used at the same time) Event Source mapping: for discarded event batches Amazon SQS Amazon SNS Note: you can send events to a DLQ directly from SQS https://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html https://docs.aws.amazon.com/lambda/latest/dg/invocation-eventsourcemapping.html","title":"Lambda - Destinations"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/17-lambda-destinations/#lambda-destinations","text":"Nov 2019: Can configure to send result to a destination Asynchronous invocations - can define destinations for successful and failed event: Amazon SNS Amazon SQS AWS Lambda Amazon EventBridge bus Note: AWS recommends you use destinations instead of DLQ now (but both can be used at the same time) Event Source mapping: for discarded event batches Amazon SQS Amazon SNS Note: you can send events to a DLQ directly from SQS https://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html https://docs.aws.amazon.com/lambda/latest/dg/invocation-eventsourcemapping.html","title":"Lambda - Destinations"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/18-lambda-destinations-hands-on/","text":"Lambda Destinations Hands On \u00b6 We can add a destination for a lambda We are going to create 2 new queues - one for successful invocations and one for failures. Then we are going to add them as destinations. Now when executing the lambda, we will see a message in the queue:","title":"Lambda Destinations Hands On"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/18-lambda-destinations-hands-on/#lambda-destinations-hands-on","text":"We can add a destination for a lambda We are going to create 2 new queues - one for successful invocations and one for failures. Then we are going to add them as destinations. Now when executing the lambda, we will see a message in the queue:","title":"Lambda Destinations Hands On"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/19-lambda-permissions-iam-roles-resource-policies/","text":"Lambda Permissions - IAM Roles & Resources Policies \u00b6 IAM Role \u00b6 Grants the Lambda Function permissions to AWS services / resources Sample managed policies for Lambda: AWSLambdaBasicExecutionRole - Upload logs to CloudWatch AWSLambdaKinesisExecutionRole - Read from Kinesis AWSLambdaDynamoDBExecutionRole - Read from DynamoDB Streams AWSLambdaSQSQueueExecutionRole - Read from SQS AWSLambdaVPCAccessExecutionRole - Deploy Lambda function in VPC AWSXRayDaemonWriteAccess - Upload trace data to X-Ray When you use an event source mapping to invoke your function, Lambda uses the execution role to read event data. Best practice: create one Lambda Execution Role per function. Resource Based Policies \u00b6 use resource-based policies to give other accounts and AWS services permission to use your Lambda resources Similar to S3 bucket policies for S3 bucket An IAM principal can access Lambda: If the IAM policy attached to the principal authorizes it (e.g. user access) Or if the resource-based policy authorizes (e.g. service access) When AWS service like Amazon S3 calls your Lambda function, the resource based policy gives it access.","title":"Lambda Permissions - IAM Roles & Resources Policies"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/19-lambda-permissions-iam-roles-resource-policies/#lambda-permissions-iam-roles-resources-policies","text":"","title":"Lambda Permissions - IAM Roles &amp; Resources Policies"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/19-lambda-permissions-iam-roles-resource-policies/#iam-role","text":"Grants the Lambda Function permissions to AWS services / resources Sample managed policies for Lambda: AWSLambdaBasicExecutionRole - Upload logs to CloudWatch AWSLambdaKinesisExecutionRole - Read from Kinesis AWSLambdaDynamoDBExecutionRole - Read from DynamoDB Streams AWSLambdaSQSQueueExecutionRole - Read from SQS AWSLambdaVPCAccessExecutionRole - Deploy Lambda function in VPC AWSXRayDaemonWriteAccess - Upload trace data to X-Ray When you use an event source mapping to invoke your function, Lambda uses the execution role to read event data. Best practice: create one Lambda Execution Role per function.","title":"IAM Role"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/19-lambda-permissions-iam-roles-resource-policies/#resource-based-policies","text":"use resource-based policies to give other accounts and AWS services permission to use your Lambda resources Similar to S3 bucket policies for S3 bucket An IAM principal can access Lambda: If the IAM policy attached to the principal authorizes it (e.g. user access) Or if the resource-based policy authorizes (e.g. service access) When AWS service like Amazon S3 calls your Lambda function, the resource based policy gives it access.","title":"Resource Based Policies"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/20-lambda-permissions-iam-roles-resource-policies-hands-on/","text":"Lambda Permissions - IAM Roles & Resource Policies - Hands On \u00b6 We can go into IAM Roles and search for lambda to see all the lambda execution roles. As for the resource based policies, we can configure that when opening our lambda Configuration - Permissions .","title":"Lambda Permissions - IAM Roles & Resource Policies - Hands On"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/20-lambda-permissions-iam-roles-resource-policies-hands-on/#lambda-permissions-iam-roles-resource-policies-hands-on","text":"We can go into IAM Roles and search for lambda to see all the lambda execution roles. As for the resource based policies, we can configure that when opening our lambda Configuration - Permissions .","title":"Lambda Permissions - IAM Roles &amp; Resource Policies - Hands On"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/21-lambda-environment-variables/","text":"Lambda Environment Variables \u00b6 Environment variable = key / value pair in string form Adjust the function behavior without updating the code The environment variables are available to your code Lambda Service adds it own system environment variables as well Helpful to store secrets (encrypted by KMS) Secrets can be encrypted by the Lambda service key, or your own CMK","title":"Lambda Environment Variables"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/21-lambda-environment-variables/#lambda-environment-variables","text":"Environment variable = key / value pair in string form Adjust the function behavior without updating the code The environment variables are available to your code Lambda Service adds it own system environment variables as well Helpful to store secrets (encrypted by KMS) Secrets can be encrypted by the Lambda service key, or your own CMK","title":"Lambda Environment Variables"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/22-lambda-environment-variables-hands-on/","text":"Lambda Environment Variables Hands On \u00b6 We are going to modify the lambda code: Then under the Configuration - Environment Variables , we can configure the variables.","title":"Lambda Environment Variables Hands On"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/22-lambda-environment-variables-hands-on/#lambda-environment-variables-hands-on","text":"We are going to modify the lambda code: Then under the Configuration - Environment Variables , we can configure the variables.","title":"Lambda Environment Variables Hands On"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/23-lambda-monitoring-x-ray-tracing/","text":"Lambda Monitoring & X-Ray Tracing \u00b6 Lambda Logging & Monitoring \u00b6 CloudWatch Logs AWS Lambda execution logs are stored in AWS CloudWatch Logs Make sure your AWS Lambda functions has an execution role with an IAM policy that authorizes writes to CloudWatch Logs CloudWatch Metrics AWS Lambda metrics are displayed in AWS CloudWatch Metrics Invocations, Durations, Concurrent Executions Error count, Success Rates, Throttles Async Delivery Failures Iterator Age (Kinesis & DynamoDB Streams) Lambda Tracing with X-Ray \u00b6 Enable in Lambda configuration (Active Tracing) Runs the X-Ray daemon for you Use AWS X-Ray SDK in Code Ensure Lambda Function has a correct IAM Execution Role The managed policy is called AWSXRayDaemonWriteAccess Environment variables to communicate with X-Ray _X_AMZN_TRACE_ID: contains the tracing header AWS_XRAY_CONTEXT_MISSING: by default, LOG_ERROR AWS_XRAY_DAEMON_ADDRESS: the X-Ray Daemon IP_ADDRESS:PORT","title":"Lambda Monitoring & X-Ray Tracing"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/23-lambda-monitoring-x-ray-tracing/#lambda-monitoring-x-ray-tracing","text":"","title":"Lambda Monitoring &amp; X-Ray Tracing"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/23-lambda-monitoring-x-ray-tracing/#lambda-logging-monitoring","text":"CloudWatch Logs AWS Lambda execution logs are stored in AWS CloudWatch Logs Make sure your AWS Lambda functions has an execution role with an IAM policy that authorizes writes to CloudWatch Logs CloudWatch Metrics AWS Lambda metrics are displayed in AWS CloudWatch Metrics Invocations, Durations, Concurrent Executions Error count, Success Rates, Throttles Async Delivery Failures Iterator Age (Kinesis & DynamoDB Streams)","title":"Lambda Logging &amp; Monitoring"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/23-lambda-monitoring-x-ray-tracing/#lambda-tracing-with-x-ray","text":"Enable in Lambda configuration (Active Tracing) Runs the X-Ray daemon for you Use AWS X-Ray SDK in Code Ensure Lambda Function has a correct IAM Execution Role The managed policy is called AWSXRayDaemonWriteAccess Environment variables to communicate with X-Ray _X_AMZN_TRACE_ID: contains the tracing header AWS_XRAY_CONTEXT_MISSING: by default, LOG_ERROR AWS_XRAY_DAEMON_ADDRESS: the X-Ray Daemon IP_ADDRESS:PORT","title":"Lambda Tracing with X-Ray"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/24-lambda-monitoring-x-ray-tracing-hands-on/","text":"Lambda Monitoring & X-Ray Tracing Hands On \u00b6 In the lambda, under Monitor - Metrics we can see the CloudWatch Metrics for our lambda. We can also see the CloudWatch Logs for the lambda invocations. We can go to Configuration - Monitoring and operations tools We can enable AWS X-Ray there. This will also create the necessary permissions: Then, we can trigger the function and after some time we will see an xray service map.","title":"Lambda Monitoring & X-Ray Tracing Hands On"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/24-lambda-monitoring-x-ray-tracing-hands-on/#lambda-monitoring-x-ray-tracing-hands-on","text":"In the lambda, under Monitor - Metrics we can see the CloudWatch Metrics for our lambda. We can also see the CloudWatch Logs for the lambda invocations. We can go to Configuration - Monitoring and operations tools We can enable AWS X-Ray there. This will also create the necessary permissions: Then, we can trigger the function and after some time we will see an xray service map.","title":"Lambda Monitoring &amp; X-Ray Tracing Hands On"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/25-lambda-in-vpc/","text":"Lambda in VPC \u00b6 Lambda by default \u00b6 By default, your Lambda function is launched outside your own VPC (in aan AWS-owned VPC) There fore it cannot access resources in your VPC (RDS, ElastiCache, internal ELB...) Lambda in VPC \u00b6 You must define the VPC ID, the Subnets and the Security Groups Lambda will create an ENI (Elastic Network Interface) in your subnets AWSLambdaVPCAccessExecutionRole Lambda in VPC - Internet Access \u00b6 A Lambda function in your VPC does not have internet access Deploying a Lambda function in a public subnet does not give it internet access or a public IP Deploying a Lambda function in a private subnet gives it internet access if you have a NAT Gateway / Instance You can use VPC endpoints to privately access AWS services without a NAT Note: Lambda CloudWatch Logs works even without endpoint or NAT Gateway","title":"Lambda in VPC"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/25-lambda-in-vpc/#lambda-in-vpc","text":"","title":"Lambda in VPC"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/25-lambda-in-vpc/#lambda-by-default","text":"By default, your Lambda function is launched outside your own VPC (in aan AWS-owned VPC) There fore it cannot access resources in your VPC (RDS, ElastiCache, internal ELB...)","title":"Lambda by default"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/25-lambda-in-vpc/#lambda-in-vpc_1","text":"You must define the VPC ID, the Subnets and the Security Groups Lambda will create an ENI (Elastic Network Interface) in your subnets AWSLambdaVPCAccessExecutionRole","title":"Lambda in VPC"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/25-lambda-in-vpc/#lambda-in-vpc-internet-access","text":"A Lambda function in your VPC does not have internet access Deploying a Lambda function in a public subnet does not give it internet access or a public IP Deploying a Lambda function in a private subnet gives it internet access if you have a NAT Gateway / Instance You can use VPC endpoints to privately access AWS services without a NAT Note: Lambda CloudWatch Logs works even without endpoint or NAT Gateway","title":"Lambda in VPC - Internet Access"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/26-lambda-in-vpc-hands-on/","text":"Lambda in VPC Hands On \u00b6 We are going to create an empty security group in EC2. Then, in Lambda, we can go to Configuration - VPC and set it up: This will fail because the lambda execution role doesn't have CreatenetworkInterface permission. So, we need to attach this permission to the execution role. After that's done, we can see the new network interfaces in EC2 - Network Interfaces .","title":"Lambda in VPC Hands On"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/26-lambda-in-vpc-hands-on/#lambda-in-vpc-hands-on","text":"We are going to create an empty security group in EC2. Then, in Lambda, we can go to Configuration - VPC and set it up: This will fail because the lambda execution role doesn't have CreatenetworkInterface permission. So, we need to attach this permission to the execution role. After that's done, we can see the new network interfaces in EC2 - Network Interfaces .","title":"Lambda in VPC Hands On"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/27-lambda-function-performance/","text":"Lambda Function Performance \u00b6 Lambda Function Configuration \u00b6 RAM From 128 MB to 10GB in 1MB increments The more RAM you add, the more vCPU credits you get At 1,792 MB, a function has the equivalent of one full vCPU After 1,792 MB, you get more than one CPU, and need to use multi-threading in your code to benefit from it. If your application is CPU-bound (computation heavy), increase RAM Timeout: default 3 seconds, maximum 900 seconds (15 minutes) Lambda Execution Context \u00b6 The execution context is a temporary runtime environment that initializes any external dependencies of your lambda code Great for database connections, HTTP clients, SDK clients... The execution context is maintained for some time in anticipation of another Lambda function invocation The next function invocation can \"re-use\" the context to execution time and save time in initializing connections objects. The execution context includes the /tmp directory. Initialize outside the handler \u00b6 BAD \u00b6 import os def get_user_handler(event, context): DB_URL = os.get_env(\"DB_URL\") db_client = db.connect(DB_URL) user = db_client.get(user_id = event[\"user_id\"]) return user The DB connection is established at every function invocation. GOOD \u00b6 import os DB_URL = os.get_env(\"DB_URL\") db_client = db.connect(DB_URL) def get_user_handler(event, context): user = db_client.get(user_id = event[\"user_id\"]) return user The DB connection is established once and re-used across invocations. Lambda Functions /tmp space \u00b6 If your Lambda function needs to download a big file to work If your Lambda function needs disk space to perform operations You can use the /tmp directory Max size is 512MB The directory content remains when the execution context is frozen, providing transient cache that can be used for multiple invocations (helpful to checkpoint your work) For permanent persistence of object (non temporary), use S3","title":"Lambda Function Performance"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/27-lambda-function-performance/#lambda-function-performance","text":"","title":"Lambda Function Performance"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/27-lambda-function-performance/#lambda-function-configuration","text":"RAM From 128 MB to 10GB in 1MB increments The more RAM you add, the more vCPU credits you get At 1,792 MB, a function has the equivalent of one full vCPU After 1,792 MB, you get more than one CPU, and need to use multi-threading in your code to benefit from it. If your application is CPU-bound (computation heavy), increase RAM Timeout: default 3 seconds, maximum 900 seconds (15 minutes)","title":"Lambda Function Configuration"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/27-lambda-function-performance/#lambda-execution-context","text":"The execution context is a temporary runtime environment that initializes any external dependencies of your lambda code Great for database connections, HTTP clients, SDK clients... The execution context is maintained for some time in anticipation of another Lambda function invocation The next function invocation can \"re-use\" the context to execution time and save time in initializing connections objects. The execution context includes the /tmp directory.","title":"Lambda Execution Context"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/27-lambda-function-performance/#initialize-outside-the-handler","text":"","title":"Initialize outside the handler"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/27-lambda-function-performance/#bad","text":"import os def get_user_handler(event, context): DB_URL = os.get_env(\"DB_URL\") db_client = db.connect(DB_URL) user = db_client.get(user_id = event[\"user_id\"]) return user The DB connection is established at every function invocation.","title":"BAD"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/27-lambda-function-performance/#good","text":"import os DB_URL = os.get_env(\"DB_URL\") db_client = db.connect(DB_URL) def get_user_handler(event, context): user = db_client.get(user_id = event[\"user_id\"]) return user The DB connection is established once and re-used across invocations.","title":"GOOD"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/27-lambda-function-performance/#lambda-functions-tmp-space","text":"If your Lambda function needs to download a big file to work If your Lambda function needs disk space to perform operations You can use the /tmp directory Max size is 512MB The directory content remains when the execution context is frozen, providing transient cache that can be used for multiple invocations (helpful to checkpoint your work) For permanent persistence of object (non temporary), use S3","title":"Lambda Functions /tmp space"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/28-lambda-function-performance-hands-on/","text":"Lambda Function Performance Hands On \u00b6 In Configuration - General Configuration we can edit the basic settings of the Lambda.","title":"Lambda Function Performance Hands On"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/28-lambda-function-performance-hands-on/#lambda-function-performance-hands-on","text":"In Configuration - General Configuration we can edit the basic settings of the Lambda.","title":"Lambda Function Performance Hands On"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/29-lambda-concurrency/","text":"Lambda Concurrency \u00b6 Concurrency limit: up to 1000 concurrent executions Can set a reserved concurrency at the function level Each invocation over the concurrency limit will trigger a throttle Throttle behavior If syncronous invocation - return ThrottleError - 429 If asynchronous invocation - retry automaticall and then go to DLQ Concurrency and Asynchronous Invocations \u00b6 If the function doesn't have enough concurrency available to process all events, additional requests are throttled. For throttling errors (429) and system errors (500-series), Lambda returns the event to the queue and attempts to run the function again for up to 6 hours. The retry interval increases exponentially from 1 second after the first attempt to a maximum of 5 minutes. Cold Starts & Provisioned Concurrency \u00b6 Cold Start New instance -> code is loaded and code outside the handler run (init) If the init is large (code, dependencies, SDK) this process can take some time First request is served by new instances has higher latency than the rest Provisioned Concurrency Concurrency is allocated before the function is invoked (in advance) So the cold start never happens and all invocations have low latency Application Auto Scaling can manage concurrency (schedule or target utilization) Note: cold starts in VPC have been dramatically reduced in Oct & Nov 2019 https://aws.amazon.com/blogs/compute/announcing-improved-vpc-networking-for-aws-lambda-functions/ Reserved and Provisioned Concurrency \u00b6 https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html","title":"Lambda Concurrency"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/29-lambda-concurrency/#lambda-concurrency","text":"Concurrency limit: up to 1000 concurrent executions Can set a reserved concurrency at the function level Each invocation over the concurrency limit will trigger a throttle Throttle behavior If syncronous invocation - return ThrottleError - 429 If asynchronous invocation - retry automaticall and then go to DLQ","title":"Lambda Concurrency"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/29-lambda-concurrency/#concurrency-and-asynchronous-invocations","text":"If the function doesn't have enough concurrency available to process all events, additional requests are throttled. For throttling errors (429) and system errors (500-series), Lambda returns the event to the queue and attempts to run the function again for up to 6 hours. The retry interval increases exponentially from 1 second after the first attempt to a maximum of 5 minutes.","title":"Concurrency and Asynchronous Invocations"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/29-lambda-concurrency/#cold-starts-provisioned-concurrency","text":"Cold Start New instance -> code is loaded and code outside the handler run (init) If the init is large (code, dependencies, SDK) this process can take some time First request is served by new instances has higher latency than the rest Provisioned Concurrency Concurrency is allocated before the function is invoked (in advance) So the cold start never happens and all invocations have low latency Application Auto Scaling can manage concurrency (schedule or target utilization) Note: cold starts in VPC have been dramatically reduced in Oct & Nov 2019 https://aws.amazon.com/blogs/compute/announcing-improved-vpc-networking-for-aws-lambda-functions/","title":"Cold Starts &amp; Provisioned Concurrency"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/29-lambda-concurrency/#reserved-and-provisioned-concurrency","text":"https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html","title":"Reserved and Provisioned Concurrency"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/30-lambda-concurrency-hands-on/","text":"Lambda Concurrency Hands On \u00b6 In Configuration - Concurrency we can configure the concurrency. This will leave 980 concurrency for other functions. Note: you can set the reserve concurrency to 0 and it will become always throttled. We can also configure provisioned concurrency.","title":"Lambda Concurrency Hands On"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/30-lambda-concurrency-hands-on/#lambda-concurrency-hands-on","text":"In Configuration - Concurrency we can configure the concurrency. This will leave 980 concurrency for other functions. Note: you can set the reserve concurrency to 0 and it will become always throttled. We can also configure provisioned concurrency.","title":"Lambda Concurrency Hands On"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/31-lambda-external-dependencies/","text":"Lambda External Dependencies \u00b6 If your Lambda function depends on external libraries: for example AWS X-Ray SDK, Database Clients, etc You need to install the packages alongside your code and sip it together For node.js use npm & node_modules directory For python use pip --target options For Java, include the relevant .jar files Upload the zip straight to Lambda if less than 50MB, else to S3 first Native libraries work: they need to be compiled on Amazon Linux AWS SDK comes by default with every Lambda function","title":"Lambda External Dependencies"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/31-lambda-external-dependencies/#lambda-external-dependencies","text":"If your Lambda function depends on external libraries: for example AWS X-Ray SDK, Database Clients, etc You need to install the packages alongside your code and sip it together For node.js use npm & node_modules directory For python use pip --target options For Java, include the relevant .jar files Upload the zip straight to Lambda if less than 50MB, else to S3 first Native libraries work: they need to be compiled on Amazon Linux AWS SDK comes by default with every Lambda function","title":"Lambda External Dependencies"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/32-lambda-external-dependencies-hands-on/","text":"Lambda External Dependencies Hands On \u00b6 const AWSXRay = require('aws-xray-sdk-core') const AWS = AWSXray.captureAWS(requre('aws-sdk')) const s3 = new AWS.S3() exports.handler = async function(event) { return s3.listBuckets().promise() } $ npm install aws-xray-sdk $ chmod a+r * $ zip -r function.zip . $ aws lambda create-function --zip-file fileb://function.zip --function-name lambda-xray-with-dependencies --runtime nodejs14.x --handler index.handler --role arn:aws:iam::...:role/DemoLambdaWithDependencies","title":"Lambda External Dependencies Hands On"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/32-lambda-external-dependencies-hands-on/#lambda-external-dependencies-hands-on","text":"const AWSXRay = require('aws-xray-sdk-core') const AWS = AWSXray.captureAWS(requre('aws-sdk')) const s3 = new AWS.S3() exports.handler = async function(event) { return s3.listBuckets().promise() } $ npm install aws-xray-sdk $ chmod a+r * $ zip -r function.zip . $ aws lambda create-function --zip-file fileb://function.zip --function-name lambda-xray-with-dependencies --runtime nodejs14.x --handler index.handler --role arn:aws:iam::...:role/DemoLambdaWithDependencies","title":"Lambda External Dependencies Hands On"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/33-lambda-and-cloudformation/","text":"Lambda and CloudFormation \u00b6 Inline \u00b6 Inline functions are very simple Use the Code.ZipFile property You cannot include function dependencies with inline functions AWSTemplateFormatVersion: '2010-09-09' Description: Lambda function inline Resources: primer: Type: AWS::Lambda::Function Properties: Runtime: python3.x Role: arn:aws:iam:...:role/lambda-role Handler: index.handler Code: ZipFile: | import os DB_URL = os.getenv(\"DB_URL\") db_client = db.connect(DB_URL) def handler(event, context): return db_client.get(user_id = event[\"user_id\"]) Through S3 \u00b6 You must store the Lambda zip in S3 You must refer the S3 zip location in the CloudFormation code S3 Bucket S3Key: full path to zip S3ObjectVersion: if versioned bucket If you update the code in S3, but don't update S3Bucket, S3Key or S3ObjectVersion, CloudFormation won't update your function. AWSTemplateFormatVersion: '2010-09-09' Description: Lambda function inline Resources: primer: Type: AWS::Lambda::Function Properties: Runtime: python3.x Role: arn:aws:iam:...:role/lambda-role Handler: index.handler Code: S3Bucket: my-bucket S3Key: function.zip S3ObjectVersion: String Through S3 multiple accounts \u00b6","title":"Lambda and CloudFormation"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/33-lambda-and-cloudformation/#lambda-and-cloudformation","text":"","title":"Lambda and CloudFormation"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/33-lambda-and-cloudformation/#inline","text":"Inline functions are very simple Use the Code.ZipFile property You cannot include function dependencies with inline functions AWSTemplateFormatVersion: '2010-09-09' Description: Lambda function inline Resources: primer: Type: AWS::Lambda::Function Properties: Runtime: python3.x Role: arn:aws:iam:...:role/lambda-role Handler: index.handler Code: ZipFile: | import os DB_URL = os.getenv(\"DB_URL\") db_client = db.connect(DB_URL) def handler(event, context): return db_client.get(user_id = event[\"user_id\"])","title":"Inline"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/33-lambda-and-cloudformation/#through-s3","text":"You must store the Lambda zip in S3 You must refer the S3 zip location in the CloudFormation code S3 Bucket S3Key: full path to zip S3ObjectVersion: if versioned bucket If you update the code in S3, but don't update S3Bucket, S3Key or S3ObjectVersion, CloudFormation won't update your function. AWSTemplateFormatVersion: '2010-09-09' Description: Lambda function inline Resources: primer: Type: AWS::Lambda::Function Properties: Runtime: python3.x Role: arn:aws:iam:...:role/lambda-role Handler: index.handler Code: S3Bucket: my-bucket S3Key: function.zip S3ObjectVersion: String","title":"Through S3"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/33-lambda-and-cloudformation/#through-s3-multiple-accounts","text":"","title":"Through S3 multiple accounts"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/34-lambda-and-cloudformation-hands-on/","text":"Lambda and CloudFormation Hands On \u00b6 Parameters: S3BucketParam: Type: String S3KeyParam: Type: String S3ObjectVersionParam: Type: String Resources: LambdaExecutionRole: Type: AWS::IAM::Role Properties: AssumeRolePolicyDocument: Version: '2012-10-17' Statement: - Effect: Allow Principal: Service: - lambda.amazonaws.com Action: - sts:AssumeRole Path: \"/\" Policies: - PolicyName: root PolicyDocument: Version: '2012-10-17' Statement: - Effect: Allow Action: - logs:* Resource: arn:aws:logs:*:*:* - Effect: Allow Action: - xray:PutTraceSegments - xray:PutTelemetryRecords - xray:GetSamplingRules - xray:GetSamplingTargets - xray:GetSamplingStatisticsSummaries Resource: \"*\" - Effect: Allow Action: - s3:Get* - s3:List* Resource: \"*\" LambdaWithXRay: Type: AWS::Lambda::Function Properties: Handler: index.handler Role: Fn::GetAtt: - LambdaExecutionRole - Arn Code: S3Bucket: Ref: S3BucketParam S3Key: Ref: S3KeyParam S3ObjectVersion: Ref: S3ObjectVersionParam Timeout: 10 TracingConfig: # enable XRay Mode: Active Now we can create a new s3 bucket, upload the function zip file to it. Then import the CloudFormation template and provide the path to the zip file as parameters.","title":"Lambda and CloudFormation Hands On"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/34-lambda-and-cloudformation-hands-on/#lambda-and-cloudformation-hands-on","text":"Parameters: S3BucketParam: Type: String S3KeyParam: Type: String S3ObjectVersionParam: Type: String Resources: LambdaExecutionRole: Type: AWS::IAM::Role Properties: AssumeRolePolicyDocument: Version: '2012-10-17' Statement: - Effect: Allow Principal: Service: - lambda.amazonaws.com Action: - sts:AssumeRole Path: \"/\" Policies: - PolicyName: root PolicyDocument: Version: '2012-10-17' Statement: - Effect: Allow Action: - logs:* Resource: arn:aws:logs:*:*:* - Effect: Allow Action: - xray:PutTraceSegments - xray:PutTelemetryRecords - xray:GetSamplingRules - xray:GetSamplingTargets - xray:GetSamplingStatisticsSummaries Resource: \"*\" - Effect: Allow Action: - s3:Get* - s3:List* Resource: \"*\" LambdaWithXRay: Type: AWS::Lambda::Function Properties: Handler: index.handler Role: Fn::GetAtt: - LambdaExecutionRole - Arn Code: S3Bucket: Ref: S3BucketParam S3Key: Ref: S3KeyParam S3ObjectVersion: Ref: S3ObjectVersionParam Timeout: 10 TracingConfig: # enable XRay Mode: Active Now we can create a new s3 bucket, upload the function zip file to it. Then import the CloudFormation template and provide the path to the zip file as parameters.","title":"Lambda and CloudFormation Hands On"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/35-lambda-layers/","text":"Lambda Layers \u00b6 Custom Runtimes Ex: C++ https://github.com/awslabs/aws-lambda-cpp Ex: Rust https://github.com/awslabs/aws-lambda-rust-runtime Externalize Dependencies to re-use them","title":"Lambda Layers"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/35-lambda-layers/#lambda-layers","text":"Custom Runtimes Ex: C++ https://github.com/awslabs/aws-lambda-cpp Ex: Rust https://github.com/awslabs/aws-lambda-rust-runtime Externalize Dependencies to re-use them","title":"Lambda Layers"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/36-lambda-layers-hands-on/","text":"Lambda Layers Hands On \u00b6 We are going to create a new function and choose From scratch type. Then we can follow the AWS tutorial on how to create the layers: https://aws.amazon.com/blogs/aws/new-for-aws-lambda-use-any-programming-language-and-share-common-components/ In our lambda function at the bottom of the page we can add a new layer. So, this is a layer with Pyhon3.8 SciPy library already compiled available to us. Now we can use code like this without uploading any dependencies: import numpy as np from scipy.spatial import ConvexHull def lambda_handler(event, context): print(\"\\nUsing NumPy\\n\") print(\"random matrix_a =\") matrix_a = np.random.randint(10, size=(4, 4)) print(matrix_a) print(\"random matrix_b =\") matrix_b = np.random.randint(10, size=(4, 4)) print(matrix_b) print(\"matrix_a * matrix_b = \") print(matrix_a.dot(matrix_b)) print(\"\\nUsing SciPy\\n\") num_points = 10 print(num_points, \"random points:\") points = np.random.rand(num_points, 2) for i, point in enumerate(points): print(i, '->', point) hull = ConvexHull(points) print(\"The smallest convex set containing all\", num_points, \"points has\", len(hull.simplices), \"sides,\\nconnecting points:\") for simplex in hull.simplices: print(simplex[0], '<->', simplex[1])","title":"Lambda Layers Hands On"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/36-lambda-layers-hands-on/#lambda-layers-hands-on","text":"We are going to create a new function and choose From scratch type. Then we can follow the AWS tutorial on how to create the layers: https://aws.amazon.com/blogs/aws/new-for-aws-lambda-use-any-programming-language-and-share-common-components/ In our lambda function at the bottom of the page we can add a new layer. So, this is a layer with Pyhon3.8 SciPy library already compiled available to us. Now we can use code like this without uploading any dependencies: import numpy as np from scipy.spatial import ConvexHull def lambda_handler(event, context): print(\"\\nUsing NumPy\\n\") print(\"random matrix_a =\") matrix_a = np.random.randint(10, size=(4, 4)) print(matrix_a) print(\"random matrix_b =\") matrix_b = np.random.randint(10, size=(4, 4)) print(matrix_b) print(\"matrix_a * matrix_b = \") print(matrix_a.dot(matrix_b)) print(\"\\nUsing SciPy\\n\") num_points = 10 print(num_points, \"random points:\") points = np.random.rand(num_points, 2) for i, point in enumerate(points): print(i, '->', point) hull = ConvexHull(points) print(\"The smallest convex set containing all\", num_points, \"points has\", len(hull.simplices), \"sides,\\nconnecting points:\") for simplex in hull.simplices: print(simplex[0], '<->', simplex[1])","title":"Lambda Layers Hands On"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/37-lambda-container-images/","text":"Lambda Container Images \u00b6 Deploy Lambda function as container images of up to 10GB from ECR Pack complex dependencies, large dependencies in a container Base images are available for Python, Node.js, Java, .NET, Go, Ruby Can create your own image as long as it implements the Lambda Runtime API Test the containers locally using the Lambda Runtime Interface Emulator Unified workflow to build apps Example: build from the base images provided by AWS FROM amazon/aws-lambda-nodejs:12 COPY app.js package*.json ./ RUN npm install CMD [\"app.lambdaHandler\"]","title":"Lambda Container Images"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/37-lambda-container-images/#lambda-container-images","text":"Deploy Lambda function as container images of up to 10GB from ECR Pack complex dependencies, large dependencies in a container Base images are available for Python, Node.js, Java, .NET, Go, Ruby Can create your own image as long as it implements the Lambda Runtime API Test the containers locally using the Lambda Runtime Interface Emulator Unified workflow to build apps Example: build from the base images provided by AWS FROM amazon/aws-lambda-nodejs:12 COPY app.js package*.json ./ RUN npm install CMD [\"app.lambdaHandler\"]","title":"Lambda Container Images"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/38-lambda-versions-and-aliases/","text":"Lambda Versions and Aliases \u00b6 Versions \u00b6 When you work on a Lambda function, we work on $LATEST When we're ready to publish a Lambda function, we create a version Versions are immutable Versions have increasing version numbers Versions get their own ARN (Amazon Resource Name) Version = code + configuration (nothing can be changed - immutable) Each version of the lambda function can be accessed Aliases \u00b6 Aliases are pointers to Lambda function versions We can define a \"dev\", \"test\", \"prod\" aliases and have them point at different lambda versions Aliases are mutable Aliases enable Blue / Green deployment by assigning weights to lambda functions Aliases enable stable configuration of our event triggers / destinations Aliases have their own ARNs Aliases cannot reference aliases","title":"Lambda Versions and Aliases"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/38-lambda-versions-and-aliases/#lambda-versions-and-aliases","text":"","title":"Lambda Versions and Aliases"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/38-lambda-versions-and-aliases/#versions","text":"When you work on a Lambda function, we work on $LATEST When we're ready to publish a Lambda function, we create a version Versions are immutable Versions have increasing version numbers Versions get their own ARN (Amazon Resource Name) Version = code + configuration (nothing can be changed - immutable) Each version of the lambda function can be accessed","title":"Versions"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/38-lambda-versions-and-aliases/#aliases","text":"Aliases are pointers to Lambda function versions We can define a \"dev\", \"test\", \"prod\" aliases and have them point at different lambda versions Aliases are mutable Aliases enable Blue / Green deployment by assigning weights to lambda functions Aliases enable stable configuration of our event triggers / destinations Aliases have their own ARNs Aliases cannot reference aliases","title":"Aliases"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/39-lambda-versions-and-aliases-hands-on/","text":"Lambda Versions and Aliases Hands On \u00b6 When working on our function, we can publish a new version. We can view all the version of our function: We can also define aliases:","title":"Lambda Versions and Aliases Hands On"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/39-lambda-versions-and-aliases-hands-on/#lambda-versions-and-aliases-hands-on","text":"When working on our function, we can publish a new version. We can view all the version of our function: We can also define aliases:","title":"Lambda Versions and Aliases Hands On"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/40-lambda-codedeploy/","text":"Lambda & CodeDeploy \u00b6 CodeDeploy can help you automate traffic shift for Lambda aliases Feature is integrated within the SAM framework Linear: grow traffic every N minutes until 100% Canary: try X percent then 100% Canary10Percent5Minutes Canary10Percent30Minutes AllAtOnce: immediate Can create Pre & Post Traffic hooks to check the health of the Lambda functions","title":"Lambda & CodeDeploy"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/40-lambda-codedeploy/#lambda-codedeploy","text":"CodeDeploy can help you automate traffic shift for Lambda aliases Feature is integrated within the SAM framework Linear: grow traffic every N minutes until 100% Canary: try X percent then 100% Canary10Percent5Minutes Canary10Percent30Minutes AllAtOnce: immediate Can create Pre & Post Traffic hooks to check the health of the Lambda functions","title":"Lambda &amp; CodeDeploy"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/41-lambda-limits/","text":"Lambda Limits \u00b6 AWS Lambda Limits to Know - per region \u00b6 Execution: Memory allocation: 128 MB - 10GB (1MB increments) Maximum execution time: 900 seconds (15 minutes) Environment variables (4KB) Disk capacity in the function \"function container\" (in /tmp): 512MB Concurrency executions: 1000 (can be increased) Deployment: Lambda function deployment size (compressed .zip): 50MB Size of uncompressed deployment (code + dependencies): 250MB Can use the /tmp directory to load other files at startup Size of environment variables: 4KB","title":"Lambda Limits"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/41-lambda-limits/#lambda-limits","text":"","title":"Lambda Limits"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/41-lambda-limits/#aws-lambda-limits-to-know-per-region","text":"Execution: Memory allocation: 128 MB - 10GB (1MB increments) Maximum execution time: 900 seconds (15 minutes) Environment variables (4KB) Disk capacity in the function \"function container\" (in /tmp): 512MB Concurrency executions: 1000 (can be increased) Deployment: Lambda function deployment size (compressed .zip): 50MB Size of uncompressed deployment (code + dependencies): 250MB Can use the /tmp directory to load other files at startup Size of environment variables: 4KB","title":"AWS Lambda Limits to Know - per region"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/42-lambda-best-practices/","text":"Lambda Best Practices \u00b6 Perform heavy-duty work outside of your function handler Connect to databases outside of your function handler Initialize the AWS SDK outside of your function handler Pull in dependencies or datasets outside your function handler Use environment variables for: Database Connection Strings, S3 bucket, etc... don't put these values in your code Passwords, sensitive values... they can be encrypted using KMS Minimize your deployment package size to it's runtime necessities Break down the function if need be Remember the AWS Lambda limits Use layers where necessary Avoid using recursive code, never have a lambda call itself. Can be a very expensive disaster.","title":"Lambda Best Practices"},{"location":"AWS/developer-associate/20-aws-serverless-lambda/42-lambda-best-practices/#lambda-best-practices","text":"Perform heavy-duty work outside of your function handler Connect to databases outside of your function handler Initialize the AWS SDK outside of your function handler Pull in dependencies or datasets outside your function handler Use environment variables for: Database Connection Strings, S3 bucket, etc... don't put these values in your code Passwords, sensitive values... they can be encrypted using KMS Minimize your deployment package size to it's runtime necessities Break down the function if need be Remember the AWS Lambda limits Use layers where necessary Avoid using recursive code, never have a lambda call itself. Can be a very expensive disaster.","title":"Lambda Best Practices"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/01-dynamodb-overview/","text":"DynamoDB Overview \u00b6 Traditional Architecture \u00b6 Traditional applications leverage RDBMS databases These databases have SQL query language Strong requirements about how the data should be modeled Ability to do query joins, aggregations, complex computations Vertical scaling (getting more powerful CPU / RAM / IO) Horizontal scaling (increasing reading capacity by adding EC2 / RDS Read Replicas) NoSQL databases \u00b6 NoSQL databases are non-relational databases and are distributed NoSQL databases include MongoDB, DynamoDB, ... NoSQL databases do not support query joins (or just limited support) All the data that is needed for a query is present in one row NoSQL databases don't perform aggregations such as SUM, AVG, ... NoSQL databases scale horizontally There's no \"right or wrong\" for NoSQL vs SQL, they just require to model the data differently and think about user queries differently. Amazon DynamoDB \u00b6 Fully managed, highly available with replication across multiple AZs NoSQL database - not a relational database Scales to massive workloads, distributed database Millions of requests per seconds, trillions of rows, 100s of TB of storage Fast and consistent in performance (low latency on retrieval) Integrated with IAM for security, authorization and administration Enables event driven programming with DynamoDB Streams Low cost and auto-scaling capabilities Standart & Infrequent Access (IA) Table Class DynamoDB - Basics \u00b6 DynamoDB is made of Tables Each table has a Primary Key (must be decided at creation time) Each table can have an infinite number of items (rows) Each item has attributes (can be added over time - can be null) Maximum size of an ittem - 400KB Data types supported are: Scalar Types - String, Number, Binary, Boolean, Null Document Types - List, Map Set Types - String Set, Number Set, Binary Set DynamoDB - Primary Keys \u00b6 Option 1: Partition Key (HASH) Partition key must be unique for each item Partition key must be diverse so that the data is distributed Example: user_id for a users table Option 2: Partition Key + Sort Key (HASH + Range) The combination must be unique for each item Data is grouped by partition key Example: users-games table, user_id for partition key and game_id for sort key. Partition Keys - Excercise \u00b6 We're building a movie database What is the best partition key to maximize data distribution? Movie_id Producer_name Leader_actor_name Movie_language Movie_id has the highest cardinality so it's a good candidate Movie language doesn't take many values and may be skewed towards English so it's not a great choice for the partition key.","title":"DynamoDB Overview"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/01-dynamodb-overview/#dynamodb-overview","text":"","title":"DynamoDB Overview"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/01-dynamodb-overview/#traditional-architecture","text":"Traditional applications leverage RDBMS databases These databases have SQL query language Strong requirements about how the data should be modeled Ability to do query joins, aggregations, complex computations Vertical scaling (getting more powerful CPU / RAM / IO) Horizontal scaling (increasing reading capacity by adding EC2 / RDS Read Replicas)","title":"Traditional Architecture"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/01-dynamodb-overview/#nosql-databases","text":"NoSQL databases are non-relational databases and are distributed NoSQL databases include MongoDB, DynamoDB, ... NoSQL databases do not support query joins (or just limited support) All the data that is needed for a query is present in one row NoSQL databases don't perform aggregations such as SUM, AVG, ... NoSQL databases scale horizontally There's no \"right or wrong\" for NoSQL vs SQL, they just require to model the data differently and think about user queries differently.","title":"NoSQL databases"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/01-dynamodb-overview/#amazon-dynamodb","text":"Fully managed, highly available with replication across multiple AZs NoSQL database - not a relational database Scales to massive workloads, distributed database Millions of requests per seconds, trillions of rows, 100s of TB of storage Fast and consistent in performance (low latency on retrieval) Integrated with IAM for security, authorization and administration Enables event driven programming with DynamoDB Streams Low cost and auto-scaling capabilities Standart & Infrequent Access (IA) Table Class","title":"Amazon DynamoDB"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/01-dynamodb-overview/#dynamodb-basics","text":"DynamoDB is made of Tables Each table has a Primary Key (must be decided at creation time) Each table can have an infinite number of items (rows) Each item has attributes (can be added over time - can be null) Maximum size of an ittem - 400KB Data types supported are: Scalar Types - String, Number, Binary, Boolean, Null Document Types - List, Map Set Types - String Set, Number Set, Binary Set","title":"DynamoDB - Basics"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/01-dynamodb-overview/#dynamodb-primary-keys","text":"Option 1: Partition Key (HASH) Partition key must be unique for each item Partition key must be diverse so that the data is distributed Example: user_id for a users table Option 2: Partition Key + Sort Key (HASH + Range) The combination must be unique for each item Data is grouped by partition key Example: users-games table, user_id for partition key and game_id for sort key.","title":"DynamoDB - Primary Keys"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/01-dynamodb-overview/#partition-keys-excercise","text":"We're building a movie database What is the best partition key to maximize data distribution? Movie_id Producer_name Leader_actor_name Movie_language Movie_id has the highest cardinality so it's a good candidate Movie language doesn't take many values and may be skewed towards English so it's not a great choice for the partition key.","title":"Partition Keys - Excercise"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/02-dynamodb-basics-hands-on/","text":"DynamoDB Basics - Hands On \u00b6 We can open up the DynamoDB service in AWS Console and create a new table. At first we need to fill out the table name and a primary key (optionally the sort key as well). Next, we can choose either the default settings or to customize them, like table class - standard or infrequent access. Next, we have a capacity calculator, at which we will look into later. Next, we can select the On-demand or Provisioned capacity mode. Note that the Provisioned is in the free-tier. Next, we can set the read and write capacity. For free tier we can turn off the auto scaling and set both to 2 provisioned capacity units. Next, we can set up the secondary indexes. We are going to look into that later. Finally, we can set up the encryption. Our table has now been created. We can open it up and have a lot of information about it. If we click on the View items, we can scan or query them. We are going to create an item - there's a button for that. It will open up a form where you can set the partition key and add additional fields. Optionally, you can choose to add the item in JSON format. In DynamoDB it is fine that most of the columns are left empty, the only mandatory field is the primary key. Next, we can set up another table UserPosts, where the primary key is the user_id and the sort key is post timestamp.","title":"DynamoDB Basics - Hands On"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/02-dynamodb-basics-hands-on/#dynamodb-basics-hands-on","text":"We can open up the DynamoDB service in AWS Console and create a new table. At first we need to fill out the table name and a primary key (optionally the sort key as well). Next, we can choose either the default settings or to customize them, like table class - standard or infrequent access. Next, we have a capacity calculator, at which we will look into later. Next, we can select the On-demand or Provisioned capacity mode. Note that the Provisioned is in the free-tier. Next, we can set the read and write capacity. For free tier we can turn off the auto scaling and set both to 2 provisioned capacity units. Next, we can set up the secondary indexes. We are going to look into that later. Finally, we can set up the encryption. Our table has now been created. We can open it up and have a lot of information about it. If we click on the View items, we can scan or query them. We are going to create an item - there's a button for that. It will open up a form where you can set the partition key and add additional fields. Optionally, you can choose to add the item in JSON format. In DynamoDB it is fine that most of the columns are left empty, the only mandatory field is the primary key. Next, we can set up another table UserPosts, where the primary key is the user_id and the sort key is post timestamp.","title":"DynamoDB Basics - Hands On"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/03-dynamodb-wcu-rcu-throughput/","text":"DynamoDB - Write Capacity Units & Read Capacity Units - Throughput \u00b6 Read/Write Capacity Modes \u00b6 Control how you manage your table's capacity (read/write throughput) Provisioned mode (default) You specify the number of reads/writes per second You need to plan capacity beforehand Pay for provisioned read & write capacity units On-Demand Mode Read/writes automatically scale up/down with your workloads No capacity planning needed Pay for what you use, more expensive ($$$) You can switch between different modes once every 24 hours R/W Capacity Modes - Provisioned \u00b6 Table must have provisioned read and write capacity units Read Capacity Units (RCU) - throughput for reads Write Capacity Units (RCU) - throughput for writes Option to setup auto-scaling of throughput to meet demand Throughput can be exceeded temporarily using \"Burst Capacity\" If Burst Capacity has been consumed, you'll get a \"ProvisionedThroughputExceededException\" It's when advised to do an exponential backoff retry. DynamoDB - Write Capacity Units (WCU) \u00b6 One Write Capacity Unit (WCU) represents one write per second for an item up to 1 KB in size If the items are larger than 1KB, more WCUs are consumed Example 1: we write 10 items per second, with item size of 2KB We need 10 * (2KB / 1 KB) = 20 WCUs Example 2: we write 6 items per second with item size 4.5KB We need 6 * (5KB / 1KB) = 30 WCUs (we round up 4.5KB to 5KB) Example 3: we write 120 items per minute, with item size of 2KB We need (120 / 60) * (2KB / 1KB) = 4 WCUs Strongly Consistent Read vs Eventually Consistent Read \u00b6 Eventually Consistent Read (default) If we read just after a write, it's possible we'll get some stale data because of replication. Strongly Consistent Read If we read just after a write, we will get the correct data Set \"ConsistendtRead\" parameter to True in API calls (GetItem, BatchGetItem, Query, Scan) Consumes twice the RCU DynamoDB - Read Capacity Units (RCU) \u00b6 One Read Capacity Unit (RCU) represents one Strongly Consistent Read per second or two Eventually Consistent Reads per second, for an item up to 4 KB in size. If the items are larger than 4KB, more RCUs are consumed. Example 1: 10 Strongly Consistent Reads per second, with item size 4 KB We need 10 * (4KB/4KB) = 10 RCUs Example 2: 16 Eventually Consistent Reads per second, with item size 12 KB We need (16 / 2) * (12 KB / 4 KB) = 24 RCUs Example 3: 10 Strongly Consistent Reads per second, with item size 6KB We need 10 * (8KB / 4 KB) = 20 RCUs (we must round up 6KB to 8KB) DynamoDB - Partitions Internal \u00b6 Data is stored in partitions Partition Keys go through a hashing algorithm to know which partition they go to To compute the number of partitions: # of partitionsByCapacity = (RCUsTotal / 3000) + (WCUsTotal/1000) # of partitionsBySize = TotalSize / 10GB # of partitions = ceil(max(# of partitionsByCapacity, # of partitionsBySize)) WCUs and RCUs are spread evenly across partitions. DynamoDB - Throttling \u00b6 If we exceed provisioned RCUs or WCUs, we get \"ProvisionedThroughputExceededException\" Reasons: Hot Keys - one partition key is being read too many times (e.g., popular item) Hot partitions Very large items, remember RCU and WCU depends on size of items Solutions Exponential backoff when exception is encountered (already in SDK) Distribute partition keys as much as possible If RCU issue, we can use DynamoDB Accelerator (DAX) R/W Capacity Modes - On Demand \u00b6 Read/writes automatically scale up/down with your workloads No capacity planning needed (WCU/RCU) Unlimited WCU&RCU, no throttle, more expensive You're charged for reads/writes that you use in terms of RRU, WRU Read Request Units (RRU) - throughput for reads (same as RCU) Write Request Units (WRU) - thoughput for writes (same as WCU) 2.5x more expensive than provisioned capacity (use with care) Use cases: unknown workloads, unpredictable application traffic, ...","title":"DynamoDB - Write Capacity Units & Read Capacity Units - Throughput"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/03-dynamodb-wcu-rcu-throughput/#dynamodb-write-capacity-units-read-capacity-units-throughput","text":"","title":"DynamoDB - Write Capacity Units &amp; Read Capacity Units - Throughput"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/03-dynamodb-wcu-rcu-throughput/#readwrite-capacity-modes","text":"Control how you manage your table's capacity (read/write throughput) Provisioned mode (default) You specify the number of reads/writes per second You need to plan capacity beforehand Pay for provisioned read & write capacity units On-Demand Mode Read/writes automatically scale up/down with your workloads No capacity planning needed Pay for what you use, more expensive ($$$) You can switch between different modes once every 24 hours","title":"Read/Write Capacity Modes"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/03-dynamodb-wcu-rcu-throughput/#rw-capacity-modes-provisioned","text":"Table must have provisioned read and write capacity units Read Capacity Units (RCU) - throughput for reads Write Capacity Units (RCU) - throughput for writes Option to setup auto-scaling of throughput to meet demand Throughput can be exceeded temporarily using \"Burst Capacity\" If Burst Capacity has been consumed, you'll get a \"ProvisionedThroughputExceededException\" It's when advised to do an exponential backoff retry.","title":"R/W Capacity Modes - Provisioned"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/03-dynamodb-wcu-rcu-throughput/#dynamodb-write-capacity-units-wcu","text":"One Write Capacity Unit (WCU) represents one write per second for an item up to 1 KB in size If the items are larger than 1KB, more WCUs are consumed Example 1: we write 10 items per second, with item size of 2KB We need 10 * (2KB / 1 KB) = 20 WCUs Example 2: we write 6 items per second with item size 4.5KB We need 6 * (5KB / 1KB) = 30 WCUs (we round up 4.5KB to 5KB) Example 3: we write 120 items per minute, with item size of 2KB We need (120 / 60) * (2KB / 1KB) = 4 WCUs","title":"DynamoDB - Write Capacity Units (WCU)"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/03-dynamodb-wcu-rcu-throughput/#strongly-consistent-read-vs-eventually-consistent-read","text":"Eventually Consistent Read (default) If we read just after a write, it's possible we'll get some stale data because of replication. Strongly Consistent Read If we read just after a write, we will get the correct data Set \"ConsistendtRead\" parameter to True in API calls (GetItem, BatchGetItem, Query, Scan) Consumes twice the RCU","title":"Strongly Consistent Read vs Eventually Consistent Read"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/03-dynamodb-wcu-rcu-throughput/#dynamodb-read-capacity-units-rcu","text":"One Read Capacity Unit (RCU) represents one Strongly Consistent Read per second or two Eventually Consistent Reads per second, for an item up to 4 KB in size. If the items are larger than 4KB, more RCUs are consumed. Example 1: 10 Strongly Consistent Reads per second, with item size 4 KB We need 10 * (4KB/4KB) = 10 RCUs Example 2: 16 Eventually Consistent Reads per second, with item size 12 KB We need (16 / 2) * (12 KB / 4 KB) = 24 RCUs Example 3: 10 Strongly Consistent Reads per second, with item size 6KB We need 10 * (8KB / 4 KB) = 20 RCUs (we must round up 6KB to 8KB)","title":"DynamoDB - Read Capacity Units (RCU)"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/03-dynamodb-wcu-rcu-throughput/#dynamodb-partitions-internal","text":"Data is stored in partitions Partition Keys go through a hashing algorithm to know which partition they go to To compute the number of partitions: # of partitionsByCapacity = (RCUsTotal / 3000) + (WCUsTotal/1000) # of partitionsBySize = TotalSize / 10GB # of partitions = ceil(max(# of partitionsByCapacity, # of partitionsBySize)) WCUs and RCUs are spread evenly across partitions.","title":"DynamoDB - Partitions Internal"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/03-dynamodb-wcu-rcu-throughput/#dynamodb-throttling","text":"If we exceed provisioned RCUs or WCUs, we get \"ProvisionedThroughputExceededException\" Reasons: Hot Keys - one partition key is being read too many times (e.g., popular item) Hot partitions Very large items, remember RCU and WCU depends on size of items Solutions Exponential backoff when exception is encountered (already in SDK) Distribute partition keys as much as possible If RCU issue, we can use DynamoDB Accelerator (DAX)","title":"DynamoDB - Throttling"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/03-dynamodb-wcu-rcu-throughput/#rw-capacity-modes-on-demand","text":"Read/writes automatically scale up/down with your workloads No capacity planning needed (WCU/RCU) Unlimited WCU&RCU, no throttle, more expensive You're charged for reads/writes that you use in terms of RRU, WRU Read Request Units (RRU) - throughput for reads (same as RCU) Write Request Units (WRU) - thoughput for writes (same as WCU) 2.5x more expensive than provisioned capacity (use with care) Use cases: unknown workloads, unpredictable application traffic, ...","title":"R/W Capacity Modes - On Demand"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/04-dynamodb-wcu-rcu-hands-on/","text":"DynamoDB WCU & RCU - Hands On \u00b6 You can set the Capacity mode while editing the table. Note that the On-demand mode is 2.5x more expensive than the provisioned one. When selecting the Provisioned mode, you can use the Capacity Calculator to calculate number of units you need. Then, you can set the capacity under it. Once done, you can see the provisioned units as well as the autoscaling activities.","title":"DynamoDB WCU & RCU - Hands On"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/04-dynamodb-wcu-rcu-hands-on/#dynamodb-wcu-rcu-hands-on","text":"You can set the Capacity mode while editing the table. Note that the On-demand mode is 2.5x more expensive than the provisioned one. When selecting the Provisioned mode, you can use the Capacity Calculator to calculate number of units you need. Then, you can set the capacity under it. Once done, you can see the provisioned units as well as the autoscaling activities.","title":"DynamoDB WCU &amp; RCU - Hands On"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/05-dynamodb-basic-apis/","text":"DynamoDB - Basic APIs \u00b6 Writing data \u00b6 PutItem Creates a new item of fully replace an old item (same primary key) Consumes WCU UpdateItem Edits an existing item's attributes or adds a new item if it doesnt exist Can be used to implement Atomic Counters - a numeric attribute that's unconditionally incremented Conditional Writes Accept a write/update/delete only of conditions are met, otherwise returns an error Helps with concurrent access to items No performance impact Reading data \u00b6 GetItem Read based on Primary Key Primary Key can be HASH or HASH+Range Eventually Consistent Read (default) Option to use Strongly Consistent Reads (more RCU - might take longer) ProjectionExpression can be specified to retrieve only certain attributes Query returns items based on: KeyConditionExpression Partition Key value (must be = operator) - required Sort key value (=, <, <=, >, >=, between, begins with) - optional FilterExpression Additional filtering after the query operation (before data is returned to you) Use only with non-key attributes (does not allow HAS or RANGE attributes) returns The number of items specified in Limit Or up to 1 MB of data Ability to do pagination on the results Can query table, a Local Secondary Index, or a Global Secondary Index Scan scan the entire table and then filter out data (inefficient) Returns up to 1MB of data - use pagination to keep on reading Consumes a lot of RCU Limit impact using Limit or reduce the size of the result and pause For faster performance, use Parallel Scan Multiple workers scan multiple data segments at the same time Increases the thoughput and RCU consumed Limit the impact of parallel scans just like you would for scans Can use ProjectionExpression & FilterExpression (no changes to RCU) Deleting data \u00b6 DeleteItem Delete an individual item Ability to perform a conditional delete DeleteTable Delete a whole table and all it's items Much quicker deletion than calling DeleteItem on all items Batch Operations \u00b6 Allows you to save in latency by reducing the number of API calls Operations are done in parallel for better efficiency Part of a batch can fail, in which case we need to try again for the failed items BatchWriteItem Up to 25 PutItem and/or DeleteItem in one call Up to 16MB of data written, up to 400KB of data per item Can't update items (use UpdateItem) BatchGetItem Return items from one or more tables Up to 100 items, up to 16MB of data Items are retrieved in parallel to minimize latency","title":"DynamoDB - Basic APIs"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/05-dynamodb-basic-apis/#dynamodb-basic-apis","text":"","title":"DynamoDB - Basic APIs"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/05-dynamodb-basic-apis/#writing-data","text":"PutItem Creates a new item of fully replace an old item (same primary key) Consumes WCU UpdateItem Edits an existing item's attributes or adds a new item if it doesnt exist Can be used to implement Atomic Counters - a numeric attribute that's unconditionally incremented Conditional Writes Accept a write/update/delete only of conditions are met, otherwise returns an error Helps with concurrent access to items No performance impact","title":"Writing data"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/05-dynamodb-basic-apis/#reading-data","text":"GetItem Read based on Primary Key Primary Key can be HASH or HASH+Range Eventually Consistent Read (default) Option to use Strongly Consistent Reads (more RCU - might take longer) ProjectionExpression can be specified to retrieve only certain attributes Query returns items based on: KeyConditionExpression Partition Key value (must be = operator) - required Sort key value (=, <, <=, >, >=, between, begins with) - optional FilterExpression Additional filtering after the query operation (before data is returned to you) Use only with non-key attributes (does not allow HAS or RANGE attributes) returns The number of items specified in Limit Or up to 1 MB of data Ability to do pagination on the results Can query table, a Local Secondary Index, or a Global Secondary Index Scan scan the entire table and then filter out data (inefficient) Returns up to 1MB of data - use pagination to keep on reading Consumes a lot of RCU Limit impact using Limit or reduce the size of the result and pause For faster performance, use Parallel Scan Multiple workers scan multiple data segments at the same time Increases the thoughput and RCU consumed Limit the impact of parallel scans just like you would for scans Can use ProjectionExpression & FilterExpression (no changes to RCU)","title":"Reading data"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/05-dynamodb-basic-apis/#deleting-data","text":"DeleteItem Delete an individual item Ability to perform a conditional delete DeleteTable Delete a whole table and all it's items Much quicker deletion than calling DeleteItem on all items","title":"Deleting data"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/05-dynamodb-basic-apis/#batch-operations","text":"Allows you to save in latency by reducing the number of API calls Operations are done in parallel for better efficiency Part of a batch can fail, in which case we need to try again for the failed items BatchWriteItem Up to 25 PutItem and/or DeleteItem in one call Up to 16MB of data written, up to 400KB of data per item Can't update items (use UpdateItem) BatchGetItem Return items from one or more tables Up to 100 items, up to 16MB of data Items are retrieved in parallel to minimize latency","title":"Batch Operations"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/06-dynamodb-indexes-gsi-lsi/","text":"DynamoDB Indexes (GSI + LSI) \u00b6 Local Secondary Index (LSI) \u00b6 Alternative Sort Key for your table (same Partition Key as that of base table) The Sort Key consists of one scalar attribute (String, Number or Binary) Up to 5 local secondary indexes per table Must be defined at table creation time Attribute Projections - can contain some or all the attributes of the base table (KEYS_ONLY, INCLUDE, ALL) Global Secondary Index (GSI) \u00b6 Alternative Primary Key (HASH or HASH+RANGE) from the base table Speed up queries on non-key attributes The Index Key consists of scalar attributes (String, Number, or Binary) Attribute Projections - some or all the attributes of the base table (KEYS_ONLY, INCLUDE, ALL) Must provision RCUs & WCUs for the index Can be added/modified after table creation. Throttling \u00b6 Global Secondary Index If the writes are throttled on the GSI, then the main table will be throttled! Even if the WCU on the main table are fine Choose your GSI partition key carefully Assign your WCU capacity carefully Local Secondary Index Uses the WCUs and RCUs of the main table No special throttling considerations","title":"DynamoDB Indexes (GSI + LSI)"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/06-dynamodb-indexes-gsi-lsi/#dynamodb-indexes-gsi-lsi","text":"","title":"DynamoDB Indexes (GSI + LSI)"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/06-dynamodb-indexes-gsi-lsi/#local-secondary-index-lsi","text":"Alternative Sort Key for your table (same Partition Key as that of base table) The Sort Key consists of one scalar attribute (String, Number or Binary) Up to 5 local secondary indexes per table Must be defined at table creation time Attribute Projections - can contain some or all the attributes of the base table (KEYS_ONLY, INCLUDE, ALL)","title":"Local Secondary Index (LSI)"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/06-dynamodb-indexes-gsi-lsi/#global-secondary-index-gsi","text":"Alternative Primary Key (HASH or HASH+RANGE) from the base table Speed up queries on non-key attributes The Index Key consists of scalar attributes (String, Number, or Binary) Attribute Projections - some or all the attributes of the base table (KEYS_ONLY, INCLUDE, ALL) Must provision RCUs & WCUs for the index Can be added/modified after table creation.","title":"Global Secondary Index (GSI)"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/06-dynamodb-indexes-gsi-lsi/#throttling","text":"Global Secondary Index If the writes are throttled on the GSI, then the main table will be throttled! Even if the WCU on the main table are fine Choose your GSI partition key carefully Assign your WCU capacity carefully Local Secondary Index Uses the WCUs and RCUs of the main table No special throttling considerations","title":"Throttling"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/07-dynamodb-indexes-gsi-lsi-hands-on/","text":"DynamoDB Indexes (GSI + LSI) - Hands On \u00b6 When creating a new table, we can define secondary indexes. When creating a global secondary index, we can customize the capacity as well. When using Query, we can choose to query either a table or the index.","title":"DynamoDB Indexes (GSI + LSI) - Hands On"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/07-dynamodb-indexes-gsi-lsi-hands-on/#dynamodb-indexes-gsi-lsi-hands-on","text":"When creating a new table, we can define secondary indexes. When creating a global secondary index, we can customize the capacity as well. When using Query, we can choose to query either a table or the index.","title":"DynamoDB Indexes (GSI + LSI) - Hands On"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/08-dynamodb-partiql/","text":"DynamoDB - PartiQL \u00b6 Use an SQL-likey syntax to manipulate DynamoDB tables SELECT * FROM \"demo_indexes\" WHERE \"user_id\" = \"partitionKeyValue\" AND \"game_ts\" = \"sortKeyValue\" Supports some (but not all) statements: INSERT UPDATE SELECT DELETE It supports Batch operations The editor can be acessed in the left sidebar.","title":"DynamoDB - PartiQL"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/08-dynamodb-partiql/#dynamodb-partiql","text":"Use an SQL-likey syntax to manipulate DynamoDB tables SELECT * FROM \"demo_indexes\" WHERE \"user_id\" = \"partitionKeyValue\" AND \"game_ts\" = \"sortKeyValue\" Supports some (but not all) statements: INSERT UPDATE SELECT DELETE It supports Batch operations The editor can be acessed in the left sidebar.","title":"DynamoDB - PartiQL"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/09-dynamodb-optimistic-locking/","text":"DynamoDB - Optimistic Locking \u00b6 DynamoDB has feature called \"Conditional Writes\" A Strategy to ensure an item hasn't changed before you update/delete it Each item has an attribute that acts as a version number.","title":"DynamoDB - Optimistic Locking"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/09-dynamodb-optimistic-locking/#dynamodb-optimistic-locking","text":"DynamoDB has feature called \"Conditional Writes\" A Strategy to ensure an item hasn't changed before you update/delete it Each item has an attribute that acts as a version number.","title":"DynamoDB - Optimistic Locking"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/10-dynamodb-dax/","text":"DynamoDB Accelerator (DAX) \u00b6 Fully-managed, highly available, seamless in-memory cache for DynamoDB Microseconds latency for cached reads & queries Doesn't require application logic modification (compatible with existing DynamoDB APIs) Solves the \"Hot Key\" problem (too many reads) 5 minutes TTL for cache (default) up to 10 nodes in the cluster Multi-AZ (3 nodes minimum recommended for production) Secure (Encryption at rest with KMS, VPC, IAM, CloudTrail) DAX vs ElastiCache \u00b6","title":"DynamoDB Accelerator (DAX)"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/10-dynamodb-dax/#dynamodb-accelerator-dax","text":"Fully-managed, highly available, seamless in-memory cache for DynamoDB Microseconds latency for cached reads & queries Doesn't require application logic modification (compatible with existing DynamoDB APIs) Solves the \"Hot Key\" problem (too many reads) 5 minutes TTL for cache (default) up to 10 nodes in the cluster Multi-AZ (3 nodes minimum recommended for production) Secure (Encryption at rest with KMS, VPC, IAM, CloudTrail)","title":"DynamoDB Accelerator (DAX)"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/10-dynamodb-dax/#dax-vs-elasticache","text":"","title":"DAX vs ElastiCache"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/11-dynamodb-dax-hands-on/","text":"DynamoDB DAX - Hands On \u00b6 In DynamoDB Console, on the left side there is DAX available. It's not in the free tier so it will cost money. You can select the node family. In the 4th step we can choose the parameter group. In them we can specify the Item TTL and Query TTL. The defaults are both 5 minutes: We can also choose the maintenence window: Once the cluster is created, it will provide a cluster endpoint. This is the endpoint our applications should leverage.","title":"DynamoDB DAX - Hands On"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/11-dynamodb-dax-hands-on/#dynamodb-dax-hands-on","text":"In DynamoDB Console, on the left side there is DAX available. It's not in the free tier so it will cost money. You can select the node family. In the 4th step we can choose the parameter group. In them we can specify the Item TTL and Query TTL. The defaults are both 5 minutes: We can also choose the maintenence window: Once the cluster is created, it will provide a cluster endpoint. This is the endpoint our applications should leverage.","title":"DynamoDB DAX - Hands On"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/13-dynamodb-streams-hands-on/","text":"DynamoDB Streams Hands On \u00b6 We can open up a dynamodb table and click on Exports and Streams . We can enable the DynamoDB stream details . We can choose which details we want Now we can define triggers:","title":"DynamoDB Streams Hands On"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/13-dynamodb-streams-hands-on/#dynamodb-streams-hands-on","text":"We can open up a dynamodb table and click on Exports and Streams . We can enable the DynamoDB stream details . We can choose which details we want Now we can define triggers:","title":"DynamoDB Streams Hands On"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/14-dynamodb-ttl/","text":"DynamoDB - Time To Live (TTL) \u00b6 Automatically delete items after an expiry timestamp Doesn't consume any WCUs (i.e., no extra cost) The TTL attribute must be a Number data type with Unix Epoch timestamp value. Expired items deleted within 48 hours of expiration Expired items, that haven't beeen deleted, appears in reads/queries/scans (if you don't want them, filter them out). Expired items are deleted from both LSIs and GSIs A delete operation for each expired item enters the DynamoDB Streams (can help recover expired items) Use cases: reduce stored data by keeping only current items, adhere to regulatory obligations...","title":"DynamoDB - Time To Live (TTL)"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/14-dynamodb-ttl/#dynamodb-time-to-live-ttl","text":"Automatically delete items after an expiry timestamp Doesn't consume any WCUs (i.e., no extra cost) The TTL attribute must be a Number data type with Unix Epoch timestamp value. Expired items deleted within 48 hours of expiration Expired items, that haven't beeen deleted, appears in reads/queries/scans (if you don't want them, filter them out). Expired items are deleted from both LSIs and GSIs A delete operation for each expired item enters the DynamoDB Streams (can help recover expired items) Use cases: reduce stored data by keeping only current items, adhere to regulatory obligations...","title":"DynamoDB - Time To Live (TTL)"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/15-dynamodb-cli/","text":"DynamoDB - Good to Know \u00b6 --projection-expression: one or more attributes to retrieve --filter-expression: filter items before returned to you General AWS CLI Pagination Options (e.g. DynamoDB, S3...) --page-size: specify AWS CLI retrieves the full list of items but with larger number of API calls instead of one API call (default: 1000 items) --max-items: max. number of items to show in the CLI (returns NextToken) --starting-token: specify the last Nexttoken to retrieve the next set ot items # Projection Expression $ aws dynamodb scan --table-name UserPosts --projection-expression \"user_id, content\" # Filter Expression $ aws dynamodb scan --table-name UserPosts --filter-expression \"user_id = :u\" --expression-attribute-values '{\":u\": {\"S\": \"john123\"}}' # Page Size - will do 1 API call if you have 3 items $ aws dynamodb scan --tablename UserPosts # Will do 3 API calls if you have 3 Items $ aws dynamodb scan --table-name UserPosts --page-size 1 # Max Items $ aws dynamodb scan --table-name UserPosts --max-items 1 # Fetch the next item $ aws dynamodb scan --table-name UserPosts --max-items 1 --starting-token eyJFeGN..","title":"DynamoDB - Good to Know"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/15-dynamodb-cli/#dynamodb-good-to-know","text":"--projection-expression: one or more attributes to retrieve --filter-expression: filter items before returned to you General AWS CLI Pagination Options (e.g. DynamoDB, S3...) --page-size: specify AWS CLI retrieves the full list of items but with larger number of API calls instead of one API call (default: 1000 items) --max-items: max. number of items to show in the CLI (returns NextToken) --starting-token: specify the last Nexttoken to retrieve the next set ot items # Projection Expression $ aws dynamodb scan --table-name UserPosts --projection-expression \"user_id, content\" # Filter Expression $ aws dynamodb scan --table-name UserPosts --filter-expression \"user_id = :u\" --expression-attribute-values '{\":u\": {\"S\": \"john123\"}}' # Page Size - will do 1 API call if you have 3 items $ aws dynamodb scan --tablename UserPosts # Will do 3 API calls if you have 3 Items $ aws dynamodb scan --table-name UserPosts --page-size 1 # Max Items $ aws dynamodb scan --table-name UserPosts --max-items 1 # Fetch the next item $ aws dynamodb scan --table-name UserPosts --max-items 1 --starting-token eyJFeGN..","title":"DynamoDB - Good to Know"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/16-dynamodb-transactions/","text":"DynamoDB Transactions \u00b6 Coordinate, all-or-nothing operations (add/update/delete) to multiple items across one or more tables. Provides Atomicity, Consistency, Isolation, and Durability (ACID) Read Modes - Eventual Consistency, String Consistency, Transactional Write Modes - Standard, Transactional Consumes 2x WCUs & RCUs DynamoDB performs 2 operations for every item (prepare & commit) Two operations: (up to 25 unique items or up to 4MB of data) TransactGetItems - one or more GetItem operations TransactWriteItems - one or more PutItem, UpdateItem and DeleteItem operations Use cases: financial transactions, managing orders, multiplayer games. Capacity Computations \u00b6 Example 1: 3 Transactional writes per second, with item size 5KB We need 3 * (5KB/1KB) * 2 (transactional cost) = 30 WCUs Example 2: 5 Transactional reads per second, with item size 5KB We need 5 * (8KB/4KB) * 2 (transaction cost) = 20 RCUs 5 gets rounded to the upper 4KB - 8KBs.","title":"DynamoDB Transactions"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/16-dynamodb-transactions/#dynamodb-transactions","text":"Coordinate, all-or-nothing operations (add/update/delete) to multiple items across one or more tables. Provides Atomicity, Consistency, Isolation, and Durability (ACID) Read Modes - Eventual Consistency, String Consistency, Transactional Write Modes - Standard, Transactional Consumes 2x WCUs & RCUs DynamoDB performs 2 operations for every item (prepare & commit) Two operations: (up to 25 unique items or up to 4MB of data) TransactGetItems - one or more GetItem operations TransactWriteItems - one or more PutItem, UpdateItem and DeleteItem operations Use cases: financial transactions, managing orders, multiplayer games.","title":"DynamoDB Transactions"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/16-dynamodb-transactions/#capacity-computations","text":"Example 1: 3 Transactional writes per second, with item size 5KB We need 3 * (5KB/1KB) * 2 (transactional cost) = 30 WCUs Example 2: 5 Transactional reads per second, with item size 5KB We need 5 * (8KB/4KB) * 2 (transaction cost) = 20 RCUs 5 gets rounded to the upper 4KB - 8KBs.","title":"Capacity Computations"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/17-dynamodb-session-state/","text":"DynamoDB as Session State Cache \u00b6 It's common to use DynamoDB to store session state vs ElastiCache: ElastiCache is in-memory, but DynamoDB is serverless Both are key/value stores vs EFS EFS must be attached to EC2 instances as a network drive vs EBS & Instance Store EBS & Instance Store can only be used for local caching, not shared caching vs S3: S3 is higher latency, and not meant for small objects","title":"DynamoDB as Session State Cache"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/17-dynamodb-session-state/#dynamodb-as-session-state-cache","text":"It's common to use DynamoDB to store session state vs ElastiCache: ElastiCache is in-memory, but DynamoDB is serverless Both are key/value stores vs EFS EFS must be attached to EC2 instances as a network drive vs EBS & Instance Store EBS & Instance Store can only be used for local caching, not shared caching vs S3: S3 is higher latency, and not meant for small objects","title":"DynamoDB as Session State Cache"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/18-dynamodb-partitioning-strategies/","text":"DynamoDB Write Sharding \u00b6 Imagine we have a voting application with two candidates, candidate A and candidate B If partion key is Candidate_ID, this results into two partitions, which will generate issues (e.g. Hot Partition) A strategy that allows for better distribution of items evenly across partitions Add a suffix to partition key value Two methods: Sharding using Random Suffix Sharding using Calculated suffix","title":"DynamoDB Write Sharding"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/18-dynamodb-partitioning-strategies/#dynamodb-write-sharding","text":"Imagine we have a voting application with two candidates, candidate A and candidate B If partion key is Candidate_ID, this results into two partitions, which will generate issues (e.g. Hot Partition) A strategy that allows for better distribution of items evenly across partitions Add a suffix to partition key value Two methods: Sharding using Random Suffix Sharding using Calculated suffix","title":"DynamoDB Write Sharding"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/19-dynamodb-conditional-writes-concurrent-writes-atomic-writes/","text":"DynamoDB Conditional Writes, Concurrent Writes & Atomic Writes \u00b6","title":"DynamoDB Conditional Writes, Concurrent Writes & Atomic Writes"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/19-dynamodb-conditional-writes-concurrent-writes-atomic-writes/#dynamodb-conditional-writes-concurrent-writes-atomic-writes","text":"","title":"DynamoDB Conditional Writes, Concurrent Writes &amp; Atomic Writes"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/20-dynamodb-patterns-with-s3/","text":"DynamoDB - Patterns with S3 \u00b6 Large Objects Pattern \u00b6 Indexing S3 Objects Metadata \u00b6","title":"DynamoDB - Patterns with S3"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/20-dynamodb-patterns-with-s3/#dynamodb-patterns-with-s3","text":"","title":"DynamoDB - Patterns with S3"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/20-dynamodb-patterns-with-s3/#large-objects-pattern","text":"","title":"Large Objects Pattern"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/20-dynamodb-patterns-with-s3/#indexing-s3-objects-metadata","text":"","title":"Indexing S3 Objects Metadata"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/21-dynamodb-operations/","text":"DynamoDB Operations \u00b6 Table Cleanup Option 1: Scan + DeleteItem Very slow, consumes RCU & WCU, expensive Option 2: Drop Table + Recreate table Fast, efficient, cheap Copying a DynamoDB table Option 1: Using AWS Data Pipeline Option 2: Backup and restore into a new table takes some time Option 3: Scan + PutItem or BatchWriteItem Write your own code","title":"DynamoDB Operations"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/21-dynamodb-operations/#dynamodb-operations","text":"Table Cleanup Option 1: Scan + DeleteItem Very slow, consumes RCU & WCU, expensive Option 2: Drop Table + Recreate table Fast, efficient, cheap Copying a DynamoDB table Option 1: Using AWS Data Pipeline Option 2: Backup and restore into a new table takes some time Option 3: Scan + PutItem or BatchWriteItem Write your own code","title":"DynamoDB Operations"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/22-dynamodb-security-and-other/","text":"DynamoDB - Security & Other Features \u00b6 Security VPC Endpoints available to access DynamoDB without using the Internet Access fully controlled by IAM Encryption at rest using AWS KMS and in-transit using SSL/TLS Backup and Restore feature available Point-in-time Recovery (PITR) like RDS No performance impact Global Tables Multi-region, multi-active, fully replicated, high performance DynamoDB Local Develop and test apps locally without acessing the DynamoDB web service (without internet) AWS Database Migration Service (AWS DMS) can be used to migrate to DynamoDB (from MongoDB, Oracle, MySQL, S3, ...) Users interact with DynamoDB Directly \u00b6 Fine-Grained Access Control \u00b6 Using Web Identity Federation or Cognito Identity Pools, each user gets AWS credentials You can assign an IAM Role to these users with a Condition to limit their API access to DynamoDB LeadingKeys - limit row-level access for users on primary key Attributes - limit specific attributes the user can see { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"dynamodb:GetItem\", \"dynamodb:BatchGetItem\", \"dynamodb:Query\", \"dynamodb:PutItem\", \"dynamodb:UpdateItem\", \"dynamodb:DeleteItem\", \"dynamodb:BatchWriteitem\" ], \"Resource\": \"arn:aws:dynamodb:us-west-2:123...:table/MyTable\", \"Condition\": { \"ForAllValues:StringEquals\": { \"dynamodb:LeadingKeys\": [\"${cognito-identity.amazonaws.com:sub\"] } } } ] } more at https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/specifying-conditions.html","title":"DynamoDB - Security & Other Features"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/22-dynamodb-security-and-other/#dynamodb-security-other-features","text":"Security VPC Endpoints available to access DynamoDB without using the Internet Access fully controlled by IAM Encryption at rest using AWS KMS and in-transit using SSL/TLS Backup and Restore feature available Point-in-time Recovery (PITR) like RDS No performance impact Global Tables Multi-region, multi-active, fully replicated, high performance DynamoDB Local Develop and test apps locally without acessing the DynamoDB web service (without internet) AWS Database Migration Service (AWS DMS) can be used to migrate to DynamoDB (from MongoDB, Oracle, MySQL, S3, ...)","title":"DynamoDB - Security &amp; Other Features"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/22-dynamodb-security-and-other/#users-interact-with-dynamodb-directly","text":"","title":"Users interact with DynamoDB Directly"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/22-dynamodb-security-and-other/#fine-grained-access-control","text":"Using Web Identity Federation or Cognito Identity Pools, each user gets AWS credentials You can assign an IAM Role to these users with a Condition to limit their API access to DynamoDB LeadingKeys - limit row-level access for users on primary key Attributes - limit specific attributes the user can see { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"dynamodb:GetItem\", \"dynamodb:BatchGetItem\", \"dynamodb:Query\", \"dynamodb:PutItem\", \"dynamodb:UpdateItem\", \"dynamodb:DeleteItem\", \"dynamodb:BatchWriteitem\" ], \"Resource\": \"arn:aws:dynamodb:us-west-2:123...:table/MyTable\", \"Condition\": { \"ForAllValues:StringEquals\": { \"dynamodb:LeadingKeys\": [\"${cognito-identity.amazonaws.com:sub\"] } } } ] } more at https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/specifying-conditions.html","title":"Fine-Grained Access Control"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/DynamoDB%20Streams/","text":"DynamoDB Streams \u00b6 Ordered stream of item-level notifications (create/update/delete) in a table Stream records can be: Sent to Kinesis Data Streams Read by AWS Lambda Read by Kinesis Client Library Applications Data Retention for up to 24 hours Use cases: react to changes in real-time (welcome email to users) analytics insert into derivative tables insert into elasticsearch implement cross-region replication Ability to choose the information that will be written to the stream: KEYS_ONLY - only the key attributes of the modified item NEW_IMAGE - the entire item, as it appears after it was modified OLD_IMAGE - the entire item, as it appeared before it was modified NEW_AND_OLD_IMAGES - both the new and the old images of the item DynamoDB Streams are made of shards, just like Kinesis Data Streams You don't provision shards, this is automated by AWS Records are not retroactively populated in a stream after enabling it. DynamoDB Streams & AWS Lambda \u00b6 You need to define an Event Source Mapping to read from a DynamoDB Streams You need to ensure the Lambda function has the appropriate permissions You Lambda function is invoked syncronously","title":"DynamoDB Streams"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/DynamoDB%20Streams/#dynamodb-streams","text":"Ordered stream of item-level notifications (create/update/delete) in a table Stream records can be: Sent to Kinesis Data Streams Read by AWS Lambda Read by Kinesis Client Library Applications Data Retention for up to 24 hours Use cases: react to changes in real-time (welcome email to users) analytics insert into derivative tables insert into elasticsearch implement cross-region replication Ability to choose the information that will be written to the stream: KEYS_ONLY - only the key attributes of the modified item NEW_IMAGE - the entire item, as it appears after it was modified OLD_IMAGE - the entire item, as it appeared before it was modified NEW_AND_OLD_IMAGES - both the new and the old images of the item DynamoDB Streams are made of shards, just like Kinesis Data Streams You don't provision shards, this is automated by AWS Records are not retroactively populated in a stream after enabling it.","title":"DynamoDB Streams"},{"location":"AWS/developer-associate/21-aws-serverless-dynamodb/DynamoDB%20Streams/#dynamodb-streams-aws-lambda","text":"You need to define an Event Source Mapping to read from a DynamoDB Streams You need to ensure the Lambda function has the appropriate permissions You Lambda function is invoked syncronously","title":"DynamoDB Streams &amp; AWS Lambda"},{"location":"Architecture/Service-Oriented%20Architecture%20%28SOA%29/Service%20Abstraction/","text":"The services act as black boxes, that is their inner logic is hidden from the consumers.","title":"Service Abstraction"},{"location":"Architecture/Service-Oriented%20Architecture%20%28SOA%29/Service%20Autonomy/","text":"Service s are independent and control the functionality they encapsulate, from a Design-time and a run-time perspective.","title":"Service Autonomy"},{"location":"Architecture/Service-Oriented%20Architecture%20%28SOA%29/Service%20Broker/","text":"Service broker, service registry or service repository Service Broker's main functionality is to make the information regarding the [[web service]] available to any potential requester. Whoever implements the broker decides the scope of the broker. - Public brokers are available anywhere and everywhere - Private brokers are only available to a limited amount of public","title":"Service Broker"},{"location":"Architecture/Service-Oriented%20Architecture%20%28SOA%29/Service%20Composability/","text":"Service s can be used to compose other services.","title":"Service Composability"},{"location":"Architecture/Service-Oriented%20Architecture%20%28SOA%29/Service%20Consumer/","text":"Service Consumer locates entries in the Service Broker using various find operations and then binds to the Service Provider in order to invoke one of its web services. Whichever service the service-consumers needs, they have to take it into the brokers, bind it with respective service and then use it. They can access multiple services if the service provides multiple services. The service consumer\u2013provider relationship is governed by a Service Contract which has a business part, a functional part and a technical part. [[Service composition patterns]] have two broad, high-level architectural styles: - [[choreography]] - [[orchestration]] Lower level enterprise integration patterns that are not bound to a particular architectural style continue to be relevant and eligible in Service-Oriented Architecture (SOA) design.","title":"Service Consumer"},{"location":"Architecture/Service-Oriented%20Architecture%20%28SOA%29/Service%20Contract/","text":"Services adhere to a standard communications agreement, as defined collectively by one or more service-description documents within a given set of services.","title":"Service Contract"},{"location":"Architecture/Service-Oriented%20Architecture%20%28SOA%29/Service%20Discovery/","text":"Service s are supplemented with [[communicative meta data]] by which they can be effectively discovered and interpreted.","title":"Service Discovery"},{"location":"Architecture/Service-Oriented%20Architecture%20%28SOA%29/Service%20Encapsulation/","text":"Many Service s which were not initially planned under Service-Oriented Architecture (SOA) , may get encapsulated or become a part of Service-Oriented Architecture (SOA) .","title":"Service Encapsulation"},{"location":"Architecture/Service-Oriented%20Architecture%20%28SOA%29/Service%20Granularity/","text":"A principle to ensure Service s have an adequate size and scope. The functionality provided by the service to the user must be relevant.","title":"Service Granularity"},{"location":"Architecture/Service-Oriented%20Architecture%20%28SOA%29/Service%20Location%20Transparency/","text":"An aspect of [[loose coupling]] Service s can be called from anywhere within the network that it is located no matter where it is present.","title":"Service Location Transparency"},{"location":"Architecture/Service-Oriented%20Architecture%20%28SOA%29/Service%20Longetivity/","text":"Service s should be designed to be long lived. Where possible services should avoid forcing consumers to change if they do not require new features, if you call a service today you should be able to call the same service tomorrow.","title":"Service Longetivity"},{"location":"Architecture/Service-Oriented%20Architecture%20%28SOA%29/Service%20Normalization/","text":"Service s are [[decomposed]] or consolidated ([[normalized]]) to minimize [[redundancy]]. In some, this may not be done. These are the cases where [[performance optimization]], access, and [[aggregation]] are required.","title":"Service Normalization"},{"location":"Architecture/Service-Oriented%20Architecture%20%28SOA%29/Service%20Provider/","text":"Service Provider creates a [[web service]] and provides its information to the [[service registry]]. Each provider debates upon a lot of hows and whys like which service to expose, which to give more importance: security or easy availability, what price to offer the service for and many more. The provider also has to decide what category the service should be listed in for a given [[broker service]] and what sort of trading partner agreements are required to use the service.","title":"Service Provider"},{"location":"Architecture/Service-Oriented%20Architecture%20%28SOA%29/Service%20Reference%20Autonomy/","text":"An aspect of [[loose coupling]] The relationship between Service s is minimized to the level that they are only aware of their existence.","title":"Service Reference Autonomy"},{"location":"Architecture/Service-Oriented%20Architecture%20%28SOA%29/Service%20Reusability/","text":"Logic is divided into various Service s, to promote [[reuse of code]].","title":"Service Reusability"},{"location":"Architecture/Service-Oriented%20Architecture%20%28SOA%29/Service%20Statelessness/","text":"Service s are [[stateless]], that is either return the requested value or give an exception hence minimizing resource use.","title":"Service Statelessness"},{"location":"Architecture/Service-Oriented%20Architecture%20%28SOA%29/Service-Oriented%20Architecture%20%28SOA%29/","text":"Architectural style that focuses on discrete Service s instead of [[monolithic design]]. Different services can be used in conjunction as a [[Service Mesh]] to provide the functionality of a large application ([[modular programming]]). SOA integrates distributes, separately maintained and deployed software components. It is enabled by technologies and standards that facilitate components' communication and cooperation over a network, especially over an IP network. SOA is related to the [[API]] idea, an interface or communication protocol between different parts of a computer program intended to simplify the implementation and maintenance of software. Principles \u00b6 There are no industry standards relating to the exact composition of a service-oriented architecture, although many industry sources have published their own principles. Patterns \u00b6 Each SOA building block can play any of the three roles: Implementation Approaches \u00b6 ... Organizational Benefits \u00b6 ... Criticisms \u00b6 ... Extensions and Variants \u00b6 ... Conspect this Add flashcards / review implement an example project with this","title":"Service Oriented Architecture (SOA)"},{"location":"Architecture/Service-Oriented%20Architecture%20%28SOA%29/Service-Oriented%20Architecture%20%28SOA%29/#principles","text":"There are no industry standards relating to the exact composition of a service-oriented architecture, although many industry sources have published their own principles.","title":"Principles"},{"location":"Architecture/Service-Oriented%20Architecture%20%28SOA%29/Service-Oriented%20Architecture%20%28SOA%29/#patterns","text":"Each SOA building block can play any of the three roles:","title":"Patterns"},{"location":"Architecture/Service-Oriented%20Architecture%20%28SOA%29/Service-Oriented%20Architecture%20%28SOA%29/#implementation-approaches","text":"...","title":"Implementation Approaches"},{"location":"Architecture/Service-Oriented%20Architecture%20%28SOA%29/Service-Oriented%20Architecture%20%28SOA%29/#organizational-benefits","text":"...","title":"Organizational Benefits"},{"location":"Architecture/Service-Oriented%20Architecture%20%28SOA%29/Service-Oriented%20Architecture%20%28SOA%29/#criticisms","text":"...","title":"Criticisms"},{"location":"Architecture/Service-Oriented%20Architecture%20%28SOA%29/Service-Oriented%20Architecture%20%28SOA%29/#extensions-and-variants","text":"... Conspect this Add flashcards / review implement an example project with this","title":"Extensions and Variants"},{"location":"Architecture/Service-Oriented%20Architecture%20%28SOA%29/Service/","text":"A service is a discrete unit of functionality that can be accessed remotely and acted upon and updated independently. A service has 4 properties according to Service-Oriented Architecture (SOA) : 1. It logically represents a repeatable business activity with a specified outcome. 2. It is self-contained. 3. It is a [[block box]] for consumers, meaning the consumer does not have to be aware of the service's inner workings. 4. It may be composed of other services.","title":"Service"},{"location":"Business/idea-generation/","text":"Idea Generation \u00b6 Learning about business idea generation Sources: - How to come up with killer business ideas: complete workshop","title":"Idea Generation"},{"location":"Business/idea-generation/#idea-generation","text":"Learning about business idea generation Sources: - How to come up with killer business ideas: complete workshop","title":"Idea Generation"},{"location":"Business/idea-generation/01-jumpstart-strategies/01-strategy-problem-based-business-ideas/","text":"Problem-based business ideas \u00b6 Virtually every business today exists because it solves a problem. What are the first 5 companies that come in mind? What do these companies solve? Uber gets you where you want to go faster and cheaper Airbnb saves you from being price-gauged by hotels and helps you being from a generic steril experience of sleeping in a motel Any video game company solves the problem of being bored Dropbox was founded because the founder was tired of bringing files to work and back Mint was founded to keep track of personal finances Take out a sheet of paper and write a list of problems you have day-to-day. Where there is a problem, there is an opportunity. Then list next them 2 things - frequency and depth. Frequency - how often do I have this pain? Rate 1-10 Depth - How big of a deal is this pain? Rate 1-10 One of these must be very high. For example, Uber doesn't solve a huge pain (depth) but the frequency of how often people need a car is high so that is why they're such a big company. Then ask yourself: - can I solve this problem? - Is it worth solving?","title":"Problem-based business ideas"},{"location":"Business/idea-generation/01-jumpstart-strategies/01-strategy-problem-based-business-ideas/#problem-based-business-ideas","text":"Virtually every business today exists because it solves a problem. What are the first 5 companies that come in mind? What do these companies solve? Uber gets you where you want to go faster and cheaper Airbnb saves you from being price-gauged by hotels and helps you being from a generic steril experience of sleeping in a motel Any video game company solves the problem of being bored Dropbox was founded because the founder was tired of bringing files to work and back Mint was founded to keep track of personal finances Take out a sheet of paper and write a list of problems you have day-to-day. Where there is a problem, there is an opportunity. Then list next them 2 things - frequency and depth. Frequency - how often do I have this pain? Rate 1-10 Depth - How big of a deal is this pain? Rate 1-10 One of these must be very high. For example, Uber doesn't solve a huge pain (depth) but the frequency of how often people need a car is high so that is why they're such a big company. Then ask yourself: - can I solve this problem? - Is it worth solving?","title":"Problem-based business ideas"},{"location":"Business/idea-generation/01-jumpstart-strategies/02-strategy-wouldnt-it-be-great-if/","text":"Wouldn't it be great if \u00b6 How it works \u00b6 Just like a thought experiment. Start with a frase Wouldn't it be great if ___? Then it is up to you to answer the question yourself. Examples: - Wouldn't it be great if I didn't have to do because it would be much easier. - Wouldn't it be great if the price for would be significantly reduced. - I do __ all the time and I wish it would take half the time. Make a list of all these items. Then ask question: Can I make this work?","title":"Wouldn't it be great if"},{"location":"Business/idea-generation/01-jumpstart-strategies/02-strategy-wouldnt-it-be-great-if/#wouldnt-it-be-great-if","text":"","title":"Wouldn't it be great if"},{"location":"Business/idea-generation/01-jumpstart-strategies/02-strategy-wouldnt-it-be-great-if/#how-it-works","text":"Just like a thought experiment. Start with a frase Wouldn't it be great if ___? Then it is up to you to answer the question yourself. Examples: - Wouldn't it be great if I didn't have to do because it would be much easier. - Wouldn't it be great if the price for would be significantly reduced. - I do __ all the time and I wish it would take half the time. Make a list of all these items. Then ask question: Can I make this work?","title":"How it works"},{"location":"Business/idea-generation/01-jumpstart-strategies/03-strategy-imagine-the-future/","text":"Imagine the future \u00b6 What great business in the future is not being built today? What big problems are being tackled by big companies in the future? Imagine what future is going to look like? (In 20 or 30 years or so) Do it in a couple of dimensions. How do you think your country will look like? How will be the politics changed? What the future will look like in terms of technology? Which of companies for these problems are not being built today?","title":"Imagine the future"},{"location":"Business/idea-generation/01-jumpstart-strategies/03-strategy-imagine-the-future/#imagine-the-future","text":"What great business in the future is not being built today? What big problems are being tackled by big companies in the future? Imagine what future is going to look like? (In 20 or 30 years or so) Do it in a couple of dimensions. How do you think your country will look like? How will be the politics changed? What the future will look like in terms of technology? Which of companies for these problems are not being built today?","title":"Imagine the future"},{"location":"Business/idea-generation/01-jumpstart-strategies/04-idea-list-builder/","text":"Idea List Builder \u00b6 Keep a scratchpad where you write down all your ideas, can be unpolished. Then polish it using following things: Describe your idea (1-2 sentences) Who is your customer? (who are you selling to?) What problem are you solving? What business model do you think you would use? Do you have experience in this field? Are you passionate in this field? Do you have any hobbies that related to this field? How enthusiastic about this idea are you (rank 1-10) When finished going through all your ideas, write a summary list of all of all of them and then rank them.","title":"Idea List Builder"},{"location":"Business/idea-generation/01-jumpstart-strategies/04-idea-list-builder/#idea-list-builder","text":"Keep a scratchpad where you write down all your ideas, can be unpolished. Then polish it using following things: Describe your idea (1-2 sentences) Who is your customer? (who are you selling to?) What problem are you solving? What business model do you think you would use? Do you have experience in this field? Are you passionate in this field? Do you have any hobbies that related to this field? How enthusiastic about this idea are you (rank 1-10) When finished going through all your ideas, write a summary list of all of all of them and then rank them.","title":"Idea List Builder"},{"location":"Business/idea-generation/02-laying-the-groundwork/01-the-idea-equation/","text":"The idea equation \u00b6 Every business idea out there consists of different components. when an idea doesn't have all of these components, it is not a complete business idea. Example of a complete idea: \u00b6 Vacation rental owners have trouble buying rental supplies like linens & towels. This problem would be solved with a subscription service business that automatically replenishes these supplies. Example of incomplete idea: \u00b6 Vacation rental owners rely too much on one or two platforms for bookings. Components \u00b6 A complete business idea consists of 3 components: The subject who in your idea is benefitting from the product / service? Group of people I'm targeting with this idea. Don't just use all people, pick a specific group. The problem / benefit Identify what the subject is having a problem with. How the group will benefit Problem example: Employees in midsize companies want to communicate with rich media, like images. Currently they cannot, and they have to use x,y,z workaround in order to do it. Benefit example: By supplying all employees in mid-size companies with bluetooth headsets, they could communicate at 3x their normal rate. Business model Have more business models for backup Complete business idea = Subject + Problem/Benefit + Business model","title":"The idea equation"},{"location":"Business/idea-generation/02-laying-the-groundwork/01-the-idea-equation/#the-idea-equation","text":"Every business idea out there consists of different components. when an idea doesn't have all of these components, it is not a complete business idea.","title":"The idea equation"},{"location":"Business/idea-generation/02-laying-the-groundwork/01-the-idea-equation/#example-of-a-complete-idea","text":"Vacation rental owners have trouble buying rental supplies like linens & towels. This problem would be solved with a subscription service business that automatically replenishes these supplies.","title":"Example of a complete idea:"},{"location":"Business/idea-generation/02-laying-the-groundwork/01-the-idea-equation/#example-of-incomplete-idea","text":"Vacation rental owners rely too much on one or two platforms for bookings.","title":"Example of incomplete idea:"},{"location":"Business/idea-generation/02-laying-the-groundwork/01-the-idea-equation/#components","text":"A complete business idea consists of 3 components: The subject who in your idea is benefitting from the product / service? Group of people I'm targeting with this idea. Don't just use all people, pick a specific group. The problem / benefit Identify what the subject is having a problem with. How the group will benefit Problem example: Employees in midsize companies want to communicate with rich media, like images. Currently they cannot, and they have to use x,y,z workaround in order to do it. Benefit example: By supplying all employees in mid-size companies with bluetooth headsets, they could communicate at 3x their normal rate. Business model Have more business models for backup Complete business idea = Subject + Problem/Benefit + Business model","title":"Components"},{"location":"Business/idea-generation/02-laying-the-groundwork/02-strategy-reverse-imagination/","text":"Reverse imagination \u00b6 When looking at the past there are multiple things that when looked at now - they seem stupid. Imagine the future and think what is stupid today. Example: PayPal was created because they thought it was stupid that we couldn't transfer money online. Look around your room and think what item is there that will be unnecessary in future. Try to predict trends and think of businesses that will facilitate it and benefit from it. https://www.therichest.com/world-entertainment/15-things-we-did-in-the-90s-that-are-no-longer-acceptable/","title":"Reverse imagination"},{"location":"Business/idea-generation/02-laying-the-groundwork/02-strategy-reverse-imagination/#reverse-imagination","text":"When looking at the past there are multiple things that when looked at now - they seem stupid. Imagine the future and think what is stupid today. Example: PayPal was created because they thought it was stupid that we couldn't transfer money online. Look around your room and think what item is there that will be unnecessary in future. Try to predict trends and think of businesses that will facilitate it and benefit from it. https://www.therichest.com/world-entertainment/15-things-we-did-in-the-90s-that-are-no-longer-acceptable/","title":"Reverse imagination"},{"location":"Business/idea-generation/02-laying-the-groundwork/03-areas-to-isolate-and-target/","text":"Areas to isolate and target \u00b6 Any business you can think of will go into one of these 4 categories: Product/Service Procurement Delivery Promotion You can break down each of these categories, analyze and innovate. Product / Service Each product has several characteristics about them and that's what it makes them unique. Procurement What went into the product? Where the materials for the product came from? How did the company obtained them? Delivery Delivery can be made in many different ways Promotion How did the customer found out about it?","title":"Areas to isolate and target"},{"location":"Business/idea-generation/02-laying-the-groundwork/03-areas-to-isolate-and-target/#areas-to-isolate-and-target","text":"Any business you can think of will go into one of these 4 categories: Product/Service Procurement Delivery Promotion You can break down each of these categories, analyze and innovate. Product / Service Each product has several characteristics about them and that's what it makes them unique. Procurement What went into the product? Where the materials for the product came from? How did the company obtained them? Delivery Delivery can be made in many different ways Promotion How did the customer found out about it?","title":"Areas to isolate and target"},{"location":"Business/idea-generation/02-laying-the-groundwork/04-strategy-cater-to-power-users/","text":"Cater to power users \u00b6 Power users - people that use a service or platform way more than average people. Platform - any online service or community or tool that brings together large groups of people. Examples: ebay and amazon. How it works \u00b6 Identify a group of power users in a platform and find out what problems are they having. Platforms can't grow if they only focus on the top power users. They need very specific things which will have no value on the rest of the user base. It's better to have a deep and narrow well than a wide and shallow well. What to look for \u00b6 Find a platform, play around in it and find a user that uses the platform a lot Ask them what problems are they facing, what frustrates them on a daily basis Try to pick a new platform, established platforms will have less oppurtunities","title":"Cater to power users"},{"location":"Business/idea-generation/02-laying-the-groundwork/04-strategy-cater-to-power-users/#cater-to-power-users","text":"Power users - people that use a service or platform way more than average people. Platform - any online service or community or tool that brings together large groups of people. Examples: ebay and amazon.","title":"Cater to power users"},{"location":"Business/idea-generation/02-laying-the-groundwork/04-strategy-cater-to-power-users/#how-it-works","text":"Identify a group of power users in a platform and find out what problems are they having. Platforms can't grow if they only focus on the top power users. They need very specific things which will have no value on the rest of the user base. It's better to have a deep and narrow well than a wide and shallow well.","title":"How it works"},{"location":"Business/idea-generation/02-laying-the-groundwork/04-strategy-cater-to-power-users/#what-to-look-for","text":"Find a platform, play around in it and find a user that uses the platform a lot Ask them what problems are they facing, what frustrates them on a daily basis Try to pick a new platform, established platforms will have less oppurtunities","title":"What to look for"},{"location":"Business/idea-generation/02-laying-the-groundwork/05-five-types-of-innovation/","text":"5 types of innovation \u00b6 All business ideas are trying to improve things or add benefits. How can you improve? Price Process innovation Same product, lower price Same product, higher price Usually works in Fashion and Education Higher price increase the perceived value Convenience Lowers friction Removes steps for customer to purchase Finding a cab - finding a cab, waiting, paying in cash. On uber you get an app, specify location and press a button. Speed How quickly the value can be delivered to customer Amazon Prime Quality Usually applies to the product and procurement segment Self-expression Do people feel better about themselves because they bought your product? GoPro - specs are lower than competition, prices are higher, no innovation. It's because of self-expressive benefit. Apple often sell more a lifestyle than the product. You can improve multiple dimensions.","title":"5 types of innovation"},{"location":"Business/idea-generation/02-laying-the-groundwork/05-five-types-of-innovation/#5-types-of-innovation","text":"All business ideas are trying to improve things or add benefits. How can you improve? Price Process innovation Same product, lower price Same product, higher price Usually works in Fashion and Education Higher price increase the perceived value Convenience Lowers friction Removes steps for customer to purchase Finding a cab - finding a cab, waiting, paying in cash. On uber you get an app, specify location and press a button. Speed How quickly the value can be delivered to customer Amazon Prime Quality Usually applies to the product and procurement segment Self-expression Do people feel better about themselves because they bought your product? GoPro - specs are lower than competition, prices are higher, no innovation. It's because of self-expressive benefit. Apple often sell more a lifestyle than the product. You can improve multiple dimensions.","title":"5 types of innovation"},{"location":"Business/idea-generation/02-laying-the-groundwork/06-strategy-disintermediation/","text":"Disintermediation \u00b6 Everything you purchase comes though different hands and businesses, a value chain. The iPhone gets manufactured in China, then it gets transferred to a port, then it goes to Apple, then gets distributed to retail locations where it gets sold by different people. You can look at the value chain and see if you remove steps from the value chain and with one of two things happening: Lower price, same quality Same price, higher quality Examples: Eye glasses You need to go to optometrist and get a prescription You try various frames at the store You wait for the lenses Pay huge chunk of money A company that solves this - Warby Parker It cuts out the whole retail side of the value chain Website where you enter your prescription Select 5 frames and you get them shipped to you, try them out and select which ones you like Furniture Shipping costs more if you are shipping assembled furniture Ikea removes this assembly process Dell were the first to skip the retail process","title":"Disintermediation"},{"location":"Business/idea-generation/02-laying-the-groundwork/06-strategy-disintermediation/#disintermediation","text":"Everything you purchase comes though different hands and businesses, a value chain. The iPhone gets manufactured in China, then it gets transferred to a port, then it goes to Apple, then gets distributed to retail locations where it gets sold by different people. You can look at the value chain and see if you remove steps from the value chain and with one of two things happening: Lower price, same quality Same price, higher quality Examples: Eye glasses You need to go to optometrist and get a prescription You try various frames at the store You wait for the lenses Pay huge chunk of money A company that solves this - Warby Parker It cuts out the whole retail side of the value chain Website where you enter your prescription Select 5 frames and you get them shipped to you, try them out and select which ones you like Furniture Shipping costs more if you are shipping assembled furniture Ikea removes this assembly process Dell were the first to skip the retail process","title":"Disintermediation"},{"location":"Business/idea-generation/03-making-it-fit/01-the-fit-quadrant-hobbies-and-passions/","text":"The Fit Quadrant: Hobbies and Passions \u00b6 Hobbies and passions are probably the biggest source of business ideas. Hobbies \u00b6 Hobby is something you do for fun at your own will. You have a natural inclination towards something. Starting a business is hard and it is even harder if you don't really enjoy what you're doing. What do you naturally find yourself doing? Hobbies are what you enjoy doing Analyze what you're doing To see if your hobby qualifies for turning it into a business, ask yourself: - Could you do that all day? - Could you manage doing supportive tasks around it but not actually doing it? E.g. Building an app for running - figuring out how it looks like etc. Passions \u00b6 Things that you spend a lot of time thinking about and really care about. You generally want to spend a large amount of time working on or working towards. Write down 5 things you care about. Skip the generic ones like friends and family.","title":"The Fit Quadrant: Hobbies and Passions"},{"location":"Business/idea-generation/03-making-it-fit/01-the-fit-quadrant-hobbies-and-passions/#the-fit-quadrant-hobbies-and-passions","text":"Hobbies and passions are probably the biggest source of business ideas.","title":"The Fit Quadrant: Hobbies and Passions"},{"location":"Business/idea-generation/03-making-it-fit/01-the-fit-quadrant-hobbies-and-passions/#hobbies","text":"Hobby is something you do for fun at your own will. You have a natural inclination towards something. Starting a business is hard and it is even harder if you don't really enjoy what you're doing. What do you naturally find yourself doing? Hobbies are what you enjoy doing Analyze what you're doing To see if your hobby qualifies for turning it into a business, ask yourself: - Could you do that all day? - Could you manage doing supportive tasks around it but not actually doing it? E.g. Building an app for running - figuring out how it looks like etc.","title":"Hobbies"},{"location":"Business/idea-generation/03-making-it-fit/01-the-fit-quadrant-hobbies-and-passions/#passions","text":"Things that you spend a lot of time thinking about and really care about. You generally want to spend a large amount of time working on or working towards. Write down 5 things you care about. Skip the generic ones like friends and family.","title":"Passions"},{"location":"Business/idea-generation/03-making-it-fit/02-the-fit-quadrant-skills-and-experiences/","text":"The Fit Quadrant: Skills and Experience \u00b6 Skills \u00b6 \"Don't follow your passions, do what you're good at\" The problem with passion is that it doesn't tell that you're actually good at it. What could you teach someone? What do you consider an expert at? I am in the top 10% at the ___ Experience \u00b6 Go to your LinkedIn and see what you've worked in. What do you have inside knowledge on? Industries you've been in","title":"The Fit Quadrant: Skills and Experience"},{"location":"Business/idea-generation/03-making-it-fit/02-the-fit-quadrant-skills-and-experiences/#the-fit-quadrant-skills-and-experience","text":"","title":"The Fit Quadrant: Skills and Experience"},{"location":"Business/idea-generation/03-making-it-fit/02-the-fit-quadrant-skills-and-experiences/#skills","text":"\"Don't follow your passions, do what you're good at\" The problem with passion is that it doesn't tell that you're actually good at it. What could you teach someone? What do you consider an expert at? I am in the top 10% at the ___","title":"Skills"},{"location":"Business/idea-generation/03-making-it-fit/02-the-fit-quadrant-skills-and-experiences/#experience","text":"Go to your LinkedIn and see what you've worked in. What do you have inside knowledge on? Industries you've been in","title":"Experience"},{"location":"Business/idea-generation/03-making-it-fit/03-activity-hobbies-passions-skills-experiences/","text":"Activity: Hobbies, Passions, Skills & Experience \u00b6 \"A good idea is not a great idea until it fits you\" List your top 10 thinks you are passionate about In your head write down 5 things you care about What are you doing when you are the happiest? List your top 10 hobbies: What do you do on the weekends? If you had nothing to do one day and were not allowed to work, what would you do? List top 10 skills: What could you teach someone? What do you consider yourself an expert at? I'm the top 10% of ___ List top 10 industries you have experience in What do you have inside knowledge on? What scoop do you have? What industry landscape can you navigate easily and have no learning curve? Of those Top 10 list Top 3 of each one","title":"Activity: Hobbies, Passions, Skills & Experience"},{"location":"Business/idea-generation/03-making-it-fit/03-activity-hobbies-passions-skills-experiences/#activity-hobbies-passions-skills-experience","text":"\"A good idea is not a great idea until it fits you\" List your top 10 thinks you are passionate about In your head write down 5 things you care about What are you doing when you are the happiest? List your top 10 hobbies: What do you do on the weekends? If you had nothing to do one day and were not allowed to work, what would you do? List top 10 skills: What could you teach someone? What do you consider yourself an expert at? I'm the top 10% of ___ List top 10 industries you have experience in What do you have inside knowledge on? What scoop do you have? What industry landscape can you navigate easily and have no learning curve? Of those Top 10 list Top 3 of each one","title":"Activity: Hobbies, Passions, Skills &amp; Experience"},{"location":"Business/idea-generation/03-making-it-fit/04-strategy-reposition-good-fast-cheap/","text":"Reposition Good / Fast / Cheap \u00b6 This one is one of the easiest way to come up with business ideas. Every business out there can be rated on 3 different dimensions: - Good - Fast - Cheap You can be only 2/3 Think of an industry you are interested in What's the last website that you used? Are they on to something? Think of a way to do what they're doing but a little bit better. Example: You're really interested in app development and contracting You find gigster app that matches contractors with companies Try to figure out what dimensions are they working on. Fast, Good Which of the other combinations could you capitalize on a different part of the market or the market that gigster is already serving?","title":"Reposition Good / Fast / Cheap"},{"location":"Business/idea-generation/03-making-it-fit/04-strategy-reposition-good-fast-cheap/#reposition-good-fast-cheap","text":"This one is one of the easiest way to come up with business ideas. Every business out there can be rated on 3 different dimensions: - Good - Fast - Cheap You can be only 2/3 Think of an industry you are interested in What's the last website that you used? Are they on to something? Think of a way to do what they're doing but a little bit better. Example: You're really interested in app development and contracting You find gigster app that matches contractors with companies Try to figure out what dimensions are they working on. Fast, Good Which of the other combinations could you capitalize on a different part of the market or the market that gigster is already serving?","title":"Reposition Good / Fast / Cheap"},{"location":"Business/idea-generation/03-making-it-fit/05-strategy-adding-technology-to-an-offline-industry/","text":"Strategy: Adding technology to an offline industry \u00b6 Look for an industry that is offline and move it online For laywers you might create an application where you can sign papers online For doctors you might create an app to save their documentations online For sales - it takes less time, less redundancy, more relient, doesn't take up extra physical storage Look where paper is used Look for fax machines Start looking for common human behaviors, can this process benefit from being online? Couchsurfing Airbnb There's always oppurtunity to specialize and tailor solution to specific area","title":"Strategy: Adding technology to an offline industry"},{"location":"Business/idea-generation/03-making-it-fit/05-strategy-adding-technology-to-an-offline-industry/#strategy-adding-technology-to-an-offline-industry","text":"Look for an industry that is offline and move it online For laywers you might create an application where you can sign papers online For doctors you might create an app to save their documentations online For sales - it takes less time, less redundancy, more relient, doesn't take up extra physical storage Look where paper is used Look for fax machines Start looking for common human behaviors, can this process benefit from being online? Couchsurfing Airbnb There's always oppurtunity to specialize and tailor solution to specific area","title":"Strategy: Adding technology to an offline industry"},{"location":"Business/idea-generation/03-making-it-fit/06-activity-list-out-yout-goals/","text":"Activity: List out your goals \u00b6 Answer these questions: - How big you want your next business to be? - As big as possible - 100+ employees - ~50 Employees - Small crew of less than 10 people - Myself + 2-3 people - Just me - How long are you willing to work on this next business? - Forever - As long as it takes - 10 years+ - 5-7 years - 2-4 years - 1-2 years - less than 1 year - How much money will the business need to make to be satisfied? - Nothing - 1k/month - 3k/month - 5k/month - 5-10k/month - I want to make millions - How many hours per week are you willing to work on this business? - As needed - 5 hours - 10-15 hours - 25 hours - ... - How long could you wait for this business to become profitable? - Forever - As long as it takes - 1 month - 2-3 months - 3-6 months - year - 2 years - How much of your resources are you willing to commit? - Everything I have + loans if needed - Everything I have - less than $20,000 - less than $10,000 - less than $1,000 - $100 - Just my time","title":"Activity: List out your goals"},{"location":"Business/idea-generation/03-making-it-fit/06-activity-list-out-yout-goals/#activity-list-out-your-goals","text":"Answer these questions: - How big you want your next business to be? - As big as possible - 100+ employees - ~50 Employees - Small crew of less than 10 people - Myself + 2-3 people - Just me - How long are you willing to work on this next business? - Forever - As long as it takes - 10 years+ - 5-7 years - 2-4 years - 1-2 years - less than 1 year - How much money will the business need to make to be satisfied? - Nothing - 1k/month - 3k/month - 5k/month - 5-10k/month - I want to make millions - How many hours per week are you willing to work on this business? - As needed - 5 hours - 10-15 hours - 25 hours - ... - How long could you wait for this business to become profitable? - Forever - As long as it takes - 1 month - 2-3 months - 3-6 months - year - 2 years - How much of your resources are you willing to commit? - Everything I have + loans if needed - Everything I have - less than $20,000 - less than $10,000 - less than $1,000 - $100 - Just my time","title":"Activity: List out your goals"},{"location":"Business/idea-generation/03-making-it-fit/07-the-three-business-types-pick-yours/","text":"The 3 Business types: pick yours \u00b6 Startup businesses ambitious business ideas very sensitive to competition High time commitment high risk, high profit potential Any business designed to grow rapidly. Lifestyle businesses medium sized ideas moderately sensitive to competition between full-time and part-time medium risk and profit Business designed to uphold a certain lifestyle of yours. Meets only your financial needs Side businesses templates, minimum innovation not sensitive to competition low time commitment low risk, low profit Create something easy to sell, use your skills and hobbies","title":"The 3 Business types: pick yours"},{"location":"Business/idea-generation/03-making-it-fit/07-the-three-business-types-pick-yours/#the-3-business-types-pick-yours","text":"Startup businesses ambitious business ideas very sensitive to competition High time commitment high risk, high profit potential Any business designed to grow rapidly. Lifestyle businesses medium sized ideas moderately sensitive to competition between full-time and part-time medium risk and profit Business designed to uphold a certain lifestyle of yours. Meets only your financial needs Side businesses templates, minimum innovation not sensitive to competition low time commitment low risk, low profit Create something easy to sell, use your skills and hobbies","title":"The 3 Business types: pick yours"},{"location":"Business/idea-generation/03-making-it-fit/08-make-it-eco-friendly/","text":"Make it eco-friendly \u00b6 Things use resources, which has a toll on environment. Look at anything that is not eco-friendly, and make it so. Offset the damage made to charities etc Car wash - make it waste less water","title":"Make it eco-friendly"},{"location":"Business/idea-generation/03-making-it-fit/08-make-it-eco-friendly/#make-it-eco-friendly","text":"Things use resources, which has a toll on environment. Look at anything that is not eco-friendly, and make it so. Offset the damage made to charities etc Car wash - make it waste less water","title":"Make it eco-friendly"},{"location":"Business/idea-generation/03-making-it-fit/09-how-good-does-my-idea-have-to-be/","text":"How good does my idea have to be? \u00b6 Good is a relative term Side business: Help people to make thumbnails for videos for instructors on udemy Need is actually real Not saturated Lifestyle business Be an instructor on udemy Topic with not enough competition Create launch plan Research on how to differentiate Startup business You want to change the world of education by creating an online education marketplace, because you think there is a growing demand for this type of education. Takes a lot of different predictions Understanding in a lot of different domains Need to be original Need to make a big impact on the subject, move the market Idea advice: - The beginning is hard for everyone - Go to your goal sheet, ask yourself - do you need a world-changing idea to be happy? - All ideas are worth considering - Startup business: insert judgement - Lifestyle business: focus on your skills - Side business: Look for something that people will buy","title":"How good does my idea have to be?"},{"location":"Business/idea-generation/03-making-it-fit/09-how-good-does-my-idea-have-to-be/#how-good-does-my-idea-have-to-be","text":"Good is a relative term Side business: Help people to make thumbnails for videos for instructors on udemy Need is actually real Not saturated Lifestyle business Be an instructor on udemy Topic with not enough competition Create launch plan Research on how to differentiate Startup business You want to change the world of education by creating an online education marketplace, because you think there is a growing demand for this type of education. Takes a lot of different predictions Understanding in a lot of different domains Need to be original Need to make a big impact on the subject, move the market Idea advice: - The beginning is hard for everyone - Go to your goal sheet, ask yourself - do you need a world-changing idea to be happy? - All ideas are worth considering - Startup business: insert judgement - Lifestyle business: focus on your skills - Side business: Look for something that people will buy","title":"How good does my idea have to be?"},{"location":"Business/idea-generation/03-making-it-fit/10-strategy-the-fit-generator/","text":"Strategy: The Fit generator \u00b6 Go to https://evankimbrell.com/ideas/ Input all your specifics, choose easy or hard mode and try to figure out how to combine the outputs of the app. After some practice you will be able to string together all these components and might get to actual viable idea.","title":"Strategy: The Fit generator"},{"location":"Business/idea-generation/03-making-it-fit/10-strategy-the-fit-generator/#strategy-the-fit-generator","text":"Go to https://evankimbrell.com/ideas/ Input all your specifics, choose easy or hard mode and try to figure out how to combine the outputs of the app. After some practice you will be able to string together all these components and might get to actual viable idea.","title":"Strategy: The Fit generator"},{"location":"Business/idea-generation/03-making-it-fit/11-the-eureka-myth/","text":"The Eureka Myth \u00b6 The \"eureka\" moment idea states that all brilliant people have their brilliant ideas and they just happen, hence the saying eureka. Like apple on Newtons head. It's complete bullshit. People wait for the perfect idea. Mist brilliant ideas started as something else. All the startup ideas started differently and over time they changed presented more knowledge on the matter. It gets better and better until you can't even recognize the original idea. Examples: - Facebook started as an application for 1 college to compare attractiveness between classmates. - Airbnb started just as system to find a floor to sleep in while in a conference and all the hotels are booked up. To meet overflow. - Groupon started as a platform for people to donate to charities, which has no relation to what it is now. - Pinterest started as an app for going to designers websites and marking things you were interested in, it would notify you when those things go on sale. - Twitter started as a podcast subscription service - Flickr started as a video game called never ending game - Instagram started as a check-in app - Nintendo started off selling vacuum cleaners Follow the lean startup concepts. \"When we're in the shower, when we're thinking about our idea, boy, does it sound brilliant. But the reality is that most of our ideas are actually terrible.\" - Eric Ries Every idea does with key assumptions, but you'll won't know for sure until you actually test them. You will be surprised on how wrong you are. Brilliant people also make dumb decisions. The only difference between you and brilliant people is that they try them, understand what they did wrong sooner, pivot and evolve these ideas and eventually get them right.","title":"The Eureka Myth"},{"location":"Business/idea-generation/03-making-it-fit/11-the-eureka-myth/#the-eureka-myth","text":"The \"eureka\" moment idea states that all brilliant people have their brilliant ideas and they just happen, hence the saying eureka. Like apple on Newtons head. It's complete bullshit. People wait for the perfect idea. Mist brilliant ideas started as something else. All the startup ideas started differently and over time they changed presented more knowledge on the matter. It gets better and better until you can't even recognize the original idea. Examples: - Facebook started as an application for 1 college to compare attractiveness between classmates. - Airbnb started just as system to find a floor to sleep in while in a conference and all the hotels are booked up. To meet overflow. - Groupon started as a platform for people to donate to charities, which has no relation to what it is now. - Pinterest started as an app for going to designers websites and marking things you were interested in, it would notify you when those things go on sale. - Twitter started as a podcast subscription service - Flickr started as a video game called never ending game - Instagram started as a check-in app - Nintendo started off selling vacuum cleaners Follow the lean startup concepts. \"When we're in the shower, when we're thinking about our idea, boy, does it sound brilliant. But the reality is that most of our ideas are actually terrible.\" - Eric Ries Every idea does with key assumptions, but you'll won't know for sure until you actually test them. You will be surprised on how wrong you are. Brilliant people also make dumb decisions. The only difference between you and brilliant people is that they try them, understand what they did wrong sooner, pivot and evolve these ideas and eventually get them right.","title":"The Eureka Myth"},{"location":"Business/idea-generation/04-ways-of-coming-up-with-ideas/01-essential-reading-for-those-focusing-on-startups/","text":"Essential reading for those focusing on startups \u00b6 Zero to One by Peter Thiel","title":"Essential reading for those focusing on startups"},{"location":"Business/idea-generation/04-ways-of-coming-up-with-ideas/01-essential-reading-for-those-focusing-on-startups/#essential-reading-for-those-focusing-on-startups","text":"Zero to One by Peter Thiel","title":"Essential reading for those focusing on startups"},{"location":"Business/idea-generation/04-ways-of-coming-up-with-ideas/02-strategy-bloated-margins/","text":"Strategy: Bloated Margins \u00b6 Margins are the difference between what it costs produce something and what it is sold for. It is the profit. In new industries you would see larger margins because you have less competition, on old industries typically the margins get smaller. It's called market erosion. Competition forces you to keep your margins low. There are situations that that's not the case - you have an old industry and the margins are high. You may call this phenomenon a bloated market. Company called Casper created a mattress that can fold and is easy to ship, reduced the bloated margins of a mattress industry. For them these margins are needed for distribution, it usually sits 6-12 months before being sold. Casper created one mattress type and made it shippable directly to the customer. Where to look: - Look at the industry you're interested in - Look at the value chain - Do any of these steps of the value chain make large margins?","title":"Strategy: Bloated Margins"},{"location":"Business/idea-generation/04-ways-of-coming-up-with-ideas/02-strategy-bloated-margins/#strategy-bloated-margins","text":"Margins are the difference between what it costs produce something and what it is sold for. It is the profit. In new industries you would see larger margins because you have less competition, on old industries typically the margins get smaller. It's called market erosion. Competition forces you to keep your margins low. There are situations that that's not the case - you have an old industry and the margins are high. You may call this phenomenon a bloated market. Company called Casper created a mattress that can fold and is easy to ship, reduced the bloated margins of a mattress industry. For them these margins are needed for distribution, it usually sits 6-12 months before being sold. Casper created one mattress type and made it shippable directly to the customer. Where to look: - Look at the industry you're interested in - Look at the value chain - Do any of these steps of the value chain make large margins?","title":"Strategy: Bloated Margins"},{"location":"Business/idea-generation/04-ways-of-coming-up-with-ideas/03-strategy-trends-in-outsourcing/","text":"Strategy: Trends in outsourcing \u00b6 Over time more and more things become automated. In virtually all scenarios there are roughly two steps to follow: - A brand new task - Automate it Essentially every new task starts with a hard task, over time it becomes easier and can be delegated, then eventually it becomes fully automated. You can find opportunities and target tasks between outsourcing and partial automated stages. Go to upwork, freelancer and scan the job listings. Look for posts that have high frequency. Sort by high end prices.","title":"Strategy: Trends in outsourcing"},{"location":"Business/idea-generation/04-ways-of-coming-up-with-ideas/03-strategy-trends-in-outsourcing/#strategy-trends-in-outsourcing","text":"Over time more and more things become automated. In virtually all scenarios there are roughly two steps to follow: - A brand new task - Automate it Essentially every new task starts with a hard task, over time it becomes easier and can be delegated, then eventually it becomes fully automated. You can find opportunities and target tasks between outsourcing and partial automated stages. Go to upwork, freelancer and scan the job listings. Look for posts that have high frequency. Sort by high end prices.","title":"Strategy: Trends in outsourcing"},{"location":"Business/idea-generation/04-ways-of-coming-up-with-ideas/04-strategy-robin-hooding/","text":"Strategy: Robin Hooding \u00b6 Take a solution that usually is offered for a price and offer them for free. Think about an industry you're interested in Look for a pattern where all the products are offered for a significant fee when the cost for it is quite low Do all of the current solutions for this space charge money? What percentage of the market is owned by the companies that do charge? Is the marginal cost of the product or service low?","title":"Strategy: Robin Hooding"},{"location":"Business/idea-generation/04-ways-of-coming-up-with-ideas/04-strategy-robin-hooding/#strategy-robin-hooding","text":"Take a solution that usually is offered for a price and offer them for free. Think about an industry you're interested in Look for a pattern where all the products are offered for a significant fee when the cost for it is quite low Do all of the current solutions for this space charge money? What percentage of the market is owned by the companies that do charge? Is the marginal cost of the product or service low?","title":"Strategy: Robin Hooding"},{"location":"Business/idea-generation/04-ways-of-coming-up-with-ideas/05-strategy-good-idea-bad-timing/","text":"Strategy: Good idea, bad timing \u00b6 There are many companies that tried to do something of value, but did it at the wrong time. Youtube wasn't the first site to host video, there was pseudo.com, but it did not look that good on a 56K modem. Look for companies that failed and ask yourself: - Did they fail because their timing wasn't right or was it something else? Some of the factors that may affect bad timing: - Cultural acceptance and cultural norms - Infrastructure - Legal issues - Technical issues Where to look - autopsy.io - cbinsights","title":"Strategy: Good idea, bad timing"},{"location":"Business/idea-generation/04-ways-of-coming-up-with-ideas/05-strategy-good-idea-bad-timing/#strategy-good-idea-bad-timing","text":"There are many companies that tried to do something of value, but did it at the wrong time. Youtube wasn't the first site to host video, there was pseudo.com, but it did not look that good on a 56K modem. Look for companies that failed and ask yourself: - Did they fail because their timing wasn't right or was it something else? Some of the factors that may affect bad timing: - Cultural acceptance and cultural norms - Infrastructure - Legal issues - Technical issues Where to look - autopsy.io - cbinsights","title":"Strategy: Good idea, bad timing"},{"location":"Business/idea-generation/04-ways-of-coming-up-with-ideas/06-strategy-information-asymmetry/","text":"Strategy: Information Asymmetry \u00b6 Any normal industry where one side has advantage over consumer by having access to certain information. Normally consumers doesn't really know what materials are going into products and what fair pricing really is. Examples: - Lawyers - Venture Capitalists There is a company called avvo to solve lawyer problem, by lawyers bidding to your problem detailing the bid. For VC, it is solved by venture hacks that help companies to get funded by better terms. If I wanted to acquire ___ would I know how to do it?","title":"Strategy: Information Asymmetry"},{"location":"Business/idea-generation/04-ways-of-coming-up-with-ideas/06-strategy-information-asymmetry/#strategy-information-asymmetry","text":"Any normal industry where one side has advantage over consumer by having access to certain information. Normally consumers doesn't really know what materials are going into products and what fair pricing really is. Examples: - Lawyers - Venture Capitalists There is a company called avvo to solve lawyer problem, by lawyers bidding to your problem detailing the bid. For VC, it is solved by venture hacks that help companies to get funded by better terms. If I wanted to acquire ___ would I know how to do it?","title":"Strategy: Information Asymmetry"},{"location":"Business/idea-generation/04-ways-of-coming-up-with-ideas/07-strategy-hobby-lobbyist/","text":"Strategy: Hobby Lobbyist \u00b6 Hobbies often turn into business industries. Think of the hobbies you have Look at other hobbies What parts of this hobby, if made simpler, would attract more people to use them / do them?","title":"Strategy: Hobby Lobbyist"},{"location":"Business/idea-generation/04-ways-of-coming-up-with-ideas/07-strategy-hobby-lobbyist/#strategy-hobby-lobbyist","text":"Hobbies often turn into business industries. Think of the hobbies you have Look at other hobbies What parts of this hobby, if made simpler, would attract more people to use them / do them?","title":"Strategy: Hobby Lobbyist"},{"location":"Business/idea-generation/04-ways-of-coming-up-with-ideas/08-strategy-the-enabler/","text":"Strategy: The Enabler \u00b6 We have a large group of people that want to do something but can't because there is a large technical investment required to get started. The bar to entry is too high. Look for solutions to make this process easier. Look at an alternative industry Research what it would take to make your first sale in that industry Check regulations Check investment required Think of how to solve it Starting in medicine / healthcare is difficult Because of regulations like HIPAA One companies trying to solve this is aptible","title":"Strategy: The Enabler"},{"location":"Business/idea-generation/04-ways-of-coming-up-with-ideas/08-strategy-the-enabler/#strategy-the-enabler","text":"We have a large group of people that want to do something but can't because there is a large technical investment required to get started. The bar to entry is too high. Look for solutions to make this process easier. Look at an alternative industry Research what it would take to make your first sale in that industry Check regulations Check investment required Think of how to solve it Starting in medicine / healthcare is difficult Because of regulations like HIPAA One companies trying to solve this is aptible","title":"Strategy: The Enabler"},{"location":"Business/idea-generation/04-ways-of-coming-up-with-ideas/09-molding-a-user/","text":"Strategy: Molding a user \u00b6 Take an industry you're interested in Find out from people: \"What are your top problems or difficulties you experience frequently throughout your day?\" Research these problems Mold a solution for that user By molding solution to 1 user, you instantly have 1 potential user and chances are that more people will want to use it too. Give yourself a top 3 problems of the day Potentially solve these problems Does anyone else want it?","title":"Strategy: Molding a user"},{"location":"Business/idea-generation/04-ways-of-coming-up-with-ideas/09-molding-a-user/#strategy-molding-a-user","text":"Take an industry you're interested in Find out from people: \"What are your top problems or difficulties you experience frequently throughout your day?\" Research these problems Mold a solution for that user By molding solution to 1 user, you instantly have 1 potential user and chances are that more people will want to use it too. Give yourself a top 3 problems of the day Potentially solve these problems Does anyone else want it?","title":"Strategy: Molding a user"},{"location":"Business/idea-generation/04-ways-of-coming-up-with-ideas/10-strategy-economies-of-service/","text":"Strategy: Economies of Service \u00b6 What service you wish you had but it's way too expensive? What services do rich people have that normal people does not? tax specialists buttlers vacation planners stylists If we didn't change the cost of this service, but instead spread it out amongst more users, could this be a realistic service offered for a lower price? Example company: Hello Alfred","title":"Strategy: Economies of Service"},{"location":"Business/idea-generation/04-ways-of-coming-up-with-ideas/10-strategy-economies-of-service/#strategy-economies-of-service","text":"What service you wish you had but it's way too expensive? What services do rich people have that normal people does not? tax specialists buttlers vacation planners stylists If we didn't change the cost of this service, but instead spread it out amongst more users, could this be a realistic service offered for a lower price? Example company: Hello Alfred","title":"Strategy: Economies of Service"},{"location":"Business/idea-generation/04-ways-of-coming-up-with-ideas/11-strategy-sloppy-duct-tape/","text":"Strategy: Sloppy Duct Tape \u00b6 Think of an examples for workarounds to get something done that are ridiculous Examples: - Man duct-tapes ethernet port when he wants to work so he cannot access internet. - People who during finals swap facebook passwords so they can focus on studies - Make a stick like device to press a start button on router from desk - Nail christmas tree to the ceiling so animals cannot reach it When finding this it is because: - These people want this problem solved - There is no solution for it","title":"Strategy: Sloppy Duct Tape"},{"location":"Business/idea-generation/04-ways-of-coming-up-with-ideas/11-strategy-sloppy-duct-tape/#strategy-sloppy-duct-tape","text":"Think of an examples for workarounds to get something done that are ridiculous Examples: - Man duct-tapes ethernet port when he wants to work so he cannot access internet. - People who during finals swap facebook passwords so they can focus on studies - Make a stick like device to press a start button on router from desk - Nail christmas tree to the ceiling so animals cannot reach it When finding this it is because: - These people want this problem solved - There is no solution for it","title":"Strategy: Sloppy Duct Tape"},{"location":"Business/idea-generation/04-ways-of-coming-up-with-ideas/12-strategy-privacy-and-the-blind-eye/","text":"Strategy: Privacy and the blind eye \u00b6 There are people that value their privacy. What are the obvious things that need to have privacy protection? Can I make this more private? Would thi actually improve privacy in any meaningful way? DuckDuckGo vs Google Abine, Burner Giving out your name when applying for a job You can obscure the name It prevents from race, age or other biases","title":"Strategy: Privacy and the blind eye"},{"location":"Business/idea-generation/04-ways-of-coming-up-with-ideas/12-strategy-privacy-and-the-blind-eye/#strategy-privacy-and-the-blind-eye","text":"There are people that value their privacy. What are the obvious things that need to have privacy protection? Can I make this more private? Would thi actually improve privacy in any meaningful way? DuckDuckGo vs Google Abine, Burner Giving out your name when applying for a job You can obscure the name It prevents from race, age or other biases","title":"Strategy: Privacy and the blind eye"},{"location":"Business/idea-generation/04-ways-of-coming-up-with-ideas/13-strategy-spotting-market-inefficiencies/","text":"Strategy: Spotting market inefficiencies \u00b6 These inefficiencies can occur because of various reasons: - The profit motive. Businesses try to maximize their profits and minimize costs, but that doesn't really align with customer needs. Examples: Private education- maximizing how much students pay buy actually learn doesn't really make sense. - Goverment regulations - rent control, rent cannot increase only 1-2% per year. - Not enough potential profit - Orphan drugs and orphan populations. What to look for: - Look for completely irrational pricing - Private education vs public education price gap - Housing prices - Drug prices - Look for weird consumer behaviours - People from Brazil comping to US to buy Apple products - Landlords actively try to evict their tenants How to build an idea: - Look for the problems caused by the inefficiency - Work on importing electronics - Can't fix the rent problem, but you can make landlords and tenants life easier.","title":"Strategy: Spotting market inefficiencies"},{"location":"Business/idea-generation/04-ways-of-coming-up-with-ideas/13-strategy-spotting-market-inefficiencies/#strategy-spotting-market-inefficiencies","text":"These inefficiencies can occur because of various reasons: - The profit motive. Businesses try to maximize their profits and minimize costs, but that doesn't really align with customer needs. Examples: Private education- maximizing how much students pay buy actually learn doesn't really make sense. - Goverment regulations - rent control, rent cannot increase only 1-2% per year. - Not enough potential profit - Orphan drugs and orphan populations. What to look for: - Look for completely irrational pricing - Private education vs public education price gap - Housing prices - Drug prices - Look for weird consumer behaviours - People from Brazil comping to US to buy Apple products - Landlords actively try to evict their tenants How to build an idea: - Look for the problems caused by the inefficiency - Work on importing electronics - Can't fix the rent problem, but you can make landlords and tenants life easier.","title":"Strategy: Spotting market inefficiencies"},{"location":"Business/idea-generation/04-ways-of-coming-up-with-ideas/14-strategy-vertical-integration/","text":"Strategy: Vertical Integration \u00b6 Very rarely a company that sells computers owns a company that manufactures them, ships and transports them. Look for combining 2 segments of value chain in an industry Does that create an advantage over a company that pays other companies for these segments? When running a card shop, you have to buy them, you have store to sell them. If a card shop also writes them and prints them - you can give them personalized cards on the spot, speed and flexibility.","title":"Strategy: Vertical Integration"},{"location":"Business/idea-generation/04-ways-of-coming-up-with-ideas/14-strategy-vertical-integration/#strategy-vertical-integration","text":"Very rarely a company that sells computers owns a company that manufactures them, ships and transports them. Look for combining 2 segments of value chain in an industry Does that create an advantage over a company that pays other companies for these segments? When running a card shop, you have to buy them, you have store to sell them. If a card shop also writes them and prints them - you can give them personalized cards on the spot, speed and flexibility.","title":"Strategy: Vertical Integration"},{"location":"Business/idea-generation/04-ways-of-coming-up-with-ideas/15-strategy-one-step-solutions/","text":"Strategy: One Step Strategy \u00b6 People like convenience. There are tasks that require multiple step process. Push for pizza. One button, when you press it, it delivers a medium-sized cheese pizza to your GPS location. Rev - send a file of a video, within 24 hours you receive subtitles etc. Amazon 1-click buy Look for industry you're interested in Think about the processes to do something Think what happens when you remove a step / all the steps It doesn't work when personalization is important","title":"Strategy: One Step Strategy"},{"location":"Business/idea-generation/04-ways-of-coming-up-with-ideas/15-strategy-one-step-solutions/#strategy-one-step-strategy","text":"People like convenience. There are tasks that require multiple step process. Push for pizza. One button, when you press it, it delivers a medium-sized cheese pizza to your GPS location. Rev - send a file of a video, within 24 hours you receive subtitles etc. Amazon 1-click buy Look for industry you're interested in Think about the processes to do something Think what happens when you remove a step / all the steps It doesn't work when personalization is important","title":"Strategy: One Step Strategy"},{"location":"Business/idea-generation/05-business-models/01-a-comprehensive-list-of-online-business-models-and-who-uses-them/","text":"A Comprehensive list of business models and who uses them \u00b6 Advertising Display ads - Yahoo Search ads - Google Text ads - Google Video ads - Hulu, Twitch.tv Audio ads - Pandora, Radio, Podcasts Promoted content - Twitter, Tumblr Paid content links - Outbrain Recruitment ads - LinkedIn Lead generation - MoneySuperMarket, ZocDoc Affiliate foos - The Wirecutter Classifieds - Craigslist Featured listings - Yelp, Super Pages E-mail ads - Yahoo, MSN Ad-retargeting - Criteo Real-time-intent ad delivery - Foursquare Sponsorships/Site take-overs - Pandora Lifetime, unique ad space - NSFWCORP Conflict Tower, milliondollarhomepage Sponsorships - Twitch.tv TODO: FINISH","title":"A Comprehensive list of business models and who uses them"},{"location":"Business/idea-generation/05-business-models/01-a-comprehensive-list-of-online-business-models-and-who-uses-them/#a-comprehensive-list-of-business-models-and-who-uses-them","text":"Advertising Display ads - Yahoo Search ads - Google Text ads - Google Video ads - Hulu, Twitch.tv Audio ads - Pandora, Radio, Podcasts Promoted content - Twitter, Tumblr Paid content links - Outbrain Recruitment ads - LinkedIn Lead generation - MoneySuperMarket, ZocDoc Affiliate foos - The Wirecutter Classifieds - Craigslist Featured listings - Yelp, Super Pages E-mail ads - Yahoo, MSN Ad-retargeting - Criteo Real-time-intent ad delivery - Foursquare Sponsorships/Site take-overs - Pandora Lifetime, unique ad space - NSFWCORP Conflict Tower, milliondollarhomepage Sponsorships - Twitch.tv TODO: FINISH","title":"A Comprehensive list of business models and who uses them"},{"location":"Business/idea-generation/05-business-models/02-saas-software-as-a-service/","text":"SaaS: Software as a Service \u00b6 Any business delivers value, you sell software as a service instead of a product. Browser version of desktop apps, charges online. Because it is in a browser: You can reach a lot more people You can grow your business a lot more easily Frees up a lot of resources you can use for marketing etc Examples: - Xero - Spotify What to look for: - Offline to online: things that are often done on paper but can be done online - Services that are used frequently - Collecting user information - convenient to store user information (evenote) - Avoid copying and being pirated (photoshop)","title":"SaaS: Software as a Service"},{"location":"Business/idea-generation/05-business-models/02-saas-software-as-a-service/#saas-software-as-a-service","text":"Any business delivers value, you sell software as a service instead of a product. Browser version of desktop apps, charges online. Because it is in a browser: You can reach a lot more people You can grow your business a lot more easily Frees up a lot of resources you can use for marketing etc Examples: - Xero - Spotify What to look for: - Offline to online: things that are often done on paper but can be done online - Services that are used frequently - Collecting user information - convenient to store user information (evenote) - Avoid copying and being pirated (photoshop)","title":"SaaS: Software as a Service"},{"location":"Business/idea-generation/05-business-models/03-subscriptions-and-memberships/","text":"Subscriptions & Memberships \u00b6 A service that charges monthly and gives an access to something. Envato Market Skillshare ZipCar When you lose money on individual sales When you have large amount of content to sell Your product need to be used frequently Costco makes 0% margin on the stuff they sell, but they make money on memberships.","title":"Subscriptions & Memberships"},{"location":"Business/idea-generation/05-business-models/03-subscriptions-and-memberships/#subscriptions-memberships","text":"A service that charges monthly and gives an access to something. Envato Market Skillshare ZipCar When you lose money on individual sales When you have large amount of content to sell Your product need to be used frequently Costco makes 0% margin on the stuff they sell, but they make money on memberships.","title":"Subscriptions &amp; Memberships"},{"location":"Business/idea-generation/05-business-models/04-pay-what-you-want/","text":"Pay what you want \u00b6 You let customers decide how much they pay. When to use this model: - When you have 0 overhead When not to use: - Your target auditory does not have a disposable income priceline humblebundle","title":"Pay what you want"},{"location":"Business/idea-generation/05-business-models/04-pay-what-you-want/#pay-what-you-want","text":"You let customers decide how much they pay. When to use this model: - When you have 0 overhead When not to use: - Your target auditory does not have a disposable income priceline humblebundle","title":"Pay what you want"},{"location":"Business/idea-generation/05-business-models/05-the-sharing-economy/","text":"The sharing economy \u00b6 Sharing economy: A company or a business idea that plans to allow individual people to monetize assets they aren't used to selling. Sharing, renting or loaning marketplace instead of selling. Uber, lyft Airbnb Getaround Relayrounds Udemy Vayable What people buy for a lot of money, don't use and can make money by offering them to other people without purchasing themselves. What is underutilized and you could monetize by sharing? Doesn't work on older generations Doesn't work on items that are too common for people to have","title":"The sharing economy"},{"location":"Business/idea-generation/05-business-models/05-the-sharing-economy/#the-sharing-economy","text":"Sharing economy: A company or a business idea that plans to allow individual people to monetize assets they aren't used to selling. Sharing, renting or loaning marketplace instead of selling. Uber, lyft Airbnb Getaround Relayrounds Udemy Vayable What people buy for a lot of money, don't use and can make money by offering them to other people without purchasing themselves. What is underutilized and you could monetize by sharing? Doesn't work on older generations Doesn't work on items that are too common for people to have","title":"The sharing economy"},{"location":"Business/idea-generation/05-business-models/06-marketplaces/","text":"Marketplaces \u00b6 Business relies on connecting buyers and sellers, facilitate the exchange, convenience innovation. Ebay Etsy Udemy Amazon Getaround Look for unmet demand who can offer it. Why aren't they already connected? Time Overhead Downsides: - Trying to build two-sided community where one won't join without another","title":"Marketplaces"},{"location":"Business/idea-generation/05-business-models/06-marketplaces/#marketplaces","text":"Business relies on connecting buyers and sellers, facilitate the exchange, convenience innovation. Ebay Etsy Udemy Amazon Getaround Look for unmet demand who can offer it. Why aren't they already connected? Time Overhead Downsides: - Trying to build two-sided community where one won't join without another","title":"Marketplaces"},{"location":"Business/idea-generation/05-business-models/07-on-demand/","text":"On demand \u00b6 Any company that takes an ordinary business and adds extreme convenience, speed. Uber Lyft Washio Spoonrocket Basic service that happens all the time that you can make more convenient and faster. Costumers will pay premium for that. Do people need this right now? Would they pay extra for it? Cater for people with extra disposable income Think of what you hate acquiring","title":"On demand"},{"location":"Business/idea-generation/05-business-models/07-on-demand/#on-demand","text":"Any company that takes an ordinary business and adds extreme convenience, speed. Uber Lyft Washio Spoonrocket Basic service that happens all the time that you can make more convenient and faster. Costumers will pay premium for that. Do people need this right now? Would they pay extra for it? Cater for people with extra disposable income Think of what you hate acquiring","title":"On demand"},{"location":"Business/idea-generation/05-business-models/08-currated-boxes/","text":"Currated boxes \u00b6 Sell lots of small things and currate them in each box that is sold each month. Usually comes with a subscription mode. You don't actually produce anything, but shop for large amounts of people. You can negotiate lower prices or even get paid for including items. Birchbox Naturebox Barkbox Bespoke post Go for products that have a high replacement rate","title":"Currated boxes"},{"location":"Business/idea-generation/05-business-models/08-currated-boxes/#currated-boxes","text":"Sell lots of small things and currate them in each box that is sold each month. Usually comes with a subscription mode. You don't actually produce anything, but shop for large amounts of people. You can negotiate lower prices or even get paid for including items. Birchbox Naturebox Barkbox Bespoke post Go for products that have a high replacement rate","title":"Currated boxes"},{"location":"Business/idea-generation/05-business-models/09-direct-to-consumer/","text":"Direct to Consumer \u00b6 When you're selling a physical product, you skip retail steps and send it directly to the consumer. This can save a lot of money. Dell computers Warby Parker What are you trying to sell? Do you need a retailer to sell them? The item size has to be proportional with the size Ship small things that are more expensive For low price items, use curated boxes","title":"Direct to Consumer"},{"location":"Business/idea-generation/05-business-models/09-direct-to-consumer/#direct-to-consumer","text":"When you're selling a physical product, you skip retail steps and send it directly to the consumer. This can save a lot of money. Dell computers Warby Parker What are you trying to sell? Do you need a retailer to sell them? The item size has to be proportional with the size Ship small things that are more expensive For low price items, use curated boxes","title":"Direct to Consumer"},{"location":"Business/idea-generation/05-business-models/10-crowdsourced-catalogs-and-inventory/","text":"Crowdsourced catalogs and inventory \u00b6 Companies allow other people to determine on what their inventory is, based on what's popular they can find on what is going to sell the best. Teespring - allows people to design t-shirts, produces them when there is enough demand. Gusto - Produce designs - people chooses what they like and the most popular are manufactured. Advantages: - Limit your risk selling things that won't work - Building a community Disadvantages: - This process may take you places that is contrary on how the company want to present themselves. Typically work where people are incentivized to create content for you","title":"Crowdsourced catalogs and inventory"},{"location":"Business/idea-generation/05-business-models/10-crowdsourced-catalogs-and-inventory/#crowdsourced-catalogs-and-inventory","text":"Companies allow other people to determine on what their inventory is, based on what's popular they can find on what is going to sell the best. Teespring - allows people to design t-shirts, produces them when there is enough demand. Gusto - Produce designs - people chooses what they like and the most popular are manufactured. Advantages: - Limit your risk selling things that won't work - Building a community Disadvantages: - This process may take you places that is contrary on how the company want to present themselves. Typically work where people are incentivized to create content for you","title":"Crowdsourced catalogs and inventory"},{"location":"Business/idea-generation/06-evaluate-and-test/01-matching-fit/","text":"Matching Fit \u00b6 Take your top 3 sheet you made previously and see if something catches your eye regarding something you're evaluating. Look at your idea and see if matches your hobbies, skills, experience, passions. Experience and skills matter more than passion. Ask yourself: - Could I do this every single day without losing interest? - If this idea failed, will I still gain valuable lessons and learn valuable skills? - Do I have a step-by-step plan for my idea? - Do you have the resources to do it? A business opportunity you can't pursue is not an opportunity. Does your idea have a window? Reflect if you can actually get in within this time window. Break your idea into pieces and analyze them What pieces do you have the ability to work on? You can also pick one piece and build a business around that Example: - Managing airbnb properties - Breaking down: - Optimize people listing descriptions - Manage cleanings - Manage checkins - Relay communication from customer to owner - Setting up a system to respond to inquiries - You can pick one and start a business on only one if it's too much","title":"Matching Fit"},{"location":"Business/idea-generation/06-evaluate-and-test/01-matching-fit/#matching-fit","text":"Take your top 3 sheet you made previously and see if something catches your eye regarding something you're evaluating. Look at your idea and see if matches your hobbies, skills, experience, passions. Experience and skills matter more than passion. Ask yourself: - Could I do this every single day without losing interest? - If this idea failed, will I still gain valuable lessons and learn valuable skills? - Do I have a step-by-step plan for my idea? - Do you have the resources to do it? A business opportunity you can't pursue is not an opportunity. Does your idea have a window? Reflect if you can actually get in within this time window. Break your idea into pieces and analyze them What pieces do you have the ability to work on? You can also pick one piece and build a business around that Example: - Managing airbnb properties - Breaking down: - Optimize people listing descriptions - Manage cleanings - Manage checkins - Relay communication from customer to owner - Setting up a system to respond to inquiries - You can pick one and start a business on only one if it's too much","title":"Matching Fit"},{"location":"Business/idea-generation/06-evaluate-and-test/02-pain-to-payment/","text":"Pain to Payment \u00b6 Pain to Payment ratio - any business idea you're creating is going to solve the problem. How big of a problem is it? How often are people used to paying money? how likely are customers are likely to pay you? Do customers understand the value of your service? Rate it 1-10","title":"Pain to Payment"},{"location":"Business/idea-generation/06-evaluate-and-test/02-pain-to-payment/#pain-to-payment","text":"Pain to Payment ratio - any business idea you're creating is going to solve the problem. How big of a problem is it? How often are people used to paying money? how likely are customers are likely to pay you? Do customers understand the value of your service? Rate it 1-10","title":"Pain to Payment"},{"location":"Business/idea-generation/06-evaluate-and-test/03-sustainability/","text":"Sustainability \u00b6 Sustainability: the longevity of your business. How long do you want to spend on your business As a startup look for sustainability What's going to happen to your market when you become successfull? Fixed launched a mobile app that will contest your parking tickets, they did get popular and cities banned the service. Haystack tried to auction off public parking spaces and the city responded the same way. Are you dependent on another platform? What happens if the platform dissapears? Branchout got very popular, facebook changed one thing and it imploded. How will your competitors react to you coming into the market? Pebble was popular but then apple, motorolla came out with a watch and it dissapeared. Do you have something specific that others can't imitate?","title":"Sustainability"},{"location":"Business/idea-generation/06-evaluate-and-test/03-sustainability/#sustainability","text":"Sustainability: the longevity of your business. How long do you want to spend on your business As a startup look for sustainability What's going to happen to your market when you become successfull? Fixed launched a mobile app that will contest your parking tickets, they did get popular and cities banned the service. Haystack tried to auction off public parking spaces and the city responded the same way. Are you dependent on another platform? What happens if the platform dissapears? Branchout got very popular, facebook changed one thing and it imploded. How will your competitors react to you coming into the market? Pebble was popular but then apple, motorolla came out with a watch and it dissapeared. Do you have something specific that others can't imitate?","title":"Sustainability"},{"location":"Business/idea-generation/06-evaluate-and-test/04-path-to-validation/","text":"Path to validation \u00b6 Validation is to get your business to a point that you can validate that there actually is a demand for your product. Required to validate your idea: - Difficulty to validate: - Protein bars (easy to validate) - You can use a landing page for validation - If you get people to buy the bar, you validated it - New credit card technology (hard to validate) - Prove the customers that will use the product - Merchants will agree to use it - Payment processors will use it - You need to validate the core assumptions: - People are having problems with their credit cards - All 3 parties have problems - People care about security about their credit cards - People are open to try new technology","title":"Path to validation"},{"location":"Business/idea-generation/06-evaluate-and-test/04-path-to-validation/#path-to-validation","text":"Validation is to get your business to a point that you can validate that there actually is a demand for your product. Required to validate your idea: - Difficulty to validate: - Protein bars (easy to validate) - You can use a landing page for validation - If you get people to buy the bar, you validated it - New credit card technology (hard to validate) - Prove the customers that will use the product - Merchants will agree to use it - Payment processors will use it - You need to validate the core assumptions: - People are having problems with their credit cards - All 3 parties have problems - People care about security about their credit cards - People are open to try new technology","title":"Path to validation"},{"location":"Business/idea-generation/06-evaluate-and-test/05-unfair-advantages/","text":"Unfair advantages \u00b6 You should plan on having competition. What prevents from competing with you? Trade secrets & patents You are an expert in the field you are in Dedication & availability First mover's advantage Flexibility","title":"Unfair advantages"},{"location":"Business/idea-generation/06-evaluate-and-test/05-unfair-advantages/#unfair-advantages","text":"You should plan on having competition. What prevents from competing with you? Trade secrets & patents You are an expert in the field you are in Dedication & availability First mover's advantage Flexibility","title":"Unfair advantages"},{"location":"Business/idea-generation/07-idea-advice-commentary/01-the-value-of-ideas/","text":"The value of ideas \u00b6 Your idea has no value by itself. Hiding ideas Ideas do not get better by sitting in your head It's better to test a bad idea than hide a good one NDAs They don't really work They're hard to write and hard to enforce Patents Patents are usually a waste of time It takes 2.5 years to make it You can never be 100% protected Focus on execution. \"Business ideas are like buses. If you miss one, there's one coming in 10 minutes\" - Richard Branson","title":"The value of ideas"},{"location":"Business/idea-generation/07-idea-advice-commentary/01-the-value-of-ideas/#the-value-of-ideas","text":"Your idea has no value by itself. Hiding ideas Ideas do not get better by sitting in your head It's better to test a bad idea than hide a good one NDAs They don't really work They're hard to write and hard to enforce Patents Patents are usually a waste of time It takes 2.5 years to make it You can never be 100% protected Focus on execution. \"Business ideas are like buses. If you miss one, there's one coming in 10 minutes\" - Richard Branson","title":"The value of ideas"},{"location":"Business/idea-generation/07-idea-advice-commentary/02-red-oceans-blue-oceans/","text":"Red oceans, blue oceans \u00b6 Red ocean - any market that has a lot of players in it. Well known profits. Blue ocean - no signs of actually being a market. More speculative, explorative. Depending on the business type you want to build, this makes a difference: - Lifestyle business - You should go for a red ocean - You probably want to be in 75% percentile of a trend adoption curve - Startups - You should go for a blue ocean - Side business - Red ocean","title":"Red oceans, blue oceans"},{"location":"Business/idea-generation/07-idea-advice-commentary/02-red-oceans-blue-oceans/#red-oceans-blue-oceans","text":"Red ocean - any market that has a lot of players in it. Well known profits. Blue ocean - no signs of actually being a market. More speculative, explorative. Depending on the business type you want to build, this makes a difference: - Lifestyle business - You should go for a red ocean - You probably want to be in 75% percentile of a trend adoption curve - Startups - You should go for a blue ocean - Side business - Red ocean","title":"Red oceans, blue oceans"},{"location":"Business/idea-generation/07-idea-advice-commentary/03-idea-frameworks-and-unreasonable-people/","text":"Idea frameworks & unreasonable people \u00b6 There is no framework for ideas. The process of idea generation is fluid and you can't really measure it. You have to actively try to look for ideas. If there was a framework, every single idea would be done. Ideas often come from unreasonable people, they think more outside of the bounds of the system already established. Work towards finding ideas. Try to be unreasonable.","title":"Idea frameworks & unreasonable people"},{"location":"Business/idea-generation/07-idea-advice-commentary/03-idea-frameworks-and-unreasonable-people/#idea-frameworks-unreasonable-people","text":"There is no framework for ideas. The process of idea generation is fluid and you can't really measure it. You have to actively try to look for ideas. If there was a framework, every single idea would be done. Ideas often come from unreasonable people, they think more outside of the bounds of the system already established. Work towards finding ideas. Try to be unreasonable.","title":"Idea frameworks &amp; unreasonable people"},{"location":"Business/idea-generation/07-idea-advice-commentary/04-west-coast-east-coast/","text":"West coast, east coast \u00b6 You should know your audience, don't get discouraged if you're coming up with ideas and people are telling you that they're bad or stupid. You need to know what your context is - is it more conservative, what metrics do people care about, what do they are good ideas, and what's their risk tolerance.","title":"West coast, east coast"},{"location":"Business/idea-generation/07-idea-advice-commentary/04-west-coast-east-coast/#west-coast-east-coast","text":"You should know your audience, don't get discouraged if you're coming up with ideas and people are telling you that they're bad or stupid. You need to know what your context is - is it more conservative, what metrics do people care about, what do they are good ideas, and what's their risk tolerance.","title":"West coast, east coast"},{"location":"Business/idea-generation/07-idea-advice-commentary/05-avoid-threshold-problems/","text":"Avoid threshold ideas \u00b6 There are ideas that require certain amount of customers before you start getting customers - that is a threshold idea. You need a lot of experience and a lot of time to pull it off. social network platforms market place platforms How bad is your threshold problem? Strategies for solving a problem: - Seeding the one side - Faking it - Lower transparency so one side doesn't see how many people are on the other side","title":"Avoid threshold ideas"},{"location":"Business/idea-generation/07-idea-advice-commentary/05-avoid-threshold-problems/#avoid-threshold-ideas","text":"There are ideas that require certain amount of customers before you start getting customers - that is a threshold idea. You need a lot of experience and a lot of time to pull it off. social network platforms market place platforms How bad is your threshold problem? Strategies for solving a problem: - Seeding the one side - Faking it - Lower transparency so one side doesn't see how many people are on the other side","title":"Avoid threshold ideas"},{"location":"Clean%20Code/3%20Rules%20of%20TDD/","text":"3 rules of TDD You are not allowed to write any production code before you have written a test that fails because the production code doesn't exist . You are not allowed to write more of a test that is sufficient to fail. Not compiling is also failing. As soon as the test does not compile, you must stop writing it and fix it . You are not allowed to write any more production code that is sufficient to pass the failing test .","title":"3 Rules of TDD"},{"location":"Clean%20Code/Clean%20architecture/","text":"The [[web]] is a [[delivery mechanism]]. [[Architecture]] tells the intent of the application. Use [[use case]]s. Use case - [[object]], [[interactor]]. Contains app-specific [[business rule]]s user \u2194 boundaries \u2190 interactors \u2192 entities [[MVC]] is real for small components like buttons etc not for pages. [[Database]] should be treated as a [[IO device]]. Implements [[Interface]], method or [[entity gateway]]. Same thing with [[ORM]]. A good architecture allows you to defer until the last responsible moment.","title":"Clean architecture"},{"location":"Clean%20Code/Kent%20Beck%27s%20game/","text":"write a failing test, try to make the code pass. If it doesn't - throw the code away and start over.","title":"Kent Beck's game"},{"location":"Clean%20Code/TDD%20example/","text":"TDD example: Create a nothing() test that asserts that an empty test is working. Create a test that creates the class you are going to work with. Create the class. Assert it's empty Add push, assert it's not empty Add pop ... so on","title":"TDD example"},{"location":"Clean%20Code/Uncle%20Bob%20Clean%20Code%20Checklist%20-%20Architecture/","text":"Architecture What is [[design]] and [[architecture]]? Roughly the same thing Goal: Minimize the human resources to build and maintain code. Quality can be measured in amount of effort to build something. If the effort increases - the design is bad. What is the goal of an architecture?::To minimize the human resources to build and maintain code. Most startups fail because they can't maintain. Slow & steady wins the race. Over confidence that we will be able to clean up the mess later - that never happens. Key to speed - not to slow down, in a steady, consistent and clean way. [[TDD]] is always faster. Solution: just be as clean as possible. Software Software must be [[changeable]], changes must be cheap. Software (does not work, changeable) > Software(works, not changeable) [[Change isolation]]: Plugin to business rules [[Plugin architecture]] Frameworks: not made for your benefit but for authors decouple from frameworks Look at the cost benefit when using them.","title":"Uncle Bob Clean Code Checklist   Architecture"},{"location":"Clean%20Code/Uncle%20Bob%20Clean%20Code%20Checklist%20-%20Comments%2C%20Naming/","text":"Add flashcards / review \u2705 2022-11-01 Comments Most of the time [[comments]] are code smell They tend to compensate for bad code They deteriorate over time and can lie Most of the time are useless There really should be following comments: Legal / copyright comments Comments when you cannot use a more descriptive name due to a design pattern Hard to understand lines like [[regex]] query When writing comments Should reveal intent Are applied to local [[scope]] only Naming names Should reveal intent Variable name length should be proportional to the scope that contains it. Function and class name length should be inverse proportional to the scope. The larger scope - the shorter it should be, more abstract. Avoid using names with \"data\", \"info\". Color comments red - read them - if useless, delete them.> Red column bar that shows you if you have too much of line-width. flashcards Why are comments considered a code smell? ? - They tend to compensate for bad code - They deteriorate over time and can lie - Most of the time are useless These are the comments that should be allowed: ? - Legal / Copyright comments - Comments when you cannot use a more descriptive name due to a design pattern - Hard to understand lines like regex query When writing comments you should: ? - Reveal intent - Apply them to the local scope only Variable names should reveal intent . Variable name length should be proportional to the scope that contains it . Function and class name length should be inverse proportional to the scope. The larger scope - the shorter it should be, more abstract . Avoid using names with::\"data\", \"info\"","title":"Uncle Bob Clean Code Checklist   Comments, Naming"},{"location":"Clean%20Code/Uncle%20Bob%20Clean%20Code%20Checklist%20-%20Functions/","text":"Functions Has same level of abstractions (high level abstractions and primitive structures together) Should follow the same methodology as news articles - the deeper you go into them, the more details you get. Should be small (usually 2-6 lines) Should do 1 thing - you cannot extract another function from it. Shouldn't have more than 1-2 indents Shouldn't have more than 3 arguments Shouldn't have bool input arguments Shouldn't have input arguments that are used as outputs Shouldn't contain switch/if-else statements (prefer [[polymorphism]], Open-Closed Principle , [[NullObject]]) Shouldn't have [[side-effects]] (file.open without close etc) Should have command/query separation. ([[Queries]] shouldn't modify the state[[,]] commands shouldn't return data). Basically split your functions into more functions until you can't. Then probably you'll see hidden classes that you need to move these functions to. (Timestamp 1:00:23) Exceptions Always prefer [[exceptions]] over [[error codes]] [[Function body]] should only contain [[try&catch block]] if there is one. Don't use nested try&catch blocks Flashcards Has same level of abstractions (high level abstractions and primitive structures together) together How big should an average function be::small (usually 2-6 lines) What should a function do?::It should do only 1 thing. You cannot extract another function from it. How many indents shouldn't the function exceed?::It shouldn't exceed 1-2 indents How many arguments should a function have::A functions shouldn't have move than 3 arguments A function shouldn't have boolean input arguments A function shouldn't input arguments that are used as outputs . A function shouldn't contain switch/if-else statements. Prefer polymorphism, open-closed principle, null-object . A functions shouldn't have side -effects like opening file without closing it . Functions should follow the command/query seperation. Queries shouldn't modify the state, commands shouldn't return data . Always prefer exceptions over error codes. Function body should only contain try&catch block if there is one. Don't use nested try&catch blocks.","title":"Uncle Bob Clean Code Checklist   Functions"},{"location":"Clean%20Code/Uncle%20Bob%20Clean%20Code%20Checklist%20-%20Rules/","text":"We will not ship shit You know it works before release The code is the highest quality proportional to the time has been allocated to it We will always be ready System is ready to deploy at the end of sprint Code, testing, documentation has been done It is shippable Stable productivity You do now slow down as the project grows longer. Usually we are getting slower as we make mess in the code. Inexpensive adaptability Cost of change should be proportional to scope. Changing text for a button shouldn't take 6 months. There should be a good suite of tests Continuous Improvement Code should improve with time not degrade. With time it should use design patterns / architectures. Fearless competence Clean code, do not let the code rot. Do not be afraid to touch the code. Tests + refactoring Check it in a bit better than you found it. We will not dump on QA Not the group that finds bugs. QA should be at the start of the process, not the end. Expect for QA to find nothing All tests should be automated We will cover for each other. You should be able to step in for someone else. Someone else should be able to cover for you. Pairing might come in handy. Give honest estimates Define a shape of what we don't know PERT estimation technique I expect you will say no when the answer is no I expect you are going to be able to deal with pressure.","title":"Uncle Bob Clean Code Checklist   Rules"},{"location":"Clean%20Code/Uncle%20Bob%20Clean%20Code%20Checklist%20-%20Test%20Driven%20Development/","text":"https://www.youtube.com/watch?v=58jGpV2Cg50 Tests are the perfect [[documentation]] for developers . When you use tdd: You cannot write code that's hard to write The code is automatically [[decoupled]] You trust the tests Separate logic from GUI Testing is like double-entry bookkeeping instead of assets and liabilities you have tests and production code. Instead of adding liabilities and assets and getting a 0, you get 0 failed tests. [[Inheritance]] - test all the lines, each and every parent/child class. [[Mutation testing]] - mutates code, expects failure. Makes sure tests actually make assertions. Write tests around the solution first. Use [[katas]] and tdd excercises to learn this. There are two strategies to tdd - [[ouside-in tdd]] (start with HTTP requests and go inwards, acceptance to unit) or [[inside-out tdd]] (starts at the domain/business model and works it's way out to the api, unit to acceptance). Tests should not mirror code (test class per production code class) .","title":"Uncle Bob Clean Code Checklist   Test Driven Development"},{"location":"Clean%20Code/Uncle%20Bob%27s%20clean%20code%20checklist/","text":"Split this into multiple things, add some kind of an embed to see it together? \ud83d\udcc5 2022-10-30 \u2705 2022-11-01 Uncle Bob Clean Code Checklist - Functions Uncle Bob Clean Code Checklist - Comments, Naming From Lesson 3 \u00b6 Uncle Bob Clean Code Checklist - Rules From Lesson 4 \u00b6 From Lesson 5 \u00b6","title":"Uncle Bob's clean code checklist"},{"location":"Clean%20Code/Uncle%20Bob%27s%20clean%20code%20checklist/#from-lesson-3","text":"Uncle Bob Clean Code Checklist - Rules","title":"From Lesson 3"},{"location":"Clean%20Code/Uncle%20Bob%27s%20clean%20code%20checklist/#from-lesson-4","text":"","title":"From Lesson 4"},{"location":"Clean%20Code/Uncle%20Bob%27s%20clean%20code%20checklist/#from-lesson-5","text":"","title":"From Lesson 5"},{"location":"Clean%20Code/Design%20Pattern/Adapter%20Design%20Pattern/","text":"Converts the [[interface]] of a [[class]] into another [[interface]] clients expect. Adapter lets classes work together that couldn't otherwise because of incompatible interfaces. Adapter example \u00b6 namespace SOLID.ISP { public interface IWideInterface { void A(); void B(); void C(); void D(); } public interface INarrowInterface { void A(); void B(); } class Adapter : INarrowInterface { private readonly IWideInterface _wide; public Adapter(IWideInterface wide) { _wide = wide; } public void A() { _wide.A(); } public void B() { _wide.B(); } } class Client { private readonly INarrowInterface _narrow; public Client(INarrowInterface narrow) { _narrow = narrow; } } }","title":"Adapter Design Pattern"},{"location":"Clean%20Code/Design%20Pattern/Adapter%20Design%20Pattern/#adapter-example","text":"namespace SOLID.ISP { public interface IWideInterface { void A(); void B(); void C(); void D(); } public interface INarrowInterface { void A(); void B(); } class Adapter : INarrowInterface { private readonly IWideInterface _wide; public Adapter(IWideInterface wide) { _wide = wide; } public void A() { _wide.A(); } public void B() { _wide.B(); } } class Client { private readonly INarrowInterface _narrow; public Client(INarrowInterface narrow) { _narrow = narrow; } } }","title":"Adapter example"},{"location":"Clean%20Code/Design%20Pattern/Template%20Method%20Design%20Pattern/","text":"There is an [[algorithm]] but small parts of the algorithm may vary. It defines the skeleton of an algorithm in an operation, deferring some steps to [[subclass]]es. Template method lets [[subclass]]es redefine certains steps of an algorithm without changing the algorithm's structure. namespace SOLID.OCP { public abstract class PaymentProcessor { public void ProcessTransaction() { WithdrawMoney(); CalculateBonus(); SendGreetings(); } protected abstract void WithdrawMoney(); protected abstract void CalculateBonus(); protected abstract void SendGreetings(); } public class OnlineProcessor : PaymentProcessor { protected override void WithdrawMoney() {} protected override void CalculateBonus() {} protected override void SendGreetings() {} } public class PosTerminalProcessor : PaymentProcessor { protected override void WithdrawMoney() {} protected override void CalculateBonus() {} protected override void SendGreetings() {} } }","title":"Template Method Design Pattern"},{"location":"Clean%20Code/Design%20Pattern/Visitor%20Design%20Pattern/","text":"Visitor pattern \u00b6 namespace SOLID.OCP.Refactored { public class BillDispenserEcdm : Device, IDevice { public BillDispenserEcdm() { Port = new SerialPort { BaudRate = 4800, Parity = Parity.Mark, Handshake = Handshake.RequestToSendXOnXOff }; } public SerialPort Port { get; } public string Find() { foreach (string portName in SerialPort.GetPortNames()) { Port.Write(\"Special code\"); if (Port.ReadByte == 120) return portName; } return null; } public override void Accept(IDeviceVisitor visitor) { visitor.Visit(this); } } public interface IDeviceVisitor { void Visit(BillDispenserEcdm billDispenser); void Visit(CoinDispenserCube4 coinDispenser); } public abstract class Device { public abstract void Accept(IDeviceVisitor visitor); } public class CloseCommandVisitor : IDeviceVisitor { public void Visit(BillDispenserEcdm billDispenser) { SerialPort port = billDispenser.Port; port.Write(new byte[] { 0x03 }, 0, 1); } public void Visit(CoinDispenserCube4 coinDispenser) { SerialPort port = coinDispenser.Port; port.Write(new byte[] { 0x12 }, 0, 1); } } public static class DeviceEx { public static void Close(this Device device) { var visitor = new CloseCommandVisitor(); device.Accept(visitor); } } public class Client { void Logic() { var device = new BillDispenserEcdm(); device.Close(); } } }","title":"Visitor Design Pattern"},{"location":"Clean%20Code/Design%20Pattern/Visitor%20Design%20Pattern/#visitor-pattern","text":"namespace SOLID.OCP.Refactored { public class BillDispenserEcdm : Device, IDevice { public BillDispenserEcdm() { Port = new SerialPort { BaudRate = 4800, Parity = Parity.Mark, Handshake = Handshake.RequestToSendXOnXOff }; } public SerialPort Port { get; } public string Find() { foreach (string portName in SerialPort.GetPortNames()) { Port.Write(\"Special code\"); if (Port.ReadByte == 120) return portName; } return null; } public override void Accept(IDeviceVisitor visitor) { visitor.Visit(this); } } public interface IDeviceVisitor { void Visit(BillDispenserEcdm billDispenser); void Visit(CoinDispenserCube4 coinDispenser); } public abstract class Device { public abstract void Accept(IDeviceVisitor visitor); } public class CloseCommandVisitor : IDeviceVisitor { public void Visit(BillDispenserEcdm billDispenser) { SerialPort port = billDispenser.Port; port.Write(new byte[] { 0x03 }, 0, 1); } public void Visit(CoinDispenserCube4 coinDispenser) { SerialPort port = coinDispenser.Port; port.Write(new byte[] { 0x12 }, 0, 1); } } public static class DeviceEx { public static void Close(this Device device) { var visitor = new CloseCommandVisitor(); device.Accept(visitor); } } public class Client { void Logic() { var device = new BillDispenserEcdm(); device.Close(); } } }","title":"Visitor pattern"},{"location":"Clean%20Code/Design%20Smell/Dependency%20Management/","text":"Most of the Design smells are caused by bad dependency management [[Object Oriented Programming]] languages can harness the power of [[dynamic dispatch]] Proper dependency management - key to a good [[architecture]]","title":"Dependency Management"},{"location":"Clean%20Code/Design%20Smell/Design%20smells/","text":"","title":"Design smells"},{"location":"Clean%20Code/Design%20Smell/Fragility/","text":"Small changes in one module cause [[bug]]s to appear in other modules. There is [[tight coupling]] between modules.","title":"Fragility"},{"location":"Clean%20Code/Design%20Smell/Immobility/","text":"Components can't be [[reused]] in other systems. There is [[tight coupling]] between components.","title":"Immobility"},{"location":"Clean%20Code/Design%20Smell/Needless%20Complexity/","text":"Developers are trying to forecast the future, introducing excessive [[points of extenstion]]. Developers should concentrate on the current [[requirements]], constructing the [[supple architecture]] which can bend to meet new requirements.","title":"Needless Complexity"},{"location":"Clean%20Code/Design%20Smell/Rigidity/","text":"The cost of making a single change is very high. There is [[tight coupling]] between modules.","title":"Rigidity"},{"location":"Clean%20Code/Design%20Smell/Viscosity/","text":"Adding a single feature evotes dealing with tons of aspects. There is [[tight coupling]] between components.","title":"Viscosity"},{"location":"Clean%20Code/Refactoring/How%20to%20refactor/","text":"It should be done as a series of small changes , where each makes the code slightly better while still leaving the program in working order. Checklist \u00b6 Code should become cleaner. If the code hasn't become cleaner - try to figure out what happened. Usually happens with a mix of small changes into one big change. Can also happen with extremely sloppy code. In this case it's worthwhile to thing about completely rewriting parts of the code. Before proceeding, it should be covered with [[tests]]. New functionality shouldn't be created during refactoring . All existing tests must pass after refactoring . Cases when tests break down after refactoring . You made an error, go fix it. Tests are too low-level. e.g. you were testing [[private method]]s. Refactor tests or write new set of [[higher-level tests]]. Write [[BDD-style tests]].","title":"How to refactor"},{"location":"Clean%20Code/Refactoring/How%20to%20refactor/#checklist","text":"Code should become cleaner. If the code hasn't become cleaner - try to figure out what happened. Usually happens with a mix of small changes into one big change. Can also happen with extremely sloppy code. In this case it's worthwhile to thing about completely rewriting parts of the code. Before proceeding, it should be covered with [[tests]]. New functionality shouldn't be created during refactoring . All existing tests must pass after refactoring . Cases when tests break down after refactoring . You made an error, go fix it. Tests are too low-level. e.g. you were testing [[private method]]s. Refactor tests or write new set of [[higher-level tests]]. Write [[BDD-style tests]].","title":"Checklist"},{"location":"Clean%20Code/Refactoring/Refactoring/","text":"What is refactoring? \u00b6 The main purpose is to fight Technical Debt , transform mess into clean and simple [[design]]. [[Clean code]]: - is obvious for other programmers. - doesn't contain duplication. - contains minimal number of moving parts - passes all tests - is easier and cheaper to maintain. How to refactor When to refactor","title":"What is refactoring?"},{"location":"Clean%20Code/Refactoring/Refactoring/#what-is-refactoring","text":"The main purpose is to fight Technical Debt , transform mess into clean and simple [[design]]. [[Clean code]]: - is obvious for other programmers. - doesn't contain duplication. - contains minimal number of moving parts - passes all tests - is easier and cheaper to maintain. How to refactor When to refactor","title":"What is refactoring?"},{"location":"Clean%20Code/Refactoring/Rule%20of%20Three/","text":"Rule of Three \u00b6 When you're doing something for the first time, just get it done. When you're doing something similar for the second time, cringe at having to repeat but do the same thing anyway. When you're doing something for the third time, start refactoring .","title":"Rule of Three"},{"location":"Clean%20Code/Refactoring/Rule%20of%20Three/#rule-of-three","text":"When you're doing something for the first time, just get it done. When you're doing something similar for the second time, cringe at having to repeat but do the same thing anyway. When you're doing something for the third time, start refactoring .","title":"Rule of Three"},{"location":"Clean%20Code/Refactoring/Technical%20Debt/","text":"What is technical debt? \u00b6 Technical debt is essentially unclean code. If you get a loan from bank - you can purchase things faster, but you have to pay for it. You can rack up so much interest that the amount of the interest exceeds our total income, making full repayment impossible. The same is with code. You temporarily speed up process without [[tests]]. Causes \u00b6 Business pressure. Need to roll out features before they're finished. Lack of understanding of the consequences of technical debt, Lack of tests. Encourages quick but risky workarounds. Lack of documentation. Slows down introduction of new people to the project. Lack of interaction between team members. People working on an outdated understanding of processes. Long-term simultaneous development in several branches. The more changes made in isolation, the greater technical debt. Delayed refactoring. Requirements are constantly changing. The longer refactoring is delayed, the more dependent code will have to be reworked in future. Lack of compliance monitoring. Everyone writes code as they see fit. Incompetence.","title":"What is technical debt?"},{"location":"Clean%20Code/Refactoring/Technical%20Debt/#what-is-technical-debt","text":"Technical debt is essentially unclean code. If you get a loan from bank - you can purchase things faster, but you have to pay for it. You can rack up so much interest that the amount of the interest exceeds our total income, making full repayment impossible. The same is with code. You temporarily speed up process without [[tests]].","title":"What is technical debt?"},{"location":"Clean%20Code/Refactoring/Technical%20Debt/#causes","text":"Business pressure. Need to roll out features before they're finished. Lack of understanding of the consequences of technical debt, Lack of tests. Encourages quick but risky workarounds. Lack of documentation. Slows down introduction of new people to the project. Lack of interaction between team members. People working on an outdated understanding of processes. Long-term simultaneous development in several branches. The more changes made in isolation, the greater technical debt. Delayed refactoring. Requirements are constantly changing. The longer refactoring is delayed, the more dependent code will have to be reworked in future. Lack of compliance monitoring. Everyone writes code as they see fit. Incompetence.","title":"Causes"},{"location":"Clean%20Code/Refactoring/When%20to%20refactor/","text":"When adding a feature \u00b6 Refactoring helps you to understand code. If you have to deal with someone else's code, [[refactor]] it first. Clean code is easier to understand, will improve for yourself and others. Makes it easier to add new features and make changes. When fixing a bug \u00b6 Bugs in code behave just like those in real life. [[Clean code]] and the [[error]]s will show up. Managers appreciate proactive refactoring as it eliminates the need for special refactoring tasks later. During a code review \u00b6 The [[code review]] may be the last chance to tidy up before it becomes available for the public.","title":"When to refactor"},{"location":"Clean%20Code/Refactoring/When%20to%20refactor/#when-adding-a-feature","text":"Refactoring helps you to understand code. If you have to deal with someone else's code, [[refactor]] it first. Clean code is easier to understand, will improve for yourself and others. Makes it easier to add new features and make changes.","title":"When adding a feature"},{"location":"Clean%20Code/Refactoring/When%20to%20refactor/#when-fixing-a-bug","text":"Bugs in code behave just like those in real life. [[Clean code]] and the [[error]]s will show up. Managers appreciate proactive refactoring as it eliminates the need for special refactoring tasks later.","title":"When fixing a bug"},{"location":"Clean%20Code/Refactoring/When%20to%20refactor/#during-a-code-review","text":"The [[code review]] may be the last chance to tidy up before it becomes available for the public.","title":"During a code review"},{"location":"Clean%20Code/SOLID/SOLID%20Principles/","text":"Solid principles are principles on how we should deal with Software Design . There can be different Design smells and it lists how to deal with them. SOLID Principles \u00b6 SRP - Single Responsibility Principle OCP - Open-Closed Principle LSP - Liskov Substitution Principle ISP - Interface Segregation Principle DIP - Dependency Inversion Principle These principles are not bound to any [[technology]], you can apply them to any language. SOLID is not a goal, it can't really be measured . Metaprinciples \u00b6 SOLID architecture and design DRY - Don't repeat yourself KISS - Keep it simple, stupid Separation Of Concerns YAGNI - You ain't gonna need it Command Query Separation Principle Law of Demeter Principle of least astonishment Encapsulation General principles building APIs SOLID vs YAGNI","title":"SOLID Principles"},{"location":"Clean%20Code/SOLID/SOLID%20Principles/#solid-principles","text":"SRP - Single Responsibility Principle OCP - Open-Closed Principle LSP - Liskov Substitution Principle ISP - Interface Segregation Principle DIP - Dependency Inversion Principle These principles are not bound to any [[technology]], you can apply them to any language. SOLID is not a goal, it can't really be measured .","title":"SOLID Principles"},{"location":"Clean%20Code/SOLID/SOLID%20Principles/#metaprinciples","text":"SOLID architecture and design DRY - Don't repeat yourself KISS - Keep it simple, stupid Separation Of Concerns YAGNI - You ain't gonna need it Command Query Separation Principle Law of Demeter Principle of least astonishment Encapsulation General principles building APIs SOLID vs YAGNI","title":"Metaprinciples"},{"location":"Clean%20Code/SOLID/SOLID%20architecture%20and%20design/","text":"Architecture \u00b6 Any software system has an [[architecture]] There's no \"the highest level of abstraction of a system\" Architecture has a social role (communicative) [[DBMS]] and [[UI]] are just tools, strive to make them irrelevant. Design \u00b6 Software design is about designing the individual modules or components What are the responsibilities, functions of module x? or class y? What can it do? what not? Architecture and Design Patterns \u00b6 [[Architecture patterns]] are standard forms of relationships between components that are deemed to be significant. [[2 tier architecture]] [[3 tier architecutre]] Service-Oriented Architecture (SOA) [[Event-Driven Architecture]] [[cloud architecture]] They can be split into - [[behavioral design patterns]], [[structural design patterns]] and [[creational design patterns]].","title":"SOLID architecture and design"},{"location":"Clean%20Code/SOLID/SOLID%20architecture%20and%20design/#architecture","text":"Any software system has an [[architecture]] There's no \"the highest level of abstraction of a system\" Architecture has a social role (communicative) [[DBMS]] and [[UI]] are just tools, strive to make them irrelevant.","title":"Architecture"},{"location":"Clean%20Code/SOLID/SOLID%20architecture%20and%20design/#design","text":"Software design is about designing the individual modules or components What are the responsibilities, functions of module x? or class y? What can it do? what not?","title":"Design"},{"location":"Clean%20Code/SOLID/SOLID%20architecture%20and%20design/#architecture-and-design-patterns","text":"[[Architecture patterns]] are standard forms of relationships between components that are deemed to be significant. [[2 tier architecture]] [[3 tier architecutre]] Service-Oriented Architecture (SOA) [[Event-Driven Architecture]] [[cloud architecture]] They can be split into - [[behavioral design patterns]], [[structural design patterns]] and [[creational design patterns]].","title":"Architecture and Design Patterns"},{"location":"Clean%20Code/SOLID/Software%20Design/","text":"What is design? \u00b6 Design of software is [[source code]] itself [[Architecture]] is the shape which code takes The [[source code]] specifies how software works Final product is a running program Building Software \u00b6 Big design upfront is very expensive in [[software development]] No guarantee that we take into account all the possible [[requirements]] Requirements tent to change very quickly Keep the design as clean as you can There can be multiple Design smells , you can deal with them using SOLID Principles .","title":"Software Design"},{"location":"Clean%20Code/SOLID/Software%20Design/#what-is-design","text":"Design of software is [[source code]] itself [[Architecture]] is the shape which code takes The [[source code]] specifies how software works Final product is a running program","title":"What is design?"},{"location":"Clean%20Code/SOLID/Software%20Design/#building-software","text":"Big design upfront is very expensive in [[software development]] No guarantee that we take into account all the possible [[requirements]] Requirements tent to change very quickly Keep the design as clean as you can There can be multiple Design smells , you can deal with them using SOLID Principles .","title":"Building Software"},{"location":"Clean%20Code/SOLID/Dependency%20Inversion%20Principle/Constructor%20Injection/","text":"Constructor Injection \u00b6 Protects the [[invariants]] Possible pitfall: Tends to accumulate many dependencies smell of Single Responsibility Principle violation, consider extracting a class Several dependencies tend to be passed in together: interface IDependency1 {} interface IDependency2 {} interface IDependency3 {} interface IDependency4 {} class ViewModel { public ViewModel( IDependency1 d1, IDependency2 d2, IDependency3 d3, IDependency4 d4) { } } class Infrastructure : IInfrastructure { public Infrastructure( IDependency1 d1, IDependency2 d2, IDependency3 d3, IDependency4 d4) { } } class ViewModel { public ViewModel(IInfrastructure infrastructure) { } } Non-obligatory dependencies public class Customer { private ILogger _logger = new Logger(); public Customer() {} public Customer(ILogger logger) { _logger = logger; } } 3rd party framework imposes a public [[default constructor]] public class Customer { private ICustomerRepo _repo; // have to expose public Customer() {} public Customer(ICustomerRepo repo) { _repo = repo; } } A certain dependency is used only in a single method use Method injection instead","title":"Constructor Injection"},{"location":"Clean%20Code/SOLID/Dependency%20Inversion%20Principle/Constructor%20Injection/#constructor-injection","text":"Protects the [[invariants]] Possible pitfall: Tends to accumulate many dependencies smell of Single Responsibility Principle violation, consider extracting a class Several dependencies tend to be passed in together: interface IDependency1 {} interface IDependency2 {} interface IDependency3 {} interface IDependency4 {} class ViewModel { public ViewModel( IDependency1 d1, IDependency2 d2, IDependency3 d3, IDependency4 d4) { } } class Infrastructure : IInfrastructure { public Infrastructure( IDependency1 d1, IDependency2 d2, IDependency3 d3, IDependency4 d4) { } } class ViewModel { public ViewModel(IInfrastructure infrastructure) { } } Non-obligatory dependencies public class Customer { private ILogger _logger = new Logger(); public Customer() {} public Customer(ILogger logger) { _logger = logger; } } 3rd party framework imposes a public [[default constructor]] public class Customer { private ICustomerRepo _repo; // have to expose public Customer() {} public Customer(ICustomerRepo repo) { _repo = repo; } } A certain dependency is used only in a single method use Method injection instead","title":"Constructor Injection"},{"location":"Clean%20Code/SOLID/Dependency%20Inversion%20Principle/Data-Centric%20Model/","text":"Data-Centric model \u00b6 Utils -----------| UI -------------|-> Database Business Logic -| The data and schema rule the world. Logic in [[SQL Stored procedures]]. [[SQL]] is suited for [[tuples processing]], not for modeling [[object relationships]].","title":"Data Centric Model"},{"location":"Clean%20Code/SOLID/Dependency%20Inversion%20Principle/Data-Centric%20Model/#data-centric-model","text":"Utils -----------| UI -------------|-> Database Business Logic -| The data and schema rule the world. Logic in [[SQL Stored procedures]]. [[SQL]] is suited for [[tuples processing]], not for modeling [[object relationships]].","title":"Data-Centric model"},{"location":"Clean%20Code/SOLID/Dependency%20Inversion%20Principle/Dependency%20Injection%20Techniques/","text":"","title":"Dependency Injection Techniques"},{"location":"Clean%20Code/SOLID/Dependency%20Inversion%20Principle/Dependency%20Injection/","text":"Dependency Injection (DI) is a set of Software Design principles and patterns that enable us to develop [[loosely coupled code]]. There are two ways of dealing with Dependency Injection:","title":"Dependency Injection"},{"location":"Clean%20Code/SOLID/Dependency%20Inversion%20Principle/Dependency%20Inversion%20Principle%20Common%20Smells%20and%20Violations/","text":"A [[class]] explicitly creates one or more dependencies hiding them from a client A class uses [[non-deterministic dependencies]] like [[DateTime]] or [[Random]] extract a class which works with non-deterministic dependencies and cover it by integration tests create an [[adapter]] A class uses [[static dependencies]], very often [[singleton]]s To remove the smells: - Extract a later of [[indirection]] and make high-level policies independent of low-level details - Adhere to the Single Responsibility Principle","title":"Dependency Inversion Principle Common Smells and Violations"},{"location":"Clean%20Code/SOLID/Dependency%20Inversion%20Principle/Dependency%20Inversion%20Principle%20Demo/","text":"This class uses 2 devices - accounter and fiscal registrator. These devices double the operations that needs to be made so to validate that there are no mistakes. This class checks whether there are any divergences between these 2 devices. namespace SOLID.DIP { public class GainDivergenceChecker { private Accounter _privateAccounter; private FiscalRegistrator _fr; public gainDivergenceChecker() { _privateAccounter = new Accounter(); _fr = new FiscalRegistrator(); } public bool HasDivergence() { decimal salesSum = _privateAccounter.GetSalesSum(); decimal sumOfReturnedTickets = _privateAccounter.GetSumOfReturnedTickets(); decimal salesSumByFiscalRegistrator = _fr.GetSalesSum(); decimal sumOfReturnedTicketsBytFiscalRegistrator = _fr.GetSumOfReturnedTickets(); return salesSum == salesSumByFiscalRegistrator && sumOfReturnedTickets == sumOfReturnedTicketsBytFiscalRegistrator; } private void ValidateDependencies(Accounter accounter, FiscalRegistrator fr) { if (accounter == null) throw new ArgumentNullException(\"accounter\"); if (fr == null) throw new ArgumentNullException(\"fr\"); } } internal class FiscalYearRegistrator { ... } internal class Accounter { ... } } If we wanted to add [[unit tests]] to this code, we couldn't really do that because it's tightly coupled with the FiscalYearRegistrator and Accounter class.","title":"Dependency Inversion Principle Demo"},{"location":"Clean%20Code/SOLID/Dependency%20Inversion%20Principle/Dependency%20Inversion%20Principle%20Refactorings%20-%20Dependency%20Injection/","text":"We are refactoring Dependency Inversion Principle Demo . public interface IFiscalRegistrator { decimal GetSalesSum(); decimal GetSumOfReturnedTickets(); } public class FiscalRegistrator : IFiscalRegistrator { public decimal GetSalesSum() { return 0; } public decimal GetSumOfReturnedTickets() { return 0; } } public interface IAccounter { decimal GetSalesSum(); decimal GetSumOfReturnedTickets(); } public class Accounter : IAccounter { public decimal GetSalesSum() { return 0; } public decimal GetSumOfReturnedTickets() { return 0; } } public class GainDivergenceChecker { // public IAccounter Accounter { get; set; } // public IFiscalRegistrator Fr { get; set; } private IAccounter _privateAccounter; private IFiscalRegistrator _fr; public GainDivergenceChecker(IAccounter accounter, IFiscalRegistrator fr) { _privateAccounter = accounter; _fr = fr; } // public bool HasDivergence(IAccounter accounter, IFiscalRegistrator fr) public bool HasDivergence () { return accounter.GetSalesSum() == fr.GetSalesSum() && accounter.GetSumOfReturnedTickets() == fr.GetSumOfReturnedTickets(); } ... } Now we can write a test. using NUnit.Frakework; using NUnit.Framework.Internal; namespace SOLID.UnitTests { [TestFixture] public class GainDivergenceCheckerTests { [Test] [TestCase(100, 100, 100, 100, true)] [TestCase(100, 200, 100, 200, true)] [TestCase(50, 100, 50, 100, true)] [TestCase(50, 100, 50, 50, false)] [TestCase(100, 100, 50, 50, false)] public void HasDivergence_ReturnsCorrectResult( decimal accounterSales, decimal accounterReturned, decimal frSales, decimal frReturned, bool expectResult ) { IAccounter accounter = new TestableAccounter() { SalesSum = accounterSales, SumOfReturnedTickets = accounterReturned, }; IFiscalRegistrator fr = new TestableFr() { SalesSum = frSales, SumOfReturnedTickets = frReturned, }; var checker = new GainDivergenceChecker(accounter, fr); bool result = checker.HasDivergence(); Assert.AreEqual(expectResult, result); } } public class TestableFr : IFiscalRegistrator { public decimal SalesSum { get; set; } public decimal SumOfReturnedTickets { get; set; } public decimal GetSalesSum() { return SalesSum; } public decimal GetSumOfReturnedTickets() { return SumOfReturnedTickets; } } public class TestableAccounter : IAccounter { public decimal SalesSum { get; set; } public decimal SumOfReturnedTickets { get; set; } public decimal GetSalesSum() { return SalesSum; } public decimal GetSumOfReturnedTickets() { return SumOfReturnedTickets; } } }","title":"Dependency Inversion Principle Refactorings   Dependency Injection"},{"location":"Clean%20Code/SOLID/Dependency%20Inversion%20Principle/Dependency%20Inversion%20Principle/","text":"Depependency Inversion Principle is a detailed version of Inversion Of Control (IoC) . Concretizes that \"High-level modules should not depend on low level modules\". Dependency Inversion Principle is all about [[decoupling]]. - Coupling indicates how dependent modules are on the inner working of each other. - Low coupling is often a sign of a well-structured computer system and a good Software Design , and when combined with [[high cohesion]], supports the general goals of [[high readability]] and [[maintainability]]. - DIP is applicable both at the [[source code]] and [[binary level]] - High-level modules should not depend on low-level modules. Both should depend on [[abstraction]]s. - [[Abstraction]]s should not depend on details. Details should depend on abstractions. Would you solder a lamp directly to the electrical outlet on your wall? You would only be able to use that socket for your lamp since they are tightly coupled. To fix the issue, we need to come up with a standard plug - then it can be plugged in the socket. Dependency Inversion Principle Demo Dependency Inversion Principle Refactorings - Dependency Injection Dependency Investion Principle - Architectural implications Dependency Inversion Principle Common Smells and Violations","title":"Dependency Inversion Principle"},{"location":"Clean%20Code/SOLID/Dependency%20Inversion%20Principle/Dependency%20Inversion%20Rules/","text":"Dependency Inversion Principle implies that high-level policies should not depend on low-level details Two major types of dependencies: stable and unstable (or volatile) Unstable dependencies are those to which we want to apply the inversion of control What Inversion Of Control (IoC) and Dependency Injection are and how they are related to the Dependency Inversion Principle and to each other 3 techniques of DI Constructor Injection , should be used 95% of the cases Property Injection Method injection Adhering to DIP leads to a [[plugin architecture]] which is known as the [[Ports and Adapters architecture]]. There should be a single place which knows everything about application dependencies and their relationships and this is the [[Main partition]]. Manual dependency injection may become tedious. That's why IoC-Container s exist, they help to simplify Dependency Injection in difficult cases.","title":"Dependency Inversion Rules"},{"location":"Clean%20Code/SOLID/Dependency%20Inversion%20Principle/Dependency%20Investion%20Principle%20-%20Architectural%20implications/","text":"By inverting dependencies you create [[bounds]] between software modules. [[Plugin]]s can be deployed and developed independently Divide the system by [[boundaries]] and invert the dependencies that cross those boundaries. Plugins define the boundaries of a system. ![[Domain-Centric Model]]","title":"Dependency Investion Principle   Architectural implications"},{"location":"Clean%20Code/SOLID/Dependency%20Inversion%20Principle/Dependency/","text":"Types of Dependencies \u00b6 [[Framework]] [[3rd party library]] [[External system]] like [[File System]], [[Database]], Any [[system resource]]. Dependency on a custom type built on top of the .NET framework Policy depends on Details \u00b6 UI -> Person -> PersonRepository High-level objects of the domain layer directly depend on low-level objects of the infrastructural layer. It's hard to replace coupled dependencies. We can solve any problem by introducing an extra level of indirection Policy doesn't depend on Details \u00b6 UI -> Person -> <interface> IPersonRepository <- PersonRepository IPersonRepository is a seam which inverts the dependencies. Volatile and Stable dependencies \u00b6 What dependencies should we abstract away? Volatile dependencies Dependency itself depends on the [[environment]] (web servers, db) Dependency doesn't yet exist and is still under development Dependency which is not installed on all machines of developers Dependencies has a [[nondeterministic behaviour]] ([[randomizer]]s inside) If none of these are true, a dependency is stable. Volatile dependencies are those which we want to abstract away by introducing levels of indirection.","title":"Dependency"},{"location":"Clean%20Code/SOLID/Dependency%20Inversion%20Principle/Dependency/#types-of-dependencies","text":"[[Framework]] [[3rd party library]] [[External system]] like [[File System]], [[Database]], Any [[system resource]]. Dependency on a custom type built on top of the .NET framework","title":"Types of Dependencies"},{"location":"Clean%20Code/SOLID/Dependency%20Inversion%20Principle/Dependency/#policy-depends-on-details","text":"UI -> Person -> PersonRepository High-level objects of the domain layer directly depend on low-level objects of the infrastructural layer. It's hard to replace coupled dependencies. We can solve any problem by introducing an extra level of indirection","title":"Policy depends on Details"},{"location":"Clean%20Code/SOLID/Dependency%20Inversion%20Principle/Dependency/#policy-doesnt-depend-on-details","text":"UI -> Person -> <interface> IPersonRepository <- PersonRepository IPersonRepository is a seam which inverts the dependencies.","title":"Policy doesn't depend on Details"},{"location":"Clean%20Code/SOLID/Dependency%20Inversion%20Principle/Dependency/#volatile-and-stable-dependencies","text":"What dependencies should we abstract away? Volatile dependencies Dependency itself depends on the [[environment]] (web servers, db) Dependency doesn't yet exist and is still under development Dependency which is not installed on all machines of developers Dependencies has a [[nondeterministic behaviour]] ([[randomizer]]s inside) If none of these are true, a dependency is stable. Volatile dependencies are those which we want to abstract away by introducing levels of indirection.","title":"Volatile and Stable dependencies"},{"location":"Clean%20Code/SOLID/Dependency%20Inversion%20Principle/Enlarged%20Domain-Centric%20Model/","text":"[[Ports and Adapters architecture]] [[Port]]s are seams we introduce by [[extracting interfaces]] [[Adapter]]s are [[plugin]]s which come from the [[boundary of a system]] Strive to keep the [[graph]] as flat as you can Who is responsible for keeping the control over the [[dependency instantiation]] and their [[lifetime]]? Answer: \"[[Main partition]] as the infrastructural point\". Conforms to [[Single Choice Principle]]. Only the [[main partition]] knows about dependencies and their relationships.","title":"Enlarged Domain Centric Model"},{"location":"Clean%20Code/SOLID/Dependency%20Inversion%20Principle/Inversion%20Of%20Control%20%28IoC%29/","text":"IoC reflects the model of relationships between a [[caller]] and a [[callee]]. A classic [[flow of control]] implies that a client has a full control over the [[environment]] and sequence of calls to library methods. IoC implies that a callee takes control over some calls between caller and callee. (callbacks is the simplest form) Frameworks rule the client's code. IoC can exist without Dependency Injection , buy the main way to perform the inversion of control is to apply Dependency Injection techniques.","title":"Inversion Of Control (IoC)"},{"location":"Clean%20Code/SOLID/Dependency%20Inversion%20Principle/IoC-Container/","text":"Use an Inversion Of Control (IoC) Container (also known as DI-Container). IoC-Container is a framework which helps to apply Dependency Injection . - Injects dependencies automatically - [[Dependency configuration]] - Knows everything about dependencies - Inversion Of Control (IoC) Container recursively creates all the required dependencies namespace SOLID.DIP { class SimpleIoC { private readonly Dictionary<Type, Type> _map = new Dictionary<Type, Type>(); public void Register(TFrom, TTo>() { _map.Add(typeof(TFrom), typeof(TTo)); } public T Resolve<T>() { return (T)Resolve(typeof(T)); } private object Resolve(Type type) { Type resolvedType = null; try { resolvedType = _map[type]; } catch { throw new ArgumentException($\"Couldn't resolve type {type}\"); } var ctor = resolvedType.GetConstructors().First(); var ctorParameters = ctor.GetParameters(); if (ctorParameters.Length == 0) { return Activator.CreateInstance(resolvedType); } var parameters = new List<object>(); foreach (var parameterToResolve in ctorParameters) { parameters.Add(Resolve(parameterToResolve.ParameterType)); } return ctor.Invoke(parameters.ToArray()); } } class Program { static void Main(string[] args) { SimpleIoC ioc = new SimpleIoC(); ioc.Register<MainViewModel, MainViewModel>(); ioc.Register<ICusomer, Customer>(); ioc.Register<ICustomerRepository, CustomerRepository>(); ioc.Register<IDbGateway, DbGateway>(); var mainViewModel = ioc.Resolve<MainViewModel>(); Console.ReadLine(); } } }","title":"IoC Container"},{"location":"Clean%20Code/SOLID/Dependency%20Inversion%20Principle/Method%20injection/","text":"Apply if only one method uses a Dependency or that dependency changes from one call to another. public interface ICurrencyRateProvider { int GetCurrencyRate(string currency); } public class PaymentService { public static Money CalculatePayment(ICurrencyRateProvider currencyRate) { return new Money(); } } Pitfalls: - Single Responsibility Principle violation - IoC-Container doesn't inject dependencies into methods","title":"Method injection"},{"location":"Clean%20Code/SOLID/Dependency%20Inversion%20Principle/Property%20Injection/","text":"Exposes dependencies by using properties via [[getters]] and [[setters]]. Allows replacing the Dependency during the [[lifetime]] of the object. The flexibility is a main advantage, but it should always be used only for [[optional dependencies]]. It can easily break the encapsulation and make dependencies hidden. Pitfalls: Breaks Encapsulation public class Customer { public Customer() {} public ILogger logger { get; set; } = new Logger(); }","title":"Property Injection"},{"location":"Clean%20Code/SOLID/Dependency%20Inversion%20Principle/Pure%20Dependency%20Injection/","text":"Manually create all the dependencies injecting them explicitly. class Program { static void Main(string[] args) { var dependency1 = new Dependency1(...); var extension = \"txt\"; var fileDateStore = new FileDateStore(dependency1, extension) ... } }","title":"Pure Dependency Injection"},{"location":"Clean%20Code/SOLID/Interface%20Segregation%20Principle/Interface%20Segregation%20Principle%20Common%20smells%20and%20Related%20Design%20Patterns/","text":"Liskov Substitution Principle violation smell often indicates a violation of Interface Segregation Principle Client's code references a [[class]] but uses only a small portion of it's [[API]] Fat [[interface]] -> segregate it Fat [[interface]] which is not under your control -> [[Facade design pattern]] Adapter Design Pattern Tips \u00b6 General algorithm of \"fixing\" fat interfaces Create narrower interface Fat interface inherits from that narrow interface Client uses the narrow interface Don't abuse ISP by creating tons of small interfaces","title":"Interface Segregation Principle Common smells and Related Design Patterns"},{"location":"Clean%20Code/SOLID/Interface%20Segregation%20Principle/Interface%20Segregation%20Principle%20Common%20smells%20and%20Related%20Design%20Patterns/#tips","text":"General algorithm of \"fixing\" fat interfaces Create narrower interface Fat interface inherits from that narrow interface Client uses the narrow interface Don't abuse ISP by creating tons of small interfaces","title":"Tips"},{"location":"Clean%20Code/SOLID/Interface%20Segregation%20Principle/Interface%20Segregation%20Principle%20Demo/","text":"Demo of the problem \u00b6 WPF APP -> Terminal1_COMServer -> BankTerminal1 -> Terminal2_COMServer -> BankTerminal2 -> Terminal3_COMServer -> BankTerminal3 namespace SOLID.ISP { public Interface IBankTerminal { void Start(); void Stop(); void Ping(); void BankHostTest(); void Purchase(decimal amount, string checkId); void CancelPayment(string checkId, decimal amount); void InterruptTransaction(); event EventHandler<PaymentOperationCompletedEventArgs> PaymentCompleted; event EventHandler<PaymentOperationCompletedEventArgs> CancellationCompleted; event EventHandler<TransactionCompletedEventArgs> TransactionCompleted; bool IsContactReaderOnPort(string comPort); bool IsNonContactReaderOnPort(string comPort); string FindContactReader(); string FindNonContactReader(); } public class TransactionCompletedEventArgs : EventArgs {} public class ZapTerminal : IBankTerminal { private ZapTerminalServiceCommunicator _service = new ZapTerminalServiceCommunicator(); public void Start() {} public void Stop() {} public void Ping() {} public void BankHostTest() {} event EventHandler<PaymentOperationCompletedEventArgs> PaymentCompleted; event EventHandler<PaymentOperationCompletedEventArgs> CancellationCompleted; event EventHandler<TransactionCompletedEventArgs> TransactionCompleted; bool IsContactReaderOnPort(string comPort) { return _service.IsContactReaderOnPort(comPort); } bool IsNonContactReaderOnPort(string comPort) { return _service.IsNonContactReaderOnPort(comPort); } string FindContactReader() { return _service.FindContactReader(); } string FindNonContactReader() { return _service.FindNonContactReader(); } } public class ZonTerminal : IBankTerminal { public void Start() {} public void Stop() {} public void Ping() {} public void BankHostTest() {} event EventHandler<PaymentOperationCompletedEventArgs> PaymentCompleted; event EventHandler<PaymentOperationCompletedEventArgs> CancellationCompleted; event EventHandler<TransactionCompletedEventArgs> TransactionCompleted; bool IsContactReaderOnPort(string comPort) { throw new NotImplementedException(); } bool IsNonContactReaderOnPort(string comPort) { throw new NotImplementedException(); } string FindContactReader() { throw new NotImplementedException(); } string FindNonContactReader() { throw new NotImplementedException(); } } public class PdqTerminal : IBankTerminal { private PdqTerminalServiceCommunicator _service = new PdqTerminalServiceCommunicator(); public void Start() {} public void Stop() {} public void Ping() {} public void BankHostTest() {} event EventHandler<PaymentOperationCompletedEventArgs> PaymentCompleted; event EventHandler<PaymentOperationCompletedEventArgs> CancellationCompleted; event EventHandler<TransactionCompletedEventArgs> TransactionCompleted; bool IsContactReaderOnPort(string comPort) { return _service.IsContactReaderOnPort(comPort); } bool IsNonContactReaderOnPort(string comPort) { return _service.IsNonContactReaderOnPort(comPort); } string FindContactReader() { return _service.FindContactReader(); } string FindNonContactReader() { return _service.FindNonContactReader(); } } } The Zon terminal is a black box terminal that physically works on it's own credit card readers. Zon doesn't expose an API. The other 2 terminals doesn't take responsibility for reading cards automatically. namespace SOLID.ISP { public class CardReadersCommunicatorViewModel { public CardReadersCommunicatorViewModel() {} public bool TestContactReaderOnPort(string port) { return false; } public bool TestNonContactReaderOnPort(string port) { return false; } public string FindContactReader() { return null; } } } This will be refactored in Interface Segregation Principle Refactorings Demo of the 2nd problem \u00b6 namespace SOLID.ISP.Config { public class Report { public string Generate() { return $\"Income: {AppConfig.Config.Income}\" + \"\\n\" + $\"Expenses: {AppConfig.Config.Expenses}\" + \"\\n\" + $\"Total Revenue: {AppConfig.Config.TotalRevenue}\"; } } [DataContract(Namespace = \"\")] public class AppConfig { private AppConfig(){...} [DataMember(IsRequired = true)] public string ServerId { get; set; } [DataMember(IsRequired = true)] public string ServerPort { get; set; } [DataMember(IsRequired = true)] public string LoggingSwitch { get; set; } [DataMember(IsRequired = true)] public int AppSkinId { get; set; } [DataMember(IsRequired = true)] public decimal Income { get; set; } [DataMember(IsRequired = true)] public decimal Expenses { get; set; } [DataMember(IsRequired = true)] public decimal TotalRevenue { get; set; } public static AppConfig Config { get; private set; } public static void Initialize() { using (Stream s = File.OpenRead(\"config.xml\")) { Config = (AppConfig) new DataContractSerializer(typeof(AppConfig)).ReadObject(s); } } } } In this example we have a problem that we have a [[configuration class]] that acts as [[singleton]] and we have a report class that is closely tied to it. If we would want to test it, we couldn't use [[unit tests]], but only [[integration tests]]. To solve this, we can use an [[interface]] for this config class. namespace SOLID.ISP.Config { public class Report { private readonly IAppConfig _appConfig; public Report(IAppConfig appConfig) { _appConfig = appConfig; } public string Generate() { return $\"Income: {_appConfig.Config.Income}\" + \"\\n\" + $\"Expenses: {_appConfig.Config.Expenses}\" + \"\\n\" + $\"Total Revenue: {_appConfig.Config.TotalRevenue}\"; } } public interface IAppConfig { string ServerId { get; set; } string ServerIP { get; set; } string ServerPort { get; set; } int LoggingSwitch { get; set; } int AppSkinId { get; set; } decimal Income { get; set; } decimal Expenses { get; set; } decimal TotalRevenue { get; set; } } [DataContract(Namespace = \"\")] public class AppConfig : IAppConfig { private AppConfig(){...} [DataMember(IsRequired = true)] public string ServerId { get; set; } [DataMember(IsRequired = true)] public string ServerPort { get; set; } [DataMember(IsRequired = true)] public string LoggingSwitch { get; set; } [DataMember(IsRequired = true)] public int AppSkinId { get; set; } [DataMember(IsRequired = true)] public decimal Income { get; set; } [DataMember(IsRequired = true)] public decimal Expenses { get; set; } [DataMember(IsRequired = true)] public decimal TotalRevenue { get; set; } public static AppConfig Config { get; private set; } public static void Initialize() { using (Stream s = File.OpenRead(\"config.xml\")) { Config = (AppConfig) new DataContractSerializer(typeof(AppConfig)).ReadObject(s); } } } } Now we can introduce an [[unit test]]: using NUnit.Framework; namespace SOLID.UnitTests { [TestFixture] public class ReportTests { [Test] public void Generate_ValidInput_GeneratesReport() { IAppConfig appConfig = new TestableAppConfig() { AppSkinId = 0, Income = 10, LoggingSwitch = 1, Expenses = 100, ServerIP = \"192.168.0.1\", ServerId = \"120888\", ServerPort = \"8080\", TotalRevenue = 1000, }; Report sut = new Report(appConfig); string report = sut.Generate(); Assert.AreEqual( \"Income: 10\\n Expenses: 100\\nTotal Revenue: 1000\" report); } } public class TestableAppConfig : IAppConfig { string ServerId { get; set; } string ServerIP { get; set; } string ServerPort { get; set; } int LoggingSwitch { get; set; } int AppSkinId { get; set; } decimal Income { get; set; } decimal Expenses { get; set; } decimal TotalRevenue { get; set; } } } This is still not ideal, we need to set all the properties of the class even though we don't actually use them. They can get added and we'll need to change the tests for them to work.","title":"Demo of the problem"},{"location":"Clean%20Code/SOLID/Interface%20Segregation%20Principle/Interface%20Segregation%20Principle%20Demo/#demo-of-the-problem","text":"WPF APP -> Terminal1_COMServer -> BankTerminal1 -> Terminal2_COMServer -> BankTerminal2 -> Terminal3_COMServer -> BankTerminal3 namespace SOLID.ISP { public Interface IBankTerminal { void Start(); void Stop(); void Ping(); void BankHostTest(); void Purchase(decimal amount, string checkId); void CancelPayment(string checkId, decimal amount); void InterruptTransaction(); event EventHandler<PaymentOperationCompletedEventArgs> PaymentCompleted; event EventHandler<PaymentOperationCompletedEventArgs> CancellationCompleted; event EventHandler<TransactionCompletedEventArgs> TransactionCompleted; bool IsContactReaderOnPort(string comPort); bool IsNonContactReaderOnPort(string comPort); string FindContactReader(); string FindNonContactReader(); } public class TransactionCompletedEventArgs : EventArgs {} public class ZapTerminal : IBankTerminal { private ZapTerminalServiceCommunicator _service = new ZapTerminalServiceCommunicator(); public void Start() {} public void Stop() {} public void Ping() {} public void BankHostTest() {} event EventHandler<PaymentOperationCompletedEventArgs> PaymentCompleted; event EventHandler<PaymentOperationCompletedEventArgs> CancellationCompleted; event EventHandler<TransactionCompletedEventArgs> TransactionCompleted; bool IsContactReaderOnPort(string comPort) { return _service.IsContactReaderOnPort(comPort); } bool IsNonContactReaderOnPort(string comPort) { return _service.IsNonContactReaderOnPort(comPort); } string FindContactReader() { return _service.FindContactReader(); } string FindNonContactReader() { return _service.FindNonContactReader(); } } public class ZonTerminal : IBankTerminal { public void Start() {} public void Stop() {} public void Ping() {} public void BankHostTest() {} event EventHandler<PaymentOperationCompletedEventArgs> PaymentCompleted; event EventHandler<PaymentOperationCompletedEventArgs> CancellationCompleted; event EventHandler<TransactionCompletedEventArgs> TransactionCompleted; bool IsContactReaderOnPort(string comPort) { throw new NotImplementedException(); } bool IsNonContactReaderOnPort(string comPort) { throw new NotImplementedException(); } string FindContactReader() { throw new NotImplementedException(); } string FindNonContactReader() { throw new NotImplementedException(); } } public class PdqTerminal : IBankTerminal { private PdqTerminalServiceCommunicator _service = new PdqTerminalServiceCommunicator(); public void Start() {} public void Stop() {} public void Ping() {} public void BankHostTest() {} event EventHandler<PaymentOperationCompletedEventArgs> PaymentCompleted; event EventHandler<PaymentOperationCompletedEventArgs> CancellationCompleted; event EventHandler<TransactionCompletedEventArgs> TransactionCompleted; bool IsContactReaderOnPort(string comPort) { return _service.IsContactReaderOnPort(comPort); } bool IsNonContactReaderOnPort(string comPort) { return _service.IsNonContactReaderOnPort(comPort); } string FindContactReader() { return _service.FindContactReader(); } string FindNonContactReader() { return _service.FindNonContactReader(); } } } The Zon terminal is a black box terminal that physically works on it's own credit card readers. Zon doesn't expose an API. The other 2 terminals doesn't take responsibility for reading cards automatically. namespace SOLID.ISP { public class CardReadersCommunicatorViewModel { public CardReadersCommunicatorViewModel() {} public bool TestContactReaderOnPort(string port) { return false; } public bool TestNonContactReaderOnPort(string port) { return false; } public string FindContactReader() { return null; } } } This will be refactored in Interface Segregation Principle Refactorings","title":"Demo of the problem"},{"location":"Clean%20Code/SOLID/Interface%20Segregation%20Principle/Interface%20Segregation%20Principle%20Demo/#demo-of-the-2nd-problem","text":"namespace SOLID.ISP.Config { public class Report { public string Generate() { return $\"Income: {AppConfig.Config.Income}\" + \"\\n\" + $\"Expenses: {AppConfig.Config.Expenses}\" + \"\\n\" + $\"Total Revenue: {AppConfig.Config.TotalRevenue}\"; } } [DataContract(Namespace = \"\")] public class AppConfig { private AppConfig(){...} [DataMember(IsRequired = true)] public string ServerId { get; set; } [DataMember(IsRequired = true)] public string ServerPort { get; set; } [DataMember(IsRequired = true)] public string LoggingSwitch { get; set; } [DataMember(IsRequired = true)] public int AppSkinId { get; set; } [DataMember(IsRequired = true)] public decimal Income { get; set; } [DataMember(IsRequired = true)] public decimal Expenses { get; set; } [DataMember(IsRequired = true)] public decimal TotalRevenue { get; set; } public static AppConfig Config { get; private set; } public static void Initialize() { using (Stream s = File.OpenRead(\"config.xml\")) { Config = (AppConfig) new DataContractSerializer(typeof(AppConfig)).ReadObject(s); } } } } In this example we have a problem that we have a [[configuration class]] that acts as [[singleton]] and we have a report class that is closely tied to it. If we would want to test it, we couldn't use [[unit tests]], but only [[integration tests]]. To solve this, we can use an [[interface]] for this config class. namespace SOLID.ISP.Config { public class Report { private readonly IAppConfig _appConfig; public Report(IAppConfig appConfig) { _appConfig = appConfig; } public string Generate() { return $\"Income: {_appConfig.Config.Income}\" + \"\\n\" + $\"Expenses: {_appConfig.Config.Expenses}\" + \"\\n\" + $\"Total Revenue: {_appConfig.Config.TotalRevenue}\"; } } public interface IAppConfig { string ServerId { get; set; } string ServerIP { get; set; } string ServerPort { get; set; } int LoggingSwitch { get; set; } int AppSkinId { get; set; } decimal Income { get; set; } decimal Expenses { get; set; } decimal TotalRevenue { get; set; } } [DataContract(Namespace = \"\")] public class AppConfig : IAppConfig { private AppConfig(){...} [DataMember(IsRequired = true)] public string ServerId { get; set; } [DataMember(IsRequired = true)] public string ServerPort { get; set; } [DataMember(IsRequired = true)] public string LoggingSwitch { get; set; } [DataMember(IsRequired = true)] public int AppSkinId { get; set; } [DataMember(IsRequired = true)] public decimal Income { get; set; } [DataMember(IsRequired = true)] public decimal Expenses { get; set; } [DataMember(IsRequired = true)] public decimal TotalRevenue { get; set; } public static AppConfig Config { get; private set; } public static void Initialize() { using (Stream s = File.OpenRead(\"config.xml\")) { Config = (AppConfig) new DataContractSerializer(typeof(AppConfig)).ReadObject(s); } } } } Now we can introduce an [[unit test]]: using NUnit.Framework; namespace SOLID.UnitTests { [TestFixture] public class ReportTests { [Test] public void Generate_ValidInput_GeneratesReport() { IAppConfig appConfig = new TestableAppConfig() { AppSkinId = 0, Income = 10, LoggingSwitch = 1, Expenses = 100, ServerIP = \"192.168.0.1\", ServerId = \"120888\", ServerPort = \"8080\", TotalRevenue = 1000, }; Report sut = new Report(appConfig); string report = sut.Generate(); Assert.AreEqual( \"Income: 10\\n Expenses: 100\\nTotal Revenue: 1000\" report); } } public class TestableAppConfig : IAppConfig { string ServerId { get; set; } string ServerIP { get; set; } string ServerPort { get; set; } int LoggingSwitch { get; set; } int AppSkinId { get; set; } decimal Income { get; set; } decimal Expenses { get; set; } decimal TotalRevenue { get; set; } } } This is still not ideal, we need to set all the properties of the class even though we don't actually use them. They can get added and we'll need to change the tests for them to work.","title":"Demo of the 2nd problem"},{"location":"Clean%20Code/SOLID/Interface%20Segregation%20Principle/Interface%20Segregation%20Principle%20Refactorings/","text":"We can refactor the Interface Segregation Principle Demo We can move the methods for the card reading from the IBankTerminal to a new interface. public interface IReadersCommunicable { bool isContactReaderOnPort(string comPort); bool IsNonContactReaderOnPort(string comPort); string FindContactReader(); string FindNonContactReader(); } Then we can delete the unimplemented methods from the ZonTerminal and implement the [[interface]] in the other terminals. public class ZapTerminal : IBankTerminal, IReadersCommunicable { private ZapTerminalServiceCommunicator _service = new ZapTerminalServiceCommunicator(); public void Start() {} public void Stop() {} public void Ping() {} public void BankHostTest() {} event EventHandler<PaymentOperationCompletedEventArgs> PaymentCompleted; event EventHandler<PaymentOperationCompletedEventArgs> CancellationCompleted; event EventHandler<TransactionCompletedEventArgs> TransactionCompleted; bool IsContactReaderOnPort(string comPort) { return _service.IsContactReaderOnPort(comPort); } bool IsNonContactReaderOnPort(string comPort) { return _service.IsNonContactReaderOnPort(comPort); } string FindContactReader() { return _service.FindContactReader(); } string FindNonContactReader() { return _service.FindNonContactReader(); } } public class ZonTerminal : IBankTerminal { public void Start() {} public void Stop() {} public void Ping() {} public void BankHostTest() {} event EventHandler<PaymentOperationCompletedEventArgs> PaymentCompleted; event EventHandler<PaymentOperationCompletedEventArgs> CancellationCompleted; event EventHandler<TransactionCompletedEventArgs> TransactionCompleted; } public class PdqTerminal : IBankTerminal, IReadersCommunicable { private PdqTerminalServiceCommunicator _service = new PdqTerminalServiceCommunicator(); public void Start() {} public void Stop() {} public void Ping() {} public void BankHostTest() {} event EventHandler<PaymentOperationCompletedEventArgs> PaymentCompleted; event EventHandler<PaymentOperationCompletedEventArgs> CancellationCompleted; event EventHandler<TransactionCompletedEventArgs> TransactionCompleted; bool IsContactReaderOnPort(string comPort) { return _service.IsContactReaderOnPort(comPort); } bool IsNonContactReaderOnPort(string comPort) { return _service.IsNonContactReaderOnPort(comPort); } string FindContactReader() { return _service.FindContactReader(); } string FindNonContactReader() { return _service.FindNonContactReader(); } } public class CardReadersCommunicatorViewModel { private readonly IReadersCommunicable _readersCommunicable; public CardReadersCommunicatorViewModel(IReadersCommunicable readersCommunicable) { _readersCommunicable = readersCommunicable; } public bool TestContactReaderOnPort(string port) { return _readersCommunicable.IsContactReaderOnPort(port); } public bool TestNonContactReaderOnPort(string port) { return _readersCommunicable.IsNonContactReaderOnPort(port); } string FindContactReader() { return _readersCommunicable.FindContactReader(); } string FindNonContactReader() { return _readersCommunicable.FindNonContactReader(); } } } We should segregate the configuration [[interface]]. The report class should only know the configuration related to the reports. namespace SOLID.ISP.Config { public class Report { private readonly IReportsConfig _reportsConfig; public Report(IReportsConfig reportsConfig) { _reportsConfig = reportsConfig; } public string Generate() { return $\"Income: {_reportsConfig.Income}\" + \"\\n\" + $\"Expenses: {_reportsConfig.Expenses}\" + \"\\n\" + $\"Total Revenue: {_reportsConfig.TotalRevenue\"; } } public interface IAppConfig { string ServerId { get; set; } string ServerIP { get; set; } string ServerPort { get; set; } int LoggingSwitch { get; set; } int AppSkinId { get; set; } } public interface IReportsConfig { decimal Income { get; set; } decimal Expenses { get; set; } decimal TotalRevenue { get; set; } } [DataContract(Namespace = \"\")] public class AppConfig : IAppConfig, IReportsConfig { private AppConfig(){...} [DataMember(IsRequired = true)] public string ServerId { get; set; } [DataMember(IsRequired = true)] public string ServerPort { get; set; } [DataMember(IsRequired = true)] public string LoggingSwitch { get; set; } [DataMember(IsRequired = true)] public int AppSkinId { get; set; } [DataMember(IsRequired = true)] public decimal Income { get; set; } [DataMember(IsRequired = true)] public decimal Expenses { get; set; } [DataMember(IsRequired = true)] public decimal TotalRevenue { get; set; } public static AppConfig Config { get; private set; } public static void Initialize() { using (Stream s = File.OpenRead(\"config.xml\")) { Config = (AppConfig) new DataContractSerializer(typeof(AppConfig)).ReadObject(s); } } } }","title":"Interface Segregation Principle Refactorings"},{"location":"Clean%20Code/SOLID/Interface%20Segregation%20Principle/Interface%20Segregation%20Principle/","text":"Interface Segregation Principle states that Clients should not be forced to depend on methods they do not use. Prefer small, [[cohesive]] [[interface]]s. Historical Background \u00b6 First public formulation belongs to [[Uncle Bob]] Uncle Bob applied Interface Segregation Principle working for Xerox That was a [[printing system]] A single job task contained fat list of tasks even though there was no use for them. [[Interface Segregation Principle Violations]] result in [[class]]es that depend on things they do not need, increasing [[coupling]] and reducing [[maintainability]]. Interface Segregation Principle Demo Interface Segregation Principle Refactorings Interface Segregation Principle Common smells and Related Design Patterns Single Responsiblility Principle vs Interface Segregation Principle","title":"Interface Segregation Principle"},{"location":"Clean%20Code/SOLID/Interface%20Segregation%20Principle/Interface%20Segregation%20Principle/#historical-background","text":"First public formulation belongs to [[Uncle Bob]] Uncle Bob applied Interface Segregation Principle working for Xerox That was a [[printing system]] A single job task contained fat list of tasks even though there was no use for them. [[Interface Segregation Principle Violations]] result in [[class]]es that depend on things they do not need, increasing [[coupling]] and reducing [[maintainability]]. Interface Segregation Principle Demo Interface Segregation Principle Refactorings Interface Segregation Principle Common smells and Related Design Patterns Single Responsiblility Principle vs Interface Segregation Principle","title":"Historical Background"},{"location":"Clean%20Code/SOLID/Liskov%20Substitution%20Principle/Contract/","text":"Contracts \u00b6 Programming to Contracts was elaborated by Bertrand Meyer \"Object-Oriented Software Construction\" by Meyer is recommended [[Eifel language]] What is Contract? \u00b6 Contracts have some [[semantic payload]] [[Interface]]s has no [[semantic payload]]s, they are not contracts Example: public abstract class CollectionContract<T> : IList<T> { public void Add(T item) { AddCore(item); count++; } public int Count { get { return count; } } protected abstract void AddCore(T item); private int count; ... } What constitutes a method's contract? \u00b6 Acceptable and unacceptable input values or types, and their meanings [[Return value]]s or [[Return type]]s, and their meanings [[Error]] and [[exception]] condition values or types that can occur, and their meanings [[Side effect]]s [[Precondition]]s [[Postcondition]]s [[Invariant]]s Example of Liskov Substitution Principle violation: - Different [[return type]]s - Different [[exception]]s - [[unused argument]]s public interface IBankTerminal { int ProcessPayment(decimal amount, string uniqueId); } public class BenkTerminal1 : IBankTerminal { private IBankTerminal1IPaymentGateway _gateway; // <returns> Response Code. Always >= 0</returns> public int ProcessPayment(decimal amount, string uniqueId) { // doesn't require uniqueId at all return (int)_gateway.ProcessPayment(amount); } public class BankTerminal2 : IBankTerminal { private IBankTerminal2IPaymentGateway _gateway; public int ProcessPayment(decimal amount, string uniqueId) { if(string.IsNullOrWhiteSpace(uniqueId)) { throw new ArgumentException(\"A client must provide a unique ID or BankTerminal2\"); } return _gateway.ProcessPayment(amount, uniqueId); } } } Code Contracts in [[Csharp]] \u00b6 You can write contracts in [[Csharp]] with a library called \"[[Code Contracts]]\". Harness the power of [[static code verification]] on correctness. [[Code Contracts]] library is not very popular though.","title":"Contracts"},{"location":"Clean%20Code/SOLID/Liskov%20Substitution%20Principle/Contract/#contracts","text":"Programming to Contracts was elaborated by Bertrand Meyer \"Object-Oriented Software Construction\" by Meyer is recommended [[Eifel language]]","title":"Contracts"},{"location":"Clean%20Code/SOLID/Liskov%20Substitution%20Principle/Contract/#what-is-contract","text":"Contracts have some [[semantic payload]] [[Interface]]s has no [[semantic payload]]s, they are not contracts Example: public abstract class CollectionContract<T> : IList<T> { public void Add(T item) { AddCore(item); count++; } public int Count { get { return count; } } protected abstract void AddCore(T item); private int count; ... }","title":"What is Contract?"},{"location":"Clean%20Code/SOLID/Liskov%20Substitution%20Principle/Contract/#what-constitutes-a-methods-contract","text":"Acceptable and unacceptable input values or types, and their meanings [[Return value]]s or [[Return type]]s, and their meanings [[Error]] and [[exception]] condition values or types that can occur, and their meanings [[Side effect]]s [[Precondition]]s [[Postcondition]]s [[Invariant]]s Example of Liskov Substitution Principle violation: - Different [[return type]]s - Different [[exception]]s - [[unused argument]]s public interface IBankTerminal { int ProcessPayment(decimal amount, string uniqueId); } public class BenkTerminal1 : IBankTerminal { private IBankTerminal1IPaymentGateway _gateway; // <returns> Response Code. Always >= 0</returns> public int ProcessPayment(decimal amount, string uniqueId) { // doesn't require uniqueId at all return (int)_gateway.ProcessPayment(amount); } public class BankTerminal2 : IBankTerminal { private IBankTerminal2IPaymentGateway _gateway; public int ProcessPayment(decimal amount, string uniqueId) { if(string.IsNullOrWhiteSpace(uniqueId)) { throw new ArgumentException(\"A client must provide a unique ID or BankTerminal2\"); } return _gateway.ProcessPayment(amount, uniqueId); } } }","title":"What constitutes a method's contract?"},{"location":"Clean%20Code/SOLID/Liskov%20Substitution%20Principle/Contract/#code-contracts-in-csharp","text":"You can write contracts in [[Csharp]] with a library called \"[[Code Contracts]]\". Harness the power of [[static code verification]] on correctness. [[Code Contracts]] library is not very popular though.","title":"Code Contracts in [[Csharp]]"},{"location":"Clean%20Code/SOLID/Liskov%20Substitution%20Principle/Covariance/","text":"Assuming the type A can be cast to type B, type X is covariant in case X\\<A> can be cast to X\\<B>. For example IBar\\<T> is covariant to T, if the following is true:. IBar<string> s = ...; IBar<object> o = s; // compiles class Animal {} class Dog : Animal {} class Cat : Animal {} public class Stack<T> { int position; T[] data = new T[100]; public void Push(T val) { data[position++] = val; } public T Pop() { return data[--position]; } } Stack<Dog> dogs = new Stack<Dog>(); Stack<Animal> animals = dogs; // compilation error // for preventing such code: animals.Push(new Cat()); // adding cat to dogs. Dog[] dogs = new Dog[10]; Animal[] animals = dogs; animals[0] = new Cat(); // runtime exception will occur","title":"Covariance"},{"location":"Clean%20Code/SOLID/Liskov%20Substitution%20Principle/Liskov%20Substitution%20Principle%20Demo/","text":"[[Object Oriented Programming]] languages can't directly map the relationships between objects in the real world into the same model of relationships between them in code. [[Child class]]es implement IS-A relationship with base classes - naive statement of [[Object Oriented Programming]]. The problem \u00b6 Implement rectangle and square. Obviously Square implements Rectangle. namespace SOLID.LSP.Violation { public class Rectangle { public int Width { get; set; } public int Height { get; set; } } public class Square : Rectangle { } public class AreaCalculator { public static int CalcSquare(Square square) => square.Height * square.Height; public static int CalcRectangle(Rectangle square) => square.Height * square.Width; } class EntryPoint { static void SuperMain(string[] args) { Rectangle rect = new Rectangle() { Width = 2, Height = 5 }; int rectArea = AreaCalculator.CalcRectangle(rect); Console.WriteLine($\"Rectangle Area = {rectArea}\"); Rectangle square = new Square { Height = 2, Width = 10 }; int squareArea = AreaCalculator.CalcRectangle(square); Console.WriteLine($\"Square Area = {squareArea}\"); } } } The problem is that we can specify different height and with for a square, therefore the you would need to implement additional business logic for that and that solution is a violation of Liskov Substitution Principle since the Rectangle is not substitutable by square. public static int CalcArea(Rectangle rect) { if (rect is Square) { return rect.Height * rect.Height; } return rect.Height * rect.Width; } This will be refactored in like so: namespace SOLID.LSP.Fixed { public interface IShape { int CalculateArea(); } public class Rectangle : IShape { public int Width { get; set; } public int Height { get; set; } public int CalculateArea() => Width * Height; } public class Square : IShape { public int SideLength { get; set; } public int CalculateArea() => SideLength * SideLength; } class EntryPoint { static void SuperMain(string[] args) { IShape rect = new Rectangle { Height = 2, Width = 5 }; int rectArea = rect.CalculateArea(); Console.WriteLine($\"Rectangle Area = {rectArea}\"); /// IShape square = new Square() { SideLength = 10 }; int squareArea = square.CalculateArea(); Console.WriteLine($\"Square Area = {squareArea}\"); } } }","title":"Liskov Substitution Principle Demo"},{"location":"Clean%20Code/SOLID/Liskov%20Substitution%20Principle/Liskov%20Substitution%20Principle%20Demo/#the-problem","text":"Implement rectangle and square. Obviously Square implements Rectangle. namespace SOLID.LSP.Violation { public class Rectangle { public int Width { get; set; } public int Height { get; set; } } public class Square : Rectangle { } public class AreaCalculator { public static int CalcSquare(Square square) => square.Height * square.Height; public static int CalcRectangle(Rectangle square) => square.Height * square.Width; } class EntryPoint { static void SuperMain(string[] args) { Rectangle rect = new Rectangle() { Width = 2, Height = 5 }; int rectArea = AreaCalculator.CalcRectangle(rect); Console.WriteLine($\"Rectangle Area = {rectArea}\"); Rectangle square = new Square { Height = 2, Width = 10 }; int squareArea = AreaCalculator.CalcRectangle(square); Console.WriteLine($\"Square Area = {squareArea}\"); } } } The problem is that we can specify different height and with for a square, therefore the you would need to implement additional business logic for that and that solution is a violation of Liskov Substitution Principle since the Rectangle is not substitutable by square. public static int CalcArea(Rectangle rect) { if (rect is Square) { return rect.Height * rect.Height; } return rect.Height * rect.Width; } This will be refactored in like so: namespace SOLID.LSP.Fixed { public interface IShape { int CalculateArea(); } public class Rectangle : IShape { public int Width { get; set; } public int Height { get; set; } public int CalculateArea() => Width * Height; } public class Square : IShape { public int SideLength { get; set; } public int CalculateArea() => SideLength * SideLength; } class EntryPoint { static void SuperMain(string[] args) { IShape rect = new Rectangle { Height = 2, Width = 5 }; int rectArea = rect.CalculateArea(); Console.WriteLine($\"Rectangle Area = {rectArea}\"); /// IShape square = new Square() { SideLength = 10 }; int squareArea = square.CalculateArea(); Console.WriteLine($\"Square Area = {squareArea}\"); } } }","title":"The problem"},{"location":"Clean%20Code/SOLID/Liskov%20Substitution%20Principle/Liskov%20Substitution%20Principle%20Rules/","text":"The Liskov Substitution Principle states that Subtypes must be substitutable for their base types. Do not violate a Contract by either [[strenghtening precondition]]s or [[weakening postcondition]]s; Do not violate Covariance /[[Contravariance]], explicitly mark the [[generic parameters]] by [[in-out keywords]] if possible. Liskov Substitution Princple Common Smells","title":"Liskov Substitution Principle Rules"},{"location":"Clean%20Code/SOLID/Liskov%20Substitution%20Principle/Liskov%20Substitution%20Principle%20Violations/","text":"Liskov Substitution Princple Common Smells Variance in C \u00b6 Starting from C# 4, generic interfaces allow variance though special keywords \"in\" and \"out\". Generic classes at the same time don't allow variance. The \"out\" keyword guarantees that inside the implementation of that interface, a generic parameter can only be used in the return statments. interface IPoppable<out T< { T Pop(); } Compiles and absolutely correct: var dogs = new Stack<Dog>(); dogs.Push(new Dog()); IPoppable<Animal> animals = dogs; // allowed The \"in\" keyword guarantees that inside the implementation of that interface, a generic parameter can only be used as the input. interface IPushable<in T> { void Push(T val); } Compiles and absolutely correct: IPushable<Animal> animals = new Stack<Animal>(); IPushable<Cat> cats = animals; // allowed cats.Push(new Cat()); Downcasts is a smell \u00b6 void PayBonus(Person p) { Customer c = (Customer)p; // downcast // or if (p is Person) { Customer c = (Customer)p; } } ## Downcasts are not always the smell Downcasts are allowed if we're absolutely sure about the type you're about to downcast to. ```csharp public void PayCashBack(int customerId, decimal amount) { Customer c = repository.GetCustomer(customerId); var pv = c as PrivilegedCustomer; if (pc != null { pc.AddMoneyToAccount(amount); } else { throw new ArgumentException(\"PayCashback on a regular customer\"); } }","title":"Liskov Substitution Principle Violations"},{"location":"Clean%20Code/SOLID/Liskov%20Substitution%20Principle/Liskov%20Substitution%20Principle%20Violations/#variance-in-c","text":"Starting from C# 4, generic interfaces allow variance though special keywords \"in\" and \"out\". Generic classes at the same time don't allow variance. The \"out\" keyword guarantees that inside the implementation of that interface, a generic parameter can only be used in the return statments. interface IPoppable<out T< { T Pop(); } Compiles and absolutely correct: var dogs = new Stack<Dog>(); dogs.Push(new Dog()); IPoppable<Animal> animals = dogs; // allowed The \"in\" keyword guarantees that inside the implementation of that interface, a generic parameter can only be used as the input. interface IPushable<in T> { void Push(T val); } Compiles and absolutely correct: IPushable<Animal> animals = new Stack<Animal>(); IPushable<Cat> cats = animals; // allowed cats.Push(new Cat());","title":"Variance in C"},{"location":"Clean%20Code/SOLID/Liskov%20Substitution%20Principle/Liskov%20Substitution%20Principle%20Violations/#downcasts-is-a-smell","text":"void PayBonus(Person p) { Customer c = (Customer)p; // downcast // or if (p is Person) { Customer c = (Customer)p; } } ## Downcasts are not always the smell Downcasts are allowed if we're absolutely sure about the type you're about to downcast to. ```csharp public void PayCashBack(int customerId, decimal amount) { Customer c = repository.GetCustomer(customerId); var pv = c as PrivilegedCustomer; if (pc != null { pc.AddMoneyToAccount(amount); } else { throw new ArgumentException(\"PayCashback on a regular customer\"); } }","title":"Downcasts is a smell"},{"location":"Clean%20Code/SOLID/Liskov%20Substitution%20Principle/Liskov%20Substitution%20Principle/","text":"If S is a subtype of T then objects of type T may be replaced with objects with type S, without breaking the problem. The liskov Substitution states that [[Subtype]]s must be [[substitutable]] for their base types. Example: - Class called Client uses an interface B. Classes A and C inherit this interface. Class doesn't care if you pass in the class A or C since all it sees is interface B. Duck typing in dynamic languages: - If a bird swims like a duck, it quacks like a duck then I call that bird a duck. Ways of breaking substitutability: - Violating a Contract - Violating Covariance /[[Contravariance]] Contract Liskov Substitution Principle Demo Liskov Substitution Principle Violations Liskov Substitution Princple Common Smells","title":"Liskov Substitution Principle"},{"location":"Clean%20Code/SOLID/Liskov%20Substitution%20Principle/Liskov%20Substitution%20Princple%20Common%20Smells/","text":"Method throws [[NotSupportedException]] Empty or [[degenerative implementation]] [[Downcast]]s Tips \u00b6 Liskov Substitution Principle is often the result of Open-Closed Principle and Interface Segregation Principle violations If two [[class]]es share some logic and they are not substitutable Create new [[base class]] Inherit those two classes from a base class Ensure that they are substitutable with the new base class","title":"Liskov Substitution Princple Common Smells"},{"location":"Clean%20Code/SOLID/Liskov%20Substitution%20Principle/Liskov%20Substitution%20Princple%20Common%20Smells/#tips","text":"Liskov Substitution Principle is often the result of Open-Closed Principle and Interface Segregation Principle violations If two [[class]]es share some logic and they are not substitutable Create new [[base class]] Inherit those two classes from a base class Ensure that they are substitutable with the new base class","title":"Tips"},{"location":"Clean%20Code/SOLID/Metaprinciples/Command%20Query%20Separation%20Principle/","text":"Every [[method]] should either be a [[command]] that performs an action, or a [[query]] that returns data to the caller, but not both. In other words, asking a question should not change the answer. There are two major types of functions: - Functions which perform commands - Functions which perform a query and return a result If we have a function that logs you in and returns the result. We cannot really say if the user was already logged in - etc. public bool LogOn(string username, string password) { } if (LogOn(\"admin\", \"qwerty\")) { } We should implement this way: public void LogOn(string username, string password) { } public bool IsLoggedOn(string username, string password) { }","title":"Command Query Separation Principle"},{"location":"Clean%20Code/SOLID/Metaprinciples/DRY%20-%20Don%27t%20repeat%20yourself/","text":"DRY - Don't repeat yourself \u00b6 A substantial number of [[bug]]s in software are caused by [[repetitive code]]. Every piece of knowledge must have a single [[unambiguous representation]] in the system. Common violations of DRY \u00b6 Magic value (number, string, etc) Duplicate logic in multiple locations","title":"DRY - Don't repeat yourself"},{"location":"Clean%20Code/SOLID/Metaprinciples/DRY%20-%20Don%27t%20repeat%20yourself/#dry-dont-repeat-yourself","text":"A substantial number of [[bug]]s in software are caused by [[repetitive code]]. Every piece of knowledge must have a single [[unambiguous representation]] in the system.","title":"DRY - Don't repeat yourself"},{"location":"Clean%20Code/SOLID/Metaprinciples/DRY%20-%20Don%27t%20repeat%20yourself/#common-violations-of-dry","text":"Magic value (number, string, etc) Duplicate logic in multiple locations","title":"Common violations of DRY"},{"location":"Clean%20Code/SOLID/Metaprinciples/Duplicate%20logic%20in%20multiple%20locations/","text":"Repeated \"if-then\" logic or multiple [[switch case]]s scattered though-out the [[code base]]. Bad: public enum Shape { Circle, Rectangle, Square, Hexagon } public class Visualizer { public void Draw(Shape shape) { switch(shape) { case Shape.Circle: break; case Shape.Hexagon: break; case Shape.Rectangle: break; case Shape.Square: break; } } public decimal CalculateArea(Shape shape) { ... switch case } } Good: abstract class Shape { abstract void Draw(); abstract decimal CalculateArea(); } public class Rectangle : Shape { public override void Draw() { } public override decimal CalculateArea() { } } public class Circle : Shape { public override void Draw() { } public override decimal CalculateArea() { } }","title":"Duplicate logic in multiple locations"},{"location":"Clean%20Code/SOLID/Metaprinciples/Encapsulation/","text":"Information hiding \u00b6 Information hiding is the principle of [[segregation]], of the design decisions in a computer program that are most likely to change, thus protecting other parts of the program from extensive modification if the design decision is changed. Information hiding is the ability to prevent certain aspects of a class or software component from being accessible to it's clients, using either programming language features (like private variables) or an explicit exporting policy. Encapsulation \u00b6 Encapsulation allows to reuse components without learning their internal details. class Customer { public event EventHandler<Customer> CustomerReceived; public Resuilt PaySalary(decimal amount) { return Result.Success(); } public Maybe<Customer> GetCustomer(int id) { // get instance from DB var customer = new Customer(); return Maybe<Customer>.From(customer); } public Result RemoveCustomer(int id) { return Result.Success(); } }","title":"Encapsulation"},{"location":"Clean%20Code/SOLID/Metaprinciples/Encapsulation/#information-hiding","text":"Information hiding is the principle of [[segregation]], of the design decisions in a computer program that are most likely to change, thus protecting other parts of the program from extensive modification if the design decision is changed. Information hiding is the ability to prevent certain aspects of a class or software component from being accessible to it's clients, using either programming language features (like private variables) or an explicit exporting policy.","title":"Information hiding"},{"location":"Clean%20Code/SOLID/Metaprinciples/Encapsulation/#encapsulation","text":"Encapsulation allows to reuse components without learning their internal details. class Customer { public event EventHandler<Customer> CustomerReceived; public Resuilt PaySalary(decimal amount) { return Result.Success(); } public Maybe<Customer> GetCustomer(int id) { // get instance from DB var customer = new Customer(); return Maybe<Customer>.From(customer); } public Result RemoveCustomer(int id) { return Result.Success(); } }","title":"Encapsulation"},{"location":"Clean%20Code/SOLID/Metaprinciples/General%20principles%20building%20APIs/","text":"API Intro \u00b6 [[API]] (Application Programming Interface) - set of functionality Types of APIs [[Private API]] (zoo) [[Public API]] (wilderness) Characteristics Simplicity Rule of thumb: \"you can always add, but never remove\". Compromise between power and simplicity: when power of an API grows, it's simplicity degrades. The only way to understand whether an API is simple or not is to estimate time spent on understanding by it's users. Expressiveness and Compromises Resources which can be allocated on API development are always limited [[API]] it is almost impossible to create [[universal API]]s [[API]] developers have to implement first things first [[Extensibility]] Reflects the capabilities to increase the power of an [[API]] without big rewrites You should be able to add new functionality and preserve backwards compatibility Open-Closed Principle In [[public API]]s we should at first preserve the [[backwards compatibility]] (if any doubts regarding a new [[API member]] - don't introduce it) [[Consitency]] [[API]] has to be logical and consistent: design decisions - strongly opinionated! Example of poor consistency in the PHP String library: str_repeat strcmp str_cplit strlen str_word_count strrev Public vs Private API \u00b6 The cost of bad decisions in public APIs may be extremely high Private APIs should be developed bearing in mind all API characteristics. Zookeepers must strive to become rangers. API Development principles \u00b6 APIs should be as simple as possible, but no simpler. A good API should allow to do a lot without learning a lot. APIs should be based on [[use case]]s. Imagine that you're a client of that API sketch API as soon as possible Provide low barrier for using an API. In practice it means that you always: Should provide the simplest constructors with default values of other required parameters. Should throw [[exception]]s with messages which explain what to do to fix the problem. Shouldn't require from clients to explicitly create more than one type for accomplishing main [[use case]]s. Shouldn't require from clients to perform a [[wide initialization]] of an object. Build self-explanatory APIs Provide a decent [[documentation]]","title":"General principles building APIs"},{"location":"Clean%20Code/SOLID/Metaprinciples/General%20principles%20building%20APIs/#api-intro","text":"[[API]] (Application Programming Interface) - set of functionality Types of APIs [[Private API]] (zoo) [[Public API]] (wilderness) Characteristics Simplicity Rule of thumb: \"you can always add, but never remove\". Compromise between power and simplicity: when power of an API grows, it's simplicity degrades. The only way to understand whether an API is simple or not is to estimate time spent on understanding by it's users. Expressiveness and Compromises Resources which can be allocated on API development are always limited [[API]] it is almost impossible to create [[universal API]]s [[API]] developers have to implement first things first [[Extensibility]] Reflects the capabilities to increase the power of an [[API]] without big rewrites You should be able to add new functionality and preserve backwards compatibility Open-Closed Principle In [[public API]]s we should at first preserve the [[backwards compatibility]] (if any doubts regarding a new [[API member]] - don't introduce it) [[Consitency]] [[API]] has to be logical and consistent: design decisions - strongly opinionated! Example of poor consistency in the PHP String library: str_repeat strcmp str_cplit strlen str_word_count strrev","title":"API Intro"},{"location":"Clean%20Code/SOLID/Metaprinciples/General%20principles%20building%20APIs/#public-vs-private-api","text":"The cost of bad decisions in public APIs may be extremely high Private APIs should be developed bearing in mind all API characteristics. Zookeepers must strive to become rangers.","title":"Public vs Private API"},{"location":"Clean%20Code/SOLID/Metaprinciples/General%20principles%20building%20APIs/#api-development-principles","text":"APIs should be as simple as possible, but no simpler. A good API should allow to do a lot without learning a lot. APIs should be based on [[use case]]s. Imagine that you're a client of that API sketch API as soon as possible Provide low barrier for using an API. In practice it means that you always: Should provide the simplest constructors with default values of other required parameters. Should throw [[exception]]s with messages which explain what to do to fix the problem. Shouldn't require from clients to explicitly create more than one type for accomplishing main [[use case]]s. Shouldn't require from clients to perform a [[wide initialization]] of an object. Build self-explanatory APIs Provide a decent [[documentation]]","title":"API Development principles"},{"location":"Clean%20Code/SOLID/Metaprinciples/KISS%20-%20Keep%20it%20simple%2C%20stupid/","text":"KISS - Keep it simple, stupid \u00b6 \"Make everything as simple as possible, but not simpler\" - Alber Einstein Simplicity is a key goal in [[design]] YAGNI - You ain't gonna need it is about removing unnecessary code; KISS is about making the simplest implementation. A simple solution is better than a complex one, even if the solution looks stupid Achieving Simplicity \u00b6 Main technique is [[Decomposition]] Decomposition underlies all the SOLID Principles SOLID are aimed at achieving simplest solutions; Abusing SOLID leads to unnecessary [[complexity]] (Unity and Struggle of [[Opposites law]]). Prefer [[composition]] over [[inheritance]] where possible. Stick with if-else and switch-case statements until you see that you need to introduce polymorphism. Avoid [[preemptive optimization]]s. In 90% of cases slower solutions work enough fast. The exceptions: app main aspect of which is the performance. Smaller classes and smaller methods are better. The best method is a one-liner. [[Extract till you drop technique]]. Don't rush to extract utility classes for private methods which are used from a single place within a class, leave it as it is until the other parts of code will require that method as well. Don't write [[parameterized general methods]], prefer methods which solve a specific problem. 01-what-is-divide-and-conquer Strive to avoid [[comments]] Write [[prototype]]s and don't be afraid to throw them away Keep the number of entities which solve a problem roughly from 5 to 7 Constantly work on simplifying your [[code base]]. This is the [[rule of a boy scout]]. Keep the amount of optimized code closer to 5-10% Accidental & Essential Complexity \u00b6 [[Complexity]] imposed by the [[domain]] itself is called the \"[[essential complexity]]\" \"[[Accidental complexity]]\" is the complexity of our solutions which are intended to solve the problems of the domain. Simplicity \u00b6 Two values of software: - Correctness - Good [[design]]","title":"KISS - Keep it simple, stupid"},{"location":"Clean%20Code/SOLID/Metaprinciples/KISS%20-%20Keep%20it%20simple%2C%20stupid/#kiss-keep-it-simple-stupid","text":"\"Make everything as simple as possible, but not simpler\" - Alber Einstein Simplicity is a key goal in [[design]] YAGNI - You ain't gonna need it is about removing unnecessary code; KISS is about making the simplest implementation. A simple solution is better than a complex one, even if the solution looks stupid","title":"KISS - Keep it simple, stupid"},{"location":"Clean%20Code/SOLID/Metaprinciples/KISS%20-%20Keep%20it%20simple%2C%20stupid/#achieving-simplicity","text":"Main technique is [[Decomposition]] Decomposition underlies all the SOLID Principles SOLID are aimed at achieving simplest solutions; Abusing SOLID leads to unnecessary [[complexity]] (Unity and Struggle of [[Opposites law]]). Prefer [[composition]] over [[inheritance]] where possible. Stick with if-else and switch-case statements until you see that you need to introduce polymorphism. Avoid [[preemptive optimization]]s. In 90% of cases slower solutions work enough fast. The exceptions: app main aspect of which is the performance. Smaller classes and smaller methods are better. The best method is a one-liner. [[Extract till you drop technique]]. Don't rush to extract utility classes for private methods which are used from a single place within a class, leave it as it is until the other parts of code will require that method as well. Don't write [[parameterized general methods]], prefer methods which solve a specific problem. 01-what-is-divide-and-conquer Strive to avoid [[comments]] Write [[prototype]]s and don't be afraid to throw them away Keep the number of entities which solve a problem roughly from 5 to 7 Constantly work on simplifying your [[code base]]. This is the [[rule of a boy scout]]. Keep the amount of optimized code closer to 5-10%","title":"Achieving Simplicity"},{"location":"Clean%20Code/SOLID/Metaprinciples/KISS%20-%20Keep%20it%20simple%2C%20stupid/#accidental-essential-complexity","text":"[[Complexity]] imposed by the [[domain]] itself is called the \"[[essential complexity]]\" \"[[Accidental complexity]]\" is the complexity of our solutions which are intended to solve the problems of the domain.","title":"Accidental &amp; Essential Complexity"},{"location":"Clean%20Code/SOLID/Metaprinciples/KISS%20-%20Keep%20it%20simple%2C%20stupid/#simplicity","text":"Two values of software: - Correctness - Good [[design]]","title":"Simplicity"},{"location":"Clean%20Code/SOLID/Metaprinciples/Law%20of%20Demeter/","text":"Law of Demeter (LoD) or principle of least knowledge is a [[design guideline]] for developing software, particularly in [[Object Oriented Programming]]. Each unit should have only limited knowledge about other units - only other units \"closely\" related to the current unit. Each unit should only talk to it's friends; not talk to strangers. A method of an object may only call methods of: - The [[object]] itself - An [[argument]] of the [[method]] - Any object created within the method - Any direct properties/fields of the object Let's pretend that we should model the business relationships between a paperboy and a customer who wants to buy magazines. A paperboy rings the doorbell, a customer opens it. The paperboy somehow has to be paid and then hand over the magazine to the customer. namespace SOLID.DemeterLaw.Before { public class Customer { public Customer(string firstName, string lastName) { FirstName = firstName; LastName = lastName; Wallet = new Wallet(1000m); } public string FirstName { get; } public string LastName { get; } public Wallet Wallet { get; } } public class Wallet { public Wallet(decimal moneyAmount) { MoneyAmount = moneyAmount; } public decimal MoneyAmount { get; private set; } public void AddMoney(decimal amount) { MoneyAmount += amount; } public void WithdrawMoney(decimal amount) { MoneyAmount -= amount; } } public class PaperBoy { public void DeliverMagazine(decimal costOfMagazine, Customer customer) { Wallet w = customer.Wallet; if (w.MoneyAmount > costOfMagazine) { w.WithdrawMoney(costOfMagazine); } else { // come back later } } } } We can improve this. This version better models the real world scenario. The paperboy doesn't have access to the wallet. The wallet class also now change and the paperboy is isolated from it. namespace SOLID.DemeterLaw.Before { public class Customer { public Customer(string firstName, string lastName) { FirstName = firstName; LastName = lastName; Wallet = new Wallet(1000m); } public string FirstName { get; } public string LastName { get; } private readonly Wallet wallet; public decimal GetPayment(decimal amount) { if (wallet.MoneyAmount >= amount) { wallet.WithdrawMoney(amount); return amount; } return 0; } } public class Wallet { public Wallet(decimal moneyAmount) { MoneyAmount = moneyAmount; } public decimal MoneyAmount { get; private set; } public void AddMoney(decimal amount) { MoneyAmount += amount; } public void WithdrawMoney(decimal amount) { MoneyAmount -= amount; } } public class PaperBoy { public void DeliverMagazine(decimal costOfMagazine, Customer customer) { Wallet w = customer.Wallet; if (w.MoneyAmount > costOfMagazine) { w.WithdrawMoney(costOfMagazine); } else { // come back later } } } } The Law of Demeter is not about the number of dots. This law is about reducing the [[coupling]] and improving the Encapsulation . It's OK to use many dots when digging the [[data structure]]s like: - ExcelDocument.Sheet.Cell","title":"Law of Demeter"},{"location":"Clean%20Code/SOLID/Metaprinciples/Magic%20value%20%28number%2C%20string%2C%20etc%29/","text":"Magic strings or any other magic values \u00b6 Bad: int responseCode = GetDeviceResponse(); if (responseCode == 188) { ... } Good (this can be reused elsewhere): const int NoConnection = 188; int responseCode = GetDeviceResponse(); if (responseCode == NoConnection) { ... } Bad: public class MagicValues { public void AcceptCard() { var d = new Device(); d.SendCommand(1); d.SendCommand(2); d.SendCommand(9); } public void DispenseCard() { var d = new Device(); d.SendCommand(1); d.SendCommand(3); d.SendCommand(9); } } Without magic values: public class NoMagic { private const int Initialize = 1; private const int Terminate = 9; public void AcceptCard() { var d = new Device(); d.SendCommand(Initialize); d.SendCommand(2); d.SendCommand(Terminate); } public void DispenseCard() { var d = new Device(); d.SendCommand(Initialize); d.SendCommand(3); d.SendCommand(Terminate); } } Good: public class NoDuplicateLogic { private const int Initialize = 1; private const int Terminate = 9; public void AcceptCard() { ExecuteCommand(2); } public void DispenseCard() { ExecuteCommand(3); } private void ExecuteCommand(byte command) { var d = new Device(); d.SendCommand(Initialize); d.SendCommand(command); d.SendCommand(Terminate); } }","title":"Magic value (number, string, etc)"},{"location":"Clean%20Code/SOLID/Metaprinciples/Magic%20value%20%28number%2C%20string%2C%20etc%29/#magic-strings-or-any-other-magic-values","text":"Bad: int responseCode = GetDeviceResponse(); if (responseCode == 188) { ... } Good (this can be reused elsewhere): const int NoConnection = 188; int responseCode = GetDeviceResponse(); if (responseCode == NoConnection) { ... } Bad: public class MagicValues { public void AcceptCard() { var d = new Device(); d.SendCommand(1); d.SendCommand(2); d.SendCommand(9); } public void DispenseCard() { var d = new Device(); d.SendCommand(1); d.SendCommand(3); d.SendCommand(9); } } Without magic values: public class NoMagic { private const int Initialize = 1; private const int Terminate = 9; public void AcceptCard() { var d = new Device(); d.SendCommand(Initialize); d.SendCommand(2); d.SendCommand(Terminate); } public void DispenseCard() { var d = new Device(); d.SendCommand(Initialize); d.SendCommand(3); d.SendCommand(Terminate); } } Good: public class NoDuplicateLogic { private const int Initialize = 1; private const int Terminate = 9; public void AcceptCard() { ExecuteCommand(2); } public void DispenseCard() { ExecuteCommand(3); } private void ExecuteCommand(byte command) { var d = new Device(); d.SendCommand(Initialize); d.SendCommand(command); d.SendCommand(Terminate); } }","title":"Magic strings or any other magic values"},{"location":"Clean%20Code/SOLID/Metaprinciples/Open-Closed%20Principle%20vs%20YAGNI/","text":"YAGNI - You ain't gonna need it means that we don't need to introduce any [[abstraction]]s until we really need them. YAGNI - You ain't gonna need it beats Open-Closed Principle in case of [[private API]]s Open-Closed Principle beats YAGNI - You ain't gonna need it in case of [[public API]]s","title":"Open Closed Principle vs YAGNI"},{"location":"Clean%20Code/SOLID/Metaprinciples/Principle%20of%20least%20astonishment/","text":"A [[component]] of a system should behave in a manner consistent with how users of that component are likely expect it to behave. Following principles and techniques are based on the principle of least astonishment: - [[Fault-Safe API]] - Command Query Separation Principle - [[Immutability]] - [[Design by Contract]] Works fine: reporting.PrintReportA(); reporting.PrintReportB(); Fails in runtime. Astonishment. reporting.PrintReportB(); reporting.PrintReportA(); Java stack exposes: - [[Push]] - [[Pop]] - Add - Where the item will be added? var array = new string[10]; var list = array as IList<string>; // this works list.Add(\"foo\"); // exception saying it's not supported","title":"Principle of least astonishment"},{"location":"Clean%20Code/SOLID/Metaprinciples/Reused%20Abstraction%20Principle/","text":"Reused Abstraction Principle (RAP) \u00b6 RAP states that there should be at least three implementers on an [[interface]] or a [[base class]] ( rule of three ) [[Abstraction]] eliminates irrelevant and amplifies the essential Basic algorithm: - Start with a concrete implementation of a specific behavior - Observe the emerging commonalities - Apply the rule of three","title":"Reused Abstraction Principle"},{"location":"Clean%20Code/SOLID/Metaprinciples/Reused%20Abstraction%20Principle/#reused-abstraction-principle-rap","text":"RAP states that there should be at least three implementers on an [[interface]] or a [[base class]] ( rule of three ) [[Abstraction]] eliminates irrelevant and amplifies the essential Basic algorithm: - Start with a concrete implementation of a specific behavior - Observe the emerging commonalities - Apply the rule of three","title":"Reused Abstraction Principle (RAP)"},{"location":"Clean%20Code/SOLID/Metaprinciples/SOLID%20vs%20YAGNI/","text":"Fixing problems we ended up with more [[complex design]] Blind application of SOLID Principles leads to needless [[complexity]] needless complexity often is the result of YAGNI - You ain't gonna need it violation \"Developers have a tendency to attemt to solve specific problems with general solutions\" - Greg Young","title":"SOLID vs YAGNI"},{"location":"Clean%20Code/SOLID/Metaprinciples/Separation%20Of%20Concerns/","text":"SoC - Separation Of Concerns \u00b6 Single Responsibility Principle and SoC are strongly related Implies separation of different [[concern]]s in different modules Allows to build [[modular system]]s Concerns we often face with: - [[UI]] - [[Business Logic]] - [[Presentation Logic]] - [[Database]] Leaking abstractions can ruin the SoC \u00b6 presentation layer is bothered by [[UI]] concerns public Color TextColor { get { bool result = Validate(text) return result ? Colors.Green : Colors.Red; } } [[Domain]] is bothered by [[Database]]: void DoWork(Customer customer1, Customer customer2) { if (customer1.Id > 0) { // do something } if (customer1.Id == customer2.Id) { // do something } } [[Layer]]s which represent different concerns should be isolated from each other in such a way that none of them should know a bout any intrinsic details of each other. [[SQL procedure]]s implementing [[business logic]] violate SoC but they're way much faster in certain scenarios.","title":"SoC - Separation Of Concerns"},{"location":"Clean%20Code/SOLID/Metaprinciples/Separation%20Of%20Concerns/#soc-separation-of-concerns","text":"Single Responsibility Principle and SoC are strongly related Implies separation of different [[concern]]s in different modules Allows to build [[modular system]]s Concerns we often face with: - [[UI]] - [[Business Logic]] - [[Presentation Logic]] - [[Database]]","title":"SoC - Separation Of Concerns"},{"location":"Clean%20Code/SOLID/Metaprinciples/Separation%20Of%20Concerns/#leaking-abstractions-can-ruin-the-soc","text":"presentation layer is bothered by [[UI]] concerns public Color TextColor { get { bool result = Validate(text) return result ? Colors.Green : Colors.Red; } } [[Domain]] is bothered by [[Database]]: void DoWork(Customer customer1, Customer customer2) { if (customer1.Id > 0) { // do something } if (customer1.Id == customer2.Id) { // do something } } [[Layer]]s which represent different concerns should be isolated from each other in such a way that none of them should know a bout any intrinsic details of each other. [[SQL procedure]]s implementing [[business logic]] violate SoC but they're way much faster in certain scenarios.","title":"Leaking abstractions can ruin the SoC"},{"location":"Clean%20Code/SOLID/Metaprinciples/Simplicity/","text":"Simplicity is the state or equality of being simple Something which is easy to understand or explain can be considered simple, in contrast to something complicated The feeling of simplicity is relative","title":"Simplicity"},{"location":"Clean%20Code/SOLID/Metaprinciples/Single%20Responsiblility%20Principle%20vs%20Interface%20Segregation%20Principle/","text":"Single Responsibility Principle implies that a class should have only one reason to change. Interface Segregation Principle at the same time tells that clients should not depend on things they do not need. Interface Segregation Principle and Single Responsibility Principle are different views on the same idea. SRP is more focused on the designer-side point of view, while ISP is more focused on the client-side point-of-view. In this example, it would be odd to separate these these responsibilities for SRP, but in ISP it makes sense to do so to hide unneeded things from clients. class Persister : IReader, IWriter { public byte[] Read(string file) {} public void Write(byte[] content) {} } interface IReader { byte[] Read(string file); } interface IWriter { void Write(byte[] content); }","title":"Single Responsiblility Principle vs Interface Segregation Principle"},{"location":"Clean%20Code/SOLID/Metaprinciples/YAGNI%20-%20You%20ain%27t%20gonna%20need%20it/","text":"YAGNI is all about avoiding [[overengineering]] There is no a well-defined criterion to measure the \"YAGNI-ness\". \"Always implement things when you actually need them, never when you just foresee that you need them\" - Ron Jeffries Worse is Better \u00b6 A model of software design and implementation which has the following characteristics: - Simplicity - [[Correctness]] - [[Consistency]] - [[Completeness]] If you do something for a future need that doesn't actually increase the complexity of the software, then there's no reason to invoke YAGNI YAGNI \u00b6 Don't follow any principles blindly YAGNI depends on supporting practices, follow other practices such as [[Continous Integration]], Refactoring , [[Unit Testing]] as well. YAGNI Violation \u00b6 A team is working on a payment system which interoperates with many devices. Working on a current version, you need to implement a driver for interoperation with the model \"Z\" of a bill dispenser At the same time, a project manager expects that in four months they will need to support the \"Y\" model of a bill dispenser. Deciding to implement such presumptive features is a classic violation of YAGNI principle! Useless Feature => all the efforts spent on analyzing, programming and testing. Useful Feature > stolen time + cost of carry + risk of wrong implementation YAGNI Violation 2 \u00b6 You're writing a fuinction within a class which should parse a custom string. Implemenmt that function right there, in that class Implement that function and extract a utility class which contains that parsing function. Ask yourself if the function (or feature) is needed to be implemented right now If the answer is \"yes\", then implement it If the answer is \"no\", ask yourself another question How much effort will it take to implement that function or feature in the future in case it will be required. If the answer is \"It will definitely be very simple\" then keep all the things as they are, don't introduce anything additional. If the answer is \"It will take enormous amount of time to rewrite many things to tweak the design to make it enough supple to introduce that feature\" - then consider performing some refactoring which will allow you to avoid massive rewriting in the future. You should always carefully plan the upcoming features No big upfront design The goal applying YAGNI is to save some time Perform refactoring to enable YAGNI Be ready to fail applying YAGNI Open-Closed Principle vs YAGNI","title":"YAGNI   You ain't gonna need it"},{"location":"Clean%20Code/SOLID/Metaprinciples/YAGNI%20-%20You%20ain%27t%20gonna%20need%20it/#worse-is-better","text":"A model of software design and implementation which has the following characteristics: - Simplicity - [[Correctness]] - [[Consistency]] - [[Completeness]] If you do something for a future need that doesn't actually increase the complexity of the software, then there's no reason to invoke YAGNI","title":"Worse is Better"},{"location":"Clean%20Code/SOLID/Metaprinciples/YAGNI%20-%20You%20ain%27t%20gonna%20need%20it/#yagni","text":"Don't follow any principles blindly YAGNI depends on supporting practices, follow other practices such as [[Continous Integration]], Refactoring , [[Unit Testing]] as well.","title":"YAGNI"},{"location":"Clean%20Code/SOLID/Metaprinciples/YAGNI%20-%20You%20ain%27t%20gonna%20need%20it/#yagni-violation","text":"A team is working on a payment system which interoperates with many devices. Working on a current version, you need to implement a driver for interoperation with the model \"Z\" of a bill dispenser At the same time, a project manager expects that in four months they will need to support the \"Y\" model of a bill dispenser. Deciding to implement such presumptive features is a classic violation of YAGNI principle! Useless Feature => all the efforts spent on analyzing, programming and testing. Useful Feature > stolen time + cost of carry + risk of wrong implementation","title":"YAGNI Violation"},{"location":"Clean%20Code/SOLID/Metaprinciples/YAGNI%20-%20You%20ain%27t%20gonna%20need%20it/#yagni-violation-2","text":"You're writing a fuinction within a class which should parse a custom string. Implemenmt that function right there, in that class Implement that function and extract a utility class which contains that parsing function. Ask yourself if the function (or feature) is needed to be implemented right now If the answer is \"yes\", then implement it If the answer is \"no\", ask yourself another question How much effort will it take to implement that function or feature in the future in case it will be required. If the answer is \"It will definitely be very simple\" then keep all the things as they are, don't introduce anything additional. If the answer is \"It will take enormous amount of time to rewrite many things to tweak the design to make it enough supple to introduce that feature\" - then consider performing some refactoring which will allow you to avoid massive rewriting in the future. You should always carefully plan the upcoming features No big upfront design The goal applying YAGNI is to save some time Perform refactoring to enable YAGNI Be ready to fail applying YAGNI Open-Closed Principle vs YAGNI","title":"YAGNI Violation 2"},{"location":"Clean%20Code/SOLID/Open-Closed%20Principle/Open-Closed%20Principle%20Common%20Smells/","text":"Common smells of OCP violations \u00b6 Many [[conditional branch]]es with if/else or switch/case statements Generally you have 3 approaches to it: [[Parameterization with delegates]]. \"[[Chain of responsibility]]\" design pattern. Classic [[Inheritance]] or Visitor Design Pattern [[Composition vs Inheritance]]. [[Strategy Design Pattern]].","title":"Common smells of OCP violations"},{"location":"Clean%20Code/SOLID/Open-Closed%20Principle/Open-Closed%20Principle%20Common%20Smells/#common-smells-of-ocp-violations","text":"Many [[conditional branch]]es with if/else or switch/case statements Generally you have 3 approaches to it: [[Parameterization with delegates]]. \"[[Chain of responsibility]]\" design pattern. Classic [[Inheritance]] or Visitor Design Pattern [[Composition vs Inheritance]]. [[Strategy Design Pattern]].","title":"Common smells of OCP violations"},{"location":"Clean%20Code/SOLID/Open-Closed%20Principle/Open-Closed%20Principle%20Demo/","text":"Demo of the problem \u00b6 namespace SOLID.OCP.Violation { public class DeviceFinder { SerialPort port = new SerialPort(); switch (model) { case DeviceModel.BillAccepterCashCode: { port.BaudRate = 9600; port.Parity = Parity.Even; port.Handshake = Handshake.RequestToSend; return Find(port) } case DeviceModel.BillDispenserEcdm: { port.BaudRate = 4800; port.Parity = Parity.Mark; port.Handshake = Handshake.RequestToSendXonXOff; return Find(port) } case DeviceModel.CoinAccepterNri: { port.BaudRate = 19200; port.Parity = Parity.Odd; port.Handshake = Handshake.XonXOff; return Find(port); } case DeviceModel.CoinDispenserCube4: { port.BaudRate = 9600; port.Parity = Parity.Space; port.Handshake = Handshake.None; return Find(port); } } } } All this does is calls the find method. If a new case appears, we will need to modify the program. Often these [[switch case]]s start appearing in other places as well which will make these modifications even harder. We can use Open-Closed Principle Refactorings .","title":"Demo of the problem"},{"location":"Clean%20Code/SOLID/Open-Closed%20Principle/Open-Closed%20Principle%20Demo/#demo-of-the-problem","text":"namespace SOLID.OCP.Violation { public class DeviceFinder { SerialPort port = new SerialPort(); switch (model) { case DeviceModel.BillAccepterCashCode: { port.BaudRate = 9600; port.Parity = Parity.Even; port.Handshake = Handshake.RequestToSend; return Find(port) } case DeviceModel.BillDispenserEcdm: { port.BaudRate = 4800; port.Parity = Parity.Mark; port.Handshake = Handshake.RequestToSendXonXOff; return Find(port) } case DeviceModel.CoinAccepterNri: { port.BaudRate = 19200; port.Parity = Parity.Odd; port.Handshake = Handshake.XonXOff; return Find(port); } case DeviceModel.CoinDispenserCube4: { port.BaudRate = 9600; port.Parity = Parity.Space; port.Handshake = Handshake.None; return Find(port); } } } } All this does is calls the find method. If a new case appears, we will need to modify the program. Often these [[switch case]]s start appearing in other places as well which will make these modifications even harder. We can use Open-Closed Principle Refactorings .","title":"Demo of the problem"},{"location":"Clean%20Code/SOLID/Open-Closed%20Principle/Open-Closed%20Principle%20Refactorings/","text":"Previously we had Open-Closed Principle Demo problem, we are going to refactor it. using System; using System.Collections.Generic; using System.Linq; using System.Text; using System.Threading.Tasks; namespace SOLID.OCP.Refactored { public interface IDevice { string Find(); } public class BillDispenserEcdm : IDevice { public string Find() { SerialPort port = new SerialPort { BaudRate = 4800, Parity = Parity.Mark, Handshake = Handshake.RequestToSendXOnXOff }; foreach(string portName in SerialPort.GetPortNames()) { // test if device can be connected port.Write(\"special code\"); if (port.ReadByte() == 120) return portName; } return null; } public class CoinDispenserCube4 : IDevice { public string Find() { SerialPort port = new SerialPort { BaudRate = 9600, Parity = Parity.Space, Handshake = Handshake.None }; foreach (string portName in SerialPort.GetPortNames()) { port.Write(\"Special code\") if (port.ReadByte == 0) return portName; } return null; } } public class DeviceFinder { private readonly IDevice _device; public DeviceFinder(IDevice device) { _device = device; } public string Find() { return _device.Find(); } } } We can't achieve a super-supple design which allows to introduce any possible features.","title":"Open Closed Principle Refactorings"},{"location":"Clean%20Code/SOLID/Open-Closed%20Principle/Open-Closed%20Principle%20Related%20Patterns/","text":"![[Strategy Design Pattern]] ![[Interface]] ![[Corollary]] ![[Abstract class]] ![[Interface vs Abstract Class]]","title":"Open Closed Principle Related Patterns"},{"location":"Clean%20Code/SOLID/Open-Closed%20Principle/Open-Closed%20Principle/","text":"Problem statement \u00b6 [[Software entities]] should be open for [[modification]], but closed for [[modification]]. When we need a change, we shouldn't dig deep into the system and change it We should be able to introduce a change by adding new code instead of changing the existing one [[Polymorphism]] is the answer to this problem. Why OCP? \u00b6 There is a high chance of introducing [[bug]]s during the modification process. It's hard to modify the behavior of an [[API]] which is already in use by many clients. When customers ask for a new feature they think that feature will be added, they don't think that developers will modify anything. We must modify the existing code if it contains a [[bug]]. Demo of OCP violation \u00b6 public class BankTerminalFactory { public static IBankTerminal CreateBankTerminal(BankTerminalModel model) { switch(model) { case BankTerminalModel.Brp; return new BrpTerminal(); case BankTerminalModel.Dcp: return new DcpTerminal(); default: throw new ArgumentException(\"Unknown model\"); } } } If we needed to add a new terminal type, we would need to change this existing code. Whenever a software system must support a set of alternatives, one and only one module in the system should know their exhaustive list. This can be solved by using Dependency Injection . Open-Closed Principle Demo Open-Closed Principle Refactorings Open-Closed Principle Related Patterns Open-Closed Principle Common Smells Open-Closed Principle vs YAGNI","title":"Open Closed Principle"},{"location":"Clean%20Code/SOLID/Open-Closed%20Principle/Open-Closed%20Principle/#problem-statement","text":"[[Software entities]] should be open for [[modification]], but closed for [[modification]]. When we need a change, we shouldn't dig deep into the system and change it We should be able to introduce a change by adding new code instead of changing the existing one [[Polymorphism]] is the answer to this problem.","title":"Problem statement"},{"location":"Clean%20Code/SOLID/Open-Closed%20Principle/Open-Closed%20Principle/#why-ocp","text":"There is a high chance of introducing [[bug]]s during the modification process. It's hard to modify the behavior of an [[API]] which is already in use by many clients. When customers ask for a new feature they think that feature will be added, they don't think that developers will modify anything. We must modify the existing code if it contains a [[bug]].","title":"Why OCP?"},{"location":"Clean%20Code/SOLID/Open-Closed%20Principle/Open-Closed%20Principle/#demo-of-ocp-violation","text":"public class BankTerminalFactory { public static IBankTerminal CreateBankTerminal(BankTerminalModel model) { switch(model) { case BankTerminalModel.Brp; return new BrpTerminal(); case BankTerminalModel.Dcp: return new DcpTerminal(); default: throw new ArgumentException(\"Unknown model\"); } } } If we needed to add a new terminal type, we would need to change this existing code. Whenever a software system must support a set of alternatives, one and only one module in the system should know their exhaustive list. This can be solved by using Dependency Injection . Open-Closed Principle Demo Open-Closed Principle Refactorings Open-Closed Principle Related Patterns Open-Closed Principle Common Smells Open-Closed Principle vs YAGNI","title":"Demo of OCP violation"},{"location":"Clean%20Code/SOLID/Open-Closed%20Principle/Open-Closed%20Princple%20Rules/","text":"Design should be opened for extensions and closed for modifications Isolate a responsibility for creating objects in a single module ([[Single Choice Principle]]) Related patterns: Template Method Design Pattern [[Strategy Design Pattern]] [[Interface]] is suppler from the client's perspective, [[Abstract class]] is suppler from the developer's perspective. To overcome the problem of predicting the future, we rely on \"[[agile design]]\"","title":"Open Closed Princple Rules"},{"location":"Clean%20Code/SOLID/Open-Closed%20Principle/Protected%20Variation%20Pattern/","text":"Identify points of [[predicted variation]] and create a [[stable interface]] around them.","title":"Protected Variation Pattern"},{"location":"Clean%20Code/SOLID/Single%20Responsibility%20Principle/Single%20Responsibility%20Principle%20Demo/","text":"Demo of the problem \u00b6 In this example we deal with an [[application]] that deals with buying tickets for trains. using System; namespace SOLID.SRP.Violation { public class PaymentModel { private decimal _cashAccepted; public void BuyTicket( TicketDetails ticket, PaymentDetails payment, onPayChangeToMobilePhone) { if(payment.Method == PaymentMethod.CreditCard) { ChargeCard(ticket, payment); } else { AcceptCash(ticket); DispenseChange(ticket, onPayChangeToMobilePhone); } } private void ChargeCard(TicketDetails ticket, PaymentDetails payment) { var gateway = new ProcessingCenterGateway(); gateway.Charge(ticket.Price, payment); } private void AcceptCash(TicketDetails ticket) { var r = new Random(); _cashAccepted = r.Next((int) ticket.Price, (int) ticket.Price + 1000); } private void DispenseChange(TicketDetails ticket, Action onPayChangeToMobilePhone) { if(_cashAccepted > ticket.Price && !TryToDispense(_cashAccepted - ticket.Price)) onPayChangeToMobilePhone?.Invoke(); } private bool TryToDispense(decimal changeAmount) { return false; //or true } } This clearly violates Single Responsibility Principle , there are two different payment methods that are coupled - paying with a credit card, cash. It depends on PaymentDetails, TicketDetails, ProcessingCenterGateway. PaymentDetails <------------- ^ | | | | | PaymentModel -----> ProcessingCenterGateway | | TicketDetails The refactoring to abide by the Single Responsibility Principle is viewed in Single Responsibility Principle Refactoring . More examples can be seen at Single Responsibility Principle Violations .","title":"Demo of the problem"},{"location":"Clean%20Code/SOLID/Single%20Responsibility%20Principle/Single%20Responsibility%20Principle%20Demo/#demo-of-the-problem","text":"In this example we deal with an [[application]] that deals with buying tickets for trains. using System; namespace SOLID.SRP.Violation { public class PaymentModel { private decimal _cashAccepted; public void BuyTicket( TicketDetails ticket, PaymentDetails payment, onPayChangeToMobilePhone) { if(payment.Method == PaymentMethod.CreditCard) { ChargeCard(ticket, payment); } else { AcceptCash(ticket); DispenseChange(ticket, onPayChangeToMobilePhone); } } private void ChargeCard(TicketDetails ticket, PaymentDetails payment) { var gateway = new ProcessingCenterGateway(); gateway.Charge(ticket.Price, payment); } private void AcceptCash(TicketDetails ticket) { var r = new Random(); _cashAccepted = r.Next((int) ticket.Price, (int) ticket.Price + 1000); } private void DispenseChange(TicketDetails ticket, Action onPayChangeToMobilePhone) { if(_cashAccepted > ticket.Price && !TryToDispense(_cashAccepted - ticket.Price)) onPayChangeToMobilePhone?.Invoke(); } private bool TryToDispense(decimal changeAmount) { return false; //or true } } This clearly violates Single Responsibility Principle , there are two different payment methods that are coupled - paying with a credit card, cash. It depends on PaymentDetails, TicketDetails, ProcessingCenterGateway. PaymentDetails <------------- ^ | | | | | PaymentModel -----> ProcessingCenterGateway | | TicketDetails The refactoring to abide by the Single Responsibility Principle is viewed in Single Responsibility Principle Refactoring . More examples can be seen at Single Responsibility Principle Violations .","title":"Demo of the problem"},{"location":"Clean%20Code/SOLID/Single%20Responsibility%20Principle/Single%20Responsibility%20Principle%20Refactoring/","text":"This lists how to [[refactor]] the Single Responsibility Principle Demo code for better Single Responsibility Principle . First we are creating an [[abstraction class]] for the payment model. namespace SOLID.SRP.Refactored { public abstract class PaymentModel { protected TicketDetails _ticketDetails; protected PaymentModel(TicketDetails ticketDetails) { _ticketDetails = ticketDetails; } public abstract void BuyTicket(); } } Then we are going to implement an [[Interface]] for credit card payments. namespace SOLID.SRP.Refactored { public interface ICanPayViaCreditCard { void Charge(TicketDetails ticketDetails, PaymentDetails paymentDetails); } } Then we are going to implement bank gateway interface namespace SOLID.SRP.Refactored { public class BankGateway : ICanPayViaCreditCard { void Charge(TicketDetails ticketDetails, PaymentDetails paymentDetails) { // charge } } } Then we are implementing OnlinePayment class that derives the PaymentModel. namespace SOLID.SRP.Refactored { private readonly PaymentDetails _payment; private readonly ICanPayViaCreditCard _bankGateway; public class OnlinePayment : PaymentModel { public OnlinePayment(TicketDetails ticketDetails, PaymentDetails paymentDetails) : base(ticketDetails) { _payment = payment; _bankGateway = new BankGateway(); } } public override void BuyTicket() { _bankGateway.ChargeCard(base._ticketDetails, _payment); } } Then we are adding an interface for paying with cash. namespace SOLID.SRP.Refactored { public interface ICanOperateWithCash { void AcceptCash(); void DispenseChange(); } } Then implement the inheritor for this interface: namespace SOLID.SRP.Refactored { public class PosTerminalPayment: PaymentModel, ICanOperateWithCash { private readonly Action _onPayChangeToMobilePhone; private decimal _cashAccepted; public PosTerminalPayment(TicketDetails ticketDetails, Action onPayChangeToMobilePhone) : base(ticketDetails) { _onPayChangeToMobilePhone = onPayChangeToMobilePhone; } public override void BuyTicket() { AcceptCash(); DispenseChange(); } public void AcceptCash() { Random r = new Random(); _cashAccepted = r.Next((int) _ticketDetails.Price, (int) _ticketDetails.Price + 1000); } public void DispenseChange() { if(_cashAccepted > _ticketDetails.Price && !TryToDispense()) { _onPayChangeToMobilePhone?.Invoke(); } } private bool TryToDispense() { // issue a command for dispensing return false; } } }","title":"Single Responsibility Principle Refactoring"},{"location":"Clean%20Code/SOLID/Single%20Responsibility%20Principle/Single%20Responsibility%20Principle%20Rules/","text":"Conclusion \u00b6 When applying Single Responsibility Principle we want to separate different [[concern]]s A [[class]] should do one thing and do it well! We can apply Single Responsibility Principle at different levels: [[Function level]], [[Object level]], [[Module level]] Classes with many responsibilities are hard to understand When Single Responsibility Principle is violated, responsibilities start to [[collate]] with each other Don't abuse Single Responsibility Principle Apply the [[Facade design pattern]] to simplify the [[API]] [[Module]]s that change frequently should be isolated from the other parts of system","title":"Conclusion"},{"location":"Clean%20Code/SOLID/Single%20Responsibility%20Principle/Single%20Responsibility%20Principle%20Rules/#conclusion","text":"When applying Single Responsibility Principle we want to separate different [[concern]]s A [[class]] should do one thing and do it well! We can apply Single Responsibility Principle at different levels: [[Function level]], [[Object level]], [[Module level]] Classes with many responsibilities are hard to understand When Single Responsibility Principle is violated, responsibilities start to [[collate]] with each other Don't abuse Single Responsibility Principle Apply the [[Facade design pattern]] to simplify the [[API]] [[Module]]s that change frequently should be isolated from the other parts of system","title":"Conclusion"},{"location":"Clean%20Code/SOLID/Single%20Responsibility%20Principle/Single%20Responsibility%20Principle%20Violations/","text":"More examples of SRP violations \u00b6 namespace SOLID.SRP.MoreExamples { class Violation1 { // example with business logic and formatting public string GetReport() { int clientsNumber = GetNumberOfClients(); decimal totalIncome = GetTotalIncome() int satisfiedClients = GetSatisfiedClients(); int unsatisfiedClients = GetUnsatisfiedClients(); // ^ gather statistical data string clientsStr = $\"Total number of Clients = {clientsNumber}\"; string incomeString $\"Total Income = {totalIncome}\"; string satisfiedClientsStr = $\"Number of satisfied clients = {satisfiedClients}\"; string unsatisfiedClientsStr = $\"Number of unsatisfied clients = {unsatisfiedClients}\"; // ^ formatting return clientsStr + Environment.NewLine + incomeString + Environment.NewLine + satisfiedClientsStr + Environment.NewLine + unsatisfiedClientsStr + Environment.NewLine; } } class Violation2 { // example with business logic and changing the state - mechanics and policy public void FindAlarmDevice() { var driver = new AlarmDriver(); string port = driver.Find(); if (!string.IsNullOrWhiteSpace(port)) { SystemState.AlarmCanBeUsed = false; } SystemState.AlarmCanBeUsed = true; } } class Violation3 { // example with drawing - calculation coordinated, setting color and filling the rectangle // violation depends on actors public void DrawRectangle() { Rectangle rect = GetRectangle(); Color color = Colors.Red; rect.Fill(color); } } } Common SRP Violations \u00b6 Mixing [[logic]] and [[infrastructure]] A class or a module serves different [[layer]]s","title":"More examples of SRP violations"},{"location":"Clean%20Code/SOLID/Single%20Responsibility%20Principle/Single%20Responsibility%20Principle%20Violations/#more-examples-of-srp-violations","text":"namespace SOLID.SRP.MoreExamples { class Violation1 { // example with business logic and formatting public string GetReport() { int clientsNumber = GetNumberOfClients(); decimal totalIncome = GetTotalIncome() int satisfiedClients = GetSatisfiedClients(); int unsatisfiedClients = GetUnsatisfiedClients(); // ^ gather statistical data string clientsStr = $\"Total number of Clients = {clientsNumber}\"; string incomeString $\"Total Income = {totalIncome}\"; string satisfiedClientsStr = $\"Number of satisfied clients = {satisfiedClients}\"; string unsatisfiedClientsStr = $\"Number of unsatisfied clients = {unsatisfiedClients}\"; // ^ formatting return clientsStr + Environment.NewLine + incomeString + Environment.NewLine + satisfiedClientsStr + Environment.NewLine + unsatisfiedClientsStr + Environment.NewLine; } } class Violation2 { // example with business logic and changing the state - mechanics and policy public void FindAlarmDevice() { var driver = new AlarmDriver(); string port = driver.Find(); if (!string.IsNullOrWhiteSpace(port)) { SystemState.AlarmCanBeUsed = false; } SystemState.AlarmCanBeUsed = true; } } class Violation3 { // example with drawing - calculation coordinated, setting color and filling the rectangle // violation depends on actors public void DrawRectangle() { Rectangle rect = GetRectangle(); Color color = Colors.Red; rect.Fill(color); } } }","title":"More examples of SRP violations"},{"location":"Clean%20Code/SOLID/Single%20Responsibility%20Principle/Single%20Responsibility%20Principle%20Violations/#common-srp-violations","text":"Mixing [[logic]] and [[infrastructure]] A class or a module serves different [[layer]]s","title":"Common SRP Violations"},{"location":"Clean%20Code/SOLID/Single%20Responsibility%20Principle/Single%20Responsibility%20Principle/","text":"SRP Definition. Problem Statement \u00b6 Every [[object]] should have a single responsibility and that responsibility should be entirely [[encapsulated]] by the class. There should never be more than one reason for a class to change. Calculating Responsibilities \u00b6 Axis of changing requirements If the class implements [[logging]] and [[caching]] on its own - it has two responsibilities. [[API]] users are the source of changes The more responsibilities a [[class]] has, the more likely it's going to be changed. Applying SRP we want to separate different [[concerns]] SRP can be applied at different levels: [[Function level]]s [[Object level]] [[Module level]] { public class PaymentProcessor { public void Charge(decimal amount) { // initialize bank terminal // send a \"Charge\" request to the terminal } public string CreateReport() { // format a report return string.Empty; } public void PrintReport() { // initialize printer's driver // send a printing command } public void SavePayment() { // saving to DB } } } The very first method Charge is responsible for dealing with a bank terminal. It needs to know how to initialise the [[connection]] and send proper [[command]]s. The CreateReport creates reports and needs to know how to format them properly. The PrintReport knows how to deal with printer's [[driver]] and send commands to it. The SavePayment deals with the [[database]] and saves the payment data. As we can see there are hidden responsibilities and we need a higher level object that orchestrates them. Classes with too many responsibilities are hard to understand When SRP is violated, responsibilities start to collate with each other. They become coupled. Gather all the same responsibilities together and separate from those what are different A set of functions or an [[Interface]] is considered cohesive when each function is closely related to another. Coupling indicates how dependent modules are on the inner working of each other. Single Responsibility Principle Demo Single Responsiblity Principle Related Patterns Single Responsiblility Principle vs Interface Segregation Principle","title":"SRP Definition. Problem Statement"},{"location":"Clean%20Code/SOLID/Single%20Responsibility%20Principle/Single%20Responsibility%20Principle/#srp-definition-problem-statement","text":"Every [[object]] should have a single responsibility and that responsibility should be entirely [[encapsulated]] by the class. There should never be more than one reason for a class to change.","title":"SRP Definition. Problem Statement"},{"location":"Clean%20Code/SOLID/Single%20Responsibility%20Principle/Single%20Responsibility%20Principle/#calculating-responsibilities","text":"Axis of changing requirements If the class implements [[logging]] and [[caching]] on its own - it has two responsibilities. [[API]] users are the source of changes The more responsibilities a [[class]] has, the more likely it's going to be changed. Applying SRP we want to separate different [[concerns]] SRP can be applied at different levels: [[Function level]]s [[Object level]] [[Module level]] { public class PaymentProcessor { public void Charge(decimal amount) { // initialize bank terminal // send a \"Charge\" request to the terminal } public string CreateReport() { // format a report return string.Empty; } public void PrintReport() { // initialize printer's driver // send a printing command } public void SavePayment() { // saving to DB } } } The very first method Charge is responsible for dealing with a bank terminal. It needs to know how to initialise the [[connection]] and send proper [[command]]s. The CreateReport creates reports and needs to know how to format them properly. The PrintReport knows how to deal with printer's [[driver]] and send commands to it. The SavePayment deals with the [[database]] and saves the payment data. As we can see there are hidden responsibilities and we need a higher level object that orchestrates them. Classes with too many responsibilities are hard to understand When SRP is violated, responsibilities start to collate with each other. They become coupled. Gather all the same responsibilities together and separate from those what are different A set of functions or an [[Interface]] is considered cohesive when each function is closely related to another. Coupling indicates how dependent modules are on the inner working of each other. Single Responsibility Principle Demo Single Responsiblity Principle Related Patterns Single Responsiblility Principle vs Interface Segregation Principle","title":"Calculating Responsibilities"},{"location":"Clean%20Code/SOLID/Single%20Responsibility%20Principle/Single%20Responsiblity%20Principle%20Related%20Patterns/","text":"SRP Related patterns \u00b6 Applying the Single Responsibility Principle leads to appearance of many small [[class]]es It is hard to understand the [[API]] of too many small classes [[Facade design pattern]] may come to the rescue Facades doesn't violate the Single Responsibility Principle since their responsibility is to bring the functionality required by a client together. Reasons for applying Facades \u00b6 Provide a simple [[API]] for client to interact with a set of complex objects Provide a cleaner [[API]] for client to interact with a poorly designed API namespace SOLID.SRP.Patterns { class Problem { void Purchase() { int id = 123; Customer c = Customer.FindCustomer(id); int goodId = 13457; Order o = Stock.Find(goodId); CreditCardInfo cci = c.GetCreditCardInfo(); BankGateway gateway = new BankGateway(); gateway.ChargeCard(cci, o); c.AddToStatistics(o); } } class PurchaseFacade { public void ProcessPurchase(int customerId, int goodId) { Customer c = Customer.Find(Customer(customerId); Order o = Stock.Find(goodId); CreditCardInfo cci = c.GetCreditCardInfo(); BankGateway gateway = new BankGateway(); gateway.ChargeCard(cci, o); c.AddToStatistics(o); } } } Related Patterns \u00b6 [[Facade design pattern]] [[Decorator design pattern]] - allows the [[behavior]] an individual object, either statically or dynamically, without affecting the behavior of other objects from the same [[class]]. [[Composite design pattern]] - allows to compose objects into [[tree structure]]s to represent part-whole hierarchies, letting clients treat individual objects and compositions of objects uniformly.","title":"SRP Related patterns"},{"location":"Clean%20Code/SOLID/Single%20Responsibility%20Principle/Single%20Responsiblity%20Principle%20Related%20Patterns/#srp-related-patterns","text":"Applying the Single Responsibility Principle leads to appearance of many small [[class]]es It is hard to understand the [[API]] of too many small classes [[Facade design pattern]] may come to the rescue Facades doesn't violate the Single Responsibility Principle since their responsibility is to bring the functionality required by a client together.","title":"SRP Related patterns"},{"location":"Clean%20Code/SOLID/Single%20Responsibility%20Principle/Single%20Responsiblity%20Principle%20Related%20Patterns/#reasons-for-applying-facades","text":"Provide a simple [[API]] for client to interact with a set of complex objects Provide a cleaner [[API]] for client to interact with a poorly designed API namespace SOLID.SRP.Patterns { class Problem { void Purchase() { int id = 123; Customer c = Customer.FindCustomer(id); int goodId = 13457; Order o = Stock.Find(goodId); CreditCardInfo cci = c.GetCreditCardInfo(); BankGateway gateway = new BankGateway(); gateway.ChargeCard(cci, o); c.AddToStatistics(o); } } class PurchaseFacade { public void ProcessPurchase(int customerId, int goodId) { Customer c = Customer.Find(Customer(customerId); Order o = Stock.Find(goodId); CreditCardInfo cci = c.GetCreditCardInfo(); BankGateway gateway = new BankGateway(); gateway.ChargeCard(cci, o); c.AddToStatistics(o); } } }","title":"Reasons for applying Facades"},{"location":"Clean%20Code/SOLID/Single%20Responsibility%20Principle/Single%20Responsiblity%20Principle%20Related%20Patterns/#related-patterns","text":"[[Facade design pattern]] [[Decorator design pattern]] - allows the [[behavior]] an individual object, either statically or dynamically, without affecting the behavior of other objects from the same [[class]]. [[Composite design pattern]] - allows to compose objects into [[tree structure]]s to represent part-whole hierarchies, letting clients treat individual objects and compositions of objects uniformly.","title":"Related Patterns"},{"location":"Data%20Science/","text":"Recapping data science \u00b6 Sources: https://www.udemy.com/course/data-science-and-machine-learning-with-python-hands-on/","title":"Recapping data science"},{"location":"Data%20Science/#recapping-data-science","text":"Sources: https://www.udemy.com/course/data-science-and-machine-learning-with-python-hands-on/","title":"Recapping data science"},{"location":"Data%20Science/01-introduction/01-installing-anaconda/","text":"Installing anaconda \u00b6 Installation checklist \u00b6 Download anaconda from https://www.anaconda.com/distribution/ Once installed, open an anaconda command prompt and install tensorflow and pydotplus conda install tensorflow conda install pydotplus If you have an nvidia graphics card, you can install tensorflow-gpu instead for better performance. Disabling conda auto activation \u00b6 conda config --set auto_activate_base false Then when needed conda activate Opening notebooks \u00b6 Navigate to the directory and run jupyter notebook .","title":"Installing anaconda"},{"location":"Data%20Science/01-introduction/01-installing-anaconda/#installing-anaconda","text":"","title":"Installing anaconda"},{"location":"Data%20Science/01-introduction/01-installing-anaconda/#installation-checklist","text":"Download anaconda from https://www.anaconda.com/distribution/ Once installed, open an anaconda command prompt and install tensorflow and pydotplus conda install tensorflow conda install pydotplus If you have an nvidia graphics card, you can install tensorflow-gpu instead for better performance.","title":"Installation checklist"},{"location":"Data%20Science/01-introduction/01-installing-anaconda/#disabling-conda-auto-activation","text":"conda config --set auto_activate_base false Then when needed conda activate","title":"Disabling conda auto activation"},{"location":"Data%20Science/01-introduction/01-installing-anaconda/#opening-notebooks","text":"Navigate to the directory and run jupyter notebook .","title":"Opening notebooks"},{"location":"Data%20Science/02-statistics-and-probability-refresher/01-types-of-data/","text":"Types of data \u00b6 Major types of data \u00b6 Numerical Represents some sort of quantitative measurement like page load times, stock prices etc. Discrete data - integer based, often counts of some event. Continuous data How much time did it take for a user to check out? Categorical Qualitative data that has no inherent mathematical meaning Gender, Yes/no, race, state of residence, product category etc You can assign numbers to categories in order to represent them more compactly, but the numbers don't have mathematical meaning. Ordinal A mixture of numerical and categorical Categorical data that has a mathematical meaning Example: movie ratings on 1-5 scale. Ratings must be 1, 2, 3, 4 or 5 But these values have mathematical meaning - 1 means it's a worse movie than 2.","title":"Types of data"},{"location":"Data%20Science/02-statistics-and-probability-refresher/01-types-of-data/#types-of-data","text":"","title":"Types of data"},{"location":"Data%20Science/02-statistics-and-probability-refresher/01-types-of-data/#major-types-of-data","text":"Numerical Represents some sort of quantitative measurement like page load times, stock prices etc. Discrete data - integer based, often counts of some event. Continuous data How much time did it take for a user to check out? Categorical Qualitative data that has no inherent mathematical meaning Gender, Yes/no, race, state of residence, product category etc You can assign numbers to categories in order to represent them more compactly, but the numbers don't have mathematical meaning. Ordinal A mixture of numerical and categorical Categorical data that has a mathematical meaning Example: movie ratings on 1-5 scale. Ratings must be 1, 2, 3, 4 or 5 But these values have mathematical meaning - 1 means it's a worse movie than 2.","title":"Major types of data"},{"location":"Data%20Science/02-statistics-and-probability-refresher/02-mean-median-mode/","text":"Mean, Median and Mode \u00b6 Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/01-mean-median-mode.ipynb Mean \u00b6 Simply the average value Calculated - sum / number of samples Example: Number of children in each house on my street 0, 2, 3, 2, 1, 0, 0, 2, 0 The MEAN is \\(\\frac{0+2+3+2+1+0+0+2+0}{9} = 1.11\\) Median \u00b6 Sort the values and take the value at the midpoint Example: 0, 2, 3, 2, 1, 0, 0, 2, 0 Sort it: 0, 0, 0, 0, 1, 2, 2, 2, 3 ^ - 1 If you have an even number of samples, take the average of the two in the middle. Median is less susceptible to outliers than the mean. Example: mean household income in the US is $72,641, but the median is only $51,939 because the mean is skewed by a handful of billionaires. Median better represents the \"typical\" american in this example. Mode \u00b6 The most common value in a data set Not relevant to continuous numerical data Back to our number of kids in each house example 0, 2, 3, 2, 1, 0, 0, 2, 0 0: 4, 1: 1, 2: 3, 3: 1 The mode is 0","title":"Mean, Median and Mode"},{"location":"Data%20Science/02-statistics-and-probability-refresher/02-mean-median-mode/#mean-median-and-mode","text":"Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/01-mean-median-mode.ipynb","title":"Mean, Median and Mode"},{"location":"Data%20Science/02-statistics-and-probability-refresher/02-mean-median-mode/#mean","text":"Simply the average value Calculated - sum / number of samples Example: Number of children in each house on my street 0, 2, 3, 2, 1, 0, 0, 2, 0 The MEAN is \\(\\frac{0+2+3+2+1+0+0+2+0}{9} = 1.11\\)","title":"Mean"},{"location":"Data%20Science/02-statistics-and-probability-refresher/02-mean-median-mode/#median","text":"Sort the values and take the value at the midpoint Example: 0, 2, 3, 2, 1, 0, 0, 2, 0 Sort it: 0, 0, 0, 0, 1, 2, 2, 2, 3 ^ - 1 If you have an even number of samples, take the average of the two in the middle. Median is less susceptible to outliers than the mean. Example: mean household income in the US is $72,641, but the median is only $51,939 because the mean is skewed by a handful of billionaires. Median better represents the \"typical\" american in this example.","title":"Median"},{"location":"Data%20Science/02-statistics-and-probability-refresher/02-mean-median-mode/#mode","text":"The most common value in a data set Not relevant to continuous numerical data Back to our number of kids in each house example 0, 2, 3, 2, 1, 0, 0, 2, 0 0: 4, 1: 1, 2: 3, 3: 1 The mode is 0","title":"Mode"},{"location":"Data%20Science/02-statistics-and-probability-refresher/03-standard-deviation-and-variance/","text":"Standard Deviation and Variance \u00b6 Python notebook https://github.com/daviskregers/data-science-recap/blob/main/02-standard-deviation-and-variance.ipynb Variance measures how \"spread-out\" the data is. \u00b6 Variance ( \\(\u03c3^2\\) ) is simply the average of the squared differences from the mean. Example: What is the variance of the data set (1, 4, 5, 4, 8) First find the mean \\(\\frac{1+4+5+4+8}{5} = 4.4\\) Now find the differences from the mean: (-3.4, -0.4, 0.6, -0.4, 3.6) Find the squared differences: (11.56, 0.16, 0.36, 0.16, 12.96) Find the average of the squared differences: \\(\u03c3^2 = \\frac{11.56 + 0.16 + 0.36 + 0.16 + 12.96}{5} = 5.04\\) Standard deviation \u03c3 is just the square root of the variance \u00b6 \\(\u03c3^2 = 5.04\\) \\(\u03c3 = \\sqrt{5.04} = 2.24\\) So the standard deviation of (1, 4, 5, 4, 8) is 2.24 This is usually used as a way to identify outliers. Data points that lie more than one standard deviation from the mean can be considered unusual. You can talk about how extreme a data point is by talking about \"how many sigmas\" away from the mean it is. Population vs Sample \u00b6 If you're working with a sample of data instead of an entire data set (the entire population) Then you want to use the \"sample variance\" instead of the \"population variance\" For N samples, you just divide the squared variances by N-1 instead of N. So, in our example, we computed the population variance like this: \\(\u03c3^2 = \\frac{11.56 + 0.16 + 0.36 + 0.16 + 12.96}{5} = 5.04\\) But the sample variance would be: \\(\u03c3^2 = \\frac{11.56 + 0.16 + 0.36 + 0.16 + 12.96}{4} = 6.3\\)","title":"Standard Deviation and Variance"},{"location":"Data%20Science/02-statistics-and-probability-refresher/03-standard-deviation-and-variance/#standard-deviation-and-variance","text":"Python notebook https://github.com/daviskregers/data-science-recap/blob/main/02-standard-deviation-and-variance.ipynb","title":"Standard Deviation and Variance"},{"location":"Data%20Science/02-statistics-and-probability-refresher/03-standard-deviation-and-variance/#variance-measures-how-spread-out-the-data-is","text":"Variance ( \\(\u03c3^2\\) ) is simply the average of the squared differences from the mean. Example: What is the variance of the data set (1, 4, 5, 4, 8) First find the mean \\(\\frac{1+4+5+4+8}{5} = 4.4\\) Now find the differences from the mean: (-3.4, -0.4, 0.6, -0.4, 3.6) Find the squared differences: (11.56, 0.16, 0.36, 0.16, 12.96) Find the average of the squared differences: \\(\u03c3^2 = \\frac{11.56 + 0.16 + 0.36 + 0.16 + 12.96}{5} = 5.04\\)","title":"Variance measures how \"spread-out\" the data is."},{"location":"Data%20Science/02-statistics-and-probability-refresher/03-standard-deviation-and-variance/#standard-deviation-is-just-the-square-root-of-the-variance","text":"\\(\u03c3^2 = 5.04\\) \\(\u03c3 = \\sqrt{5.04} = 2.24\\) So the standard deviation of (1, 4, 5, 4, 8) is 2.24 This is usually used as a way to identify outliers. Data points that lie more than one standard deviation from the mean can be considered unusual. You can talk about how extreme a data point is by talking about \"how many sigmas\" away from the mean it is.","title":"Standard deviation \u03c3 is just the square root of the variance"},{"location":"Data%20Science/02-statistics-and-probability-refresher/03-standard-deviation-and-variance/#population-vs-sample","text":"If you're working with a sample of data instead of an entire data set (the entire population) Then you want to use the \"sample variance\" instead of the \"population variance\" For N samples, you just divide the squared variances by N-1 instead of N. So, in our example, we computed the population variance like this: \\(\u03c3^2 = \\frac{11.56 + 0.16 + 0.36 + 0.16 + 12.96}{5} = 5.04\\) But the sample variance would be: \\(\u03c3^2 = \\frac{11.56 + 0.16 + 0.36 + 0.16 + 12.96}{4} = 6.3\\)","title":"Population vs Sample"},{"location":"Data%20Science/02-statistics-and-probability-refresher/04-probability-density-function/","text":"Probability Density function \u00b6 Python nootebook: https://github.com/daviskregers/data-science-recap/blob/main/03-distributions.ipynb Probability density function \u00b6 Speak to a probability of a given range of values happening. The probability distribution function gives you the probability of a data point falling within some given range of a given value. (-\u03c3; 0) = 34.1% (0; \u03c3) = 34.1% (-2\u03c3; -\u03c3) = 13.6% (1\u03c3; 2\u03c3) = 13.6% etc... Probability mass function \u00b6 When dealing with discrete data, we use probability mass function. It basically is a histogram that shows how many times each event has happened.","title":"Probability Density function"},{"location":"Data%20Science/02-statistics-and-probability-refresher/04-probability-density-function/#probability-density-function","text":"Python nootebook: https://github.com/daviskregers/data-science-recap/blob/main/03-distributions.ipynb","title":"Probability Density function"},{"location":"Data%20Science/02-statistics-and-probability-refresher/04-probability-density-function/#probability-density-function_1","text":"Speak to a probability of a given range of values happening. The probability distribution function gives you the probability of a data point falling within some given range of a given value. (-\u03c3; 0) = 34.1% (0; \u03c3) = 34.1% (-2\u03c3; -\u03c3) = 13.6% (1\u03c3; 2\u03c3) = 13.6% etc...","title":"Probability density function"},{"location":"Data%20Science/02-statistics-and-probability-refresher/04-probability-density-function/#probability-mass-function","text":"When dealing with discrete data, we use probability mass function. It basically is a histogram that shows how many times each event has happened.","title":"Probability mass function"},{"location":"Data%20Science/02-statistics-and-probability-refresher/05-percentiles-and-moments/","text":"Percentiles and moments \u00b6 Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/04-percentiles-and-moments.ipynb Percentiles \u00b6 In a data set, what's the point at which X^ of the values are less than that value? Example: income distribution 99% percent of people make less than $506,553 50% of people make less than $42,327 Moments \u00b6 Quantitative measures of the shape of a probability density function Mathematically they are a bit hard to wrap your head around: $$ \\mu n = \\int^{+\\infty} {-\\infty}(x - c)^nf(x)dx $$ (for moment n around value c) But intuitively, it's a lot simpler in statistics The first moment is the mean. Second moment is the variance Third moment is is \"skew\" ( \\(\\gamma\\) ) How \"lopsided\" is the distribution? A distribution with a longer tail on the left will be skewed left, and have a negative skew. 4th m oment is \"kurtosis\" How thick is the tail and how sharp is the peak, compared to a normal distribution? Example: higher peaks have higher kurtosis","title":"Percentiles and moments"},{"location":"Data%20Science/02-statistics-and-probability-refresher/05-percentiles-and-moments/#percentiles-and-moments","text":"Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/04-percentiles-and-moments.ipynb","title":"Percentiles and moments"},{"location":"Data%20Science/02-statistics-and-probability-refresher/05-percentiles-and-moments/#percentiles","text":"In a data set, what's the point at which X^ of the values are less than that value? Example: income distribution 99% percent of people make less than $506,553 50% of people make less than $42,327","title":"Percentiles"},{"location":"Data%20Science/02-statistics-and-probability-refresher/05-percentiles-and-moments/#moments","text":"Quantitative measures of the shape of a probability density function Mathematically they are a bit hard to wrap your head around: $$ \\mu n = \\int^{+\\infty} {-\\infty}(x - c)^nf(x)dx $$ (for moment n around value c) But intuitively, it's a lot simpler in statistics The first moment is the mean. Second moment is the variance Third moment is is \"skew\" ( \\(\\gamma\\) ) How \"lopsided\" is the distribution? A distribution with a longer tail on the left will be skewed left, and have a negative skew. 4th m oment is \"kurtosis\" How thick is the tail and how sharp is the peak, compared to a normal distribution? Example: higher peaks have higher kurtosis","title":"Moments"},{"location":"Data%20Science/02-statistics-and-probability-refresher/06-matplotlib-crash-course/","text":"Crash course into matplotlib \u00b6 Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/05-matplotlib.ipynb","title":"Crash course into matplotlib"},{"location":"Data%20Science/02-statistics-and-probability-refresher/06-matplotlib-crash-course/#crash-course-into-matplotlib","text":"Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/05-matplotlib.ipynb","title":"Crash course into matplotlib"},{"location":"Data%20Science/02-statistics-and-probability-refresher/07-advanced-visualization-with-seaborn/","text":"Advanced visualization with Seaborn \u00b6 Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/06-advanced-visualization-with-seaborn.ipynb","title":"Advanced visualization with Seaborn"},{"location":"Data%20Science/02-statistics-and-probability-refresher/07-advanced-visualization-with-seaborn/#advanced-visualization-with-seaborn","text":"Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/06-advanced-visualization-with-seaborn.ipynb","title":"Advanced visualization with Seaborn"},{"location":"Data%20Science/02-statistics-and-probability-refresher/08-covariance-and-correlation/","text":"Covariance and correlation \u00b6 Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/07-covariance-and-correlation.ipynb Covariance \u00b6 It measures how two variables vary in tandem for their means. Measuring covariance Think of the data sets for the two variables as high dimensional vectors Convert these to vectors of variances from the mean Take the dot product (cosine of the angle between them) of the two vectors Divide by the sample size Interpreting covariance is hard We know a small covariance, close to 0, means there isn't much correlation between the two variables. And large covariances - that is, far from 0 (could be negative for inverse relationships) mean there is a correlation. Then the question is - how large does it have to be? Correltation \u00b6 Just divide the covariance by the standard deviations of both variables and that normalizes things. So a correlation results: 0 means no correlation 1 means perfect correlation -1 means perfect inverse correlation Remember that correlation does not imply causation. Only a controlled, randomized experiment can give you insights on causation. Use correlation to decide what experiments to conduct!","title":"Covariance and correlation"},{"location":"Data%20Science/02-statistics-and-probability-refresher/08-covariance-and-correlation/#covariance-and-correlation","text":"Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/07-covariance-and-correlation.ipynb","title":"Covariance and correlation"},{"location":"Data%20Science/02-statistics-and-probability-refresher/08-covariance-and-correlation/#covariance","text":"It measures how two variables vary in tandem for their means. Measuring covariance Think of the data sets for the two variables as high dimensional vectors Convert these to vectors of variances from the mean Take the dot product (cosine of the angle between them) of the two vectors Divide by the sample size Interpreting covariance is hard We know a small covariance, close to 0, means there isn't much correlation between the two variables. And large covariances - that is, far from 0 (could be negative for inverse relationships) mean there is a correlation. Then the question is - how large does it have to be?","title":"Covariance"},{"location":"Data%20Science/02-statistics-and-probability-refresher/08-covariance-and-correlation/#correltation","text":"Just divide the covariance by the standard deviations of both variables and that normalizes things. So a correlation results: 0 means no correlation 1 means perfect correlation -1 means perfect inverse correlation Remember that correlation does not imply causation. Only a controlled, randomized experiment can give you insights on causation. Use correlation to decide what experiments to conduct!","title":"Correltation"},{"location":"Data%20Science/02-statistics-and-probability-refresher/09-conditional-probability/","text":"Conditional Probability \u00b6 Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/08-conditional-probability.ipynb If I have two events that depend on each other, what's the probability that both will occur? Notation: \\(P(A, B)\\) is the probability of A and B both occuring \\(P(B|A)\\) - probability of B given that A has occurred. \\[P(B|A) = \\frac{P(A,B)}{P(A)}\\] Example \u00b6 I give my students two tests. 60% of my students passed both tests, but the first test was easier. 80% passed that one. What percentage of students who passed the first test also the second one? A - passing the first test, B - passing the second test So we are asking for \\(P(B|A)\\) - the probability of B given A \\(P(B|A) = \\frac{P(A,B)}{P(A)} = \\frac{0.6}{0.8} = 0.75\\) 75% of students who passed the first test passed the second as well.","title":"Conditional Probability"},{"location":"Data%20Science/02-statistics-and-probability-refresher/09-conditional-probability/#conditional-probability","text":"Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/08-conditional-probability.ipynb If I have two events that depend on each other, what's the probability that both will occur? Notation: \\(P(A, B)\\) is the probability of A and B both occuring \\(P(B|A)\\) - probability of B given that A has occurred. \\[P(B|A) = \\frac{P(A,B)}{P(A)}\\]","title":"Conditional Probability"},{"location":"Data%20Science/02-statistics-and-probability-refresher/09-conditional-probability/#example","text":"I give my students two tests. 60% of my students passed both tests, but the first test was easier. 80% passed that one. What percentage of students who passed the first test also the second one? A - passing the first test, B - passing the second test So we are asking for \\(P(B|A)\\) - the probability of B given A \\(P(B|A) = \\frac{P(A,B)}{P(A)} = \\frac{0.6}{0.8} = 0.75\\) 75% of students who passed the first test passed the second as well.","title":"Example"},{"location":"Data%20Science/02-statistics-and-probability-refresher/10-bayes-theorem/","text":"Bayes' theorem \u00b6 \\[ P(A|B) = \\frac{P(A)P(B|A)}{P(B)} \\] The probability of A given B, is the probability of A times the probability of B given A over a probability of B. The key insight is that the probability of something that depends on B depends very much on the base probability of B and A. Examples \u00b6 Drug testing is a common example. Even a \"highly accurate\" drug test can produce more false positives than true positives. Let's say we have a drug test that can accurately identify users of a drug 99% of the time and accurately has a negative result for 99% of non-users. But only 0.3 of the overall population actually uses this drug. Event A - is a user of the drug, Event B - tested positively for the drug. We can work out from that information that P(B) is 1.3% (0.99 * 0.003 + 0.01 * 0.997) - the probability of testing positive if you don't) \\(P(A|B) = \\frac{P(A)P(B|A)}{P(B)} = \\frac{0.003*0.99}{0.013}=22.8%\\) So the odds of someone being an actual user of the drug given that they tested positive is only 22.8% Even though P(B|A) is high (99%), it doesn't mean P(A|B) is high.","title":"Bayes' theorem"},{"location":"Data%20Science/02-statistics-and-probability-refresher/10-bayes-theorem/#bayes-theorem","text":"\\[ P(A|B) = \\frac{P(A)P(B|A)}{P(B)} \\] The probability of A given B, is the probability of A times the probability of B given A over a probability of B. The key insight is that the probability of something that depends on B depends very much on the base probability of B and A.","title":"Bayes' theorem"},{"location":"Data%20Science/02-statistics-and-probability-refresher/10-bayes-theorem/#examples","text":"Drug testing is a common example. Even a \"highly accurate\" drug test can produce more false positives than true positives. Let's say we have a drug test that can accurately identify users of a drug 99% of the time and accurately has a negative result for 99% of non-users. But only 0.3 of the overall population actually uses this drug. Event A - is a user of the drug, Event B - tested positively for the drug. We can work out from that information that P(B) is 1.3% (0.99 * 0.003 + 0.01 * 0.997) - the probability of testing positive if you don't) \\(P(A|B) = \\frac{P(A)P(B|A)}{P(B)} = \\frac{0.003*0.99}{0.013}=22.8%\\) So the odds of someone being an actual user of the drug given that they tested positive is only 22.8% Even though P(B|A) is high (99%), it doesn't mean P(A|B) is high.","title":"Examples"},{"location":"Data%20Science/03-predictive-models/01-linear-regression/","text":"Linear regression \u00b6 Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/09-linear-regression.ipynb Fit a line to a data set of observations Use this line to predict unobserved values How does it work? \u00b6 Usually using \"least squares\" Minimizes the squared-error between each point and the line Remember the slope-intercept equation of a line? y=mx+b The slope is the correlation between the two variables times the standard deviation in Y, all divided by the standard deviation in X. The intercept is the mean of Y minus the slope times the mean of X But python will do all that for you. There are more than one way to do it Gradient descent is an alternate method to least squares Basically iterates to find the line that best follows the contours defined by the data. Can make sense when dealing with 3D data. Easy to try in python and just compare the results to least squares But usually least squares is a perfecly good choice. Measuring error with r-squared \u00b6 How do we measure how well our line fits our data? R-squared (aka coefficient of determination) measures: The fraction of the total variation in Y that is captured by the model. \\[1.0-\\frac{sum of squared errors}{sum of squared variation from mean}\\] Interpreting r-squared \u00b6 Ranges from 0 to 1 0 is bad (none of the variance is captured), 1 is good (all of the variance is captured).","title":"Linear regression"},{"location":"Data%20Science/03-predictive-models/01-linear-regression/#linear-regression","text":"Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/09-linear-regression.ipynb Fit a line to a data set of observations Use this line to predict unobserved values","title":"Linear regression"},{"location":"Data%20Science/03-predictive-models/01-linear-regression/#how-does-it-work","text":"Usually using \"least squares\" Minimizes the squared-error between each point and the line Remember the slope-intercept equation of a line? y=mx+b The slope is the correlation between the two variables times the standard deviation in Y, all divided by the standard deviation in X. The intercept is the mean of Y minus the slope times the mean of X But python will do all that for you. There are more than one way to do it Gradient descent is an alternate method to least squares Basically iterates to find the line that best follows the contours defined by the data. Can make sense when dealing with 3D data. Easy to try in python and just compare the results to least squares But usually least squares is a perfecly good choice.","title":"How does it work?"},{"location":"Data%20Science/03-predictive-models/01-linear-regression/#measuring-error-with-r-squared","text":"How do we measure how well our line fits our data? R-squared (aka coefficient of determination) measures: The fraction of the total variation in Y that is captured by the model. \\[1.0-\\frac{sum of squared errors}{sum of squared variation from mean}\\]","title":"Measuring error with r-squared"},{"location":"Data%20Science/03-predictive-models/01-linear-regression/#interpreting-r-squared","text":"Ranges from 0 to 1 0 is bad (none of the variance is captured), 1 is good (all of the variance is captured).","title":"Interpreting r-squared"},{"location":"Data%20Science/03-predictive-models/02-polynomial-regression/","text":"Polynomial regression \u00b6 Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/10-polynomial-regression.ipynb Not all relationships are linear We can use higher orders of polynomials to produce more complex curves. First order: \\(y = mx + b\\) Second order: \\(y = ax^2 + bx + c\\) Third order: \\(y = ax^3 + bx^2 + cx + d\\) Beware of overfitting \u00b6 Don't use more degrees than you need to Visualize your data first to see how complex of a curve there might be Visualize the fit - is your curve going out of it's wat to accommodate outliers? A high r-squared simply means your curve fits your training data well, but it may not be a good predictor. Later we'll talk about more principled ways to detect overfitting (train/test)","title":"Polynomial regression"},{"location":"Data%20Science/03-predictive-models/02-polynomial-regression/#polynomial-regression","text":"Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/10-polynomial-regression.ipynb Not all relationships are linear We can use higher orders of polynomials to produce more complex curves. First order: \\(y = mx + b\\) Second order: \\(y = ax^2 + bx + c\\) Third order: \\(y = ax^3 + bx^2 + cx + d\\)","title":"Polynomial regression"},{"location":"Data%20Science/03-predictive-models/02-polynomial-regression/#beware-of-overfitting","text":"Don't use more degrees than you need to Visualize your data first to see how complex of a curve there might be Visualize the fit - is your curve going out of it's wat to accommodate outliers? A high r-squared simply means your curve fits your training data well, but it may not be a good predictor. Later we'll talk about more principled ways to detect overfitting (train/test)","title":"Beware of overfitting"},{"location":"Data%20Science/03-predictive-models/03-multiple-regression/","text":"Multiple regression \u00b6 Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/11-multiple-regression.ipynb What if more than one variable influences the one you're interested in? Example: predicting a price for a car based on it's many attributes (body style, brand, mileage, etc) If you also have multiple dependent variables - things you're trying to predict - that's a \"multivariate regression\" It sill uses least squares \u00b6 We just end up with coefficients for each factor. For example, price \\(\\alpha+\\beta_1mileage + \\beta_2age + \\beta_3doors\\) These coefficients imply how important each factor is (if the data is all normalized!) Get rid of ones that doesn't matter! Can still measure fit with r-squared Need to assume the different factors are not themselves dependent on each other.","title":"Multiple regression"},{"location":"Data%20Science/03-predictive-models/03-multiple-regression/#multiple-regression","text":"Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/11-multiple-regression.ipynb What if more than one variable influences the one you're interested in? Example: predicting a price for a car based on it's many attributes (body style, brand, mileage, etc) If you also have multiple dependent variables - things you're trying to predict - that's a \"multivariate regression\"","title":"Multiple regression"},{"location":"Data%20Science/03-predictive-models/03-multiple-regression/#it-sill-uses-least-squares","text":"We just end up with coefficients for each factor. For example, price \\(\\alpha+\\beta_1mileage + \\beta_2age + \\beta_3doors\\) These coefficients imply how important each factor is (if the data is all normalized!) Get rid of ones that doesn't matter! Can still measure fit with r-squared Need to assume the different factors are not themselves dependent on each other.","title":"It sill uses least squares"},{"location":"Data%20Science/03-predictive-models/04-multi-level-models/","text":"Multi-level models \u00b6 The concept is that some effects happen at various levels. Example: your health depends on a hierarchy of the health of your cells, organs, you as a whole, your family, city and the world you live in. Your wealth depends on your own work, that your parents did, what your grandparents did etc. Multi-level models attemt to model and account for these interdependencies. Modeling multiple levels \u00b6 You must identify the factors that affect the ourcome you're trying to predict at each level. For example - SAT scores might be predicted based on the genetics of individual children, the home environment, the crime rate of the neighborhood they live in, the quality of the teachers, the funding of the school district and the education policies of the state. Some of these factors affect more than one level. For example, crime rate might influence the home environment too.","title":"Multi-level models"},{"location":"Data%20Science/03-predictive-models/04-multi-level-models/#multi-level-models","text":"The concept is that some effects happen at various levels. Example: your health depends on a hierarchy of the health of your cells, organs, you as a whole, your family, city and the world you live in. Your wealth depends on your own work, that your parents did, what your grandparents did etc. Multi-level models attemt to model and account for these interdependencies.","title":"Multi-level models"},{"location":"Data%20Science/03-predictive-models/04-multi-level-models/#modeling-multiple-levels","text":"You must identify the factors that affect the ourcome you're trying to predict at each level. For example - SAT scores might be predicted based on the genetics of individual children, the home environment, the crime rate of the neighborhood they live in, the quality of the teachers, the funding of the school district and the education policies of the state. Some of these factors affect more than one level. For example, crime rate might influence the home environment too.","title":"Modeling multiple levels"},{"location":"Data%20Science/04-machine-learning-with-python/01-supervised-vs-unsupervised-learning/","text":"Supervised vs Unsupervised learning \u00b6 What is machine learning? \u00b6 Algorithms that can learn from observational data and can make predictions based on it. Pretty much what your own brain does too. Unsupervised learning \u00b6 The model is not given any \"answers\" to learn from. It must make sense of the data just given the observations themselves. Example: group (cluster) some objects together into 2 different sets. But I don't tell you what the \"right\" set is for any object ahead of time. Maybe you don't know what you're looking for - you're looking for latent variables. Example: clustering users on a dating site based on their information and behaviour. Perhaps you'll find there are groups of people that emerge that don't conform to your known stereotypes. Cluster movies based on their properties. Perhaps our current concepts of genre are outdated? Analyze the test of product descriptions to find the terms that carry the most meaning for a certain category. Supervised learning \u00b6 In supervised learing, the algorithm looks at the given observations and tries to predict the correct answers. The model created is then used to predict the answer for new, unknown values. Example: You can train a model for predicting car prices based on car attributes using historical sales data. That model can then predict the optimal price for new cars that haven't been sold before. Evaluating Supervised Learning \u00b6 If you have a set of training data that includes the value you're trying to predict - you don't have to guess if the resulting model is good or not. If you have enough training data, you can split it into two parts: a training set and test set. You then train the model using only the training set. And then measure (using r-squared or some other metric) the model's accuracy by asking it to predict values for the test set and compare that to the known, true values.","title":"Supervised vs Unsupervised learning"},{"location":"Data%20Science/04-machine-learning-with-python/01-supervised-vs-unsupervised-learning/#supervised-vs-unsupervised-learning","text":"","title":"Supervised vs Unsupervised learning"},{"location":"Data%20Science/04-machine-learning-with-python/01-supervised-vs-unsupervised-learning/#what-is-machine-learning","text":"Algorithms that can learn from observational data and can make predictions based on it. Pretty much what your own brain does too.","title":"What is machine learning?"},{"location":"Data%20Science/04-machine-learning-with-python/01-supervised-vs-unsupervised-learning/#unsupervised-learning","text":"The model is not given any \"answers\" to learn from. It must make sense of the data just given the observations themselves. Example: group (cluster) some objects together into 2 different sets. But I don't tell you what the \"right\" set is for any object ahead of time. Maybe you don't know what you're looking for - you're looking for latent variables. Example: clustering users on a dating site based on their information and behaviour. Perhaps you'll find there are groups of people that emerge that don't conform to your known stereotypes. Cluster movies based on their properties. Perhaps our current concepts of genre are outdated? Analyze the test of product descriptions to find the terms that carry the most meaning for a certain category.","title":"Unsupervised learning"},{"location":"Data%20Science/04-machine-learning-with-python/01-supervised-vs-unsupervised-learning/#supervised-learning","text":"In supervised learing, the algorithm looks at the given observations and tries to predict the correct answers. The model created is then used to predict the answer for new, unknown values. Example: You can train a model for predicting car prices based on car attributes using historical sales data. That model can then predict the optimal price for new cars that haven't been sold before.","title":"Supervised learning"},{"location":"Data%20Science/04-machine-learning-with-python/01-supervised-vs-unsupervised-learning/#evaluating-supervised-learning","text":"If you have a set of training data that includes the value you're trying to predict - you don't have to guess if the resulting model is good or not. If you have enough training data, you can split it into two parts: a training set and test set. You then train the model using only the training set. And then measure (using r-squared or some other metric) the model's accuracy by asking it to predict values for the test set and compare that to the known, true values.","title":"Evaluating Supervised Learning"},{"location":"Data%20Science/04-machine-learning-with-python/02-train-test-split/","text":"Train/test split \u00b6 Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/12-train-test-split.ipynb Train / test in practice \u00b6 Need to ensure both sets are large enough to contain representatives of all the variations and outliers in the data you care about The data sets must be selected randomly Train/test is a great way to guard against overfitting Train/test is not infallible \u00b6 Maybe your sample sizes are too small Or due to random chance your train and test sets look remarkably similar Overfitting can still happen K-fold Cross validation \u00b6 One way to further protect against overfitting is K-fold cross validation Sounds complicated, but it's not: Split your data into K randomly assigned segments Reserve one segment as your test data Train on each of the remaining K-1 segments and measure their performance against the test set. Take the average of the K-1 r-squared scores","title":"Train/test split"},{"location":"Data%20Science/04-machine-learning-with-python/02-train-test-split/#traintest-split","text":"Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/12-train-test-split.ipynb","title":"Train/test split"},{"location":"Data%20Science/04-machine-learning-with-python/02-train-test-split/#train-test-in-practice","text":"Need to ensure both sets are large enough to contain representatives of all the variations and outliers in the data you care about The data sets must be selected randomly Train/test is a great way to guard against overfitting","title":"Train / test in practice"},{"location":"Data%20Science/04-machine-learning-with-python/02-train-test-split/#traintest-is-not-infallible","text":"Maybe your sample sizes are too small Or due to random chance your train and test sets look remarkably similar Overfitting can still happen","title":"Train/test is not infallible"},{"location":"Data%20Science/04-machine-learning-with-python/02-train-test-split/#k-fold-cross-validation","text":"One way to further protect against overfitting is K-fold cross validation Sounds complicated, but it's not: Split your data into K randomly assigned segments Reserve one segment as your test data Train on each of the remaining K-1 segments and measure their performance against the test set. Take the average of the K-1 r-squared scores","title":"K-fold Cross validation"},{"location":"Data%20Science/04-machine-learning-with-python/03-bayesian-methods/","text":"Bayesian methods: Concepts \u00b6 Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/14-naive-bayes-spam-classifier.ipynb Remember Baye's theorem? \u00b6 \\(P(A|B) = \\frac{P(A)P(B|A)}{P(B)}\\) Let's use it for machine learning! I want a spam classifier. Example: How would we express the probability of an email being spam if it contains the word \"free\"? \\(P(Spam|Free)=\\frac{P(Spam)P(Free|Spam)}{P(Free)}\\) The numerator is the probability of a message being spam and containing the word \"free\" (this is subtly different from what we're looking for). The denominator is the overall probability of an email containing the word \"free\". (Equivalent to \\(P(Free|Spam)P(Spam) + P(Free|Not Spam)P(Not Spam)\\) ) So together - this ratio is the % of emails with the word \"free\" that are spam. What about all the other words? \u00b6 We can construct \\(P(Spam | Word)\\) for every meaningful word we encounter during training Then multiply these together when analyzing a new email to get the probability of it being spam. Assumes the presence of different words are independent of each other - one reason this is called \"Naive Bayes\". Sounds like a lot of work. \u00b6 Scikit-learn to the rescue! The CountVectorizer lets us operate on lots of words at once, and MultinomialNB does all the heavy lifting on Naive Bayes. We'll train it on known sets of spam and \"ham\" (non-spam) emails.","title":"Bayesian methods: Concepts"},{"location":"Data%20Science/04-machine-learning-with-python/03-bayesian-methods/#bayesian-methods-concepts","text":"Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/14-naive-bayes-spam-classifier.ipynb","title":"Bayesian methods: Concepts"},{"location":"Data%20Science/04-machine-learning-with-python/03-bayesian-methods/#remember-bayes-theorem","text":"\\(P(A|B) = \\frac{P(A)P(B|A)}{P(B)}\\) Let's use it for machine learning! I want a spam classifier. Example: How would we express the probability of an email being spam if it contains the word \"free\"? \\(P(Spam|Free)=\\frac{P(Spam)P(Free|Spam)}{P(Free)}\\) The numerator is the probability of a message being spam and containing the word \"free\" (this is subtly different from what we're looking for). The denominator is the overall probability of an email containing the word \"free\". (Equivalent to \\(P(Free|Spam)P(Spam) + P(Free|Not Spam)P(Not Spam)\\) ) So together - this ratio is the % of emails with the word \"free\" that are spam.","title":"Remember Baye's theorem?"},{"location":"Data%20Science/04-machine-learning-with-python/03-bayesian-methods/#what-about-all-the-other-words","text":"We can construct \\(P(Spam | Word)\\) for every meaningful word we encounter during training Then multiply these together when analyzing a new email to get the probability of it being spam. Assumes the presence of different words are independent of each other - one reason this is called \"Naive Bayes\".","title":"What about all the other words?"},{"location":"Data%20Science/04-machine-learning-with-python/03-bayesian-methods/#sounds-like-a-lot-of-work","text":"Scikit-learn to the rescue! The CountVectorizer lets us operate on lots of words at once, and MultinomialNB does all the heavy lifting on Naive Bayes. We'll train it on known sets of spam and \"ham\" (non-spam) emails.","title":"Sounds like a lot of work."},{"location":"Data%20Science/04-machine-learning-with-python/04-k-means-clustering/","text":"K-means clustering \u00b6 Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/14-naive-bayes-spam-classifier.ipynb Attempts to split data into K groups that are closest to K centroids Unsupervised learning - uses only the positions of each data point. Can uncover interesting groupings of people / things / behaviour Example: Where do millionaires live? What genres of music / movies / etc naturally fall out of data? Create your own stereotypes from demographic data How it works \u00b6 Randomly pick K centroids (k-means) Assign each data point to the centroid it's closest to Recompute the centroids based on the average position of centroid's points Iterate until points stop changing assignment to centroids If you want to predict the cluster for new points, just find the centroid they're closest to. Gotchas \u00b6 Choosing K Try increasing K values until you stop getting large reductions in squared error (distances from each point to their centroids) Avoiding local minima The random choice of initial centroids can yield different results Run it a few times just to make sure your initial results aren't wacky Labeling the clusters K-means does not attempt to assign any meaning to the clusters you find It's up to you to dig into the data and try to determine that","title":"K-means clustering"},{"location":"Data%20Science/04-machine-learning-with-python/04-k-means-clustering/#k-means-clustering","text":"Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/14-naive-bayes-spam-classifier.ipynb Attempts to split data into K groups that are closest to K centroids Unsupervised learning - uses only the positions of each data point. Can uncover interesting groupings of people / things / behaviour Example: Where do millionaires live? What genres of music / movies / etc naturally fall out of data? Create your own stereotypes from demographic data","title":"K-means clustering"},{"location":"Data%20Science/04-machine-learning-with-python/04-k-means-clustering/#how-it-works","text":"Randomly pick K centroids (k-means) Assign each data point to the centroid it's closest to Recompute the centroids based on the average position of centroid's points Iterate until points stop changing assignment to centroids If you want to predict the cluster for new points, just find the centroid they're closest to.","title":"How it works"},{"location":"Data%20Science/04-machine-learning-with-python/04-k-means-clustering/#gotchas","text":"Choosing K Try increasing K values until you stop getting large reductions in squared error (distances from each point to their centroids) Avoiding local minima The random choice of initial centroids can yield different results Run it a few times just to make sure your initial results aren't wacky Labeling the clusters K-means does not attempt to assign any meaning to the clusters you find It's up to you to dig into the data and try to determine that","title":"Gotchas"},{"location":"Data%20Science/04-machine-learning-with-python/05-measuring-entropy/","text":"Measuring Entropy \u00b6 A measure of a data set's disorder - how same or different it is. If we classify a data set into N different classes (example: a data set of animal attributes and their species) The entropy is 0 if all of the classes in that data are the same (everyone is an iguana) The entropy is high if they're all different $$H(S) = -p_1 ln p_1 - ... - p_n ln p_n $$ - \\(p_i\\) represents the proportion of the data labeled for each class. - Each term looks like this.","title":"Measuring Entropy"},{"location":"Data%20Science/04-machine-learning-with-python/05-measuring-entropy/#measuring-entropy","text":"A measure of a data set's disorder - how same or different it is. If we classify a data set into N different classes (example: a data set of animal attributes and their species) The entropy is 0 if all of the classes in that data are the same (everyone is an iguana) The entropy is high if they're all different $$H(S) = -p_1 ln p_1 - ... - p_n ln p_n $$ - \\(p_i\\) represents the proportion of the data labeled for each class. - Each term looks like this.","title":"Measuring Entropy"},{"location":"Data%20Science/04-machine-learning-with-python/06-installing-graphviz/","text":"Installing graphviz \u00b6 On windows \u00b6 It is already installed on anaconda. On mac \u00b6 Install homebrew from https://brew.sh/ brew install graphviz On linux \u00b6 sudo apt install graphviz sudo pacman -S graphviz","title":"Installing graphviz"},{"location":"Data%20Science/04-machine-learning-with-python/06-installing-graphviz/#installing-graphviz","text":"","title":"Installing graphviz"},{"location":"Data%20Science/04-machine-learning-with-python/06-installing-graphviz/#on-windows","text":"It is already installed on anaconda.","title":"On windows"},{"location":"Data%20Science/04-machine-learning-with-python/06-installing-graphviz/#on-mac","text":"Install homebrew from https://brew.sh/ brew install graphviz","title":"On mac"},{"location":"Data%20Science/04-machine-learning-with-python/06-installing-graphviz/#on-linux","text":"sudo apt install graphviz sudo pacman -S graphviz","title":"On linux"},{"location":"Data%20Science/04-machine-learning-with-python/07-decision-trees/","text":"Decision trees \u00b6 Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/16-decision-trees.ipynb You can construct a flowchart to help you decide a classification for something with machine learning This is called a decision tree Another form of supervised learing Give if some sample data and the resulting classifications Example \u00b6 You want to build a system to filter out resumes based on historical hiring data You have a database of some important attributes of job candidates and you know which ones were hired and which ones weren't You can train a decision tree on this data, and arrive at a system for predicting whether a candidate will get hired based on it! How Decision Trees work \u00b6 At each step, find the attribute we can use to partition the data set to minimize the entropy of the data at the next step Fancy term for this simple algorithm: ID3 It's a greedy algorithm - as it goes down the tree, it just picks the decision that reduce entropy the most at that stage. That might not actually result in an optimal tree It works Random forests \u00b6 Decision trees are very susceptible to overfitting To fight this, we can construct several alternate decision trees and let them \"vote\" on the final classification Randomly re-sample the input data for each tree (fancy term for this: bootstrap aggregating or bagging) Randomize a subset of the attributes each step is allowed to choose from","title":"Decision trees"},{"location":"Data%20Science/04-machine-learning-with-python/07-decision-trees/#decision-trees","text":"Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/16-decision-trees.ipynb You can construct a flowchart to help you decide a classification for something with machine learning This is called a decision tree Another form of supervised learing Give if some sample data and the resulting classifications","title":"Decision trees"},{"location":"Data%20Science/04-machine-learning-with-python/07-decision-trees/#example","text":"You want to build a system to filter out resumes based on historical hiring data You have a database of some important attributes of job candidates and you know which ones were hired and which ones weren't You can train a decision tree on this data, and arrive at a system for predicting whether a candidate will get hired based on it!","title":"Example"},{"location":"Data%20Science/04-machine-learning-with-python/07-decision-trees/#how-decision-trees-work","text":"At each step, find the attribute we can use to partition the data set to minimize the entropy of the data at the next step Fancy term for this simple algorithm: ID3 It's a greedy algorithm - as it goes down the tree, it just picks the decision that reduce entropy the most at that stage. That might not actually result in an optimal tree It works","title":"How Decision Trees work"},{"location":"Data%20Science/04-machine-learning-with-python/07-decision-trees/#random-forests","text":"Decision trees are very susceptible to overfitting To fight this, we can construct several alternate decision trees and let them \"vote\" on the final classification Randomly re-sample the input data for each tree (fancy term for this: bootstrap aggregating or bagging) Randomize a subset of the attributes each step is allowed to choose from","title":"Random forests"},{"location":"Data%20Science/04-machine-learning-with-python/08-ensemble-learning/","text":"Ensemble learning \u00b6 Random forests was an example of ensemble learning It just means we use multiple models to try and solve the same problem, and let them vote on the results. Random Forests uses bagging (bootstrap aggregating) to implement ensemble learning. Many models are built by training or randomly-drawn subsets of the data. Boosting is an alternate technique where each subsequent model in the ensemble boosts attributes that address data mis-classified by the previous model A bucket of models trains several different models using training data and picks the one that works best with the test data Stacking runs multiple models at once on the data and combines the results together This is how the netflix prize was won. Advanced Ensemble learning \u00b6 Bayes Optimal Classifier Theoretically the best - but always impractical Bayesian Parameter Averaging Attemts to make BOC practical - but it's still misunderstood, susceptible to overfitting and often outperformed by the simpler bagging approach Bayesian Model Combination Tries to address all of those problem But in the end, it's about the same as using cross-validation to find the best combination of models","title":"Ensemble learning"},{"location":"Data%20Science/04-machine-learning-with-python/08-ensemble-learning/#ensemble-learning","text":"Random forests was an example of ensemble learning It just means we use multiple models to try and solve the same problem, and let them vote on the results. Random Forests uses bagging (bootstrap aggregating) to implement ensemble learning. Many models are built by training or randomly-drawn subsets of the data. Boosting is an alternate technique where each subsequent model in the ensemble boosts attributes that address data mis-classified by the previous model A bucket of models trains several different models using training data and picks the one that works best with the test data Stacking runs multiple models at once on the data and combines the results together This is how the netflix prize was won.","title":"Ensemble learning"},{"location":"Data%20Science/04-machine-learning-with-python/08-ensemble-learning/#advanced-ensemble-learning","text":"Bayes Optimal Classifier Theoretically the best - but always impractical Bayesian Parameter Averaging Attemts to make BOC practical - but it's still misunderstood, susceptible to overfitting and often outperformed by the simpler bagging approach Bayesian Model Combination Tries to address all of those problem But in the end, it's about the same as using cross-validation to find the best combination of models","title":"Advanced Ensemble learning"},{"location":"Data%20Science/04-machine-learning-with-python/09-xgboost/","text":"XGBoost \u00b6 Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/17-xgboost.ipynb stands for eXtreme Gradient Boosted trees Remember boosting is an ensemble method Each tree boosts attributes that led to mis-classifications of previous tree It is amazing routinely wins Kaggle competitions Easy to use Fast A good choice for an algorithm to start with Features of XGBoost \u00b6 Regularized boosting (prevents overfitting) Can handle missing values automatically Parallel processing Can cross-validate at each iteration Enables early stopping, finding optimal number of iterations. Incremental training Can plug in your own optimization objectives Tree pruning Generally results in deeper, but optimized trees Using XGBoost \u00b6 pip install xgboost Also CLI, C++, R, Julia, JVM interfaces It's not just made for scikit_learn so it has it's own interface Uses DMatrix structure to hold features & labels Can create this easily from a numpy array though All parameters passed via a dictionary Call train, then predict. XGBoost hyperparameters \u00b6 Booster gbtree or gblinear Objective (ie multi:softmax, multi:softprob) Eta (learning rate - adjusts weights on each step) max_depth (depth of the tree) min_child_weight (can control overfitting, but too high will underfit) ... many others It's almost all that you need to know for ML in practical terms, at least for simple classification or regression problems.","title":"XGBoost"},{"location":"Data%20Science/04-machine-learning-with-python/09-xgboost/#xgboost","text":"Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/17-xgboost.ipynb stands for eXtreme Gradient Boosted trees Remember boosting is an ensemble method Each tree boosts attributes that led to mis-classifications of previous tree It is amazing routinely wins Kaggle competitions Easy to use Fast A good choice for an algorithm to start with","title":"XGBoost"},{"location":"Data%20Science/04-machine-learning-with-python/09-xgboost/#features-of-xgboost","text":"Regularized boosting (prevents overfitting) Can handle missing values automatically Parallel processing Can cross-validate at each iteration Enables early stopping, finding optimal number of iterations. Incremental training Can plug in your own optimization objectives Tree pruning Generally results in deeper, but optimized trees","title":"Features of XGBoost"},{"location":"Data%20Science/04-machine-learning-with-python/09-xgboost/#using-xgboost","text":"pip install xgboost Also CLI, C++, R, Julia, JVM interfaces It's not just made for scikit_learn so it has it's own interface Uses DMatrix structure to hold features & labels Can create this easily from a numpy array though All parameters passed via a dictionary Call train, then predict.","title":"Using XGBoost"},{"location":"Data%20Science/04-machine-learning-with-python/09-xgboost/#xgboost-hyperparameters","text":"Booster gbtree or gblinear Objective (ie multi:softmax, multi:softprob) Eta (learning rate - adjusts weights on each step) max_depth (depth of the tree) min_child_weight (can control overfitting, but too high will underfit) ... many others It's almost all that you need to know for ML in practical terms, at least for simple classification or regression problems.","title":"XGBoost hyperparameters"},{"location":"Data%20Science/04-machine-learning-with-python/10-support-vector-machines/","text":"Support Vector Machines \u00b6 Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/18-support-vector-machines.ipynb Works well for classifying higher-dimensional data (lots of features) Finds higher dimensional support vecors accross which to divide the data (mathematically, these support vectors define hyperplanes.) Uses something called kernel trick to represent data in higher-dimensional spaces to find hyperplanes that might not be apparent in lower dimensions. Support Vector Classification \u00b6 In practice you'll use something called SVC to classify data using SVM. You can use different \"kernels\" with SVC. Some will work better than others for a given data set.","title":"Support Vector Machines"},{"location":"Data%20Science/04-machine-learning-with-python/10-support-vector-machines/#support-vector-machines","text":"Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/18-support-vector-machines.ipynb Works well for classifying higher-dimensional data (lots of features) Finds higher dimensional support vecors accross which to divide the data (mathematically, these support vectors define hyperplanes.) Uses something called kernel trick to represent data in higher-dimensional spaces to find hyperplanes that might not be apparent in lower dimensions.","title":"Support Vector Machines"},{"location":"Data%20Science/04-machine-learning-with-python/10-support-vector-machines/#support-vector-classification","text":"In practice you'll use something called SVC to classify data using SVM. You can use different \"kernels\" with SVC. Some will work better than others for a given data set.","title":"Support Vector Classification"},{"location":"Data%20Science/05-recommender-systems/01-user-based-collaborative-filtering/","text":"User-based collaborative filtering \u00b6 Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/19-movie-similarity-recommender-systems.ipynb build a matrix of things each user bought/viewed/rated Compute a similarity scores between users Find users similar to you Recommend stuff they bought/viewed/rated that you haven't yet. Problems with User-Based CF \u00b6 People are fickle, tastes change There are usually many more people than things - computationally more expensive than it should most of the time. People do bad things - easy to manipulate by creating fake personas on the platform.","title":"User-based collaborative filtering"},{"location":"Data%20Science/05-recommender-systems/01-user-based-collaborative-filtering/#user-based-collaborative-filtering","text":"Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/19-movie-similarity-recommender-systems.ipynb build a matrix of things each user bought/viewed/rated Compute a similarity scores between users Find users similar to you Recommend stuff they bought/viewed/rated that you haven't yet.","title":"User-based collaborative filtering"},{"location":"Data%20Science/05-recommender-systems/01-user-based-collaborative-filtering/#problems-with-user-based-cf","text":"People are fickle, tastes change There are usually many more people than things - computationally more expensive than it should most of the time. People do bad things - easy to manipulate by creating fake personas on the platform.","title":"Problems with User-Based CF"},{"location":"Data%20Science/05-recommender-systems/02-item-based-collaborative-filtering/","text":"Item-Based Collaborative Filtering \u00b6 Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/20-item-based-movie-similarity.ipynb What if we based recommendations on relationships between things instead of people? A movie will always be the same movie - it doesn't change. There are usually fewer things than people (less computation to do) Harder to game the system Find every pair of movies that were watched by the same person Measure the similarity of their ratings accross all users who watched both Sort by movie, then by similarity strength (this is just one way to do it.)","title":"Item-Based Collaborative Filtering"},{"location":"Data%20Science/05-recommender-systems/02-item-based-collaborative-filtering/#item-based-collaborative-filtering","text":"Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/20-item-based-movie-similarity.ipynb What if we based recommendations on relationships between things instead of people? A movie will always be the same movie - it doesn't change. There are usually fewer things than people (less computation to do) Harder to game the system Find every pair of movies that were watched by the same person Measure the similarity of their ratings accross all users who watched both Sort by movie, then by similarity strength (this is just one way to do it.)","title":"Item-Based Collaborative Filtering"},{"location":"Data%20Science/06-more-data-mining-and-machine-learning-techniques/01-k-nearest-neighbors/","text":"K-nearest neighbors (KNN) \u00b6 Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/21-knn-movie-rating-prediction.ipynb Used to classify new data points based on \"distance\" to known data Find the K nearest neighbors, based on your distance metric Let them all vote on the classification Although it's one of the simplest machine learning models there is - it qualifies as \"supervised learning\" But we can also do some complex tasks with it like finding movie similarities based on metadata.","title":"K-nearest neighbors (KNN)"},{"location":"Data%20Science/06-more-data-mining-and-machine-learning-techniques/01-k-nearest-neighbors/#k-nearest-neighbors-knn","text":"Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/21-knn-movie-rating-prediction.ipynb Used to classify new data points based on \"distance\" to known data Find the K nearest neighbors, based on your distance metric Let them all vote on the classification Although it's one of the simplest machine learning models there is - it qualifies as \"supervised learning\" But we can also do some complex tasks with it like finding movie similarities based on metadata.","title":"K-nearest neighbors (KNN)"},{"location":"Data%20Science/06-more-data-mining-and-machine-learning-techniques/02-dimensionality-reduction-pca/","text":"Dimensionality reduction. Principal Component Analysis. (PCA) \u00b6 Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/22-principal-component-analysis.ipynb What is the curse of dimensionality? \u00b6 Many problems can be thought of as having a huge number of dimensions. For example, in recommending movies, the rating vector for each movie may represent a dimension - every movie is it's own dimension! Dimensionality reduction attemts to distill higher-dimensional data down to a smaller number of dimensions, while preserving as much of the variance in the data as possible. K-means clustering algorithm can be an example of dimensionality reduction algorithm. It reduces data down to K dimensions. Another way: Principal Component Analysis (PCA) \u00b6 Involves fancy math - but at high level: Finds \"eigenvectors\" in the higher dimensional data These define hyperplanes that split the data while preserving the most variance in it The data gets projected onto these hyperplanes, which represent the lower dimensions you want to represent A popular implementation of this is called Singular Value Decomposition (SVD). Also really useful for things like image compression and facial recognition. Example: visualizing 4-D iris flower data \u00b6 The \"Iris dataset\" comes with scikit-learn An iris flower has petals and sepals (the lower, supportive part of the flower) We know the length and width of petals and sepals for many iris specimens. That's four dimensions. We also know the subspecies classification of each flower. PCA lets us visualize this in 2 dimensions instead of 4, while still preserving the variance.","title":"Dimensionality reduction. Principal Component Analysis. (PCA)"},{"location":"Data%20Science/06-more-data-mining-and-machine-learning-techniques/02-dimensionality-reduction-pca/#dimensionality-reduction-principal-component-analysis-pca","text":"Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/22-principal-component-analysis.ipynb","title":"Dimensionality reduction. Principal Component Analysis. (PCA)"},{"location":"Data%20Science/06-more-data-mining-and-machine-learning-techniques/02-dimensionality-reduction-pca/#what-is-the-curse-of-dimensionality","text":"Many problems can be thought of as having a huge number of dimensions. For example, in recommending movies, the rating vector for each movie may represent a dimension - every movie is it's own dimension! Dimensionality reduction attemts to distill higher-dimensional data down to a smaller number of dimensions, while preserving as much of the variance in the data as possible. K-means clustering algorithm can be an example of dimensionality reduction algorithm. It reduces data down to K dimensions.","title":"What is the curse of dimensionality?"},{"location":"Data%20Science/06-more-data-mining-and-machine-learning-techniques/02-dimensionality-reduction-pca/#another-way-principal-component-analysis-pca","text":"Involves fancy math - but at high level: Finds \"eigenvectors\" in the higher dimensional data These define hyperplanes that split the data while preserving the most variance in it The data gets projected onto these hyperplanes, which represent the lower dimensions you want to represent A popular implementation of this is called Singular Value Decomposition (SVD). Also really useful for things like image compression and facial recognition.","title":"Another  way: Principal Component Analysis (PCA)"},{"location":"Data%20Science/06-more-data-mining-and-machine-learning-techniques/02-dimensionality-reduction-pca/#example-visualizing-4-d-iris-flower-data","text":"The \"Iris dataset\" comes with scikit-learn An iris flower has petals and sepals (the lower, supportive part of the flower) We know the length and width of petals and sepals for many iris specimens. That's four dimensions. We also know the subspecies classification of each flower. PCA lets us visualize this in 2 dimensions instead of 4, while still preserving the variance.","title":"Example: visualizing 4-D iris flower data"},{"location":"Data%20Science/06-more-data-mining-and-machine-learning-techniques/03-data-warehousing-etl-and-elt/","text":"Data warehousing overview: ETL and ELT \u00b6 What is Data Warehousing? \u00b6 A large, centralized database that contains information from many sources Often used for business analysis in large corporations or organizations Queried via SQL or tools (i.e. Tableau) Often entire departments are dedicated to maintaining a data warehouse Data normalization is tricky - how dows all of this data relate to each other? What views do people need? Maintaining the data feeds is a lot of work Scaling is tricky ETL: Extract, Transform, Load \u00b6 ETL and ELT refer to how data gets into a data warehouse. Traditionally, the flow was Extract, Transform, Load: Raw data from operational systems is first periodically extracted Then the data is transformed into the schema needed by the data warehouse Finally, the data is loaded into the data warehouse, already in the structure needed But what if we're dealing with \"big data\"? That transform step can turn into a big problem. ELT: Extract, Load, Transform \u00b6 Today, a huge oracle instance isn't the only choice for a large data warehouse Things like Hive let you host massive databases on a Hadoop cluster Or, you might store in a large, distributed NoSQL data store and query it using things like Spark or MapReduce. The scalability of Hadoop let's you flip the loading process on it's head. Extract raw data as before Load it as is Then use the power of hadoop to transform it in place.","title":"Data warehousing overview: ETL and ELT"},{"location":"Data%20Science/06-more-data-mining-and-machine-learning-techniques/03-data-warehousing-etl-and-elt/#data-warehousing-overview-etl-and-elt","text":"","title":"Data warehousing overview: ETL and ELT"},{"location":"Data%20Science/06-more-data-mining-and-machine-learning-techniques/03-data-warehousing-etl-and-elt/#what-is-data-warehousing","text":"A large, centralized database that contains information from many sources Often used for business analysis in large corporations or organizations Queried via SQL or tools (i.e. Tableau) Often entire departments are dedicated to maintaining a data warehouse Data normalization is tricky - how dows all of this data relate to each other? What views do people need? Maintaining the data feeds is a lot of work Scaling is tricky","title":"What is Data Warehousing?"},{"location":"Data%20Science/06-more-data-mining-and-machine-learning-techniques/03-data-warehousing-etl-and-elt/#etl-extract-transform-load","text":"ETL and ELT refer to how data gets into a data warehouse. Traditionally, the flow was Extract, Transform, Load: Raw data from operational systems is first periodically extracted Then the data is transformed into the schema needed by the data warehouse Finally, the data is loaded into the data warehouse, already in the structure needed But what if we're dealing with \"big data\"? That transform step can turn into a big problem.","title":"ETL: Extract, Transform, Load"},{"location":"Data%20Science/06-more-data-mining-and-machine-learning-techniques/03-data-warehousing-etl-and-elt/#elt-extract-load-transform","text":"Today, a huge oracle instance isn't the only choice for a large data warehouse Things like Hive let you host massive databases on a Hadoop cluster Or, you might store in a large, distributed NoSQL data store and query it using things like Spark or MapReduce. The scalability of Hadoop let's you flip the loading process on it's head. Extract raw data as before Load it as is Then use the power of hadoop to transform it in place.","title":"ELT: Extract, Load, Transform"},{"location":"Data%20Science/06-more-data-mining-and-machine-learning-techniques/04-reinforcement-learning/","text":"Reinforcement Learning \u00b6 Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/22-reinforcement-learning.ipynb You have some sort of agent that explores some space As it goes, it learns the value of different state changes in different conditions Those values inform subsequent behaviour of the agent Examples: Pac-Man, Cat & mouse game Yields fast on-line performance once the space has been expored Q-learning \u00b6 A specific implementation of reinforcement learning You have: A set of enviromental variables s A set of possible actions in those states a A value of each state/action Q Start off with Q values of 0 Explore the space As bad things happen after a given state/action, reduce it's Q As rewards happen after a given state/action - increase it's Q What are some state/actions here? Pac-man has a wall to the west Pac-man dies if he moves one step south Pac-man just continues to live if going north or east You can \"look ahead\" more than one step by using a discount factor when computing Q (here s is previous state, s' is current state. \\(Q(s,a) += discount * (reward(s,a) + max(Q(s')) - Q(s,a))\\) The exploration problem \u00b6 How do we efficiently explore all of the possible states? Simple approach: always choose the action for a given state with the highest Q. If there's a tie, choose at random. But that's really inefficient, and you might miss a lot of paths that way. Better way: If a random number is less than epsilon, don't follow the highest Q, but choose at random That way, exploration never totally stops Choosing epsilon can be tricky. Recap \u00b6 You can make an intellingeng pac-man in a few steps: Have it semi-randomly explore different choices of movement (actions) given different conditions (states) Keep track of the reward or penalty associated with each choice for a given state/action (Q) Use those stored Q values to inform it's future choices Implementing Reinforcement Learning \u00b6 Python Markov Decision Process toolbox: https://pymdptoolbox.readthedocs.io/en/latest/ Cat&mouse example: https://github.com/studywolf/blog/tree/master/RL/Cat%20vs%20Mouse%20exploration","title":"Reinforcement Learning"},{"location":"Data%20Science/06-more-data-mining-and-machine-learning-techniques/04-reinforcement-learning/#reinforcement-learning","text":"Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/22-reinforcement-learning.ipynb You have some sort of agent that explores some space As it goes, it learns the value of different state changes in different conditions Those values inform subsequent behaviour of the agent Examples: Pac-Man, Cat & mouse game Yields fast on-line performance once the space has been expored","title":"Reinforcement Learning"},{"location":"Data%20Science/06-more-data-mining-and-machine-learning-techniques/04-reinforcement-learning/#q-learning","text":"A specific implementation of reinforcement learning You have: A set of enviromental variables s A set of possible actions in those states a A value of each state/action Q Start off with Q values of 0 Explore the space As bad things happen after a given state/action, reduce it's Q As rewards happen after a given state/action - increase it's Q What are some state/actions here? Pac-man has a wall to the west Pac-man dies if he moves one step south Pac-man just continues to live if going north or east You can \"look ahead\" more than one step by using a discount factor when computing Q (here s is previous state, s' is current state. \\(Q(s,a) += discount * (reward(s,a) + max(Q(s')) - Q(s,a))\\)","title":"Q-learning"},{"location":"Data%20Science/06-more-data-mining-and-machine-learning-techniques/04-reinforcement-learning/#the-exploration-problem","text":"How do we efficiently explore all of the possible states? Simple approach: always choose the action for a given state with the highest Q. If there's a tie, choose at random. But that's really inefficient, and you might miss a lot of paths that way. Better way: If a random number is less than epsilon, don't follow the highest Q, but choose at random That way, exploration never totally stops Choosing epsilon can be tricky.","title":"The exploration problem"},{"location":"Data%20Science/06-more-data-mining-and-machine-learning-techniques/04-reinforcement-learning/#recap","text":"You can make an intellingeng pac-man in a few steps: Have it semi-randomly explore different choices of movement (actions) given different conditions (states) Keep track of the reward or penalty associated with each choice for a given state/action (Q) Use those stored Q values to inform it's future choices","title":"Recap"},{"location":"Data%20Science/06-more-data-mining-and-machine-learning-techniques/04-reinforcement-learning/#implementing-reinforcement-learning","text":"Python Markov Decision Process toolbox: https://pymdptoolbox.readthedocs.io/en/latest/ Cat&mouse example: https://github.com/studywolf/blog/tree/master/RL/Cat%20vs%20Mouse%20exploration","title":"Implementing Reinforcement Learning"},{"location":"Data%20Science/06-more-data-mining-and-machine-learning-techniques/05-understanding-a-confusion-matrix/","text":"Understanding a confusion matrix \u00b6 Sometimes accuracy doesn't tell the whole story \u00b6 A test for a rare disease can be 99.9% accurate by just guessing \"no\" all the time. We need to understand true positives and true negative as well as false positives and false negatives. A confusion matrix shows this. Actual YES Actual NO Predicted YES True positives False positives Predicted NO False negatives True negatives","title":"Understanding a confusion matrix"},{"location":"Data%20Science/06-more-data-mining-and-machine-learning-techniques/05-understanding-a-confusion-matrix/#understanding-a-confusion-matrix","text":"","title":"Understanding a confusion matrix"},{"location":"Data%20Science/06-more-data-mining-and-machine-learning-techniques/05-understanding-a-confusion-matrix/#sometimes-accuracy-doesnt-tell-the-whole-story","text":"A test for a rare disease can be 99.9% accurate by just guessing \"no\" all the time. We need to understand true positives and true negative as well as false positives and false negatives. A confusion matrix shows this. Actual YES Actual NO Predicted YES True positives False positives Predicted NO False negatives True negatives","title":"Sometimes accuracy doesn't tell the whole story"},{"location":"Data%20Science/06-more-data-mining-and-machine-learning-techniques/06-measuring-classifiers-precision-recall-f1-roc-auc/","text":"Measuring Classifiers (Precision, Recall, F1, ROC, AUC) \u00b6 Recall \u00b6 \\[ \\frac{true positives}{true positives+false negatives}\\] Aka sensitivity, true positive rate, completeness Percent of positives correctly predicted Good choice of metric when you care a lot about false negatives i.e. fraud detection Actual fraud Actual not fraud Predicted fraud 5 20 Predicted not fraud 10 100 \\[ Recall = \\frac{TP}{TP+FN} = \\frac{5}{5+10} = \\frac{1}{3} = 33\\% \\] Precision \u00b6 \\[ \\frac{true positives}{true positives + false positives} \\] Aka correct positives Percent of relevant results Good choice of metric when you care a lot about false positives i.e., medical screening, drug testing Other metrics \u00b6 Specificity \\(\\frac{TN}{TN+FP} = true negative rate\\) F1 score \\(\\frac{2TP}{2TP+FP+FN}\\) \\(2 * \\frac{precision*recall}{precision+recall}\\) Harmonic mean of precision and sensitivity When you care about precision and recall RMSE Root mean squared error Accuracy measurement Only cares about right & wrong answers ROC Curve \u00b6 Receiver Operating Characteristic Curve Plot of true positive rate (recall) vs false positive rate at various threshold settings. Points above the diagonal represent good classigication (better than random) Ideal curve would just be a point in the upper left corner The more i's bent toward the upper left the better. AUC \u00b6 The are under the ROC curve Equal to probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one ROC AUC of 0.5 is a useless classifier, 1.0 is perfect. Commonly used metric for comparing classifiers.","title":"Measuring Classifiers (Precision, Recall, F1, ROC, AUC)"},{"location":"Data%20Science/06-more-data-mining-and-machine-learning-techniques/06-measuring-classifiers-precision-recall-f1-roc-auc/#measuring-classifiers-precision-recall-f1-roc-auc","text":"","title":"Measuring Classifiers (Precision, Recall, F1, ROC, AUC)"},{"location":"Data%20Science/06-more-data-mining-and-machine-learning-techniques/06-measuring-classifiers-precision-recall-f1-roc-auc/#recall","text":"\\[ \\frac{true positives}{true positives+false negatives}\\] Aka sensitivity, true positive rate, completeness Percent of positives correctly predicted Good choice of metric when you care a lot about false negatives i.e. fraud detection Actual fraud Actual not fraud Predicted fraud 5 20 Predicted not fraud 10 100 \\[ Recall = \\frac{TP}{TP+FN} = \\frac{5}{5+10} = \\frac{1}{3} = 33\\% \\]","title":"Recall"},{"location":"Data%20Science/06-more-data-mining-and-machine-learning-techniques/06-measuring-classifiers-precision-recall-f1-roc-auc/#precision","text":"\\[ \\frac{true positives}{true positives + false positives} \\] Aka correct positives Percent of relevant results Good choice of metric when you care a lot about false positives i.e., medical screening, drug testing","title":"Precision"},{"location":"Data%20Science/06-more-data-mining-and-machine-learning-techniques/06-measuring-classifiers-precision-recall-f1-roc-auc/#other-metrics","text":"Specificity \\(\\frac{TN}{TN+FP} = true negative rate\\) F1 score \\(\\frac{2TP}{2TP+FP+FN}\\) \\(2 * \\frac{precision*recall}{precision+recall}\\) Harmonic mean of precision and sensitivity When you care about precision and recall RMSE Root mean squared error Accuracy measurement Only cares about right & wrong answers","title":"Other metrics"},{"location":"Data%20Science/06-more-data-mining-and-machine-learning-techniques/06-measuring-classifiers-precision-recall-f1-roc-auc/#roc-curve","text":"Receiver Operating Characteristic Curve Plot of true positive rate (recall) vs false positive rate at various threshold settings. Points above the diagonal represent good classigication (better than random) Ideal curve would just be a point in the upper left corner The more i's bent toward the upper left the better.","title":"ROC Curve"},{"location":"Data%20Science/06-more-data-mining-and-machine-learning-techniques/06-measuring-classifiers-precision-recall-f1-roc-auc/#auc","text":"The are under the ROC curve Equal to probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one ROC AUC of 0.5 is a useless classifier, 1.0 is perfect. Commonly used metric for comparing classifiers.","title":"AUC"},{"location":"Data%20Science/07-dealing-with-real-world-data/01-bias-variance-tradeoff/","text":"Bias/Variance tradeoff \u00b6 Bias is how far removed the mean of your predicated values is from the real answer Variance is how scattered your predicted values are from real answer Describe the bias and variance of these four cases (assuming the center is correct) Often you need to choose between bias and variance It comes down to overfitting vs underfitting your data. But what you really case about is error - Bias and variance both contribute to error - \\(error = bias^2 + variance\\) - But it's error you want to minimize not bias or variance specifically - A complex model wil have high variance and low bias - A too-simple model will have low variance and high bias - But both may have the same error - the optimal complexity is in the middle. Trying it to earlier lessons \u00b6 Increasing K in KNN decreases variance and increases bias (by averaging together more neighbors) A single decision tree is prone to overfitting - high variance But a random forest decreases that variance.","title":"Bias/Variance tradeoff"},{"location":"Data%20Science/07-dealing-with-real-world-data/01-bias-variance-tradeoff/#biasvariance-tradeoff","text":"Bias is how far removed the mean of your predicated values is from the real answer Variance is how scattered your predicted values are from real answer Describe the bias and variance of these four cases (assuming the center is correct) Often you need to choose between bias and variance It comes down to overfitting vs underfitting your data. But what you really case about is error - Bias and variance both contribute to error - \\(error = bias^2 + variance\\) - But it's error you want to minimize not bias or variance specifically - A complex model wil have high variance and low bias - A too-simple model will have low variance and high bias - But both may have the same error - the optimal complexity is in the middle.","title":"Bias/Variance tradeoff"},{"location":"Data%20Science/07-dealing-with-real-world-data/01-bias-variance-tradeoff/#trying-it-to-earlier-lessons","text":"Increasing K in KNN decreases variance and increases bias (by averaging together more neighbors) A single decision tree is prone to overfitting - high variance But a random forest decreases that variance.","title":"Trying it to earlier lessons"},{"location":"Data%20Science/07-dealing-with-real-world-data/02-k-fold-cross-validation/","text":"K-fold cross validation to avoid overfitting \u00b6 Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/23-k-fold-cross-validation.ipynb One way to further protect against overfitting is K-fold cross validation Split your data into K randomly assigned segments Reserve one as your test data Train on the combined remaining K-1 segments and measure their performance against the test set Repeat for each segment Take the average of the K r-squared scores Prevents you from overfitting to a single train/test split","title":"K-fold cross validation to avoid overfitting"},{"location":"Data%20Science/07-dealing-with-real-world-data/02-k-fold-cross-validation/#k-fold-cross-validation-to-avoid-overfitting","text":"Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/23-k-fold-cross-validation.ipynb One way to further protect against overfitting is K-fold cross validation Split your data into K randomly assigned segments Reserve one as your test data Train on the combined remaining K-1 segments and measure their performance against the test set Repeat for each segment Take the average of the K r-squared scores Prevents you from overfitting to a single train/test split","title":"K-fold cross validation to avoid overfitting"},{"location":"Data%20Science/07-dealing-with-real-world-data/03-data-cleaning-and-normalization/","text":"Data cleaning and normalization \u00b6 Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/24-cleaning-data.ipynb The reality is, much of your time as a data scientist will be spent preparing and cleaning your data. Outliers Missing data Malicious data Erroneous data Irrelevant data Inconsistent data Formatting Garbage in, garbage out \u00b6 Look at your data, examine it. Question your results. And always do this - not just when you don't get a result that you like.","title":"Data cleaning and normalization"},{"location":"Data%20Science/07-dealing-with-real-world-data/03-data-cleaning-and-normalization/#data-cleaning-and-normalization","text":"Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/24-cleaning-data.ipynb The reality is, much of your time as a data scientist will be spent preparing and cleaning your data. Outliers Missing data Malicious data Erroneous data Irrelevant data Inconsistent data Formatting","title":"Data cleaning and normalization"},{"location":"Data%20Science/07-dealing-with-real-world-data/03-data-cleaning-and-normalization/#garbage-in-garbage-out","text":"Look at your data, examine it. Question your results. And always do this - not just when you don't get a result that you like.","title":"Garbage in, garbage out"},{"location":"Data%20Science/07-dealing-with-real-world-data/04-normalizing-numerical-data/","text":"Normalizing numerical data \u00b6 The importance of normalizing data \u00b6 If your model is based on several numerical attributes - are they comparable? Example: ages mayrange from 0-100 and icomes from 0 to billions Some models may not perform well when different attributes are on very different scales It can result in some attributes counting more than others Bias in the attributes can also be a problem Examples \u00b6 Scikit-learn's PCA implementation has a \"whiten\" option that does tihs for you. use it. Scikit-learn has a preprocessing module with hany normalize and scale functions Your data may have \"yes\" and \"no\" that needs to be converted to \"1\" and \"0\". Read the docks \u00b6 Most data mining and machine learning techniques work fine with raw, un-normalized data But double check the one you're using before you start Don't forget to re-scale your results when you're done.","title":"Normalizing numerical data"},{"location":"Data%20Science/07-dealing-with-real-world-data/04-normalizing-numerical-data/#normalizing-numerical-data","text":"","title":"Normalizing numerical data"},{"location":"Data%20Science/07-dealing-with-real-world-data/04-normalizing-numerical-data/#the-importance-of-normalizing-data","text":"If your model is based on several numerical attributes - are they comparable? Example: ages mayrange from 0-100 and icomes from 0 to billions Some models may not perform well when different attributes are on very different scales It can result in some attributes counting more than others Bias in the attributes can also be a problem","title":"The importance of normalizing data"},{"location":"Data%20Science/07-dealing-with-real-world-data/04-normalizing-numerical-data/#examples","text":"Scikit-learn's PCA implementation has a \"whiten\" option that does tihs for you. use it. Scikit-learn has a preprocessing module with hany normalize and scale functions Your data may have \"yes\" and \"no\" that needs to be converted to \"1\" and \"0\".","title":"Examples"},{"location":"Data%20Science/07-dealing-with-real-world-data/04-normalizing-numerical-data/#read-the-docks","text":"Most data mining and machine learning techniques work fine with raw, un-normalized data But double check the one you're using before you start Don't forget to re-scale your results when you're done.","title":"Read the docks"},{"location":"Data%20Science/07-dealing-with-real-world-data/05-detecting-outliers/","text":"Detecting outliers \u00b6 Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/25-dealing-with-outliers.ipynb Sometimes it's appropriate to remove outliers from your training data Do this responsibly! Understand why you are doing this. For example: in collaborative filtering a single user who rates thousands of movies could have a big effect on everyone else's ratings. That may not be desirable. Another example: in web log data, outliers may represent bots or other agents that shouldn't be discarded. But if someone really wants the mean income of US citizens for example, don't toss out billionaires just because you want to. Dealing with outliers \u00b6 Standard deviation provides a principled way to classify outliers. Find data points more than some multiple of standard deviation in your training data. What multiple? You just have to use common sense.","title":"Detecting outliers"},{"location":"Data%20Science/07-dealing-with-real-world-data/05-detecting-outliers/#detecting-outliers","text":"Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/25-dealing-with-outliers.ipynb Sometimes it's appropriate to remove outliers from your training data Do this responsibly! Understand why you are doing this. For example: in collaborative filtering a single user who rates thousands of movies could have a big effect on everyone else's ratings. That may not be desirable. Another example: in web log data, outliers may represent bots or other agents that shouldn't be discarded. But if someone really wants the mean income of US citizens for example, don't toss out billionaires just because you want to.","title":"Detecting outliers"},{"location":"Data%20Science/07-dealing-with-real-world-data/05-detecting-outliers/#dealing-with-outliers","text":"Standard deviation provides a principled way to classify outliers. Find data points more than some multiple of standard deviation in your training data. What multiple? You just have to use common sense.","title":"Dealing with outliers"},{"location":"Data%20Science/07-dealing-with-real-world-data/06-feature-engineering-and-curse-of-dimensionality/","text":"Feature Engineering and Curse of Dimensionality \u00b6 What is feature engineering? \u00b6 Applying your knowledge of the data - and the model you're using - to create better features to train your model with. Which features should I use? Do I need to transform these features in some way? Should I create new features from the existing ones? You can't just throw in raw data and expect good results This is the art of machine learning; where expertise is applied \"Applied machine learning is basically feature engineering\" - Andrew Ng The curse of dimensionality \u00b6 Too many features can be a problem - leads to sparse data Every feature is a new dimension Much of feature engineering is selecting the features most relevant to the problem at hand This often is where domain knowledge comes into play UNsupervised dimensionality reduction techniques can also be employed to distill many features into fewer features PCA K-means","title":"Feature Engineering and Curse of Dimensionality"},{"location":"Data%20Science/07-dealing-with-real-world-data/06-feature-engineering-and-curse-of-dimensionality/#feature-engineering-and-curse-of-dimensionality","text":"","title":"Feature Engineering and Curse of Dimensionality"},{"location":"Data%20Science/07-dealing-with-real-world-data/06-feature-engineering-and-curse-of-dimensionality/#what-is-feature-engineering","text":"Applying your knowledge of the data - and the model you're using - to create better features to train your model with. Which features should I use? Do I need to transform these features in some way? Should I create new features from the existing ones? You can't just throw in raw data and expect good results This is the art of machine learning; where expertise is applied \"Applied machine learning is basically feature engineering\" - Andrew Ng","title":"What is feature engineering?"},{"location":"Data%20Science/07-dealing-with-real-world-data/06-feature-engineering-and-curse-of-dimensionality/#the-curse-of-dimensionality","text":"Too many features can be a problem - leads to sparse data Every feature is a new dimension Much of feature engineering is selecting the features most relevant to the problem at hand This often is where domain knowledge comes into play UNsupervised dimensionality reduction techniques can also be employed to distill many features into fewer features PCA K-means","title":"The curse of dimensionality"},{"location":"Data%20Science/07-dealing-with-real-world-data/07-imputation-techniques-for-missing-data/","text":"Imputation techniques for missing data \u00b6 Mean replacement \u00b6 Replace missing values with the mean value from the rest of the column (columns, not rows. A column represents a single feature.) Fast & easy, won't affect mean or sample size of overall data set. Median may be a better choice than mean when outliers are present But it's generally pretty terrible Only works on column level, misses correlations between features Can't use on categorical features (imputing the most frequent value can work in this case, thoough) Not very accurate Dropping \u00b6 If not many rows contain missing data And dropping those rows doesn't bias your data And you don't have a lot of time But it's never going to be the right answer for the best approach. Almost anything is better. Can you substitute another similar fields perhaps? (i.e. review summary vs full text) Machine learning \u00b6 KNN: Find K nearest (most similar) rows and average their values Assumes numerical data, not categorical There are ways to handle categorical data (hamming distance), but categorical data is probably better served by Deep Learning Build a machine learning model to impute data for your machine learning model. Works well for categorical data. Really well, but it's complicated. Regression Find linear or non-linear relationships between the missing feature and other features Most advanced technique: MICE (Multiple Imputation by Chained Equations) Just get more data \u00b6 What's better than imputing data? Getting more real data! Sometimes you just have to try harder or collect more data.","title":"Imputation techniques for missing data"},{"location":"Data%20Science/07-dealing-with-real-world-data/07-imputation-techniques-for-missing-data/#imputation-techniques-for-missing-data","text":"","title":"Imputation techniques for missing data"},{"location":"Data%20Science/07-dealing-with-real-world-data/07-imputation-techniques-for-missing-data/#mean-replacement","text":"Replace missing values with the mean value from the rest of the column (columns, not rows. A column represents a single feature.) Fast & easy, won't affect mean or sample size of overall data set. Median may be a better choice than mean when outliers are present But it's generally pretty terrible Only works on column level, misses correlations between features Can't use on categorical features (imputing the most frequent value can work in this case, thoough) Not very accurate","title":"Mean replacement"},{"location":"Data%20Science/07-dealing-with-real-world-data/07-imputation-techniques-for-missing-data/#dropping","text":"If not many rows contain missing data And dropping those rows doesn't bias your data And you don't have a lot of time But it's never going to be the right answer for the best approach. Almost anything is better. Can you substitute another similar fields perhaps? (i.e. review summary vs full text)","title":"Dropping"},{"location":"Data%20Science/07-dealing-with-real-world-data/07-imputation-techniques-for-missing-data/#machine-learning","text":"KNN: Find K nearest (most similar) rows and average their values Assumes numerical data, not categorical There are ways to handle categorical data (hamming distance), but categorical data is probably better served by Deep Learning Build a machine learning model to impute data for your machine learning model. Works well for categorical data. Really well, but it's complicated. Regression Find linear or non-linear relationships between the missing feature and other features Most advanced technique: MICE (Multiple Imputation by Chained Equations)","title":"Machine learning"},{"location":"Data%20Science/07-dealing-with-real-world-data/07-imputation-techniques-for-missing-data/#just-get-more-data","text":"What's better than imputing data? Getting more real data! Sometimes you just have to try harder or collect more data.","title":"Just get more data"},{"location":"Data%20Science/07-dealing-with-real-world-data/08-handling-unbalanced-data/","text":"Handling unbalanced data \u00b6 What is unbalanced data? \u00b6 Large discreptancy between \"positive\" and \"negatrive\" cases i.e., fraud detection. Fraid is rate and most rows will be non-fraud. Mainly a problem with neural networks. Oversampling \u00b6 Duplicate samples from the minority class Can be done at random Undersampling \u00b6 Instead of creating more positive samples, remove the negative ones Throwing data away is usually not the right answer Unless you are specifically trying to avoid \"big data\" scaling issues. SMOTE \u00b6 Synthetic minority over-sampling technique Artificially generate new samples of the minority class using nearest neighbors Run KNN of each sample of the minority class Create a new sample from the KNN result (mean of the neighbors) Both generates new samples and undersamples majority class Generally better than just oversampling Adjusting thresholds \u00b6 When making predictions about a classification (fraud / not fraud), you have some sort of threshold of probability at which point you'll flag something as the positive case (fraud). If you have too many false positives, one way to fix that is to simply increase that threshold. Guaranteed to reduce false positives But, could result in more false negatives","title":"Handling unbalanced data"},{"location":"Data%20Science/07-dealing-with-real-world-data/08-handling-unbalanced-data/#handling-unbalanced-data","text":"","title":"Handling unbalanced data"},{"location":"Data%20Science/07-dealing-with-real-world-data/08-handling-unbalanced-data/#what-is-unbalanced-data","text":"Large discreptancy between \"positive\" and \"negatrive\" cases i.e., fraud detection. Fraid is rate and most rows will be non-fraud. Mainly a problem with neural networks.","title":"What is unbalanced data?"},{"location":"Data%20Science/07-dealing-with-real-world-data/08-handling-unbalanced-data/#oversampling","text":"Duplicate samples from the minority class Can be done at random","title":"Oversampling"},{"location":"Data%20Science/07-dealing-with-real-world-data/08-handling-unbalanced-data/#undersampling","text":"Instead of creating more positive samples, remove the negative ones Throwing data away is usually not the right answer Unless you are specifically trying to avoid \"big data\" scaling issues.","title":"Undersampling"},{"location":"Data%20Science/07-dealing-with-real-world-data/08-handling-unbalanced-data/#smote","text":"Synthetic minority over-sampling technique Artificially generate new samples of the minority class using nearest neighbors Run KNN of each sample of the minority class Create a new sample from the KNN result (mean of the neighbors) Both generates new samples and undersamples majority class Generally better than just oversampling","title":"SMOTE"},{"location":"Data%20Science/07-dealing-with-real-world-data/08-handling-unbalanced-data/#adjusting-thresholds","text":"When making predictions about a classification (fraud / not fraud), you have some sort of threshold of probability at which point you'll flag something as the positive case (fraud). If you have too many false positives, one way to fix that is to simply increase that threshold. Guaranteed to reduce false positives But, could result in more false negatives","title":"Adjusting thresholds"},{"location":"Data%20Science/07-dealing-with-real-world-data/09-binning-transforming-encoding-scaling-and-shuffling/","text":"Binning, Transforming, Encoding, Scaling and Shuffling \u00b6 Binning \u00b6 Bucket observations together based on ranges of values Example: estimated ages of people Put all 20-somethings in one classification, 30-seomethings in other etc Quantile binning categorizes data by their place in the data distribution Ensures even sizes of bins Transforms numeric data to ordinal data Especially useful when there is uncertainty in the measurements Transforming \u00b6 Feature data with an exponential trend may benefit from a logarithmic transform Applying some function to feature to make it better suited for training Example: YouTube recommendations A numeric feature x is also represented by x^2 and sqrt(x) This allows learning of super and sub-linear functions Encoding \u00b6 Transforming data into some new representation required by the model One-hot encoding Create buckets for every category The bucket for your category has a 1, all other have 0 Very common in deep learning, where categories are represented by individual output neurons. Scaling / normalization \u00b6 Some models prefer feature data to be normally distributed around 0 (most neural nets) Most models require feature data to at least be scaled to comparable values Otherwise features with larger magnitudes will have more weight than they should Example: modeling age and income as features - incomes will be much higher than ages Scikit-learn has a preprocessor module that helps (MinMaxScaler, etc) Remember to scale your results back up. Shuffling \u00b6 Many algorithms benefit from shuffling their training data Otherwiuse they may learn from residual signals in the training data resulting from the order which they were collected","title":"Binning, Transforming, Encoding, Scaling and Shuffling"},{"location":"Data%20Science/07-dealing-with-real-world-data/09-binning-transforming-encoding-scaling-and-shuffling/#binning-transforming-encoding-scaling-and-shuffling","text":"","title":"Binning, Transforming, Encoding, Scaling and Shuffling"},{"location":"Data%20Science/07-dealing-with-real-world-data/09-binning-transforming-encoding-scaling-and-shuffling/#binning","text":"Bucket observations together based on ranges of values Example: estimated ages of people Put all 20-somethings in one classification, 30-seomethings in other etc Quantile binning categorizes data by their place in the data distribution Ensures even sizes of bins Transforms numeric data to ordinal data Especially useful when there is uncertainty in the measurements","title":"Binning"},{"location":"Data%20Science/07-dealing-with-real-world-data/09-binning-transforming-encoding-scaling-and-shuffling/#transforming","text":"Feature data with an exponential trend may benefit from a logarithmic transform Applying some function to feature to make it better suited for training Example: YouTube recommendations A numeric feature x is also represented by x^2 and sqrt(x) This allows learning of super and sub-linear functions","title":"Transforming"},{"location":"Data%20Science/07-dealing-with-real-world-data/09-binning-transforming-encoding-scaling-and-shuffling/#encoding","text":"Transforming data into some new representation required by the model One-hot encoding Create buckets for every category The bucket for your category has a 1, all other have 0 Very common in deep learning, where categories are represented by individual output neurons.","title":"Encoding"},{"location":"Data%20Science/07-dealing-with-real-world-data/09-binning-transforming-encoding-scaling-and-shuffling/#scaling-normalization","text":"Some models prefer feature data to be normally distributed around 0 (most neural nets) Most models require feature data to at least be scaled to comparable values Otherwise features with larger magnitudes will have more weight than they should Example: modeling age and income as features - incomes will be much higher than ages Scikit-learn has a preprocessor module that helps (MinMaxScaler, etc) Remember to scale your results back up.","title":"Scaling / normalization"},{"location":"Data%20Science/07-dealing-with-real-world-data/09-binning-transforming-encoding-scaling-and-shuffling/#shuffling","text":"Many algorithms benefit from shuffling their training data Otherwiuse they may learn from residual signals in the training data resulting from the order which they were collected","title":"Shuffling"},{"location":"Data%20Science/08-apache-spark-machine-learning-on-big-data/01-installing-spark/","text":"Installing Spark \u00b6 Mac \u00b6 brew install apache-spark Create a log4j.properties file cd /usr/local/Cellar/apache-spark/2.0.0/libexec/conf cp log4j.properties.template log4j.properties Edit the log4j.properties file and change the log level from INFO to ERROR on log4j.rootCategory Linux \u00b6 https://www.tutorialspoint.com/apache_spark/apache_spark_installation.htm yay -S apache-spark Test it \u00b6 cd into spark installation directory look for a text file we can play with like README.md or CHANGES.txt enter pyspark rdd = sc.textFile(\"README.md\") you should get a count of the number of lines in that file. quit()","title":"Installing Spark"},{"location":"Data%20Science/08-apache-spark-machine-learning-on-big-data/01-installing-spark/#installing-spark","text":"","title":"Installing Spark"},{"location":"Data%20Science/08-apache-spark-machine-learning-on-big-data/01-installing-spark/#mac","text":"brew install apache-spark Create a log4j.properties file cd /usr/local/Cellar/apache-spark/2.0.0/libexec/conf cp log4j.properties.template log4j.properties Edit the log4j.properties file and change the log level from INFO to ERROR on log4j.rootCategory","title":"Mac"},{"location":"Data%20Science/08-apache-spark-machine-learning-on-big-data/01-installing-spark/#linux","text":"https://www.tutorialspoint.com/apache_spark/apache_spark_installation.htm yay -S apache-spark","title":"Linux"},{"location":"Data%20Science/08-apache-spark-machine-learning-on-big-data/01-installing-spark/#test-it","text":"cd into spark installation directory look for a text file we can play with like README.md or CHANGES.txt enter pyspark rdd = sc.textFile(\"README.md\") you should get a count of the number of lines in that file. quit()","title":"Test it"},{"location":"Data%20Science/08-apache-spark-machine-learning-on-big-data/02-spark-introduction/","text":"Spark introduction \u00b6 Spark is a fast an general engine for large-scale data processing. It's scalable, consists of: Driver Program (spark context) Cluster manager (spark, yarn) Executors (cache, tasks) It's fast Run programs up to 100x faster than hadoop mapreduce in memory, or 10x faster on disk DAG Engine (directed acyclic graph) optimizes workflows It's hot Amazon Ebay: log analysis and aggregation NASA JPL: Deep Space Network Groupon TripAdviser Yahoo Many others it's not hard Code in python, java or scala Built around one main concept - the resilient distributed dataset (RDD) Components: Spark streaming Spark SQL MLLib GraphX Spark Core Python vs scala Why python? No need to compile, manage dependencies etc Less coding overhead You already know python Lets us focus on the concepts instead of a new language But... Scala is probably more popular choice with spark Spark is built in scala so coding scala is native to spark New features, libraries tend to be scala first.","title":"Spark introduction"},{"location":"Data%20Science/08-apache-spark-machine-learning-on-big-data/02-spark-introduction/#spark-introduction","text":"Spark is a fast an general engine for large-scale data processing. It's scalable, consists of: Driver Program (spark context) Cluster manager (spark, yarn) Executors (cache, tasks) It's fast Run programs up to 100x faster than hadoop mapreduce in memory, or 10x faster on disk DAG Engine (directed acyclic graph) optimizes workflows It's hot Amazon Ebay: log analysis and aggregation NASA JPL: Deep Space Network Groupon TripAdviser Yahoo Many others it's not hard Code in python, java or scala Built around one main concept - the resilient distributed dataset (RDD) Components: Spark streaming Spark SQL MLLib GraphX Spark Core Python vs scala Why python? No need to compile, manage dependencies etc Less coding overhead You already know python Lets us focus on the concepts instead of a new language But... Scala is probably more popular choice with spark Spark is built in scala so coding scala is native to spark New features, libraries tend to be scala first.","title":"Spark introduction"},{"location":"Data%20Science/08-apache-spark-machine-learning-on-big-data/03-spark-and-the-resilient-distributed-dataset/","text":"Spark and the Resilient Distributed Dataset (RDD) \u00b6 The SparkContext \u00b6 Created by your dirver program Is responsible for making RDDs resilient and distributed Creates RDD's The spark shell creates a \"sc\" object for you Creating rdd's \u00b6 nums = parallelize([1,2,3,4]) sc.testFile(\"file:///somewhere/test.txt\") or s3n://, hdfs:// hiveCtx = HiveContext(sc) rows = hiveCtx.sql(\"SELECT name, age FROM users\") Can also create from: JDBC Cassandra HBase Elasticsearch JSON, CSV, sequence files, object files, various compressed formats Transforming RDD's \u00b6 Map rdd = sc.parallelize([1,2,3,4]) rdd.map(lambda x: x*x) this yields 1,4,9,16 Flatmap Filter Distinct Sample Union, intersection, subtract, cartesian RDD Actions \u00b6 Collect Count CountByValue Take Top Reduce and more Lazy evaluation \u00b6 Nothing actually happens in your driver program until an action is called.","title":"Spark and the Resilient Distributed Dataset (RDD)"},{"location":"Data%20Science/08-apache-spark-machine-learning-on-big-data/03-spark-and-the-resilient-distributed-dataset/#spark-and-the-resilient-distributed-dataset-rdd","text":"","title":"Spark and the Resilient Distributed Dataset (RDD)"},{"location":"Data%20Science/08-apache-spark-machine-learning-on-big-data/03-spark-and-the-resilient-distributed-dataset/#the-sparkcontext","text":"Created by your dirver program Is responsible for making RDDs resilient and distributed Creates RDD's The spark shell creates a \"sc\" object for you","title":"The SparkContext"},{"location":"Data%20Science/08-apache-spark-machine-learning-on-big-data/03-spark-and-the-resilient-distributed-dataset/#creating-rdds","text":"nums = parallelize([1,2,3,4]) sc.testFile(\"file:///somewhere/test.txt\") or s3n://, hdfs:// hiveCtx = HiveContext(sc) rows = hiveCtx.sql(\"SELECT name, age FROM users\") Can also create from: JDBC Cassandra HBase Elasticsearch JSON, CSV, sequence files, object files, various compressed formats","title":"Creating rdd's"},{"location":"Data%20Science/08-apache-spark-machine-learning-on-big-data/03-spark-and-the-resilient-distributed-dataset/#transforming-rdds","text":"Map rdd = sc.parallelize([1,2,3,4]) rdd.map(lambda x: x*x) this yields 1,4,9,16 Flatmap Filter Distinct Sample Union, intersection, subtract, cartesian","title":"Transforming RDD's"},{"location":"Data%20Science/08-apache-spark-machine-learning-on-big-data/03-spark-and-the-resilient-distributed-dataset/#rdd-actions","text":"Collect Count CountByValue Take Top Reduce and more","title":"RDD Actions"},{"location":"Data%20Science/08-apache-spark-machine-learning-on-big-data/03-spark-and-the-resilient-distributed-dataset/#lazy-evaluation","text":"Nothing actually happens in your driver program until an action is called.","title":"Lazy evaluation"},{"location":"Data%20Science/08-apache-spark-machine-learning-on-big-data/04-mllib-introduction/","text":"MLLib introduction \u00b6 Some capabilities \u00b6 Feature extraction Term frequency / inverse document frequency useful for search Basic statistics Chi-squared test, pearson or spearman correlation, min, max, mean, variance Linear regression, logistic regression Support Vector Machines Naive Bayes Classifier Decision trees K-mean clustering Principal component analysis, singular value decomposition Recommendations using alternating least squares Special MLLib Data Types \u00b6 Vector (dense or sparse) LabeledPoint Rating","title":"MLLib introduction"},{"location":"Data%20Science/08-apache-spark-machine-learning-on-big-data/04-mllib-introduction/#mllib-introduction","text":"","title":"MLLib introduction"},{"location":"Data%20Science/08-apache-spark-machine-learning-on-big-data/04-mllib-introduction/#some-capabilities","text":"Feature extraction Term frequency / inverse document frequency useful for search Basic statistics Chi-squared test, pearson or spearman correlation, min, max, mean, variance Linear regression, logistic regression Support Vector Machines Naive Bayes Classifier Decision trees K-mean clustering Principal component analysis, singular value decomposition Recommendations using alternating least squares","title":"Some capabilities"},{"location":"Data%20Science/08-apache-spark-machine-learning-on-big-data/04-mllib-introduction/#special-mllib-data-types","text":"Vector (dense or sparse) LabeledPoint Rating","title":"Special MLLib Data Types"},{"location":"Data%20Science/08-apache-spark-machine-learning-on-big-data/05-decision-trees-in-spark/","text":"Introduction to Decision Trees in Spark \u00b6 The python file available here: https://github.com/daviskregers/data-science-recap/blob/main/26-spark-decision-tree.py","title":"Introduction to Decision Trees in Spark"},{"location":"Data%20Science/08-apache-spark-machine-learning-on-big-data/05-decision-trees-in-spark/#introduction-to-decision-trees-in-spark","text":"The python file available here: https://github.com/daviskregers/data-science-recap/blob/main/26-spark-decision-tree.py","title":"Introduction to Decision Trees in Spark"},{"location":"Data%20Science/08-apache-spark-machine-learning-on-big-data/06-k-means-clustering-spark/","text":"K-means clustering in Spark \u00b6 The python file available here: https://github.com/daviskregers/data-science-recap/blob/main/27-spark-k-means-clustering.py","title":"K-means clustering in Spark"},{"location":"Data%20Science/08-apache-spark-machine-learning-on-big-data/06-k-means-clustering-spark/#k-means-clustering-in-spark","text":"The python file available here: https://github.com/daviskregers/data-science-recap/blob/main/27-spark-k-means-clustering.py","title":"K-means clustering in Spark"},{"location":"Data%20Science/08-apache-spark-machine-learning-on-big-data/07-tf-idf/","text":"TF / IDF \u00b6 The python file available here: https://github.com/daviskregers/data-science-recap/blob/main/28-spark-wikipedia-search-tf-idf.py TF-IDF \u00b6 Stands for Term Frequency and Inverse Document Frequency Important data for search - figures out what terms are most relevant for a document Term Frequency just measures how often a word occurs in a document A word that occurs frequently is probably important to that document's meaning Document Frequency is how often, a word occurs in an entire set of documents, i.e., all of Wikipedia or every web page This tells us about common words that just appear everywhere no matter what the topic, like \"a\", \"the\", \"and\" etc. So a measure of the relevancy of a word to a document might be \\( \\(\\frac{term frequency}{document frequency}\\) \\) Or $$ Term frequency * Inverse Document Frequency $$ That is, take how often the word appears in a document, over how often it just appears everywhere. That gives you a measure of how important and unique this word is for this document. TF-IDF in practice \u00b6 We actually use the log of the IDF, since word frequencies are distributed exponentially. That gives us a better weighting of a words overall popularity. TF-IDF assumes document is just a \"bag of words\" Parsing documents into a bag of words can be most of the work Words can be represented as a has value (number) for efficiency What about synonyms? Various tenses? Abbreviations? Capitalizations? Misspelling? Doing this at scale is the hard part That's where Spark comes in! Using TF-IDF \u00b6 A very simple search algorithm could be: Compute TF-IDF for every word in a corpus For a given search word, sort the documents by their TF-DF score for that word Display the results","title":"TF / IDF"},{"location":"Data%20Science/08-apache-spark-machine-learning-on-big-data/07-tf-idf/#tf-idf","text":"The python file available here: https://github.com/daviskregers/data-science-recap/blob/main/28-spark-wikipedia-search-tf-idf.py","title":"TF / IDF"},{"location":"Data%20Science/08-apache-spark-machine-learning-on-big-data/07-tf-idf/#tf-idf_1","text":"Stands for Term Frequency and Inverse Document Frequency Important data for search - figures out what terms are most relevant for a document Term Frequency just measures how often a word occurs in a document A word that occurs frequently is probably important to that document's meaning Document Frequency is how often, a word occurs in an entire set of documents, i.e., all of Wikipedia or every web page This tells us about common words that just appear everywhere no matter what the topic, like \"a\", \"the\", \"and\" etc. So a measure of the relevancy of a word to a document might be \\( \\(\\frac{term frequency}{document frequency}\\) \\) Or $$ Term frequency * Inverse Document Frequency $$ That is, take how often the word appears in a document, over how often it just appears everywhere. That gives you a measure of how important and unique this word is for this document.","title":"TF-IDF"},{"location":"Data%20Science/08-apache-spark-machine-learning-on-big-data/07-tf-idf/#tf-idf-in-practice","text":"We actually use the log of the IDF, since word frequencies are distributed exponentially. That gives us a better weighting of a words overall popularity. TF-IDF assumes document is just a \"bag of words\" Parsing documents into a bag of words can be most of the work Words can be represented as a has value (number) for efficiency What about synonyms? Various tenses? Abbreviations? Capitalizations? Misspelling? Doing this at scale is the hard part That's where Spark comes in!","title":"TF-IDF in practice"},{"location":"Data%20Science/08-apache-spark-machine-learning-on-big-data/07-tf-idf/#using-tf-idf","text":"A very simple search algorithm could be: Compute TF-IDF for every word in a corpus For a given search word, sort the documents by their TF-DF score for that word Display the results","title":"Using TF-IDF"},{"location":"Data%20Science/08-apache-spark-machine-learning-on-big-data/08-linear-regression/","text":"Linear regression on Spark \u00b6 The python file available here: https://github.com/daviskregers/data-science-recap/blob/main/29-spark-linear-regression.py","title":"Linear regression on Spark"},{"location":"Data%20Science/08-apache-spark-machine-learning-on-big-data/08-linear-regression/#linear-regression-on-spark","text":"The python file available here: https://github.com/daviskregers/data-science-recap/blob/main/29-spark-linear-regression.py","title":"Linear regression on Spark"},{"location":"Data%20Science/09-experimental-design-ml-in-real-world/01-deploying-models-to-real-time-systems/","text":"Deploying models to Real-time systems \u00b6 How do I use my model in an app? \u00b6 Your external apps can't justr run a notebook Separate your training from your predictions Train the model periodically offline Push the model - or it's results - to a web service Your app calls the web service Example: Google Cloud ML \u00b6 Dump your trained classifier using sklearn.externals from sklearn.externals import joblib joblib.dump(clf, 'model.joblib') Upload model.joblib to Google Cloud storage, specifying the scikit-learn framework Cloud ML Engine exposes ar REST API that you call to make predictions in real-time Example: AWS (recommender system) \u00b6 Server logs -> Amazon Kinesis Data Firehose -> Amazon S3 -> Amazon EMR -> Amazon DynamoDB -> AWS Lambda -> Amazon API Gateway -> Client app Other approaches \u00b6 Roll your own web service with Flask or another framework Then you have servers to provision and maintain. Go all-in with a platform Amazon SageMaker Amazon Comprehend Amazon Lex Amazon Polly Amazon Rekognition Amazon Rekognition Image Amazon Rekognition Video Amazon Translate Amazon Transcribe AWS Deep Learning APIs AWS DeepLens","title":"Deploying models to Real-time systems"},{"location":"Data%20Science/09-experimental-design-ml-in-real-world/01-deploying-models-to-real-time-systems/#deploying-models-to-real-time-systems","text":"","title":"Deploying models to Real-time systems"},{"location":"Data%20Science/09-experimental-design-ml-in-real-world/01-deploying-models-to-real-time-systems/#how-do-i-use-my-model-in-an-app","text":"Your external apps can't justr run a notebook Separate your training from your predictions Train the model periodically offline Push the model - or it's results - to a web service Your app calls the web service","title":"How do I use my model in an app?"},{"location":"Data%20Science/09-experimental-design-ml-in-real-world/01-deploying-models-to-real-time-systems/#example-google-cloud-ml","text":"Dump your trained classifier using sklearn.externals from sklearn.externals import joblib joblib.dump(clf, 'model.joblib') Upload model.joblib to Google Cloud storage, specifying the scikit-learn framework Cloud ML Engine exposes ar REST API that you call to make predictions in real-time","title":"Example: Google Cloud ML"},{"location":"Data%20Science/09-experimental-design-ml-in-real-world/01-deploying-models-to-real-time-systems/#example-aws-recommender-system","text":"Server logs -> Amazon Kinesis Data Firehose -> Amazon S3 -> Amazon EMR -> Amazon DynamoDB -> AWS Lambda -> Amazon API Gateway -> Client app","title":"Example: AWS (recommender system)"},{"location":"Data%20Science/09-experimental-design-ml-in-real-world/01-deploying-models-to-real-time-systems/#other-approaches","text":"Roll your own web service with Flask or another framework Then you have servers to provision and maintain. Go all-in with a platform Amazon SageMaker Amazon Comprehend Amazon Lex Amazon Polly Amazon Rekognition Amazon Rekognition Image Amazon Rekognition Video Amazon Translate Amazon Transcribe AWS Deep Learning APIs AWS DeepLens","title":"Other approaches"},{"location":"Data%20Science/09-experimental-design-ml-in-real-world/02-ab-testing-concepts/","text":"A/B Testiting concepts \u00b6 What is an A/B test? \u00b6 A controlled experiment, usually in the context of a website You test the performance of some change to your website (the variant) and measure conversion relative to your unchanged site (the control) What sorts of things can you test? \u00b6 Design changes UI flow Algorithmic changes Pricing changes You name it How do you measure conversion \u00b6 Ideally choose what you are trying to influence Order amounts Profit Ad clicks Order quantity But attributing actions downstream from your change can be hard Especially if you're running more than one experiment Variance is your enemy \u00b6 Common mistake Run a test for some small period of time that results in a few purchases to analyze You take the mean order amount from A and B and declare victory or defeat But, there\u0161 so much random variation in order amounts to begin with that your result was just based on chance You then fool yourself into thinking some change to your website, which could actually be harmful, has made tons of money. Sometimes you need to also look at conversion metrics with less variance Order quantities vs order dollar amounts for example","title":"A/B Testiting concepts"},{"location":"Data%20Science/09-experimental-design-ml-in-real-world/02-ab-testing-concepts/#ab-testiting-concepts","text":"","title":"A/B Testiting concepts"},{"location":"Data%20Science/09-experimental-design-ml-in-real-world/02-ab-testing-concepts/#what-is-an-ab-test","text":"A controlled experiment, usually in the context of a website You test the performance of some change to your website (the variant) and measure conversion relative to your unchanged site (the control)","title":"What is an A/B test?"},{"location":"Data%20Science/09-experimental-design-ml-in-real-world/02-ab-testing-concepts/#what-sorts-of-things-can-you-test","text":"Design changes UI flow Algorithmic changes Pricing changes You name it","title":"What sorts of things can you test?"},{"location":"Data%20Science/09-experimental-design-ml-in-real-world/02-ab-testing-concepts/#how-do-you-measure-conversion","text":"Ideally choose what you are trying to influence Order amounts Profit Ad clicks Order quantity But attributing actions downstream from your change can be hard Especially if you're running more than one experiment","title":"How do you measure conversion"},{"location":"Data%20Science/09-experimental-design-ml-in-real-world/02-ab-testing-concepts/#variance-is-your-enemy","text":"Common mistake Run a test for some small period of time that results in a few purchases to analyze You take the mean order amount from A and B and declare victory or defeat But, there\u0161 so much random variation in order amounts to begin with that your result was just based on chance You then fool yourself into thinking some change to your website, which could actually be harmful, has made tons of money. Sometimes you need to also look at conversion metrics with less variance Order quantities vs order dollar amounts for example","title":"Variance is your enemy"},{"location":"Data%20Science/09-experimental-design-ml-in-real-world/03-t-tests-and-p-values/","text":"T-Tests and P-Values \u00b6 Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/30-t-tests-and-p-values.ipynb Determining significance \u00b6 So, how do we know if a result is likely to be real as opposed to just random variation? T-tests and p-values. The T-Statistic \u00b6 A measure of the difference between the two sets expressed in units of standard error The size of the difference relative to the variance in the data A high t value means there's probably a real difference between the two sets Assumes a normal distribution of behaviour This is a good assumption if you're measuring revenue as conversion See also: Fisher's exact test (for clicktrough rates), E-test (for transactions per user) and chi-squared test (for product quantities purchased) The P-value \u00b6 Think of it as the probability of A and B satisfying the \"null hypothesis\" So, a low P-value implies significance It's the probability of an observation lying at an extreme t-value assuming the null hypothesis Using P-values \u00b6 Choose some threshold for significance before your experiment 1%? 5%? When your experiment is over: Measure your P-value If it's less than your significance threshold, then you can reject the null hypothesis If it's a positive change, roll it out. If it's a negative change, discard it before you lose more money.","title":"T-Tests and P-Values"},{"location":"Data%20Science/09-experimental-design-ml-in-real-world/03-t-tests-and-p-values/#t-tests-and-p-values","text":"Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/30-t-tests-and-p-values.ipynb","title":"T-Tests and P-Values"},{"location":"Data%20Science/09-experimental-design-ml-in-real-world/03-t-tests-and-p-values/#determining-significance","text":"So, how do we know if a result is likely to be real as opposed to just random variation? T-tests and p-values.","title":"Determining significance"},{"location":"Data%20Science/09-experimental-design-ml-in-real-world/03-t-tests-and-p-values/#the-t-statistic","text":"A measure of the difference between the two sets expressed in units of standard error The size of the difference relative to the variance in the data A high t value means there's probably a real difference between the two sets Assumes a normal distribution of behaviour This is a good assumption if you're measuring revenue as conversion See also: Fisher's exact test (for clicktrough rates), E-test (for transactions per user) and chi-squared test (for product quantities purchased)","title":"The T-Statistic"},{"location":"Data%20Science/09-experimental-design-ml-in-real-world/03-t-tests-and-p-values/#the-p-value","text":"Think of it as the probability of A and B satisfying the \"null hypothesis\" So, a low P-value implies significance It's the probability of an observation lying at an extreme t-value assuming the null hypothesis","title":"The P-value"},{"location":"Data%20Science/09-experimental-design-ml-in-real-world/03-t-tests-and-p-values/#using-p-values","text":"Choose some threshold for significance before your experiment 1%? 5%? When your experiment is over: Measure your P-value If it's less than your significance threshold, then you can reject the null hypothesis If it's a positive change, roll it out. If it's a negative change, discard it before you lose more money.","title":"Using P-values"},{"location":"Data%20Science/09-experimental-design-ml-in-real-world/04-determining-how-long-to-run-an-experiment/","text":"Determining how long to run an experiment \u00b6 How do I know when I'm done with an A/B test? \u00b6 You have achieved significance (positive or negative) You no longer observe meaningful trends in your p-value That is, you don't see any indication that your experiment will \"converge\" on a result over time You reach some pre-established upper bound on time","title":"Determining how long to run an experiment"},{"location":"Data%20Science/09-experimental-design-ml-in-real-world/04-determining-how-long-to-run-an-experiment/#determining-how-long-to-run-an-experiment","text":"","title":"Determining how long to run an experiment"},{"location":"Data%20Science/09-experimental-design-ml-in-real-world/04-determining-how-long-to-run-an-experiment/#how-do-i-know-when-im-done-with-an-ab-test","text":"You have achieved significance (positive or negative) You no longer observe meaningful trends in your p-value That is, you don't see any indication that your experiment will \"converge\" on a result over time You reach some pre-established upper bound on time","title":"How do I know when I'm done with an A/B test?"},{"location":"Data%20Science/09-experimental-design-ml-in-real-world/05-ab-test-gotchas/","text":"A/B test gotchas \u00b6 Correlation does not imply causation \u00b6 Even your low p-value score on well-designed experiment does not imply causation! It could still be random chance Other factors could be at play It's your duty to ensure business owners understand this Novelty effects \u00b6 Changes to a website will catch the attention of previous users who are used to the way it used to be They might click on something simply because it's new But this attention won't last forever Good idea to re-run experiments much later and validate their impact Often the \"old\" website will outperform the new one after a while, simply because it is a change Seasonal effects \u00b6 An experiment run over a short period of time may only be valid for that period of time Example: Consumer behaviour near Christmas is very different that other times of year An experiment run near christmas may not present behaviour during the rest of the year Selection Bias \u00b6 Sometimes your random selection of customers for A or B isn't really random For example: assignments is based somehow on customer ID But customers with low ID's are better customers than ones with high ID's Run an A/A test periodically to check Audit your segment assignment algorithms Data Pollution \u00b6 Are robots (both self-identified and malicious) affecting your experiment? Good reason to measure conversion based on something that requires spending real money More generally, are outliers skewing the result? Attribution Errors \u00b6 Often there are errors in how conversion is attributed to an experiment Using a widely used A/B test platform can help mitigate that risk If your is home-grown, it deserves auditing Watch for \"gray areas\" Are you counting purchases toward an experiment within some given time frame of exposure to it? Is that time too large? Could other changes downstream from the change you \u0157e measuring affect your results? Are you running multiple experiments at once?","title":"A/B test gotchas"},{"location":"Data%20Science/09-experimental-design-ml-in-real-world/05-ab-test-gotchas/#ab-test-gotchas","text":"","title":"A/B test gotchas"},{"location":"Data%20Science/09-experimental-design-ml-in-real-world/05-ab-test-gotchas/#correlation-does-not-imply-causation","text":"Even your low p-value score on well-designed experiment does not imply causation! It could still be random chance Other factors could be at play It's your duty to ensure business owners understand this","title":"Correlation does not imply causation"},{"location":"Data%20Science/09-experimental-design-ml-in-real-world/05-ab-test-gotchas/#novelty-effects","text":"Changes to a website will catch the attention of previous users who are used to the way it used to be They might click on something simply because it's new But this attention won't last forever Good idea to re-run experiments much later and validate their impact Often the \"old\" website will outperform the new one after a while, simply because it is a change","title":"Novelty effects"},{"location":"Data%20Science/09-experimental-design-ml-in-real-world/05-ab-test-gotchas/#seasonal-effects","text":"An experiment run over a short period of time may only be valid for that period of time Example: Consumer behaviour near Christmas is very different that other times of year An experiment run near christmas may not present behaviour during the rest of the year","title":"Seasonal effects"},{"location":"Data%20Science/09-experimental-design-ml-in-real-world/05-ab-test-gotchas/#selection-bias","text":"Sometimes your random selection of customers for A or B isn't really random For example: assignments is based somehow on customer ID But customers with low ID's are better customers than ones with high ID's Run an A/A test periodically to check Audit your segment assignment algorithms","title":"Selection Bias"},{"location":"Data%20Science/09-experimental-design-ml-in-real-world/05-ab-test-gotchas/#data-pollution","text":"Are robots (both self-identified and malicious) affecting your experiment? Good reason to measure conversion based on something that requires spending real money More generally, are outliers skewing the result?","title":"Data Pollution"},{"location":"Data%20Science/09-experimental-design-ml-in-real-world/05-ab-test-gotchas/#attribution-errors","text":"Often there are errors in how conversion is attributed to an experiment Using a widely used A/B test platform can help mitigate that risk If your is home-grown, it deserves auditing Watch for \"gray areas\" Are you counting purchases toward an experiment within some given time frame of exposure to it? Is that time too large? Could other changes downstream from the change you \u0157e measuring affect your results? Are you running multiple experiments at once?","title":"Attribution Errors"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/01-deep-learning-prerequisites/","text":"Deep Learning Pre-Requisites \u00b6 Gradient descent requires knowledge of the gradient from your cost function (MSE) Mathematically we need the first partial derivative of all the inputs This is hard and inefficient if you just throw calculus at the problem Reverse-mode autodiff can be used Optimized for many inputs + few outputs (lika neuron) Computes all partial derivatives in # of outputs + 1 graph traversals Still fundamentally a calculus trick - it's complicated but it works This is what tensorflow uses Softmax \u00b6 Used for classification Given a score for each class It produces a probability of each class The class with the highest probability is the answer you get. \\[h_0(x) = \\frac{1}{1+exp(-\\theta^T x)}\\] x is a vector of input values, theta is a vector of weights. In review \u00b6 Gradient descent is an algorithm for minimizing error over multiple steps Autodiff is a calculus trick for finding the gradients in gradient descent Softmax is a function for choosing the most probable classification given several input types","title":"Deep Learning Pre-Requisites"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/01-deep-learning-prerequisites/#deep-learning-pre-requisites","text":"Gradient descent requires knowledge of the gradient from your cost function (MSE) Mathematically we need the first partial derivative of all the inputs This is hard and inefficient if you just throw calculus at the problem Reverse-mode autodiff can be used Optimized for many inputs + few outputs (lika neuron) Computes all partial derivatives in # of outputs + 1 graph traversals Still fundamentally a calculus trick - it's complicated but it works This is what tensorflow uses","title":"Deep Learning Pre-Requisites"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/01-deep-learning-prerequisites/#softmax","text":"Used for classification Given a score for each class It produces a probability of each class The class with the highest probability is the answer you get. \\[h_0(x) = \\frac{1}{1+exp(-\\theta^T x)}\\] x is a vector of input values, theta is a vector of weights.","title":"Softmax"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/01-deep-learning-prerequisites/#in-review","text":"Gradient descent is an algorithm for minimizing error over multiple steps Autodiff is a calculus trick for finding the gradients in gradient descent Softmax is a function for choosing the most probable classification given several input types","title":"In review"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/02-the-history-of-artifical-neural-networks/","text":"The history of artificial neural networks \u00b6 The biological inspiration \u00b6 Neurons in your cerebral cortex are connected via axons A neuron fires to the neurons it's connected to, when enough of it's input signals are activated. Very simple at the individual neuron level - but layers of neurons connected in this way can yield learning behavior. Billions of neurons, each with thousands of connections, yields a mind. Cortical columns \u00b6 Neurons in your cortex seem to be arranged in many stacks or columns that process information in parallel Mini-columns of around 100 neurons are organized into larger hyper columns. There are 100 million mini columns in your cortex This is coincidentally similar to how GPU's work The first artificial neurons \u00b6 Goes all the way back to year 1943. An artificial neuron fires if more than N input connections are active. Depending on the number of connections from each input neuron, and whether a connection activates or suppresses a neuron, you can construct AND, OR, or NOT logical constructs this way. The Linear Threshold Unit (LTU) \u00b6 Made in 1957 Adds weights to the inputs, output is given by step function Sum up the products of the inputs and their weights. Output 1 if sum is >= 0 The perceptron \u00b6 A layer of LTU's A perceptron can learn by reinforcing weights that lead to correct behavior during training This too has a biological basis (cells that fire together, wire together) Multi-layer perceptrons \u00b6 ADdition of \"hidden layers\" This is a deep neural network Training them is trickier A modern deep neural network \u00b6 Replaces step activation function with something better Apply softmax to the output Training using gradient descent","title":"The history of artificial neural networks"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/02-the-history-of-artifical-neural-networks/#the-history-of-artificial-neural-networks","text":"","title":"The history of artificial neural networks"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/02-the-history-of-artifical-neural-networks/#the-biological-inspiration","text":"Neurons in your cerebral cortex are connected via axons A neuron fires to the neurons it's connected to, when enough of it's input signals are activated. Very simple at the individual neuron level - but layers of neurons connected in this way can yield learning behavior. Billions of neurons, each with thousands of connections, yields a mind.","title":"The biological inspiration"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/02-the-history-of-artifical-neural-networks/#cortical-columns","text":"Neurons in your cortex seem to be arranged in many stacks or columns that process information in parallel Mini-columns of around 100 neurons are organized into larger hyper columns. There are 100 million mini columns in your cortex This is coincidentally similar to how GPU's work","title":"Cortical columns"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/02-the-history-of-artifical-neural-networks/#the-first-artificial-neurons","text":"Goes all the way back to year 1943. An artificial neuron fires if more than N input connections are active. Depending on the number of connections from each input neuron, and whether a connection activates or suppresses a neuron, you can construct AND, OR, or NOT logical constructs this way.","title":"The first artificial neurons"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/02-the-history-of-artifical-neural-networks/#the-linear-threshold-unit-ltu","text":"Made in 1957 Adds weights to the inputs, output is given by step function Sum up the products of the inputs and their weights. Output 1 if sum is >= 0","title":"The Linear Threshold Unit (LTU)"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/02-the-history-of-artifical-neural-networks/#the-perceptron","text":"A layer of LTU's A perceptron can learn by reinforcing weights that lead to correct behavior during training This too has a biological basis (cells that fire together, wire together)","title":"The perceptron"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/02-the-history-of-artifical-neural-networks/#multi-layer-perceptrons","text":"ADdition of \"hidden layers\" This is a deep neural network Training them is trickier","title":"Multi-layer perceptrons"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/02-the-history-of-artifical-neural-networks/#a-modern-deep-neural-network","text":"Replaces step activation function with something better Apply softmax to the output Training using gradient descent","title":"A modern deep neural network"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/03-deep-learning-details/","text":"Deep Learning Details \u00b6 Backpropogation \u00b6 How do you train a MLP's weights? How does it learn? Backgropogation or specifically gradient descent using reverse-mode autodiff. For each training step Compute the output error Compute how much each neuron in the previous hidden layer contributed Back-propogate that error in a reverse pass Tweak wights to reduce the error using gradient descent Activation functions (aka rectifier) \u00b6 Step functions don't work with gradient descent - there is no gradient. Mathematically they have no useful derivative. Alternatives Logistic function Hyperbolic tangent function Exponential linear unit (ELU) ReLU function (Rectified Linear Unit) ReLU is common. Fast to compute and works well. Also, \"Leaky ReLU\", \"Noisy ReLU\" ELU can sometimes lead to faster learning though. Optimization functions \u00b6 There are faster (as in faster learning) optimizers than gradient descent Momentum optimization Introduces momentum term to the descent, so it slows down as things start to flatten and speeds up as the slope is steep. Nesterov Accelerated Gradient A small tweak on momentum optimization - computes momentum based on the gradient slightly ahead of you, not where you are. RMSProp Adaptive learning rate tyo help point toward the minimum Adam Adaptive moment estimation - momentum + RMSProp combined Popular choice today, easy to use Avoiding Overfitting \u00b6 With thousands of weights to tune, overfitting is a problem Early stopping (when performance starts dropping during training Dropout - ignore say 50% of all neurons randomly at each training step Works surprisingly well Forces your model to spread out it's learning Tuning your topology \u00b6 Trial & error is one way Evaluate a smaller network with less neurons in the hidden layers Evaluate a larger network with more layers Try reducing the size of each layer as you progress - form a funnel More layers can yield faster learning Or just use more layers and neurons than your need, and don't care because your use early stopping Use \"model zoos\"","title":"Deep Learning Details"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/03-deep-learning-details/#deep-learning-details","text":"","title":"Deep Learning Details"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/03-deep-learning-details/#backpropogation","text":"How do you train a MLP's weights? How does it learn? Backgropogation or specifically gradient descent using reverse-mode autodiff. For each training step Compute the output error Compute how much each neuron in the previous hidden layer contributed Back-propogate that error in a reverse pass Tweak wights to reduce the error using gradient descent","title":"Backpropogation"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/03-deep-learning-details/#activation-functions-aka-rectifier","text":"Step functions don't work with gradient descent - there is no gradient. Mathematically they have no useful derivative. Alternatives Logistic function Hyperbolic tangent function Exponential linear unit (ELU) ReLU function (Rectified Linear Unit) ReLU is common. Fast to compute and works well. Also, \"Leaky ReLU\", \"Noisy ReLU\" ELU can sometimes lead to faster learning though.","title":"Activation functions (aka rectifier)"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/03-deep-learning-details/#optimization-functions","text":"There are faster (as in faster learning) optimizers than gradient descent Momentum optimization Introduces momentum term to the descent, so it slows down as things start to flatten and speeds up as the slope is steep. Nesterov Accelerated Gradient A small tweak on momentum optimization - computes momentum based on the gradient slightly ahead of you, not where you are. RMSProp Adaptive learning rate tyo help point toward the minimum Adam Adaptive moment estimation - momentum + RMSProp combined Popular choice today, easy to use","title":"Optimization functions"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/03-deep-learning-details/#avoiding-overfitting","text":"With thousands of weights to tune, overfitting is a problem Early stopping (when performance starts dropping during training Dropout - ignore say 50% of all neurons randomly at each training step Works surprisingly well Forces your model to spread out it's learning","title":"Avoiding Overfitting"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/03-deep-learning-details/#tuning-your-topology","text":"Trial & error is one way Evaluate a smaller network with less neurons in the hidden layers Evaluate a larger network with more layers Try reducing the size of each layer as you progress - form a funnel More layers can yield faster learning Or just use more layers and neurons than your need, and don't care because your use early stopping Use \"model zoos\"","title":"Tuning your topology"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/04-indroducing-tensorflow/","text":"Introducing tensorflow \u00b6 Why tensorflow? \u00b6 It's not specifically for neural networks - it's more generally an architecture for executing a graph of numberical operations Tensorflow can optimize the processing of that graph, and distribute it's processing across a network Sounds a lot like Apache Spark It can also distribute work accross GPUs Can handle massive scale - it was made by Google Runs on about anything Highly efficient C++ code with easy to use Python APIs Tensorflow basics \u00b6 Install conda with conda install tensorflow or conda install tensorflow-gpu A tensor is just a fancy name for an array of matrix of values To use tensorflow you must Construct a graph to compute your tensors Initialize your variables Execute that graph - nothing actually heppens until then import tensorflow as tf a = tf.Variable(1, name=\"a\") b = tf.Variable(2, name\"b\") f = a + b tf.print(f) Creating a neural network with tensorflow \u00b6 Mathematical insights All those interconeected arrows multiplying weights can be thought as a big matrix multiplication The bias term can just be added onto the result of that matrix multiplication So in tensorflow, we can define layer of a neural network as output = tf.matmul(previous_layer, layer_weights) + layer_biases By using tensorflow directly we're kinda doing this the hard way. Load up our training and testing data Construct a graph describing our neural network Associate an optimizer (ie gradient descent) to the network Run the optimizer with your training data Evaluate your trained network with your testing data. Make sure your features are normalized \u00b6 Neural networks usually work best if your input data is normalized That is, 0 mean and unit variance The real goal is that every input feature is comparable in terms of magnitude ssikit_learn's StandardScaler can do this for you. Many data sets are normalized to begin with - such as the one we're about to use.","title":"Introducing tensorflow"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/04-indroducing-tensorflow/#introducing-tensorflow","text":"","title":"Introducing tensorflow"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/04-indroducing-tensorflow/#why-tensorflow","text":"It's not specifically for neural networks - it's more generally an architecture for executing a graph of numberical operations Tensorflow can optimize the processing of that graph, and distribute it's processing across a network Sounds a lot like Apache Spark It can also distribute work accross GPUs Can handle massive scale - it was made by Google Runs on about anything Highly efficient C++ code with easy to use Python APIs","title":"Why tensorflow?"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/04-indroducing-tensorflow/#tensorflow-basics","text":"Install conda with conda install tensorflow or conda install tensorflow-gpu A tensor is just a fancy name for an array of matrix of values To use tensorflow you must Construct a graph to compute your tensors Initialize your variables Execute that graph - nothing actually heppens until then import tensorflow as tf a = tf.Variable(1, name=\"a\") b = tf.Variable(2, name\"b\") f = a + b tf.print(f)","title":"Tensorflow basics"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/04-indroducing-tensorflow/#creating-a-neural-network-with-tensorflow","text":"Mathematical insights All those interconeected arrows multiplying weights can be thought as a big matrix multiplication The bias term can just be added onto the result of that matrix multiplication So in tensorflow, we can define layer of a neural network as output = tf.matmul(previous_layer, layer_weights) + layer_biases By using tensorflow directly we're kinda doing this the hard way. Load up our training and testing data Construct a graph describing our neural network Associate an optimizer (ie gradient descent) to the network Run the optimizer with your training data Evaluate your trained network with your testing data.","title":"Creating a neural network with tensorflow"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/04-indroducing-tensorflow/#make-sure-your-features-are-normalized","text":"Neural networks usually work best if your input data is normalized That is, 0 mean and unit variance The real goal is that every input feature is comparable in terms of magnitude ssikit_learn's StandardScaler can do this for you. Many data sets are normalized to begin with - such as the one we're about to use.","title":"Make sure your features are normalized"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/05-using-tensorflow/","text":"Using tensorflow \u00b6 Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/31-using-tensorflow.ipynb","title":"Using tensorflow"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/05-using-tensorflow/#using-tensorflow","text":"Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/31-using-tensorflow.ipynb","title":"Using tensorflow"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/06-introducing-keras/","text":"Introducting Keras \u00b6 Why keras? \u00b6 Easy and fast prototyping Available as a higher-level API in tensorflow 1.9+ scikit_learn integration Less to think about - which often yields better results without even trying The faster you can experiment, the better your results are. Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/32-using-keras.ipynb","title":"Introducting Keras"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/06-introducing-keras/#introducting-keras","text":"","title":"Introducting Keras"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/06-introducing-keras/#why-keras","text":"Easy and fast prototyping Available as a higher-level API in tensorflow 1.9+ scikit_learn integration Less to think about - which often yields better results without even trying The faster you can experiment, the better your results are. Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/32-using-keras.ipynb","title":"Why keras?"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/07-other-keras-examples/","text":"Other keras examples \u00b6 Example: multi-class classification \u00b6 MNIST is an example of multi-class classification. model = Sequential() model.add(Dense(64, activation='relu', input_dim=20) model.add(Dropout(0.5)) model.add(Dense(64, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(10, activation='softmax')) sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy']) Example: binary classification \u00b6 model = Sequential() model.add(Dense(64, input_dim=20, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(64, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(1, activation='sigmoid')) model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy']) Integratin Keras with scikit_learn \u00b6 from tesorflow.keras.wrappers.scikit_learn import KerasClassifier def create_model(): model = Sequential() model.add(Dense(64, input_dim=20, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(64, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(1, activation='sigmoid')) model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) return model estimator = KerasClassifier(build_fn=create_model, epochs, verbose=0) cv_scores = cross_val_score(estimator, labels, cv=10) print(cv_scores.mean()) Trying to predict political parties with Keras \u00b6 Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/33-using-keras-to-predict-political-parties.ipynb","title":"Other keras examples"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/07-other-keras-examples/#other-keras-examples","text":"","title":"Other keras examples"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/07-other-keras-examples/#example-multi-class-classification","text":"MNIST is an example of multi-class classification. model = Sequential() model.add(Dense(64, activation='relu', input_dim=20) model.add(Dropout(0.5)) model.add(Dense(64, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(10, activation='softmax')) sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])","title":"Example: multi-class classification"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/07-other-keras-examples/#example-binary-classification","text":"model = Sequential() model.add(Dense(64, input_dim=20, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(64, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(1, activation='sigmoid')) model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])","title":"Example: binary classification"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/07-other-keras-examples/#integratin-keras-with-scikit_learn","text":"from tesorflow.keras.wrappers.scikit_learn import KerasClassifier def create_model(): model = Sequential() model.add(Dense(64, input_dim=20, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(64, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(1, activation='sigmoid')) model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) return model estimator = KerasClassifier(build_fn=create_model, epochs, verbose=0) cv_scores = cross_val_score(estimator, labels, cv=10) print(cv_scores.mean())","title":"Integratin Keras with scikit_learn"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/07-other-keras-examples/#trying-to-predict-political-parties-with-keras","text":"Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/33-using-keras-to-predict-political-parties.ipynb","title":"Trying to predict political parties with Keras"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/08-convolutional-neural-networks/","text":"Convolutional neural networks \u00b6 Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/34-keras-convolutional-networks.ipynb CNN's: what are they for? \u00b6 When you have data that doesn't neatly align into columns Images that you want to find features within Machine translation Sentence classification Sentiment analysis They can find features that aren't in specific spot Like a stop sign in a picture Or words within a sentence They are \"feature-location invariant\" How do they work? \u00b6 Inspired by the biology of the visual cortex Local receptive fields are groups of neurons that only respond to a part of what your eyes see (subsampling) They overlap each other to cover the entire field (convolutions) They feed into a higher layers that identify increasingly complex images Some receptive fields identify horizontal lines, lines at different angles, etc. (filters) These would feed into a layer that identifies shapes Which might feed into a layer that identifies objects For color images, extra layers for red and blue How do we know that's a stop sign? \u00b6 Individual local receptive fields scan the image looking for edges, and pick up the edges of the stop sign in a layer Those edges in turn get picked up by a higher level convolution that identifies the stop sign's shape (and letters too) This shape then gets matched against your pattern of what a stop sign looks like, also using the strong red signal coming from your red layers That information keeps getting processed upward until your foot hits the brake A CNN works the same way CNNs with keras \u00b6 Source data must be of appropriate dimensions ie with x length x colors channels Conv2D layer type does the actual convolution on a 2D image Conv1D and Conv3D also available - doesn't have to be image data. MaxPooling2D layers can be used to reduce a 2d layer down by taking the minimum value in a given block Flatten layers will convert the 2D layer to a 1D layer for passing into a flat hidden layer of neurons Typical usage: Conv2D -> MaxPooling2D -> Dropout -> Flatten -> Dense -> Dropout -> Softmax CNNs are hard \u00b6 Very resource-intensive (CPU, GPU and RAM) Lots of hyperparameters Kernel sizes, many layers with different numbers of units, amount of pooling... in addition to the usual stuff like number of layers, choice of optimizer Getting the training data is often the hardest part. As well as storing and accessing it. Specialized CNN architectures \u00b6 Defines specific arrangement of layers, padding, and hyperparameters LeNet-5 Good for handwriting recognition AlexNet Image classification, deeper than LeNet GoogLeNet Even deeper, buty with better performance Introduces inception modules (groups of convolution layers) ResNet (Residual Network) Even deeper - maintains performance via skip connections.","title":"Convolutional neural networks"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/08-convolutional-neural-networks/#convolutional-neural-networks","text":"Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/34-keras-convolutional-networks.ipynb","title":"Convolutional neural networks"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/08-convolutional-neural-networks/#cnns-what-are-they-for","text":"When you have data that doesn't neatly align into columns Images that you want to find features within Machine translation Sentence classification Sentiment analysis They can find features that aren't in specific spot Like a stop sign in a picture Or words within a sentence They are \"feature-location invariant\"","title":"CNN's: what are they for?"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/08-convolutional-neural-networks/#how-do-they-work","text":"Inspired by the biology of the visual cortex Local receptive fields are groups of neurons that only respond to a part of what your eyes see (subsampling) They overlap each other to cover the entire field (convolutions) They feed into a higher layers that identify increasingly complex images Some receptive fields identify horizontal lines, lines at different angles, etc. (filters) These would feed into a layer that identifies shapes Which might feed into a layer that identifies objects For color images, extra layers for red and blue","title":"How do they work?"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/08-convolutional-neural-networks/#how-do-we-know-thats-a-stop-sign","text":"Individual local receptive fields scan the image looking for edges, and pick up the edges of the stop sign in a layer Those edges in turn get picked up by a higher level convolution that identifies the stop sign's shape (and letters too) This shape then gets matched against your pattern of what a stop sign looks like, also using the strong red signal coming from your red layers That information keeps getting processed upward until your foot hits the brake A CNN works the same way","title":"How do we know that's a stop sign?"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/08-convolutional-neural-networks/#cnns-with-keras","text":"Source data must be of appropriate dimensions ie with x length x colors channels Conv2D layer type does the actual convolution on a 2D image Conv1D and Conv3D also available - doesn't have to be image data. MaxPooling2D layers can be used to reduce a 2d layer down by taking the minimum value in a given block Flatten layers will convert the 2D layer to a 1D layer for passing into a flat hidden layer of neurons Typical usage: Conv2D -> MaxPooling2D -> Dropout -> Flatten -> Dense -> Dropout -> Softmax","title":"CNNs with keras"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/08-convolutional-neural-networks/#cnns-are-hard","text":"Very resource-intensive (CPU, GPU and RAM) Lots of hyperparameters Kernel sizes, many layers with different numbers of units, amount of pooling... in addition to the usual stuff like number of layers, choice of optimizer Getting the training data is often the hardest part. As well as storing and accessing it.","title":"CNNs are hard"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/08-convolutional-neural-networks/#specialized-cnn-architectures","text":"Defines specific arrangement of layers, padding, and hyperparameters LeNet-5 Good for handwriting recognition AlexNet Image classification, deeper than LeNet GoogLeNet Even deeper, buty with better performance Introduces inception modules (groups of convolution layers) ResNet (Residual Network) Even deeper - maintains performance via skip connections.","title":"Specialized CNN architectures"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/09-recurrent-neural-networks/","text":"Recurrent neural networks \u00b6 Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/35-rnn-sentiment-analysis.ipynb RNNs: what are they for? \u00b6 Time series data When you want to predict future behavior based on past behavior Web logs, sensor logs, stock trades Where to drive your self-driving car based on past trajectories Data that consists of sequences of arbitrary length Machine translation Image captions Machine generated music A Reccurrent neuron \u00b6 As we run a training step on a neuron, it will apply a step function. Usually we would output the result, but now we will feed it back to the same neuron It will influence the next step of the computation. RNN Topologies \u00b6 Sequence to sequence i.e., predict stock prices based on series of historical data Sequence to vector i.e., words in sentence to sentiment Vector to sequence i.e., create captions from an image Encoder to Decoder Sequence -> vector -> sequence i.e., machine translation Training RNNs \u00b6 Backpropogation through time Just like backpropogation on MLPs, but applied to each time step All those time steps add up fast Ends up looking like a really, really deep neural network. Can limit backpropogation to a limited number of time steps (truncated backpropogation though time) State from earlier time steps get dilluted over time This can be a problem for example when learning sentence structures LSTM Cell Long Short-Term Memory Cell Maintains separate short-term and long-term states GRU Cell Gated Recurrent Unit Simplified LSTM cell that performs about as well It's really hard Very sensitive to topologies, choice of hyperparameters Very resource intensive A wrong choice can lead to a RNN that doesn't converge at all.","title":"Recurrent neural networks"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/09-recurrent-neural-networks/#recurrent-neural-networks","text":"Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/35-rnn-sentiment-analysis.ipynb","title":"Recurrent neural networks"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/09-recurrent-neural-networks/#rnns-what-are-they-for","text":"Time series data When you want to predict future behavior based on past behavior Web logs, sensor logs, stock trades Where to drive your self-driving car based on past trajectories Data that consists of sequences of arbitrary length Machine translation Image captions Machine generated music","title":"RNNs: what are they for?"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/09-recurrent-neural-networks/#a-reccurrent-neuron","text":"As we run a training step on a neuron, it will apply a step function. Usually we would output the result, but now we will feed it back to the same neuron It will influence the next step of the computation.","title":"A Reccurrent neuron"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/09-recurrent-neural-networks/#rnn-topologies","text":"Sequence to sequence i.e., predict stock prices based on series of historical data Sequence to vector i.e., words in sentence to sentiment Vector to sequence i.e., create captions from an image Encoder to Decoder Sequence -> vector -> sequence i.e., machine translation","title":"RNN Topologies"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/09-recurrent-neural-networks/#training-rnns","text":"Backpropogation through time Just like backpropogation on MLPs, but applied to each time step All those time steps add up fast Ends up looking like a really, really deep neural network. Can limit backpropogation to a limited number of time steps (truncated backpropogation though time) State from earlier time steps get dilluted over time This can be a problem for example when learning sentence structures LSTM Cell Long Short-Term Memory Cell Maintains separate short-term and long-term states GRU Cell Gated Recurrent Unit Simplified LSTM cell that performs about as well It's really hard Very sensitive to topologies, choice of hyperparameters Very resource intensive A wrong choice can lead to a RNN that doesn't converge at all.","title":"Training RNNs"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/10-transfer-learning/","text":"Transfer learning \u00b6 Re-using trained models \u00b6 For may common problems, you can import pre-trained models and just use them. Image classification (ResNet, Inception, MobileNet, Oxford VGG) NLP (word2vec, GloVe) Use them as-is, or tune them for your application Model Zoos Caffe Model Zoo Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/36-keras-transfer-learning.ipynb","title":"Transfer learning"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/10-transfer-learning/#transfer-learning","text":"","title":"Transfer learning"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/10-transfer-learning/#re-using-trained-models","text":"For may common problems, you can import pre-trained models and just use them. Image classification (ResNet, Inception, MobileNet, Oxford VGG) NLP (word2vec, GloVe) Use them as-is, or tune them for your application Model Zoos Caffe Model Zoo Python notebook: https://github.com/daviskregers/data-science-recap/blob/main/36-keras-transfer-learning.ipynb","title":"Re-using trained models"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/11-tuning-neural-networks/","text":"Tuning Neural Networks: Learning Rate and Batch Size hyperparameters \u00b6 Learing Rate \u00b6 Neural networks are trained by gradient descent (or similar means) We start at some random point and sample different solutions (weights) seeking to minimize some cost function, over many epochs How far apart these samples are is the learning rate. Effect of learning rate \u00b6 Too high a learning rate means you might overshoot the optimal solution Too small learning rate will take too long to find the optimal solution Learning rate is an example of a hyperparameter Batch Size \u00b6 How many training samples are used within each epoch Somewhat counter intuitively: Smaller batch sizes can work their way our of \"local minima\" more easily Batch sizes that are too large can end up getting stuck in the wrong solution Random shuffling at each epoch can make this look like very inconsistent results from run to run To Recap \u00b6 Small batch sizes tend to not get stuck in local minima Large batch sizes can converge on the wrong solution at random Large learning rates can overshoot the correct solution Small learning rates increase training time.","title":"Tuning Neural Networks: Learning Rate and Batch Size hyperparameters"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/11-tuning-neural-networks/#tuning-neural-networks-learning-rate-and-batch-size-hyperparameters","text":"","title":"Tuning Neural Networks: Learning Rate and Batch Size hyperparameters"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/11-tuning-neural-networks/#learing-rate","text":"Neural networks are trained by gradient descent (or similar means) We start at some random point and sample different solutions (weights) seeking to minimize some cost function, over many epochs How far apart these samples are is the learning rate.","title":"Learing Rate"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/11-tuning-neural-networks/#effect-of-learning-rate","text":"Too high a learning rate means you might overshoot the optimal solution Too small learning rate will take too long to find the optimal solution Learning rate is an example of a hyperparameter","title":"Effect of learning rate"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/11-tuning-neural-networks/#batch-size","text":"How many training samples are used within each epoch Somewhat counter intuitively: Smaller batch sizes can work their way our of \"local minima\" more easily Batch sizes that are too large can end up getting stuck in the wrong solution Random shuffling at each epoch can make this look like very inconsistent results from run to run","title":"Batch Size"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/11-tuning-neural-networks/#to-recap","text":"Small batch sizes tend to not get stuck in local minima Large batch sizes can converge on the wrong solution at random Large learning rates can overshoot the correct solution Small learning rates increase training time.","title":"To Recap"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/12-regularization-with-dropout-and-early-stopping/","text":"Regularization with Dropout and Early Stopping \u00b6 What is regularization? \u00b6 Prevents overfitting Models that are good at making predictions on the data they were trained on, but not on new data it hasn't seen before Overfitted models learned patterns in the training data that don't generalize to the real world Often seen as high accuracy on training data set, but lower accuracy on test or evaluation data set. When training and evaluating a model, we use training, evaluation and testind data sets. Regularization techniques are intended to prevent overfitting. One of regularization techniques is to simplify the model and use less layers, neurons. Another technique is to use Dropout. It removes some neurons at each network, it forces the NN to spread out it's learning. Early stopping. In the training process, when finishing the epoch, at some time we start to increasing the accuracy at the training data set, but the accuracy doesn't improve in the validation data set. It starts to overfit, we might want to stop the training process early.","title":"Regularization with Dropout and Early Stopping"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/12-regularization-with-dropout-and-early-stopping/#regularization-with-dropout-and-early-stopping","text":"","title":"Regularization with Dropout and Early Stopping"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/12-regularization-with-dropout-and-early-stopping/#what-is-regularization","text":"Prevents overfitting Models that are good at making predictions on the data they were trained on, but not on new data it hasn't seen before Overfitted models learned patterns in the training data that don't generalize to the real world Often seen as high accuracy on training data set, but lower accuracy on test or evaluation data set. When training and evaluating a model, we use training, evaluation and testind data sets. Regularization techniques are intended to prevent overfitting. One of regularization techniques is to simplify the model and use less layers, neurons. Another technique is to use Dropout. It removes some neurons at each network, it forces the NN to spread out it's learning. Early stopping. In the training process, when finishing the epoch, at some time we start to increasing the accuracy at the training data set, but the accuracy doesn't improve in the validation data set. It starts to overfit, we might want to stop the training process early.","title":"What is regularization?"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/13-ethics-of-deep-learning/","text":"Ethics of Deep Learning \u00b6 Types of errors \u00b6 Accuracy doesn't tell the whole story Type 1: False positive Unnecessary surgery Slam on the brakes for no reason Type 2: False negative Untreated conditions You crash into the car in front of you Think about the ramifications of different types of errors from your model, tune it accordingly. Hidden biases \u00b6 Just because your model isn't human doesn't mean it's inherently fair Example: train a model on what sort of job applicants get hired, use it to screen resumes. Past biases toward gender / age / race will be reflected in your model, because it was reflected in the data your trained the model with. Is it really better than a human? \u00b6 Don't oversell the capabilities of an algorithm in your excitement Example: medical diagnostics that are almost, but not quite, as good as a human doctor Another example: self-driving cars that can kill people Unintended applications of your research \u00b6 Think of how can this be twisted and be used in a different way.","title":"Ethics of Deep Learning"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/13-ethics-of-deep-learning/#ethics-of-deep-learning","text":"","title":"Ethics of Deep Learning"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/13-ethics-of-deep-learning/#types-of-errors","text":"Accuracy doesn't tell the whole story Type 1: False positive Unnecessary surgery Slam on the brakes for no reason Type 2: False negative Untreated conditions You crash into the car in front of you Think about the ramifications of different types of errors from your model, tune it accordingly.","title":"Types of errors"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/13-ethics-of-deep-learning/#hidden-biases","text":"Just because your model isn't human doesn't mean it's inherently fair Example: train a model on what sort of job applicants get hired, use it to screen resumes. Past biases toward gender / age / race will be reflected in your model, because it was reflected in the data your trained the model with.","title":"Hidden biases"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/13-ethics-of-deep-learning/#is-it-really-better-than-a-human","text":"Don't oversell the capabilities of an algorithm in your excitement Example: medical diagnostics that are almost, but not quite, as good as a human doctor Another example: self-driving cars that can kill people","title":"Is it really better than a human?"},{"location":"Data%20Science/10-deep-learning-and-neural-networks/13-ethics-of-deep-learning/#unintended-applications-of-your-research","text":"Think of how can this be twisted and be used in a different way.","title":"Unintended applications of your research"},{"location":"Data%20Structures%20And%20Algorithms/","text":"Data Structures and Algorithms Recap Sources: - Data Structures & Algorithms !","title":"Index"},{"location":"Data%20Structures%20And%20Algorithms/Big-O%20Algorithm%20Complexity/","text":"Cheatsheet: https://www.bigocheatsheet.com/","title":"Big O Algorithm Complexity"},{"location":"Data%20Structures%20And%20Algorithms/01-introduction/01-course-breakdown/","text":"Course Breakdown \u00b6 This course is broken down into 5 sections: Basics Introduction Recursion Algorithm Runtime Analysis Physical Datastructures Arrays LinkedLists Logical Datastructures Tree Hashing Stacks Queues Graphs Miscellaneous topics Sorting Algorithm techniques Magic Framework Greedy algorithm Divide & Conquer Dynamic programming In the Introduction section first we will understand: - What is a Data Structure - Examples of Data structures in day to day life - Data Structures vs Algorithms - Why companies ask Data Structure & Algorithm knowledge in interviews - Which companies as Data Structure knowledge in interviews - Types of Data Structures In the Recursion section we will understand: - What is recursion - Why should we learn recursion - Examples of recursion - How recursion works internally - Recursion vs iterations - When to use/avoid recursion - Practical use of Recursion In the Algorithm Run Time Analysis we will understand: - What & Why of Algorithm Runtime Analysis - Types of Time Complexity - What are Notations - How to Calculate Time Complexity - Iterative / Recursive Arrays: - What is an array and why use it - Types of arrays - How is an Array represented in memory - Common operations like Creating/Inserting/Deleting/Searching etc - Practical uses of Array - Pros and cons of Array Linked Lists: - What is a linked list and why use it - Types of Linked Lists - How is a Linked List represented in memory - Common operations like Creating/Inserting/Deleting/Searching/etc - Practical use of Linked Lists - Pros and sons of Linked Lists Stacks: - What is a stack - Why should we learn stack - Common operations - When to use/avoid stacks - Practical uses of stacks Queue: - What is a queue - Why should we learn queues - Common operations - When to use/avoid queues - Practical uses of queues Trees: - What is a try and why use it - Types of trees: - Binary tree - BST - Heap - AVL Tree - Trie Graphs: - What is a graph and why use it - Types of graphs - Graph traversal techniques - Topological sorting - Single source shortest path - All pair shortest path - Minimum spanning tree - Pros & cons, practical uses, comparisons. Hashing: - What is hashing and why use it - Sample good Hash function - Collision Resolution techniques - Hashing vs Array vs Linked List vs Tree - What happens when Hash Table is full - Pros & Cons of Collision Resultution techniques - When to use which Collision Resulution Technique - When to use/avoid hashing - Practical uses of hashing Sorting: - What is Sorting? - Practical uses of sorting - Types of sorting - in-place, out-place, stable, unstable - Why should we read so many sorting techniques - Problems & Solutions (Bubble sort, Selection Sort, Insertion Sort, Bucket Sort, Merge Sort, Quick Sort, Heap Sort)","title":"Course Breakdown"},{"location":"Data%20Structures%20And%20Algorithms/01-introduction/01-course-breakdown/#course-breakdown","text":"This course is broken down into 5 sections: Basics Introduction Recursion Algorithm Runtime Analysis Physical Datastructures Arrays LinkedLists Logical Datastructures Tree Hashing Stacks Queues Graphs Miscellaneous topics Sorting Algorithm techniques Magic Framework Greedy algorithm Divide & Conquer Dynamic programming In the Introduction section first we will understand: - What is a Data Structure - Examples of Data structures in day to day life - Data Structures vs Algorithms - Why companies ask Data Structure & Algorithm knowledge in interviews - Which companies as Data Structure knowledge in interviews - Types of Data Structures In the Recursion section we will understand: - What is recursion - Why should we learn recursion - Examples of recursion - How recursion works internally - Recursion vs iterations - When to use/avoid recursion - Practical use of Recursion In the Algorithm Run Time Analysis we will understand: - What & Why of Algorithm Runtime Analysis - Types of Time Complexity - What are Notations - How to Calculate Time Complexity - Iterative / Recursive Arrays: - What is an array and why use it - Types of arrays - How is an Array represented in memory - Common operations like Creating/Inserting/Deleting/Searching etc - Practical uses of Array - Pros and cons of Array Linked Lists: - What is a linked list and why use it - Types of Linked Lists - How is a Linked List represented in memory - Common operations like Creating/Inserting/Deleting/Searching/etc - Practical use of Linked Lists - Pros and sons of Linked Lists Stacks: - What is a stack - Why should we learn stack - Common operations - When to use/avoid stacks - Practical uses of stacks Queue: - What is a queue - Why should we learn queues - Common operations - When to use/avoid queues - Practical uses of queues Trees: - What is a try and why use it - Types of trees: - Binary tree - BST - Heap - AVL Tree - Trie Graphs: - What is a graph and why use it - Types of graphs - Graph traversal techniques - Topological sorting - Single source shortest path - All pair shortest path - Minimum spanning tree - Pros & cons, practical uses, comparisons. Hashing: - What is hashing and why use it - Sample good Hash function - Collision Resolution techniques - Hashing vs Array vs Linked List vs Tree - What happens when Hash Table is full - Pros & Cons of Collision Resultution techniques - When to use which Collision Resulution Technique - When to use/avoid hashing - Practical uses of hashing Sorting: - What is Sorting? - Practical uses of sorting - Types of sorting - in-place, out-place, stable, unstable - Why should we read so many sorting techniques - Problems & Solutions (Bubble sort, Selection Sort, Insertion Sort, Bucket Sort, Merge Sort, Quick Sort, Heap Sort)","title":"Course Breakdown"},{"location":"Data%20Structures%20And%20Algorithms/01-introduction/02-what-are-data-structures-and-algorithms/","text":"What are Data Structures and Algorithms \u00b6 Data structures is a way to organize data in a way that enables it to be processed in an efficient time. Some of the common Data Structures are: - Array - Linked List - Stack - Queue - Tree - Graphs An algorithm is a set of rules to be followed to solve a problem","title":"What are Data Structures and Algorithms"},{"location":"Data%20Structures%20And%20Algorithms/01-introduction/02-what-are-data-structures-and-algorithms/#what-are-data-structures-and-algorithms","text":"Data structures is a way to organize data in a way that enables it to be processed in an efficient time. Some of the common Data Structures are: - Array - Linked List - Stack - Queue - Tree - Graphs An algorithm is a set of rules to be followed to solve a problem","title":"What are Data Structures and Algorithms"},{"location":"Data%20Structures%20And%20Algorithms/01-introduction/03-why-companies-ask-for-data-structures/","text":"Why companies ask for Data Structures \u00b6 They check for Problem Solving skills They check for Coding and Testing skills Mostly product based companies like Microsoft, Google and Facebook ask for knowledge in Data Structures. Service based companies don't really emphasize much on DS as they want a quick solution instead of an efficient one.","title":"Why companies ask for Data Structures"},{"location":"Data%20Structures%20And%20Algorithms/01-introduction/03-why-companies-ask-for-data-structures/#why-companies-ask-for-data-structures","text":"They check for Problem Solving skills They check for Coding and Testing skills Mostly product based companies like Microsoft, Google and Facebook ask for knowledge in Data Structures. Service based companies don't really emphasize much on DS as they want a quick solution instead of an efficient one.","title":"Why companies ask for Data Structures"},{"location":"Data%20Structures%20And%20Algorithms/01-introduction/04-DS-in-every-day-life/","text":"DS inf every day life \u00b6 Here will be a list of data structures in non-computing fields. Queue. Fans assembled for an autograph. Tree. Parent-child relationship/generalization-specialization Graph. Shortest route from one city to another city.","title":"DS inf every day life"},{"location":"Data%20Structures%20And%20Algorithms/01-introduction/04-DS-in-every-day-life/#ds-inf-every-day-life","text":"Here will be a list of data structures in non-computing fields. Queue. Fans assembled for an autograph. Tree. Parent-child relationship/generalization-specialization Graph. Shortest route from one city to another city.","title":"DS inf every day life"},{"location":"Data%20Structures%20And%20Algorithms/01-introduction/05-types-of-DS/","text":"Types of DS \u00b6 On the high level Data Structures can be divided in two categories - Primitive DS and Non-Primitive DS. Primitive DS are provided by given programming language - integer, float, character, boolean. Non-primitive categories can be divided into another two categories - physical DS (Array, LinkedList) and Logical DS (Stack, Queue, Tree, Graph).","title":"Types of DS"},{"location":"Data%20Structures%20And%20Algorithms/01-introduction/05-types-of-DS/#types-of-ds","text":"On the high level Data Structures can be divided in two categories - Primitive DS and Non-Primitive DS. Primitive DS are provided by given programming language - integer, float, character, boolean. Non-primitive categories can be divided into another two categories - physical DS (Array, LinkedList) and Logical DS (Stack, Queue, Tree, Graph).","title":"Types of DS"},{"location":"Data%20Structures%20And%20Algorithms/02-recursion/01-what-is-recursion/","text":"What is recursion? \u00b6 When it comes to recursion, there are 3 basic properties: - Same operation is performed multiple times with different inputs - In every step we try to make the problem smaller - We mandatorily need to have a base condition, which tells system when to stop the recursion. The recursion can be applied to this binary search tree, when searching for value 4 . search(root, valueToSearch) if(root == null) return null else if (root.value == valueToSearch) return root else if (valueToSearch < root.value) search(root.left, valueToSearch) else search(root.right, valueToSearch)","title":"What is recursion?"},{"location":"Data%20Structures%20And%20Algorithms/02-recursion/01-what-is-recursion/#what-is-recursion","text":"When it comes to recursion, there are 3 basic properties: - Same operation is performed multiple times with different inputs - In every step we try to make the problem smaller - We mandatorily need to have a base condition, which tells system when to stop the recursion. The recursion can be applied to this binary search tree, when searching for value 4 . search(root, valueToSearch) if(root == null) return null else if (root.value == valueToSearch) return root else if (valueToSearch < root.value) search(root.left, valueToSearch) else search(root.right, valueToSearch)","title":"What is recursion?"},{"location":"Data%20Structures%20And%20Algorithms/02-recursion/02-why-learn-recursion/","text":"Why learn recursion \u00b6 Because it makes the code easy to write (compared to iterative) whenevert a given problem can be broken down into a similar sub-problem. Because it is heavily sed in Data Structures like Trees, Graphs etc. It is heavily used in techniques like Divide and Conquer , Greedy , Dynamic programming .","title":"Why learn recursion"},{"location":"Data%20Structures%20And%20Algorithms/02-recursion/02-why-learn-recursion/#why-learn-recursion","text":"Because it makes the code easy to write (compared to iterative) whenevert a given problem can be broken down into a similar sub-problem. Because it is heavily sed in Data Structures like Trees, Graphs etc. It is heavily used in techniques like Divide and Conquer , Greedy , Dynamic programming .","title":"Why learn recursion"},{"location":"Data%20Structures%20And%20Algorithms/02-recursion/03-format-of-recursive-functions/","text":"Format of recursive functions \u00b6 A recursive function is based on two elements: - Recursive case. Case where the function recurs. - Base case. Case where the function doesnt recur. Example: SampleRecursion (parameter) { if ( base case is satisfied ) return some base case value else SampleRecusion(modified parameter) }","title":"Format of recursive functions"},{"location":"Data%20Structures%20And%20Algorithms/02-recursion/03-format-of-recursive-functions/#format-of-recursive-functions","text":"A recursive function is based on two elements: - Recursive case. Case where the function recurs. - Base case. Case where the function doesnt recur. Example: SampleRecursion (parameter) { if ( base case is satisfied ) return some base case value else SampleRecusion(modified parameter) }","title":"Format of recursive functions"},{"location":"Data%20Structures%20And%20Algorithms/02-recursion/04-how-recursion-works-internally/","text":"How Recursion works internally \u00b6 Given an example foo(n) { if (n < 1) return else foo(n-1) print \"Hello World\" + n } main() { foo(3) } The function calls will be stored in a stack memory like so:","title":"How Recursion works internally"},{"location":"Data%20Structures%20And%20Algorithms/02-recursion/04-how-recursion-works-internally/#how-recursion-works-internally","text":"Given an example foo(n) { if (n < 1) return else foo(n-1) print \"Hello World\" + n } main() { foo(3) } The function calls will be stored in a stack memory like so:","title":"How Recursion works internally"},{"location":"Data%20Structures%20And%20Algorithms/02-recursion/05-finding-a-factorial-using-recursion/","text":"Finding a factorial using recursion \u00b6 Factorial definition: - It can only be found for non-negative integers - Denoted by n! - Is the product of all positive integers from 1 to n Examples: - 5! = 5 * 4 * 3 * 2 * 1 = 120 - 10! = 10 * 9 * 8 * 7 * 6 * 5 * 4 * 3 * 2 * 1 = 3628800 Factorial(n): if n == 0 return 1 return (n * factorial(n-1))","title":"Finding a factorial using recursion"},{"location":"Data%20Structures%20And%20Algorithms/02-recursion/05-finding-a-factorial-using-recursion/#finding-a-factorial-using-recursion","text":"Factorial definition: - It can only be found for non-negative integers - Denoted by n! - Is the product of all positive integers from 1 to n Examples: - 5! = 5 * 4 * 3 * 2 * 1 = 120 - 10! = 10 * 9 * 8 * 7 * 6 * 5 * 4 * 3 * 2 * 1 = 3628800 Factorial(n): if n == 0 return 1 return (n * factorial(n-1))","title":"Finding a factorial using recursion"},{"location":"Data%20Structures%20And%20Algorithms/02-recursion/06-finding-fibonnaci-using-recursion/","text":"Finding Fibonacci series using Recursion \u00b6 Fibonacci series: - A series of numbers in which each number is the sum of the two preceding numbers. - The first 2 numbers by the definition are 0 and 1 Example: - 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, ... ``` fib(n) if (n < 1) return error message else if n == 1 or n == 2 return n - 1 else return fib(n - 1) + fib(n - 2)","title":"Finding Fibonacci series using Recursion"},{"location":"Data%20Structures%20And%20Algorithms/02-recursion/06-finding-fibonnaci-using-recursion/#finding-fibonacci-series-using-recursion","text":"Fibonacci series: - A series of numbers in which each number is the sum of the two preceding numbers. - The first 2 numbers by the definition are 0 and 1 Example: - 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, ... ``` fib(n) if (n < 1) return error message else if n == 1 or n == 2 return n - 1 else return fib(n - 1) + fib(n - 2)","title":"Finding Fibonacci series using Recursion"},{"location":"Data%20Structures%20And%20Algorithms/02-recursion/07-recursion-vs-iteration/","text":"Recursion vs iteration \u00b6 Recursion Iteration Space efficient? No Yes Time efficcient? No Yes Ease of code (to solve sub-problems)? Yes No When it comes to space efficiency, the recursion pushes all recursive calls to the memory stack, but in iterations we do not need to store them in the stack, we can use a basic loop. In time efficiency since recursion uses a memory stack, it will require to use push and pop functions of the memory stack, as well as pointer operations which will take up time. These operations can be avoided when using iterations with a simple loop. When it comes to ease of code, when we have a problem that can be easily broken up into similar sub-problems, in these kind of cases the recursion is a better approach.","title":"Recursion vs iteration"},{"location":"Data%20Structures%20And%20Algorithms/02-recursion/07-recursion-vs-iteration/#recursion-vs-iteration","text":"Recursion Iteration Space efficient? No Yes Time efficcient? No Yes Ease of code (to solve sub-problems)? Yes No When it comes to space efficiency, the recursion pushes all recursive calls to the memory stack, but in iterations we do not need to store them in the stack, we can use a basic loop. In time efficiency since recursion uses a memory stack, it will require to use push and pop functions of the memory stack, as well as pointer operations which will take up time. These operations can be avoided when using iterations with a simple loop. When it comes to ease of code, when we have a problem that can be easily broken up into similar sub-problems, in these kind of cases the recursion is a better approach.","title":"Recursion vs iteration"},{"location":"Data%20Structures%20And%20Algorithms/02-recursion/08-when-to-use-and-avoid-recursion/","text":"When to use and avoid recursion \u00b6 When to use recursion: - When we can easily break down a problem into similar subproblems. - When we are ok with extra overhead (both time and space) that comes with it. - When we need a quick working solution instead of an efficient one. When to avoid recursion: - If the response to any of the above statements are NO, we should not go with recursion.","title":"When to use and avoid recursion"},{"location":"Data%20Structures%20And%20Algorithms/02-recursion/08-when-to-use-and-avoid-recursion/#when-to-use-and-avoid-recursion","text":"When to use recursion: - When we can easily break down a problem into similar subproblems. - When we are ok with extra overhead (both time and space) that comes with it. - When we need a quick working solution instead of an efficient one. When to avoid recursion: - If the response to any of the above statements are NO, we should not go with recursion.","title":"When to use and avoid recursion"},{"location":"Data%20Structures%20And%20Algorithms/02-recursion/09-practical-uses-of-recursion/","text":"Practical uses of recursion \u00b6 Stacks Trees - traversal/searching/insertion/deletion Sorting - Quick Sort, Merge Sort Divide and Conquer Dynamic programming etc","title":"Practical uses of recursion"},{"location":"Data%20Structures%20And%20Algorithms/02-recursion/09-practical-uses-of-recursion/#practical-uses-of-recursion","text":"Stacks Trees - traversal/searching/insertion/deletion Sorting - Quick Sort, Merge Sort Divide and Conquer Dynamic programming etc","title":"Practical uses of recursion"},{"location":"Data%20Structures%20And%20Algorithms/03-algorithm-runtime-analysis/01-what-is-algorithm-runtime-analysis/","text":"What is algorithm runtime analysis \u00b6 Algorithm runtime analysis is a study of a given algorithm's running time, by identifying it's behaviour as the input size for the algorithm increases. In laymans terms - it measures algorithm's performance. Runtime analysis is used to measure efficiency of any given algorithm.","title":"What is algorithm runtime analysis"},{"location":"Data%20Structures%20And%20Algorithms/03-algorithm-runtime-analysis/01-what-is-algorithm-runtime-analysis/#what-is-algorithm-runtime-analysis","text":"Algorithm runtime analysis is a study of a given algorithm's running time, by identifying it's behaviour as the input size for the algorithm increases. In laymans terms - it measures algorithm's performance. Runtime analysis is used to measure efficiency of any given algorithm.","title":"What is algorithm runtime analysis"},{"location":"Data%20Structures%20And%20Algorithms/03-algorithm-runtime-analysis/02-notations/","text":"Notations for Algorithm runtime analysis \u00b6 There are 3 notations for runtime analysis: - Omega (\u03a9): - This notation gives the tighter lower bound of a given algorithm - In laymans terms - we can say that for any given input, running time of a given algorithm will not be less than given time. - Big-O (O) - This notation gives the tighter upper bound of a given algorithm - For any given input, running time of a given algorithm will not be more than given time. - Theta (\u0398) - This notation decides whether upper bound and lower bound of a given algorithm are the same of not. - For any given input, running time of a given algorithm will on average be equal to given time. Examples for notations \u00b6 We can try to understand these 3 notations with a simple array example. It is not a sorted one. Our task is to find a given number in the array. We have to start at the first cell and check if it matches the given number, if it does - return the cell index. If not, repeat the process on the next cell until we have found the number. If we reach the end of the array and still don't find the number - return an error message. So in total, we have n number of cells. To check all the cells, it will take n * 1 = n units of time. Omega (\u03a9): \u03a9(1) Big-O (O) O(n) Theta (\u0398) \u0398(n/2) Most of the time we will only be concerned about Big-O notations, but others are still used in academic work.","title":"Notations for Algorithm runtime analysis"},{"location":"Data%20Structures%20And%20Algorithms/03-algorithm-runtime-analysis/02-notations/#notations-for-algorithm-runtime-analysis","text":"There are 3 notations for runtime analysis: - Omega (\u03a9): - This notation gives the tighter lower bound of a given algorithm - In laymans terms - we can say that for any given input, running time of a given algorithm will not be less than given time. - Big-O (O) - This notation gives the tighter upper bound of a given algorithm - For any given input, running time of a given algorithm will not be more than given time. - Theta (\u0398) - This notation decides whether upper bound and lower bound of a given algorithm are the same of not. - For any given input, running time of a given algorithm will on average be equal to given time.","title":"Notations for Algorithm runtime analysis"},{"location":"Data%20Structures%20And%20Algorithms/03-algorithm-runtime-analysis/02-notations/#examples-for-notations","text":"We can try to understand these 3 notations with a simple array example. It is not a sorted one. Our task is to find a given number in the array. We have to start at the first cell and check if it matches the given number, if it does - return the cell index. If not, repeat the process on the next cell until we have found the number. If we reach the end of the array and still don't find the number - return an error message. So in total, we have n number of cells. To check all the cells, it will take n * 1 = n units of time. Omega (\u03a9): \u03a9(1) Big-O (O) O(n) Theta (\u0398) \u0398(n/2) Most of the time we will only be concerned about Big-O notations, but others are still used in academic work.","title":"Examples for notations"},{"location":"Data%20Structures%20And%20Algorithms/03-algorithm-runtime-analysis/03-examples-of-runtime-complexities/","text":"Examples of runtime complexities \u00b6 Time complexity Name Example O(1) Constant Adding an element in front of a linked list O(log n) Logarithmic Finding an element in sorted array O(n) Linear Finding an element in unsorted array O(n log n) Linear logarithmic Merge Sort O(n^2) Quadratic Shortest path between 2 nodes in a graph O(n^3) Cubic Matrix multiplication O(2^n) Exponential Tower of Hanoi Problem","title":"Examples of runtime complexities"},{"location":"Data%20Structures%20And%20Algorithms/03-algorithm-runtime-analysis/03-examples-of-runtime-complexities/#examples-of-runtime-complexities","text":"Time complexity Name Example O(1) Constant Adding an element in front of a linked list O(log n) Logarithmic Finding an element in sorted array O(n) Linear Finding an element in unsorted array O(n log n) Linear logarithmic Merge Sort O(n^2) Quadratic Shortest path between 2 nodes in a graph O(n^3) Cubic Matrix multiplication O(2^n) Exponential Tower of Hanoi Problem","title":"Examples of runtime complexities"},{"location":"Data%20Structures%20And%20Algorithms/03-algorithm-runtime-analysis/04-finding-time-complexity-of-iterative-algorithm/","text":"Finding Time Complexity of an iterative algorithm \u00b6 We have a 1D array of unsorted values: findBiggestNumber( int arr[] ): biggestNumber = arr[0] loop: i = 1 to length(arr) - 1 if arr[i] > biggestNumber biggestNumber = arr[i] return biggestNumber findBiggestNumber( int arr[] ): biggestNumber = arr[0] ------------------ O(1) loop: i = 1 to length(arr) - 1 ---------- O(n) if arr[i] > biggestNumber ----------- O(1) biggestNumber = arr[i] ---------- O(1) return biggestNumber -------------------- O(1) Time complexity = O(1) + O(n) + O(1) = O(n) When calculating time complexity, we are avoiding constant values, so we do not take in to account them. Same will be in cases like these: O(n-1) = O(n) O(2n) = O(n) O(10) = O(1) O(2000) = O(1)","title":"Finding Time Complexity of an iterative algorithm"},{"location":"Data%20Structures%20And%20Algorithms/03-algorithm-runtime-analysis/04-finding-time-complexity-of-iterative-algorithm/#finding-time-complexity-of-an-iterative-algorithm","text":"We have a 1D array of unsorted values: findBiggestNumber( int arr[] ): biggestNumber = arr[0] loop: i = 1 to length(arr) - 1 if arr[i] > biggestNumber biggestNumber = arr[i] return biggestNumber findBiggestNumber( int arr[] ): biggestNumber = arr[0] ------------------ O(1) loop: i = 1 to length(arr) - 1 ---------- O(n) if arr[i] > biggestNumber ----------- O(1) biggestNumber = arr[i] ---------- O(1) return biggestNumber -------------------- O(1) Time complexity = O(1) + O(n) + O(1) = O(n) When calculating time complexity, we are avoiding constant values, so we do not take in to account them. Same will be in cases like these: O(n-1) = O(n) O(2n) = O(n) O(10) = O(1) O(2000) = O(1)","title":"Finding Time Complexity of an iterative algorithm"},{"location":"Data%20Structures%20And%20Algorithms/03-algorithm-runtime-analysis/05-finding-time-complexity-of-recursive-algorithm/","text":"Findint time complexity of a recursive algorithm \u00b6 Example 1 \u00b6 findBiggestNumber(A, n): static highest = Integer.Min if n == -1 return highest else if A[n] > highest update highest return FindBiggestNumber(A, n - 1) findBiggestNumber(A, n): ------------------------ T(n) static highest = Integer.Min ---------------- O(1) if n == -1 ---------------------------------- O(1) return highest -------------------------- O(1) else ---------------------------------------- O(1) if A[n] > highest ----------------------- O(1) update highest ---------------------- O(1) return FindBiggestNumber(A, n - 1) ---------- T(n-1) When we encounter a recursive function, we assume it will take T(n) time. Back substitution: T(n) = O(1) + T(n-1) ----------- Equation #1 T(-1) = O(1) -------------------- Base condition T(n-1) = O(1) + T((n-1)-1) ------- Equation #2 T(n-2) = O(1) + T((n-2)-1) ------- Equation #3 T(n) = 1 + T(n-1) = 1 + (1 + T((n-1) -1)) = 2 + T(n-2) = 2 + 1 + T((n-2)-1) = 3 + T(n-3) ... = k + T(n-k) ---- replace k with (n+1) = (n + 1) + T(n-(n+1)) = n + 1 + T(-1) = n + 2 = O(n) Example 2 \u00b6 Given a sorted array of 11 numbers, find number 110. BinarySearch(int findNumber, int arr[], start, end): if start == end if arr[start] == findNumber return start else return error message that number does not exist in array mid = findMid(arr[], start, end) if mid > findNumber BinarySearch(int findNumber, int arr[], start, mid) else if mid < findNumber BinarySearch(int findNumber, int arr[], mid, end) else if mid = findNumber return mid BinarySearch(int findNumber, int arr[], start, end): ------------------- T(n) if start == end ---------------------------------------------------- O(1) if arr[start] == findNumber ------------------------------------ O(1) return start ----------------------------------------------- O(1) else return error that number does not exist in array ---------- O(1) mid = findMid(arr[], start, end) ----------------------------------- O(1) if mid > findNumber ------------------------------------------------ O(1) BinarySearch(int findNumber, int arr[], start, mid) ------------ T(n/2) else if mid < findNumber ------------------------------------------- O(1) BinarySearch(int findNumber, int arr[], mid, end) -------------- T(n/2) else if mid = findNumber ------------------------------------------- O(1) return mid ----------------------------------------------------- O(1) Time complexity = T(n) = O(1) + T(n/2) Back substitution T(n) = T(n/2) + 1 -------------- Equation #1 T(1) = 1 ----------------------- Base Condition T(n/2) = T(n/4) + 1 -------------- Equation #2 T(n/4) = T(n/8) + 1 -------------- Equation #3 T(n) = T(n/2) + 1 = (T(n/4) + 1) + 1 = T(n/4) + 2 = (T(n/8) + 1) + 2 = T(n/8) + 3 = T(n/2^k) + k ----------- n/2^k = 1 ... n = 2^k ... k = logn = T(1) + log(n) = 1 + logn = O(logn)","title":"Findint time complexity of a recursive algorithm"},{"location":"Data%20Structures%20And%20Algorithms/03-algorithm-runtime-analysis/05-finding-time-complexity-of-recursive-algorithm/#findint-time-complexity-of-a-recursive-algorithm","text":"","title":"Findint time complexity of a recursive algorithm"},{"location":"Data%20Structures%20And%20Algorithms/03-algorithm-runtime-analysis/05-finding-time-complexity-of-recursive-algorithm/#example-1","text":"findBiggestNumber(A, n): static highest = Integer.Min if n == -1 return highest else if A[n] > highest update highest return FindBiggestNumber(A, n - 1) findBiggestNumber(A, n): ------------------------ T(n) static highest = Integer.Min ---------------- O(1) if n == -1 ---------------------------------- O(1) return highest -------------------------- O(1) else ---------------------------------------- O(1) if A[n] > highest ----------------------- O(1) update highest ---------------------- O(1) return FindBiggestNumber(A, n - 1) ---------- T(n-1) When we encounter a recursive function, we assume it will take T(n) time. Back substitution: T(n) = O(1) + T(n-1) ----------- Equation #1 T(-1) = O(1) -------------------- Base condition T(n-1) = O(1) + T((n-1)-1) ------- Equation #2 T(n-2) = O(1) + T((n-2)-1) ------- Equation #3 T(n) = 1 + T(n-1) = 1 + (1 + T((n-1) -1)) = 2 + T(n-2) = 2 + 1 + T((n-2)-1) = 3 + T(n-3) ... = k + T(n-k) ---- replace k with (n+1) = (n + 1) + T(n-(n+1)) = n + 1 + T(-1) = n + 2 = O(n)","title":"Example 1"},{"location":"Data%20Structures%20And%20Algorithms/03-algorithm-runtime-analysis/05-finding-time-complexity-of-recursive-algorithm/#example-2","text":"Given a sorted array of 11 numbers, find number 110. BinarySearch(int findNumber, int arr[], start, end): if start == end if arr[start] == findNumber return start else return error message that number does not exist in array mid = findMid(arr[], start, end) if mid > findNumber BinarySearch(int findNumber, int arr[], start, mid) else if mid < findNumber BinarySearch(int findNumber, int arr[], mid, end) else if mid = findNumber return mid BinarySearch(int findNumber, int arr[], start, end): ------------------- T(n) if start == end ---------------------------------------------------- O(1) if arr[start] == findNumber ------------------------------------ O(1) return start ----------------------------------------------- O(1) else return error that number does not exist in array ---------- O(1) mid = findMid(arr[], start, end) ----------------------------------- O(1) if mid > findNumber ------------------------------------------------ O(1) BinarySearch(int findNumber, int arr[], start, mid) ------------ T(n/2) else if mid < findNumber ------------------------------------------- O(1) BinarySearch(int findNumber, int arr[], mid, end) -------------- T(n/2) else if mid = findNumber ------------------------------------------- O(1) return mid ----------------------------------------------------- O(1) Time complexity = T(n) = O(1) + T(n/2) Back substitution T(n) = T(n/2) + 1 -------------- Equation #1 T(1) = 1 ----------------------- Base Condition T(n/2) = T(n/4) + 1 -------------- Equation #2 T(n/4) = T(n/8) + 1 -------------- Equation #3 T(n) = T(n/2) + 1 = (T(n/4) + 1) + 1 = T(n/4) + 2 = (T(n/8) + 1) + 2 = T(n/8) + 3 = T(n/2^k) + k ----------- n/2^k = 1 ... n = 2^k ... k = logn = T(1) + log(n) = 1 + logn = O(logn)","title":"Example 2"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/01-what-is-an-array/","text":"What is an array \u00b6 Array is a data structure consisting of a collection of elements, each identified by array index. An array is stored in such way that the position of each element can be computed from it's index cell by a mathematical formula. Array properties: - Array can store data of specified data type (integer, long, double). - It has contiguous memory location - Every cell of an Array has an unique index - Indexes start with 0 - Size of array needs to be specified mandatorily and can not be modified. Why do we need an array? \u00b6 Problem: We want to store 1 million similar data types in memory. We can create 1 million variables of a primitive data structure like integer, but maintaining it would be borderline impossible. Instead, we can declare an array with a length of 1 million. Then we need only to reference the cell number of the array and we can access that cell.","title":"What is an array"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/01-what-is-an-array/#what-is-an-array","text":"Array is a data structure consisting of a collection of elements, each identified by array index. An array is stored in such way that the position of each element can be computed from it's index cell by a mathematical formula. Array properties: - Array can store data of specified data type (integer, long, double). - It has contiguous memory location - Every cell of an Array has an unique index - Indexes start with 0 - Size of array needs to be specified mandatorily and can not be modified.","title":"What is an array"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/01-what-is-an-array/#why-do-we-need-an-array","text":"Problem: We want to store 1 million similar data types in memory. We can create 1 million variables of a primitive data structure like integer, but maintaining it would be borderline impossible. Instead, we can declare an array with a length of 1 million. Then we need only to reference the cell number of the array and we can access that cell.","title":"Why do we need an array?"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/02-array-types/","text":"Array types \u00b6 An array can be one dimensional or multi-dimensional. In one dimensional array, each element is represented by a single subscript. The elements are stored in consecutive memory locations. In multi-dimensional arrays, the simplest form is the two dimensional array, where each element is represented by two subscripts. Thus a two dimensional m x n array has m rows and n columns and contains m*n elements. Three dimensional array - each element is represented by three subscripts. Thus a three dimensional m x n x l array contains m*n*l elements.","title":"Array types"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/02-array-types/#array-types","text":"An array can be one dimensional or multi-dimensional. In one dimensional array, each element is represented by a single subscript. The elements are stored in consecutive memory locations. In multi-dimensional arrays, the simplest form is the two dimensional array, where each element is represented by two subscripts. Thus a two dimensional m x n array has m rows and n columns and contains m*n elements. Three dimensional array - each element is represented by three subscripts. Thus a three dimensional m x n x l array contains m*n*l elements.","title":"Array types"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/03-how-array-is-represented-in-memory/","text":"How array is represented in memory \u00b6 One dimensional array \u00b6 Arr[col] Arr[10] = {0,1,2,3,4,5,6,7,8,8,9}; When we tell that we need an array of 10 cells, computer find space for 10 cells in RAM and allocates memory for it. Starting location can vary depending on current memory allocation. Two dimensional array \u00b6 Arr[rows][cols] Array[5][2] = {{00,10}, {20,30}, {40, 50}, {60, 70}, {80, 90}} Same principle can be applied with two dimensional arrays, but it does not allocate memory like this: Instead it allocates it like this: Three dimensional arrays \u00b6 Same principle goes with 3D arrays. Array[width][row][col] int arr[2][3][3] = { { {11,12,13},{15,16,17},{19,20,21} }, { {23,24,26},{27,28,29},{31,32,33} } };","title":"How array is represented in memory"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/03-how-array-is-represented-in-memory/#how-array-is-represented-in-memory","text":"","title":"How array is represented in memory"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/03-how-array-is-represented-in-memory/#one-dimensional-array","text":"Arr[col] Arr[10] = {0,1,2,3,4,5,6,7,8,8,9}; When we tell that we need an array of 10 cells, computer find space for 10 cells in RAM and allocates memory for it. Starting location can vary depending on current memory allocation.","title":"One dimensional array"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/03-how-array-is-represented-in-memory/#two-dimensional-array","text":"Arr[rows][cols] Array[5][2] = {{00,10}, {20,30}, {40, 50}, {60, 70}, {80, 90}} Same principle can be applied with two dimensional arrays, but it does not allocate memory like this: Instead it allocates it like this:","title":"Two dimensional array"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/03-how-array-is-represented-in-memory/#three-dimensional-arrays","text":"Same principle goes with 3D arrays. Array[width][row][col] int arr[2][3][3] = { { {11,12,13},{15,16,17},{19,20,21} }, { {23,24,26},{27,28,29},{31,32,33} } };","title":"Three dimensional arrays"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/04-creating-an-array/","text":"Creating an array \u00b6 When we create an array, first thing we do it declare it. It creates a reference variable like arr that basically gives a name to the array. The second thing is array instantiation, which allocates memory for the array in RAM. Then it updates the reference variable with an address of the first cell of the array. This is used because we can use this address to reference each cell of the array. For example if the first cell address is X102 , then we can find the second by using X102 + 1 , the third X102 + 2 and so on. The last step is to initialize the array - which basically means to assign values to the array cells. In java we can declare 1D arrays by doing the following: dataType[] name int[] arr1 float[] arr2 char[] arr3 Then we can instantiate it: arrayRefVar = new datatype[size] arr = new int[5] Then we can initialize it: a[0] = 10; a[1] = 20; a[2] = 30; a[3] = 40; a[4] = 50; We can combine all of these steps into one line by using: int a[] = {10,20,30,40,50}; Time complexity when declaring, instantiating, initializing 1D arrays. int[] arr -------------------------------------- O(1) arr = new int[5] ------------------------------- O(1) arr[0] = 10; -------------------------- O(1) -| arr[1] = 20; -------------------------- O(1) -| arr[2] = 30; -------------------------- O(1) -|- O(n) arr[3] = 40; -------------------------- O(1) -| arr[4] = 50; -------------------------- O(1) -| int arr[] = {10,20,30,40,50} ------------------- O(1)","title":"Creating an array"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/04-creating-an-array/#creating-an-array","text":"When we create an array, first thing we do it declare it. It creates a reference variable like arr that basically gives a name to the array. The second thing is array instantiation, which allocates memory for the array in RAM. Then it updates the reference variable with an address of the first cell of the array. This is used because we can use this address to reference each cell of the array. For example if the first cell address is X102 , then we can find the second by using X102 + 1 , the third X102 + 2 and so on. The last step is to initialize the array - which basically means to assign values to the array cells. In java we can declare 1D arrays by doing the following: dataType[] name int[] arr1 float[] arr2 char[] arr3 Then we can instantiate it: arrayRefVar = new datatype[size] arr = new int[5] Then we can initialize it: a[0] = 10; a[1] = 20; a[2] = 30; a[3] = 40; a[4] = 50; We can combine all of these steps into one line by using: int a[] = {10,20,30,40,50}; Time complexity when declaring, instantiating, initializing 1D arrays. int[] arr -------------------------------------- O(1) arr = new int[5] ------------------------------- O(1) arr[0] = 10; -------------------------- O(1) -| arr[1] = 20; -------------------------- O(1) -| arr[2] = 30; -------------------------- O(1) -|- O(n) arr[3] = 40; -------------------------- O(1) -| arr[4] = 50; -------------------------- O(1) -| int arr[] = {10,20,30,40,50} ------------------- O(1)","title":"Creating an array"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/05-insert-traverse-in-1d-array/","text":"Insert and traverse in 1d array \u00b6 Inserting a value in 1D array \u00b6 To insert a value in a cell, all we have to do is to point at the cell and tell it's value. arr[4] = 50; arr[5] = 60; We can write a function for it: insert (arr, valueToBeInserted, location): if(arr[location] is ocupied) return error // location is already occupied ... additional validation else arr[location] = valueToBeInserted Time complexity insert (arr, valueToBeInserted, location): if(arr[location] is ocupied) ------------------------ O(1) ----| return error // location is already occupied ---- O(1) ----| ... additional validation -------------------------------------|- O(1) else ------------------------------------------------ O(1) ----| arr[location] = valueToBeInserted --------------- O(1) ----| Total time complexity - O(1) Space complexity - O(1) Traversing a given 1D array \u00b6 By traversing it is meant to visit each and every cell of the array for printing or other purpose. We can write a function that does that by doing following: TraverseArray(arr): loop: i = 0 to arr.length print arr[i] Time complexity: TraverseArray(arr): loop: i = 0 to arr.length ------ O(n) print arr[i] --------------- O(1) Total time complexity - O(n) Space complexity - O(1)","title":"Insert and traverse in 1d array"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/05-insert-traverse-in-1d-array/#insert-and-traverse-in-1d-array","text":"","title":"Insert and traverse in 1d array"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/05-insert-traverse-in-1d-array/#inserting-a-value-in-1d-array","text":"To insert a value in a cell, all we have to do is to point at the cell and tell it's value. arr[4] = 50; arr[5] = 60; We can write a function for it: insert (arr, valueToBeInserted, location): if(arr[location] is ocupied) return error // location is already occupied ... additional validation else arr[location] = valueToBeInserted Time complexity insert (arr, valueToBeInserted, location): if(arr[location] is ocupied) ------------------------ O(1) ----| return error // location is already occupied ---- O(1) ----| ... additional validation -------------------------------------|- O(1) else ------------------------------------------------ O(1) ----| arr[location] = valueToBeInserted --------------- O(1) ----| Total time complexity - O(1) Space complexity - O(1)","title":"Inserting a value in 1D array"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/05-insert-traverse-in-1d-array/#traversing-a-given-1d-array","text":"By traversing it is meant to visit each and every cell of the array for printing or other purpose. We can write a function that does that by doing following: TraverseArray(arr): loop: i = 0 to arr.length print arr[i] Time complexity: TraverseArray(arr): loop: i = 0 to arr.length ------ O(n) print arr[i] --------------- O(1) Total time complexity - O(n) Space complexity - O(1)","title":"Traversing a given 1D array"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/06-accessing-cell-searching-and-deleting-given-value/","text":"Accessing cell, searching and deleting given value \u00b6 Accessing given cell of 1D array \u00b6 AccessingCell(arr, cellNumber): if (cellNumber > sizeof(arr)) return exception // cell number cannot be bigger than size of array else return arr[cellNumber] Total time complexity = O(1) Total space complexity = O(1) Searching a given value in 1D array \u00b6 We can search the given array for a value and return it's index if it's found. SearchInArray(arr, valueToSearch): loop: i = 0 to arr.length if( arr[i] equals valueToSearch ) return i return error // value not found Total time complexity - O(n) Space complexity - O(1) Deleting a given value from 1D array \u00b6 Practically speaking, we can never delete a value from an array, but you can assign a value that can be assumed as a blank value like 0 or other value that will not be used. DeletingValueFromArray(arr, location): if(arr[location] is occupied) arr[location] = Integer.MinValue else return // location is already blank Total time complexity = O(1) Total space complexity = O(1)","title":"Accessing cell, searching and deleting given value"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/06-accessing-cell-searching-and-deleting-given-value/#accessing-cell-searching-and-deleting-given-value","text":"","title":"Accessing cell, searching and deleting given value"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/06-accessing-cell-searching-and-deleting-given-value/#accessing-given-cell-of-1d-array","text":"AccessingCell(arr, cellNumber): if (cellNumber > sizeof(arr)) return exception // cell number cannot be bigger than size of array else return arr[cellNumber] Total time complexity = O(1) Total space complexity = O(1)","title":"Accessing given cell of 1D array"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/06-accessing-cell-searching-and-deleting-given-value/#searching-a-given-value-in-1d-array","text":"We can search the given array for a value and return it's index if it's found. SearchInArray(arr, valueToSearch): loop: i = 0 to arr.length if( arr[i] equals valueToSearch ) return i return error // value not found Total time complexity - O(n) Space complexity - O(1)","title":"Searching a given value in 1D array"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/06-accessing-cell-searching-and-deleting-given-value/#deleting-a-given-value-from-1d-array","text":"Practically speaking, we can never delete a value from an array, but you can assign a value that can be assumed as a blank value like 0 or other value that will not be used. DeletingValueFromArray(arr, location): if(arr[location] is occupied) arr[location] = Integer.MinValue else return // location is already blank Total time complexity = O(1) Total space complexity = O(1)","title":"Deleting a given value from 1D array"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/07-coding-1d-arrays/","text":"Coding 1D arrays \u00b6 We can create 2 files - SingleDimensionArray.java and SingleDimensionArrayMain.java where the first one is a class that implements all the logic for arrays and the second is used to run it. SingleDimensionArray: package array; public class SingleDimensionArray { int arr[] = null; // Constructor public SingleDimensionArray(int sizeOfArray) { arr = new int[sizeOfArray]; for (int i = 0; i < arr.length; i++) { arr[i] = Integer.MIN_VALUE; } } // Print the array public void traverseArray() { try { for( int i = 0; i < arr.length; i++) { System.out.print(arr[i] + \" \"); } } catch ( Exception e ) { System.out.println(\"Array no longer exists!\"); } } // Insert value in the Array public void insert(int location, int valueToBeInserted) { try { if( arr[location] == Integer.MIN_VALUE ) { arr[location] = valueToBeInserted; System.out.println(\"Successfully inserted \" + valueToBeInserted + \" at location: \" + location); } else { System.out.println(\"This cell is already occupied by another value.\"); } } catch( ArrayIndexOutOfBoundsException e ) { System.out.println(\"Invalid index to access array!\"); } } // Access a particular element of an array public void accessingCell(int cellNumber) { try { System.out.println(arr[cellNumber]); } catch( ArrayIndexOutOfBoundsException e ) { System.out.println(\"Invalid index to access array!\"); } } // Search for an element in the given Array public void searchInAnArray(int valueToSearch) { for (int i = 0; i < arr.length; i++) { if(arr[i] == valueToSearch) { System.out.println(\"Value found!\"); System.out.println(\"Index of \" + valueToSearch + \" is \" + i); return; } } System.out.println(valueToSearch + \" is not found!\"); } // Delete value from given Array public void deleteValueFromArray(int deleteValueFromThisCell) { try { arr[deleteValueFromThisCell] = Integer.MIN_VALUE; } catch (ArrayIndexOutOfBoundsException e) { System.out.println(\"Invalid index to access array!\"); } } } SingleDimensionArrayMain package array; import array.SingleDimensionArray; public class SingleDimensionArrayMain { public static void main(String[] args) { System.out.println(\"Creating a blank array of size 10...\"); SingleDimensionArray sda = new SingleDimensionArray(10); System.out.println(\"Printing the Array...\"); sda.traverseArray(); System.out.println(); System.out.println(); System.out.println(\"Inserting few values in the array...\"); sda.insert(0,0); sda.insert(1,10); sda.insert(2,20); sda.insert(3,30); sda.insert(4,40); sda.insert(5,50); sda.insert(6,60); sda.insert(7,70); sda.insert(8,80); sda.insert(1,100); sda.insert(12,120); System.out.println(); System.out.println(); System.out.println(\"Accessing cell number #1\"); sda.accessingCell(1); System.out.println(); System.out.println(); System.out.println(\"Searching for 400 in the array...\"); sda.searchInAnArray(400); System.out.println(\"Searching for 40 in the array...\"); sda.searchInAnArray(40); System.out.println(); System.out.println(); System.out.println(\"Deleting cell number #3\"); System.out.println(\"Before deleting:\"); sda.traverseArray(); sda.deleteValueFromArray(3); System.out.println(\"After deleting:\"); sda.traverseArray(); System.out.println(); System.out.println(); } } Now we can compile and run it: \u279c 01-array git:(master) \u2717 ls array SingleDimensionArray.java SingleDimensionArrayMain.java \u279c 01-array git:(master) \u2717 javac array/*.java \u279c 01-array git:(master) \u2717 java array.SingleDimensionArrayMain Creating a blank array of size 10... Printing the Array... -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 Inserting few values in the array... Successfully inserted 0 at location: 0 Successfully inserted 10 at location: 1 Successfully inserted 20 at location: 2 Successfully inserted 30 at location: 3 Successfully inserted 40 at location: 4 Successfully inserted 50 at location: 5 Successfully inserted 60 at location: 6 Successfully inserted 70 at location: 7 Successfully inserted 80 at location: 8 This cell is already occupied by another value. Invalid index to access array! Accessing cell number #1 10 Searching for 400 in the array... 400 is not found! Searching for 40 in the array... Value found! Index of 40 is 4 Deleting cell number #3 Before deleting: 0 10 20 30 40 50 60 70 80 -2147483648 After deleting: 0 10 20 -2147483648 40 50 60 70 80 -2147483648","title":"Coding 1D arrays"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/07-coding-1d-arrays/#coding-1d-arrays","text":"We can create 2 files - SingleDimensionArray.java and SingleDimensionArrayMain.java where the first one is a class that implements all the logic for arrays and the second is used to run it. SingleDimensionArray: package array; public class SingleDimensionArray { int arr[] = null; // Constructor public SingleDimensionArray(int sizeOfArray) { arr = new int[sizeOfArray]; for (int i = 0; i < arr.length; i++) { arr[i] = Integer.MIN_VALUE; } } // Print the array public void traverseArray() { try { for( int i = 0; i < arr.length; i++) { System.out.print(arr[i] + \" \"); } } catch ( Exception e ) { System.out.println(\"Array no longer exists!\"); } } // Insert value in the Array public void insert(int location, int valueToBeInserted) { try { if( arr[location] == Integer.MIN_VALUE ) { arr[location] = valueToBeInserted; System.out.println(\"Successfully inserted \" + valueToBeInserted + \" at location: \" + location); } else { System.out.println(\"This cell is already occupied by another value.\"); } } catch( ArrayIndexOutOfBoundsException e ) { System.out.println(\"Invalid index to access array!\"); } } // Access a particular element of an array public void accessingCell(int cellNumber) { try { System.out.println(arr[cellNumber]); } catch( ArrayIndexOutOfBoundsException e ) { System.out.println(\"Invalid index to access array!\"); } } // Search for an element in the given Array public void searchInAnArray(int valueToSearch) { for (int i = 0; i < arr.length; i++) { if(arr[i] == valueToSearch) { System.out.println(\"Value found!\"); System.out.println(\"Index of \" + valueToSearch + \" is \" + i); return; } } System.out.println(valueToSearch + \" is not found!\"); } // Delete value from given Array public void deleteValueFromArray(int deleteValueFromThisCell) { try { arr[deleteValueFromThisCell] = Integer.MIN_VALUE; } catch (ArrayIndexOutOfBoundsException e) { System.out.println(\"Invalid index to access array!\"); } } } SingleDimensionArrayMain package array; import array.SingleDimensionArray; public class SingleDimensionArrayMain { public static void main(String[] args) { System.out.println(\"Creating a blank array of size 10...\"); SingleDimensionArray sda = new SingleDimensionArray(10); System.out.println(\"Printing the Array...\"); sda.traverseArray(); System.out.println(); System.out.println(); System.out.println(\"Inserting few values in the array...\"); sda.insert(0,0); sda.insert(1,10); sda.insert(2,20); sda.insert(3,30); sda.insert(4,40); sda.insert(5,50); sda.insert(6,60); sda.insert(7,70); sda.insert(8,80); sda.insert(1,100); sda.insert(12,120); System.out.println(); System.out.println(); System.out.println(\"Accessing cell number #1\"); sda.accessingCell(1); System.out.println(); System.out.println(); System.out.println(\"Searching for 400 in the array...\"); sda.searchInAnArray(400); System.out.println(\"Searching for 40 in the array...\"); sda.searchInAnArray(40); System.out.println(); System.out.println(); System.out.println(\"Deleting cell number #3\"); System.out.println(\"Before deleting:\"); sda.traverseArray(); sda.deleteValueFromArray(3); System.out.println(\"After deleting:\"); sda.traverseArray(); System.out.println(); System.out.println(); } } Now we can compile and run it: \u279c 01-array git:(master) \u2717 ls array SingleDimensionArray.java SingleDimensionArrayMain.java \u279c 01-array git:(master) \u2717 javac array/*.java \u279c 01-array git:(master) \u2717 java array.SingleDimensionArrayMain Creating a blank array of size 10... Printing the Array... -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 Inserting few values in the array... Successfully inserted 0 at location: 0 Successfully inserted 10 at location: 1 Successfully inserted 20 at location: 2 Successfully inserted 30 at location: 3 Successfully inserted 40 at location: 4 Successfully inserted 50 at location: 5 Successfully inserted 60 at location: 6 Successfully inserted 70 at location: 7 Successfully inserted 80 at location: 8 This cell is already occupied by another value. Invalid index to access array! Accessing cell number #1 10 Searching for 400 in the array... 400 is not found! Searching for 40 in the array... Value found! Index of 40 is 4 Deleting cell number #3 Before deleting: 0 10 20 30 40 50 60 70 80 -2147483648 After deleting: 0 10 20 -2147483648 40 50 60 70 80 -2147483648","title":"Coding 1D arrays"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/08-time-complexity-of-1d-array/","text":"Time complexity of 1D array \u00b6 Time complexity Space complexity Creating an empty array O(1) O(n) Inserting a value in array O(1) O(1) Traversing array O(n) O(1) Accessing given cell O(1) O(1) Searching a given value O(n) O(1) Deleting a given value O(1) O(1)","title":"Time complexity of 1D array"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/08-time-complexity-of-1d-array/#time-complexity-of-1d-array","text":"Time complexity Space complexity Creating an empty array O(1) O(n) Inserting a value in array O(1) O(1) Traversing array O(n) O(1) Accessing given cell O(1) O(1) Searching a given value O(n) O(1) Deleting a given value O(1) O(1)","title":"Time complexity of 1D array"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/09-create-2d-array/","text":"Create 2D array \u00b6 Declare \u00b6 dataType[][] arr int[][] arr Instantiate \u00b6 ArrayRefVar = new datatype[row][col] arr = new int[2][3] Initialize \u00b6 a[0][0] = 10; a[0][1] = 20; a[0][2] = 30; a[1][0] = 40; a[1][1] = 50; a[1][2] = 60; Declaration, instantiation and initialization \u00b6 int[][] arr = {{10,20,30},{40,50,60}} Time complexity \u00b6 int[][] arr ------------------------------ O(1) arr = new int[2][3] ---------------------- O(1) a[0][0] = 10; --------------------------| a[0][1] = 20; --------------------------| a[0][2] = 30; --------------------------|- O(mn) a[1][0] = 40; --------------------------| a[1][1] = 50; --------------------------| a[1][2] = 60; --------------------------| int[][] arr = {{10,20,30},{40,50,60}} ---- O(1)","title":"Create 2D array"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/09-create-2d-array/#create-2d-array","text":"","title":"Create 2D array"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/09-create-2d-array/#declare","text":"dataType[][] arr int[][] arr","title":"Declare"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/09-create-2d-array/#instantiate","text":"ArrayRefVar = new datatype[row][col] arr = new int[2][3]","title":"Instantiate"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/09-create-2d-array/#initialize","text":"a[0][0] = 10; a[0][1] = 20; a[0][2] = 30; a[1][0] = 40; a[1][1] = 50; a[1][2] = 60;","title":"Initialize"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/09-create-2d-array/#declaration-instantiation-and-initialization","text":"int[][] arr = {{10,20,30},{40,50,60}}","title":"Declaration, instantiation and initialization"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/09-create-2d-array/#time-complexity","text":"int[][] arr ------------------------------ O(1) arr = new int[2][3] ---------------------- O(1) a[0][0] = 10; --------------------------| a[0][1] = 20; --------------------------| a[0][2] = 30; --------------------------|- O(mn) a[1][0] = 40; --------------------------| a[1][1] = 50; --------------------------| a[1][2] = 60; --------------------------| int[][] arr = {{10,20,30},{40,50,60}} ---- O(1)","title":"Time complexity"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/10-2d-array-operations/","text":"2D array operations \u00b6 Inserting a value in 2D array \u00b6 Insert(arr, valueToBeInserted, rowNumber, colNumber): if(arr[rowNumber][colNumber] is occupied) return error // already ocupied else arr[rowNumber][colNumber] = valueToBeInserted Time complexity \u00b6 Insert(arr, valueToBeInserted, rowNumber, colNumber): if(arr[rowNumber][colNumber] is occupied) ---------- O(1) return error // already ocupied ---------------- O(1) else ----------------------------------------------- O(1) arr[rowNumber][colNumber] = valueToBeInserted -- O(1) Time complexity O(1) Space complexity O(1) Traversing \u00b6 TraverseArray(arr): loop: row = 0 to rows loop: col = 0 to rows print arr[row][col] Time complexity \u00b6 TraverseArray(arr): loop: row = 0 to rows ----------- O(m) loop: col = 0 to rows ------- O(n) print arr[row][col] ----- O(1) Time complexity O(mn) Space complexity O(1) Searching a given value in 2D array \u00b6 SearchInArray(arr, valueToSearch): loop: row = 0 to rows loop: col = 0 to cols if( arr[row][col]] equals valueToSearch ) print(row,col); return; print (value not found) Time complexity \u00b6 SearchInArray(arr, valueToSearch): loop: row = 0 to rows --------------------------------- O(m) loop: col = 0 to cols ----------------------------- O(n) if( arr[row][col]] equals valueToSearch ) ----- O(1) print(row,col); return; ------------------- O(1) print (value not found) ------------------------------- O(1) Time complexity O(mn) Space complexity O(1) Deleting a given value \u00b6 DeletingValue(arr, rowNumber, colNumber): arr[rowNumber][colNumber] = Integer.MIN_VALUE; Time complexity \u00b6 DeletingValue(arr, rowNumber, colNumber): arr[rowNumber][colNumber] = Integer.MIN_VALUE; ---------------- O(1) Time complexity O(1) Space complexity O(1)","title":"2D array operations"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/10-2d-array-operations/#2d-array-operations","text":"","title":"2D array operations"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/10-2d-array-operations/#inserting-a-value-in-2d-array","text":"Insert(arr, valueToBeInserted, rowNumber, colNumber): if(arr[rowNumber][colNumber] is occupied) return error // already ocupied else arr[rowNumber][colNumber] = valueToBeInserted","title":"Inserting a value in 2D array"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/10-2d-array-operations/#time-complexity","text":"Insert(arr, valueToBeInserted, rowNumber, colNumber): if(arr[rowNumber][colNumber] is occupied) ---------- O(1) return error // already ocupied ---------------- O(1) else ----------------------------------------------- O(1) arr[rowNumber][colNumber] = valueToBeInserted -- O(1) Time complexity O(1) Space complexity O(1)","title":"Time complexity"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/10-2d-array-operations/#traversing","text":"TraverseArray(arr): loop: row = 0 to rows loop: col = 0 to rows print arr[row][col]","title":"Traversing"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/10-2d-array-operations/#time-complexity_1","text":"TraverseArray(arr): loop: row = 0 to rows ----------- O(m) loop: col = 0 to rows ------- O(n) print arr[row][col] ----- O(1) Time complexity O(mn) Space complexity O(1)","title":"Time complexity"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/10-2d-array-operations/#searching-a-given-value-in-2d-array","text":"SearchInArray(arr, valueToSearch): loop: row = 0 to rows loop: col = 0 to cols if( arr[row][col]] equals valueToSearch ) print(row,col); return; print (value not found)","title":"Searching a given value in 2D array"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/10-2d-array-operations/#time-complexity_2","text":"SearchInArray(arr, valueToSearch): loop: row = 0 to rows --------------------------------- O(m) loop: col = 0 to cols ----------------------------- O(n) if( arr[row][col]] equals valueToSearch ) ----- O(1) print(row,col); return; ------------------- O(1) print (value not found) ------------------------------- O(1) Time complexity O(mn) Space complexity O(1)","title":"Time complexity"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/10-2d-array-operations/#deleting-a-given-value","text":"DeletingValue(arr, rowNumber, colNumber): arr[rowNumber][colNumber] = Integer.MIN_VALUE;","title":"Deleting a given value"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/10-2d-array-operations/#time-complexity_3","text":"DeletingValue(arr, rowNumber, colNumber): arr[rowNumber][colNumber] = Integer.MIN_VALUE; ---------------- O(1) Time complexity O(1) Space complexity O(1)","title":"Time complexity"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/11-time-complexity-of-2d-array/","text":"Time complexity of 1D array \u00b6 Time complexity Space complexity Creating an empty array O(1) O(mn) Inserting a value in array O(1) O(1) Traversing array O(mn) O(1) Accessing given cell O(1) O(1) Searching a given value O(mn) O(1) Deleting a given value O(1) O(1)","title":"Time complexity of 1D array"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/11-time-complexity-of-2d-array/#time-complexity-of-1d-array","text":"Time complexity Space complexity Creating an empty array O(1) O(mn) Inserting a value in array O(1) O(1) Traversing array O(mn) O(1) Accessing given cell O(1) O(1) Searching a given value O(mn) O(1) Deleting a given value O(1) O(1)","title":"Time complexity of 1D array"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/12-when-to-use-avoid-array/","text":"When to use / avoid arrays \u00b6 When to use: - When there is a need to store data of similar type - When random access is regular affair When to avoid: - When data is not homogenous - When the size of the data is not known","title":"When to use / avoid arrays"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/12-when-to-use-avoid-array/#when-to-use-avoid-arrays","text":"When to use: - When there is a need to store data of similar type - When random access is regular affair When to avoid: - When data is not homogenous - When the size of the data is not known","title":"When to use / avoid arrays"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/13-coding-2d-arrays/","text":"Coding 2D arrays \u00b6 We can create 2 files - TwoDimensionalArray.java and TwoDimensionalArrayMain.java where the first one is a class that implements all the logic for arrays and the second is used to run it. TwoDimensionalArray: package array; public class TwoDimensionalArray { int arr[][] = null; // Constructor public TwoDimensionalArray(int numberOfRows, int numberOfColumns) { this.arr = new int[numberOfRows][numberOfColumns]; for( int row = 0; row < arr.length; row++) { for( int col = 0; col < arr[row].length; col++) { arr[row][col] = Integer.MIN_VALUE; } } } // Traverse array public void traverseArray() { try { System.out.println(\"Printing the array now...\"); for(int row = 0; row < arr.length; row++) { for(int col = 0; col < arr[row].length; col++) { System.out.print(arr[row][col] + \" \"); } System.out.println(); } System.out.println(\"\\n\"); } catch ( Exception e ) { System.out.println(\"Array does not exist\"); } } // Insert value public void insertValueInArray(int row, int col, int value) { try { if(arr[row][col] == Integer.MIN_VALUE) { arr[row][col] = value; System.out.println(\"Successfully inserted \" + value + \" in the array\"); } else { System.out.println(\"This cell is already occupied by another value\"); } } catch (ArrayIndexOutOfBoundsException e) { System.out.println(\"Invalid index to access array!\"); } } // access cell public void accessCell(int row, int col) { System.out.println(\"\\nAccessing row \" + row +\", col \" + col); try { System.out.println(\"Cell value is: \" + arr[row][col]); } catch (ArrayIndexOutOfBoundsException e) { System.out.println(\"Invalid index to access array!\"); } } // search value public void searchValue(int value) { System.out.println(\"\\n Searching for value \" + value); for(int row = 0; row < arr.length; row++) { for(int col = 0; col < arr[row].length; col++) { if(arr[row][col] == value) { System.out.println(\"Value has been found at location [\" + row + \"][\" + col + \"]\" ); return; } } } System.out.println(\"The value was not found\"); } // delete value public void deleteValue(int row, int col) { System.out.println(\"Deleting value from row \" + row + \", col \" + col); try { System.out.println(\"Successfully deleted: \" + arr[row][col]); arr[row][col] = Integer.MIN_VALUE; } catch (ArrayIndexOutOfBoundsException e) { System.out.println(\"Invalid index to access array!\"); } } // delete entire array public void deleteArray() { arr = null; System.out.println(\"Array has been deleted\"); } } TwoDimensionalArrayMain package array; import array.TwoDimensionalArray; public class TwoDimensionalArrayMain { public static void main(String[] args) { System.out.println(\"Creating a blank array of size 5x5\"); TwoDimensionalArray sda = new TwoDimensionalArray(5,5); sda.traverseArray(); sda.insertValueInArray(0,2, 10000001); sda.traverseArray(); sda.insertValueInArray(0,2, 10000001); sda.traverseArray(); sda.accessCell(0,2); sda.accessCell(6,2); sda.accessCell(2,2); sda.searchValue(10); sda.searchValue(Integer.MIN_VALUE); sda.searchValue(10000001); sda.deleteValue(0,2); sda.traverseArray(); sda.deleteArray(); sda.traverseArray(); } } Now we can compile and run it: \u279c 02-2d-array git:(master) \u2717 javac array/*.java && java array.TwoDimensionalArrayMain Creating a blank array of size 5x5 Printing the array now... -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 Successfully inserted 10000001 in the array Printing the array now... -2147483648 -2147483648 10000001 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 This cell is already occupied by another value Printing the array now... -2147483648 -2147483648 10000001 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 Accessing row 0, col 2 Cell value is: 10000001 Accessing row 6, col 2 Invalid index to access array! Accessing row 2, col 2 Cell value is: -2147483648 Searching for value 10 The value was not found Searching for value -2147483648 Value has been found at location [0][0] Searching for value 10000001 Value has been found at location [0][2] Deleting value from row 0, col 2 Successfully deleted: 10000001 Printing the array now... -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 Array has been deleted Printing the array now... Array does not exist","title":"Coding 2D arrays"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/13-coding-2d-arrays/#coding-2d-arrays","text":"We can create 2 files - TwoDimensionalArray.java and TwoDimensionalArrayMain.java where the first one is a class that implements all the logic for arrays and the second is used to run it. TwoDimensionalArray: package array; public class TwoDimensionalArray { int arr[][] = null; // Constructor public TwoDimensionalArray(int numberOfRows, int numberOfColumns) { this.arr = new int[numberOfRows][numberOfColumns]; for( int row = 0; row < arr.length; row++) { for( int col = 0; col < arr[row].length; col++) { arr[row][col] = Integer.MIN_VALUE; } } } // Traverse array public void traverseArray() { try { System.out.println(\"Printing the array now...\"); for(int row = 0; row < arr.length; row++) { for(int col = 0; col < arr[row].length; col++) { System.out.print(arr[row][col] + \" \"); } System.out.println(); } System.out.println(\"\\n\"); } catch ( Exception e ) { System.out.println(\"Array does not exist\"); } } // Insert value public void insertValueInArray(int row, int col, int value) { try { if(arr[row][col] == Integer.MIN_VALUE) { arr[row][col] = value; System.out.println(\"Successfully inserted \" + value + \" in the array\"); } else { System.out.println(\"This cell is already occupied by another value\"); } } catch (ArrayIndexOutOfBoundsException e) { System.out.println(\"Invalid index to access array!\"); } } // access cell public void accessCell(int row, int col) { System.out.println(\"\\nAccessing row \" + row +\", col \" + col); try { System.out.println(\"Cell value is: \" + arr[row][col]); } catch (ArrayIndexOutOfBoundsException e) { System.out.println(\"Invalid index to access array!\"); } } // search value public void searchValue(int value) { System.out.println(\"\\n Searching for value \" + value); for(int row = 0; row < arr.length; row++) { for(int col = 0; col < arr[row].length; col++) { if(arr[row][col] == value) { System.out.println(\"Value has been found at location [\" + row + \"][\" + col + \"]\" ); return; } } } System.out.println(\"The value was not found\"); } // delete value public void deleteValue(int row, int col) { System.out.println(\"Deleting value from row \" + row + \", col \" + col); try { System.out.println(\"Successfully deleted: \" + arr[row][col]); arr[row][col] = Integer.MIN_VALUE; } catch (ArrayIndexOutOfBoundsException e) { System.out.println(\"Invalid index to access array!\"); } } // delete entire array public void deleteArray() { arr = null; System.out.println(\"Array has been deleted\"); } } TwoDimensionalArrayMain package array; import array.TwoDimensionalArray; public class TwoDimensionalArrayMain { public static void main(String[] args) { System.out.println(\"Creating a blank array of size 5x5\"); TwoDimensionalArray sda = new TwoDimensionalArray(5,5); sda.traverseArray(); sda.insertValueInArray(0,2, 10000001); sda.traverseArray(); sda.insertValueInArray(0,2, 10000001); sda.traverseArray(); sda.accessCell(0,2); sda.accessCell(6,2); sda.accessCell(2,2); sda.searchValue(10); sda.searchValue(Integer.MIN_VALUE); sda.searchValue(10000001); sda.deleteValue(0,2); sda.traverseArray(); sda.deleteArray(); sda.traverseArray(); } } Now we can compile and run it: \u279c 02-2d-array git:(master) \u2717 javac array/*.java && java array.TwoDimensionalArrayMain Creating a blank array of size 5x5 Printing the array now... -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 Successfully inserted 10000001 in the array Printing the array now... -2147483648 -2147483648 10000001 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 This cell is already occupied by another value Printing the array now... -2147483648 -2147483648 10000001 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 Accessing row 0, col 2 Cell value is: 10000001 Accessing row 6, col 2 Invalid index to access array! Accessing row 2, col 2 Cell value is: -2147483648 Searching for value 10 The value was not found Searching for value -2147483648 Value has been found at location [0][0] Searching for value 10000001 Value has been found at location [0][2] Deleting value from row 0, col 2 Successfully deleted: 10000001 Printing the array now... -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 Array has been deleted Printing the array now... Array does not exist","title":"Coding 2D arrays"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/14-practical-uses-of-arrays/","text":"Practical uses of arrays \u00b6 One of the places we use arrays very heavily is dynamic programming , it will be talked about later, but basically it is a technique where we compromise space complexity to achieve better time complexity. Another place where arrays are heavily used - hashtables, which basically is an array of hash keys.","title":"Practical uses of arrays"},{"location":"Data%20Structures%20And%20Algorithms/04-arrays/14-practical-uses-of-arrays/#practical-uses-of-arrays","text":"One of the places we use arrays very heavily is dynamic programming , it will be talked about later, but basically it is a technique where we compromise space complexity to achieve better time complexity. Another place where arrays are heavily used - hashtables, which basically is an array of hash keys.","title":"Practical uses of arrays"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/01-what-is-a-linked-list/","text":"What is a linked list \u00b6 A linked list is a linear data structure where each element is a separate object. Each element (node) of a list compromises of two items - the data and a reference to the next node. The most powerful feature of Linked List is that it is of variable size. Example:","title":"What is a linked list"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/01-what-is-a-linked-list/#what-is-a-linked-list","text":"A linked list is a linear data structure where each element is a separate object. Each element (node) of a list compromises of two items - the data and a reference to the next node. The most powerful feature of Linked List is that it is of variable size. Example:","title":"What is a linked list"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/02-linked-list-vs-array/","text":"Linked list vs array \u00b6 The main difference between a linked list and array is that each element of the linked list is a separate object where the elements in array are not seperate objects - the array is the object. For example, when we need to delete an element from the linked list, that can be done, but we can't really delete an element from an array. Also, the linked list can be variable size. Random access. If we need the 3rd element of the array, we just access it. But in order to access it in a linked list, we have to traverse.","title":"Linked list vs array"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/02-linked-list-vs-array/#linked-list-vs-array","text":"The main difference between a linked list and array is that each element of the linked list is a separate object where the elements in array are not seperate objects - the array is the object. For example, when we need to delete an element from the linked list, that can be done, but we can't really delete an element from an array. Also, the linked list can be variable size. Random access. If we need the 3rd element of the array, we just access it. But in order to access it in a linked list, we have to traverse.","title":"Linked list vs array"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/03-components-of-a-linked-list/","text":"Components of a linked list \u00b6 Node - contains data & reference to the next node Head - Reference to the first node in the list Tail - Reference to the last node in the list","title":"Components of a linked list"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/03-components-of-a-linked-list/#components-of-a-linked-list","text":"Node - contains data & reference to the next node Head - Reference to the first node in the list Tail - Reference to the last node in the list","title":"Components of a linked list"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/04-types-of-linked-lists/","text":"Types of linked lists \u00b6 Single linked list each node stores data and reference to the next node Circular single linked list Same as the Single linked list, but the last element also holds a reference to the first element. Double linked list Each node holds 2 references - previous and next node. Circular double linked list Same as double linked list, but last element points to front and front points to end.","title":"Types of linked lists"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/04-types-of-linked-lists/#types-of-linked-lists","text":"Single linked list each node stores data and reference to the next node Circular single linked list Same as the Single linked list, but the last element also holds a reference to the first element. Double linked list Each node holds 2 references - previous and next node. Circular double linked list Same as double linked list, but last element points to front and front points to end.","title":"Types of linked lists"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/05-how-is-linked-list-stored-in-memory/","text":"How is Linked List stored in memory \u00b6 Here is a representation on how a linked list of 7 elements can be stored in RAM. We can see that there is a difference on how a linked list is stored in memory vs how an array is stored in memory. Each element can be located at a random memory address, this is the reason why we cannot access n-th element of a linked list without traversing like we can in arrays.","title":"How is Linked List stored in memory"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/05-how-is-linked-list-stored-in-memory/#how-is-linked-list-stored-in-memory","text":"Here is a representation on how a linked list of 7 elements can be stored in RAM. We can see that there is a difference on how a linked list is stored in memory vs how an array is stored in memory. Each element can be located at a random memory address, this is the reason why we cannot access n-th element of a linked list without traversing like we can in arrays.","title":"How is Linked List stored in memory"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/06-single-linked-list/","text":"Single linked list \u00b6 Creating \u00b6 When we want to create a single linked list, it consists of a few steps: 1. We create a blank head reference 2. We create a blank tail reference 3. We initialize them both with null 4. We create a blank node 5. We initialize it with null reference 6. We initialize the data part of it 7. We update the head reference to the node 8. We update the tail reference to the node createSingleLinkedList(node): create a head, tail pointer and initialize with NULL create a blank node node.value = nodeValue head = node tail = node createSingleLinkedList(node): create a head, tail pointer and initialize with NULL ---- O(1) create a blank node ------------------------------------- O(1) node.value = nodeValue ---------------------------------- O(1) head = node --------------------------------------------- O(1) tail = node --------------------------------------------- O(1) Time complexity O(1) Space complexity O(1) Inserting \u00b6 We can assume that we have a Single Linked List with 6 elements in it. When we want to insert a new node, we can have 3 scenarios: - Inserting at the start of linked list We have to create a new node, hold a reference to the first element and update the head to reference it. Inserting at the end of linked list We have to create a new node, update the previous element as well as the tail to point to it. Inserting at a specified location in linked list We have to create a new node, update the previous element to point to it, hold the reference to the next element. insertInLinkedList(head, nodeValue, location) create a blank node node.value = nodeValue if( !existsLinkedList(head) ) return error // LinkedList does not exist else if (location equals 0) // insert at first position node.next = head head = node else if (location equals last) // insert at last position node.next = null last.next = node last = node // to keep track of the last node else loop: tmpNode = 0 to location - 1 node.next = tmpNode.next tmpNode.next = node insertInLinkedList(head, nodeValue, location) create a blank node ------------------------------------------- O(1) node.value = nodeValue ---------------------------------------- O(1) if( !existsLinkedList(head) ) --------------------------------- O(1) return error // LinkedList does not exist ----------------- O(1) else if (location equals 0) // insert at first position ------- O(1) node.next = head ------------------------------------------ O(1) head = node ----------------------------------------------- O(1) else if (location equals last) // insert at last position ----- O(1) node.next = null ------------------------------------------ O(1) last.next = node ------------------------------------------ O(1) last = node // to keep track of the last node ------------- O(1) else --------------------------------------------------------- O(1) loop: tmpNode = 0 to location - 1 ------------------------- O(n) node.next = tmpNode.next ---------------------------------- O(1) tmpNode.next = node --------------------------------------- O(1) Time complexity O(n) Space complexity O(1) Traversal \u00b6 TraverseLinkedList(head): if head == null return; loop: head to tail print currentNode.value TraverseLinkedList(head): if head == null return; -------------- O(1) loop: head to tail ------------------- O(n) print currentNode.value ---------- O(1) Time complexity - O(n) Space complexity - O(1) Search \u00b6 SearchNode(head, nodeValue): loop: tmpNode = start to tail if( tmpNode.value equals nodeValue ) print tmpNode.value // node value found return return // nodeValue node found SearchNode(head, nodeValue): loop: tmpNode = start to tail ------------------------- O(n) if( tmpNode.value equals nodeValue ) ---------| print tmpNode.value // node value found --|- O(1) return -----------------------------------| return // nodeValue node found ------------------------ O(1) Time complexity O(n) Space complexity O(1) Deleting a node \u00b6 There can be 3 cases: - Delete first element Delete node, update head to point to second element. Delete last element Delete node, update tail to point to previous element. Delete a specified node Delete node, update previous element to point to next. DeleteNode(head, location): if(!existsLinkedList(head)) return error // linked list does not exist else if (location equals 0) // delete first node head = head.next if this is the only element in the list, update tail = null else if (location >= last) if current node is only node in list head = tail = null return; else loop till 2nd last node (tmpNode) tail = nmpNode; tmpNode.next = null; else loop: tnpNode = start to location - 1 tmpNode.next = tmpNode.next DeleteNode(head, location): if(!existsLinkedList(head)) --------------------------------------- O(1) return error // linked list does not exist -------------------- O(1) else if (location equals 0) // delete first node ------------------ O(1) head = head.next ---------------------------------------------- O(1) if this is the only element in the list, update tail = null --- O(1) else if (location >= last) ---------------------------------------- O(1) if current node is only node in list -------------------------- O(1) head = tail = null return; -------------------------------- O(1) else --------------------------------------------------------- O(1) loop till 2nd last node (tmpNode) ------------------------- O(n) tail = nmpNode; tmpNode.next = null; -------------------------- O(1) else -------------------------------------------------------------- O(1) loop: tnpNode = start to location - 1 ------------------------- O(n) tmpNode.next = tmpNode.next ----------------------------------- O(1) Time complexity O(n) Space complexity O(1) Delete entire Single Linked List \u00b6 DeleteLinkedList(head, tail): head = null tail = null ` Time complexity O(1) Space complexity O(1) Time complexity \u00b6 Time complexity Space complexity Creating O(1) O(1) Insertion O(n) O(1) Searching O(n) O(1) Traversing O(n) O(1) Deletion O(n) O(1) Deleting whole list O(1) O(1)","title":"Single linked list"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/06-single-linked-list/#single-linked-list","text":"","title":"Single linked list"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/06-single-linked-list/#creating","text":"When we want to create a single linked list, it consists of a few steps: 1. We create a blank head reference 2. We create a blank tail reference 3. We initialize them both with null 4. We create a blank node 5. We initialize it with null reference 6. We initialize the data part of it 7. We update the head reference to the node 8. We update the tail reference to the node createSingleLinkedList(node): create a head, tail pointer and initialize with NULL create a blank node node.value = nodeValue head = node tail = node createSingleLinkedList(node): create a head, tail pointer and initialize with NULL ---- O(1) create a blank node ------------------------------------- O(1) node.value = nodeValue ---------------------------------- O(1) head = node --------------------------------------------- O(1) tail = node --------------------------------------------- O(1) Time complexity O(1) Space complexity O(1)","title":"Creating"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/06-single-linked-list/#inserting","text":"We can assume that we have a Single Linked List with 6 elements in it. When we want to insert a new node, we can have 3 scenarios: - Inserting at the start of linked list We have to create a new node, hold a reference to the first element and update the head to reference it. Inserting at the end of linked list We have to create a new node, update the previous element as well as the tail to point to it. Inserting at a specified location in linked list We have to create a new node, update the previous element to point to it, hold the reference to the next element. insertInLinkedList(head, nodeValue, location) create a blank node node.value = nodeValue if( !existsLinkedList(head) ) return error // LinkedList does not exist else if (location equals 0) // insert at first position node.next = head head = node else if (location equals last) // insert at last position node.next = null last.next = node last = node // to keep track of the last node else loop: tmpNode = 0 to location - 1 node.next = tmpNode.next tmpNode.next = node insertInLinkedList(head, nodeValue, location) create a blank node ------------------------------------------- O(1) node.value = nodeValue ---------------------------------------- O(1) if( !existsLinkedList(head) ) --------------------------------- O(1) return error // LinkedList does not exist ----------------- O(1) else if (location equals 0) // insert at first position ------- O(1) node.next = head ------------------------------------------ O(1) head = node ----------------------------------------------- O(1) else if (location equals last) // insert at last position ----- O(1) node.next = null ------------------------------------------ O(1) last.next = node ------------------------------------------ O(1) last = node // to keep track of the last node ------------- O(1) else --------------------------------------------------------- O(1) loop: tmpNode = 0 to location - 1 ------------------------- O(n) node.next = tmpNode.next ---------------------------------- O(1) tmpNode.next = node --------------------------------------- O(1) Time complexity O(n) Space complexity O(1)","title":"Inserting"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/06-single-linked-list/#traversal","text":"TraverseLinkedList(head): if head == null return; loop: head to tail print currentNode.value TraverseLinkedList(head): if head == null return; -------------- O(1) loop: head to tail ------------------- O(n) print currentNode.value ---------- O(1) Time complexity - O(n) Space complexity - O(1)","title":"Traversal"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/06-single-linked-list/#search","text":"SearchNode(head, nodeValue): loop: tmpNode = start to tail if( tmpNode.value equals nodeValue ) print tmpNode.value // node value found return return // nodeValue node found SearchNode(head, nodeValue): loop: tmpNode = start to tail ------------------------- O(n) if( tmpNode.value equals nodeValue ) ---------| print tmpNode.value // node value found --|- O(1) return -----------------------------------| return // nodeValue node found ------------------------ O(1) Time complexity O(n) Space complexity O(1)","title":"Search"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/06-single-linked-list/#deleting-a-node","text":"There can be 3 cases: - Delete first element Delete node, update head to point to second element. Delete last element Delete node, update tail to point to previous element. Delete a specified node Delete node, update previous element to point to next. DeleteNode(head, location): if(!existsLinkedList(head)) return error // linked list does not exist else if (location equals 0) // delete first node head = head.next if this is the only element in the list, update tail = null else if (location >= last) if current node is only node in list head = tail = null return; else loop till 2nd last node (tmpNode) tail = nmpNode; tmpNode.next = null; else loop: tnpNode = start to location - 1 tmpNode.next = tmpNode.next DeleteNode(head, location): if(!existsLinkedList(head)) --------------------------------------- O(1) return error // linked list does not exist -------------------- O(1) else if (location equals 0) // delete first node ------------------ O(1) head = head.next ---------------------------------------------- O(1) if this is the only element in the list, update tail = null --- O(1) else if (location >= last) ---------------------------------------- O(1) if current node is only node in list -------------------------- O(1) head = tail = null return; -------------------------------- O(1) else --------------------------------------------------------- O(1) loop till 2nd last node (tmpNode) ------------------------- O(n) tail = nmpNode; tmpNode.next = null; -------------------------- O(1) else -------------------------------------------------------------- O(1) loop: tnpNode = start to location - 1 ------------------------- O(n) tmpNode.next = tmpNode.next ----------------------------------- O(1) Time complexity O(n) Space complexity O(1)","title":"Deleting a node"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/06-single-linked-list/#delete-entire-single-linked-list","text":"DeleteLinkedList(head, tail): head = null tail = null ` Time complexity O(1) Space complexity O(1)","title":"Delete entire Single Linked List"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/06-single-linked-list/#time-complexity","text":"Time complexity Space complexity Creating O(1) O(1) Insertion O(n) O(1) Searching O(n) O(1) Traversing O(n) O(1) Deletion O(n) O(1) Deleting whole list O(1) O(1)","title":"Time complexity"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/07-circular-single-linked-list/","text":"Circular single linked list \u00b6 The same principes as the Single Linked List applies to the Circular Single Linked List. The only difference is that the last element points to the first one. Creating \u00b6 When we want to create a circular single linked list, it consists of a few steps: We create a blank head reference We create a blank tail reference We initialize them both with null We create a blank node We initialize it with reference to the first node (self) We initialize the data part of it We update the head reference to the node We update the tail reference to the node createSingleLinkedList(nodeValue): create a blank node node.value = nodeValue node.next = node head = node tail = node createSingleLinkedList(nodeValue): create a blank node -------------- O(1) node.value = nodeValue ----------- O(1) node.next = node ----------------- O(1) head = node ---------------------- O(1) tail = node ---------------------- O(1) Time complexity O(1) Space complexity O(1) Inserting data in Circular Single Linked List \u00b6 When we want to insert a new node, we can have 3 scenarios: - Inserting at the start of linked list We have to create a new node, hold a reference to the first element and update the head to reference it. Inserting at the end of linked list We have to create a new node, update the previous element as well as the tail to point to it, reference self to the first element. Inserting at a specified location in linked list We have to create a new node, update the previous element to point to it, hold the reference to the next element. insertLinkedList(head, nodeValue, location): create a blank node node.value = nodeValue if (!existsLinkedList(head)) return error // Linked List does not exist else if (location equals 0) // insert at first position node.next = head head = node; tail.next = head else if (location equals last) // insert at last position node.next = head tail.next = node tail = node // to keep track of last node else loop: tmpNode = 0 to location - 1 node.next = tmpNode.next tmpNode.next = node Traversing in Circular Single Linked Lists \u00b6 TraverseLinkedList(head): if head == NULL return; loop: head to tail print curentNode.value TraverseLinkedList(head): if head == NULL ---------------- O(1) return; -------------------- O(1) loop: head to tail ------------- O(n) print curentNode.value -- O(1) Time complexity O(n) Space complexity O(1) Searching a node in Circular Single Linked List \u00b6 SearchNode(head, nodeValue): loop: tmpNode = start to tail if (tmpNode.value == nodeValue) print tmpNode.value return true return false SearchNode(head, nodeValue): loop: tmpNode = start to tail --------------- O(n) | if (tmpNode.value == nodeValue) -- O(1)| |- O(n) print tmpNode.value ---------- O(1)|- O(1) | return true ------------------ O(1)| return false ---------------------------------------- O(1) Time complexity - O(n) Space complexity - O(1) Delete a node from a Circular Single Linked List \u00b6 There can be 3 cases: - Delete first node - Delete last node - Delete any other node deleteNode(head, Location): if( !existsLinkedList(head)) return error // Linked List does not exist else if (location == 0) // delete first element head = head.next; tail.next = head; if (this was the only element in the list): head = tail = node.next = null else if (location >= last) if( current node is only node in list ): head = tail = node.next = null; return; loop until 2nd last node (tmpNode) tail = tmpNode; tmpNode.next = head; else // if any internal node needs to be deleted loop: tmpNode = start to location -1 tmpNode.next = tmpNode.next.next deleteNode(head, Location): if( !existsLinkedList(head)) -------------------------- O(1) return error // Linked List does not exist -------- O(1) else if (location == 0) // delete first element ------- O(1) head = head.next; -------------------------------- O(1) tail.next = head; --------------------------------- O(1) if (this was the only element in the list): ------- O(1) head = tail = node.next = null ---------------- O(1) else if (location >= last) ---------------------------- O(1) if( current node is only node in list ): ---------- O(1) head = tail = node.next = null; return; ------- O(1) loop until 2nd last node (tmpNode) ---------------- O(n) tail = tmpNode; ------------------------------ O(1) tmpNode.next = head; -------------------------- O(1) else // if any internal node needs to be deleted ------ O(1) loop: tmpNode = start to location -1 -------------- O(n) tmpNode.next = tmpNode.next.next ------------------ O(1) Time Complexity - O(n) Space Complexity - O(1) Deletion of entire cicular single linked list \u00b6 DeleteLinkedList(head, tail): head = null tail.next = null last = null DeleteLinkedList(head, tail): ----- O(1) head = null ------------------- O(1) tail.next = null -------------- O(1) last = null ------------------- O(1) Time complexity - O(1) Space complexity - O(1) Time & Space complexity of Circular Single Linked List \u00b6 Time complexity Space complexity Creating O(1) O(1) Insertion O(n) O(1) Searching O(n) O(1) Traversing O(n) O(1) Deletion O(n) O(1) Deleting whole list O(1) O(1)","title":"Circular single linked list"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/07-circular-single-linked-list/#circular-single-linked-list","text":"The same principes as the Single Linked List applies to the Circular Single Linked List. The only difference is that the last element points to the first one.","title":"Circular single linked list"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/07-circular-single-linked-list/#creating","text":"When we want to create a circular single linked list, it consists of a few steps: We create a blank head reference We create a blank tail reference We initialize them both with null We create a blank node We initialize it with reference to the first node (self) We initialize the data part of it We update the head reference to the node We update the tail reference to the node createSingleLinkedList(nodeValue): create a blank node node.value = nodeValue node.next = node head = node tail = node createSingleLinkedList(nodeValue): create a blank node -------------- O(1) node.value = nodeValue ----------- O(1) node.next = node ----------------- O(1) head = node ---------------------- O(1) tail = node ---------------------- O(1) Time complexity O(1) Space complexity O(1)","title":"Creating"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/07-circular-single-linked-list/#inserting-data-in-circular-single-linked-list","text":"When we want to insert a new node, we can have 3 scenarios: - Inserting at the start of linked list We have to create a new node, hold a reference to the first element and update the head to reference it. Inserting at the end of linked list We have to create a new node, update the previous element as well as the tail to point to it, reference self to the first element. Inserting at a specified location in linked list We have to create a new node, update the previous element to point to it, hold the reference to the next element. insertLinkedList(head, nodeValue, location): create a blank node node.value = nodeValue if (!existsLinkedList(head)) return error // Linked List does not exist else if (location equals 0) // insert at first position node.next = head head = node; tail.next = head else if (location equals last) // insert at last position node.next = head tail.next = node tail = node // to keep track of last node else loop: tmpNode = 0 to location - 1 node.next = tmpNode.next tmpNode.next = node","title":"Inserting data in Circular Single Linked List"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/07-circular-single-linked-list/#traversing-in-circular-single-linked-lists","text":"TraverseLinkedList(head): if head == NULL return; loop: head to tail print curentNode.value TraverseLinkedList(head): if head == NULL ---------------- O(1) return; -------------------- O(1) loop: head to tail ------------- O(n) print curentNode.value -- O(1) Time complexity O(n) Space complexity O(1)","title":"Traversing in Circular Single Linked Lists"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/07-circular-single-linked-list/#searching-a-node-in-circular-single-linked-list","text":"SearchNode(head, nodeValue): loop: tmpNode = start to tail if (tmpNode.value == nodeValue) print tmpNode.value return true return false SearchNode(head, nodeValue): loop: tmpNode = start to tail --------------- O(n) | if (tmpNode.value == nodeValue) -- O(1)| |- O(n) print tmpNode.value ---------- O(1)|- O(1) | return true ------------------ O(1)| return false ---------------------------------------- O(1) Time complexity - O(n) Space complexity - O(1)","title":"Searching a node in Circular Single Linked List"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/07-circular-single-linked-list/#delete-a-node-from-a-circular-single-linked-list","text":"There can be 3 cases: - Delete first node - Delete last node - Delete any other node deleteNode(head, Location): if( !existsLinkedList(head)) return error // Linked List does not exist else if (location == 0) // delete first element head = head.next; tail.next = head; if (this was the only element in the list): head = tail = node.next = null else if (location >= last) if( current node is only node in list ): head = tail = node.next = null; return; loop until 2nd last node (tmpNode) tail = tmpNode; tmpNode.next = head; else // if any internal node needs to be deleted loop: tmpNode = start to location -1 tmpNode.next = tmpNode.next.next deleteNode(head, Location): if( !existsLinkedList(head)) -------------------------- O(1) return error // Linked List does not exist -------- O(1) else if (location == 0) // delete first element ------- O(1) head = head.next; -------------------------------- O(1) tail.next = head; --------------------------------- O(1) if (this was the only element in the list): ------- O(1) head = tail = node.next = null ---------------- O(1) else if (location >= last) ---------------------------- O(1) if( current node is only node in list ): ---------- O(1) head = tail = node.next = null; return; ------- O(1) loop until 2nd last node (tmpNode) ---------------- O(n) tail = tmpNode; ------------------------------ O(1) tmpNode.next = head; -------------------------- O(1) else // if any internal node needs to be deleted ------ O(1) loop: tmpNode = start to location -1 -------------- O(n) tmpNode.next = tmpNode.next.next ------------------ O(1) Time Complexity - O(n) Space Complexity - O(1)","title":"Delete a node from a Circular Single Linked List"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/07-circular-single-linked-list/#deletion-of-entire-cicular-single-linked-list","text":"DeleteLinkedList(head, tail): head = null tail.next = null last = null DeleteLinkedList(head, tail): ----- O(1) head = null ------------------- O(1) tail.next = null -------------- O(1) last = null ------------------- O(1) Time complexity - O(1) Space complexity - O(1)","title":"Deletion of entire cicular single linked list"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/07-circular-single-linked-list/#time-space-complexity-of-circular-single-linked-list","text":"Time complexity Space complexity Creating O(1) O(1) Insertion O(n) O(1) Searching O(n) O(1) Traversing O(n) O(1) Deletion O(n) O(1) Deleting whole list O(1) O(1)","title":"Time &amp; Space complexity of Circular Single Linked List"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/08-double-linked-list/","text":"Double linked list \u00b6 In double linked list each node contains two references that reference both previous and next nodes, instead of pointing just to the next node like single linked lists do. Creation of a double linked list \u00b6 Declare head reference Declare Tail reference Create an empty node Initialize with some value, head and tail references should be null. Update head to reference the node Update the tail to reference the node CreateDoubleLinkedList(nodeValue): create a blank node node.value = nodeValue head = node tail = node node.next = node.prev = null; Time complexity - O(1) Space complexity - O(1) Insert a node in double linked list \u00b6 There can be 3 cases: - Insert at the start of the linked list - Insert at the end of linked list - Insert at any other location insertInLinkedList(head, nodeValue, location): create a blank node node.value = nodeValue if(!existsLinkedList(head)) return error else if (location == 0) // insert at first position node.next = head node.prev = null head.next = node head = node else if (location == last) node.next = null node.prev = tail tail.next = node tail = node else // any other location loop: tmpNode = 0 to location -1 node.next = tmpNode.next; node.prev = tmpNode; tmpNode.next = node; node.next.prev = node; Time complexity - O(n) Space complexity - O(n) Traverse \u00b6 traverseLinkedList() if (head == null) return loop: head to tail print currentNode.value Time complexity - O(n) Space complexity - O(1) Search \u00b6 searchNOde(head, nodeValue): loop: tmpNode = head to tail if(tmpNode.value == nodeValue) print tmpNode.value return true return false Time complexity - O(n) Space complexity - O(1) Delete \u00b6 deleteNode(head, location): if(!existsLinkedList(head)) return error else if (location == 0) // first element if (this was the only element in list) head = tail = null return head = head.next; head.prev = null else if (location >= last) if (this was the only element in list) head = tail = null return tail = tail.prev; tail.next = null else // any other location loop: tmpNode = start to location - 1 tmpNode.next = tmpNode.next.next tmpNode.next.prev = tmpNode Time complexity O(n) Space complexity O(1) Delete entire DLL \u00b6 deleteLinkedList(head, tail): loop(tmp : head to tail) tmp.prev = null; head = tail = null; Time complexity - O(n) Space complexity - O(1) TIme and space complexityies \u00b6 Time complexity Space complexity Creating O(1) O(1) Insertion O(n) O(1) Searching O(n) O(1) Traversing O(n) O(1) Deletion O(n) O(1) Deleting whole list O(n) O(1)","title":"Double linked list"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/08-double-linked-list/#double-linked-list","text":"In double linked list each node contains two references that reference both previous and next nodes, instead of pointing just to the next node like single linked lists do.","title":"Double linked list"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/08-double-linked-list/#creation-of-a-double-linked-list","text":"Declare head reference Declare Tail reference Create an empty node Initialize with some value, head and tail references should be null. Update head to reference the node Update the tail to reference the node CreateDoubleLinkedList(nodeValue): create a blank node node.value = nodeValue head = node tail = node node.next = node.prev = null; Time complexity - O(1) Space complexity - O(1)","title":"Creation of a double linked list"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/08-double-linked-list/#insert-a-node-in-double-linked-list","text":"There can be 3 cases: - Insert at the start of the linked list - Insert at the end of linked list - Insert at any other location insertInLinkedList(head, nodeValue, location): create a blank node node.value = nodeValue if(!existsLinkedList(head)) return error else if (location == 0) // insert at first position node.next = head node.prev = null head.next = node head = node else if (location == last) node.next = null node.prev = tail tail.next = node tail = node else // any other location loop: tmpNode = 0 to location -1 node.next = tmpNode.next; node.prev = tmpNode; tmpNode.next = node; node.next.prev = node; Time complexity - O(n) Space complexity - O(n)","title":"Insert a node in double linked list"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/08-double-linked-list/#traverse","text":"traverseLinkedList() if (head == null) return loop: head to tail print currentNode.value Time complexity - O(n) Space complexity - O(1)","title":"Traverse"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/08-double-linked-list/#search","text":"searchNOde(head, nodeValue): loop: tmpNode = head to tail if(tmpNode.value == nodeValue) print tmpNode.value return true return false Time complexity - O(n) Space complexity - O(1)","title":"Search"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/08-double-linked-list/#delete","text":"deleteNode(head, location): if(!existsLinkedList(head)) return error else if (location == 0) // first element if (this was the only element in list) head = tail = null return head = head.next; head.prev = null else if (location >= last) if (this was the only element in list) head = tail = null return tail = tail.prev; tail.next = null else // any other location loop: tmpNode = start to location - 1 tmpNode.next = tmpNode.next.next tmpNode.next.prev = tmpNode Time complexity O(n) Space complexity O(1)","title":"Delete"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/08-double-linked-list/#delete-entire-dll","text":"deleteLinkedList(head, tail): loop(tmp : head to tail) tmp.prev = null; head = tail = null; Time complexity - O(n) Space complexity - O(1)","title":"Delete entire DLL"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/08-double-linked-list/#time-and-space-complexityies","text":"Time complexity Space complexity Creating O(1) O(1) Insertion O(n) O(1) Searching O(n) O(1) Traversing O(n) O(1) Deletion O(n) O(1) Deleting whole list O(n) O(1)","title":"TIme and space complexityies"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/09-circular-double-linked-list/","text":"Circular double linked list \u00b6 A linked list that is similar to circular single linked list, but it has 2 references - to previous and next nodes. Creating \u00b6 createCircularDoubleLinkedList(nodeValue): create a blank node node.value = nodeValue head = node tail = node node.next = node.prev = node Time complexity - O(1) Space complexity - O(1) Insert \u00b6 insert(head, nodeValue, location): create a blank node node.value = nodeValue if(!existsLinkedList(head)) return error else if (locations == 0) node.next = head node.prev = tail head.prev = node head = node; tail.next = node else if (location == last) node.next = head; node.prev = last; head.prev = node last.next = node tail = node else // at specific location loop: tmpNode = 0 to location -1 node.next = tmpNode.next; node.prev = tmpNode tmpNode.next = node; node.next.prev = node Time complexity - O(n) Space comlexity - O(1) Traverse \u00b6 traverse(): if head == null return loop: head to tail print currentNode.value Time complexity - O(n) Space complexity - O(1) Search \u00b6 searchNode(nodeValue): loop: tmpNode = head to tail if(tmpNode.value == nodeValue) print tmpNode.value return true return false Time complexity - O(n) Space complexity - O(1) Delete \u00b6 delete(head, location): if(!existsLinkedList(head)) return error else if (location == 0) if(only one element) head.next = head.prev = head = tail = null; return head = head.next head.prev = null tail.next = head else if (location >= last) if(only one element) head.next = head.prev = head = tail = null; return tail = tail.prev; tail.next = head; head.prev = tail else loop: tmpNode = head to location -1 tmpNode.next = tmpNode.next.next tmpNode.next.prev = tmpNode Time complexity - O(n) Space complexity - O(1) Delete entire CDLL \u00b6 deleteLinkedList(head, tail): tail.next = null loop(tmp:head to tail) tmp.prev = null head = tail = null Time complexity - O(n) Space complexity - O(1) Time and space complexityies \u00b6 Time complexity Space complexity Creating O(1) O(1) Insertion O(n) O(1) Searching O(n) O(1) Traversing O(n) O(1) Deletion O(n) O(1) Deleting whole list O(n) O(1)","title":"Circular double linked list"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/09-circular-double-linked-list/#circular-double-linked-list","text":"A linked list that is similar to circular single linked list, but it has 2 references - to previous and next nodes.","title":"Circular double linked list"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/09-circular-double-linked-list/#creating","text":"createCircularDoubleLinkedList(nodeValue): create a blank node node.value = nodeValue head = node tail = node node.next = node.prev = node Time complexity - O(1) Space complexity - O(1)","title":"Creating"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/09-circular-double-linked-list/#insert","text":"insert(head, nodeValue, location): create a blank node node.value = nodeValue if(!existsLinkedList(head)) return error else if (locations == 0) node.next = head node.prev = tail head.prev = node head = node; tail.next = node else if (location == last) node.next = head; node.prev = last; head.prev = node last.next = node tail = node else // at specific location loop: tmpNode = 0 to location -1 node.next = tmpNode.next; node.prev = tmpNode tmpNode.next = node; node.next.prev = node Time complexity - O(n) Space comlexity - O(1)","title":"Insert"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/09-circular-double-linked-list/#traverse","text":"traverse(): if head == null return loop: head to tail print currentNode.value Time complexity - O(n) Space complexity - O(1)","title":"Traverse"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/09-circular-double-linked-list/#search","text":"searchNode(nodeValue): loop: tmpNode = head to tail if(tmpNode.value == nodeValue) print tmpNode.value return true return false Time complexity - O(n) Space complexity - O(1)","title":"Search"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/09-circular-double-linked-list/#delete","text":"delete(head, location): if(!existsLinkedList(head)) return error else if (location == 0) if(only one element) head.next = head.prev = head = tail = null; return head = head.next head.prev = null tail.next = head else if (location >= last) if(only one element) head.next = head.prev = head = tail = null; return tail = tail.prev; tail.next = head; head.prev = tail else loop: tmpNode = head to location -1 tmpNode.next = tmpNode.next.next tmpNode.next.prev = tmpNode Time complexity - O(n) Space complexity - O(1)","title":"Delete"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/09-circular-double-linked-list/#delete-entire-cdll","text":"deleteLinkedList(head, tail): tail.next = null loop(tmp:head to tail) tmp.prev = null head = tail = null Time complexity - O(n) Space complexity - O(1)","title":"Delete entire CDLL"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/09-circular-double-linked-list/#time-and-space-complexityies","text":"Time complexity Space complexity Creating O(1) O(1) Insertion O(n) O(1) Searching O(n) O(1) Traversing O(n) O(1) Deletion O(n) O(1) Deleting whole list O(n) O(1)","title":"Time and space complexityies"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/10-time-complexity-linkedlist-vs-array/","text":"Time complexity (linked list vs array) \u00b6 Array LinkedList Create O(1) O(1) Insertion at 1st position O(1) O(1) Insertion at last position O(1) O(1) Insertion at n-th position O(1) O(n) Deletion from 1st position O(1) O(1) Deletion from last position O(1) O(n) / O(1) Deletion from n-th position O(1) O(n) Searhing in unsorted data O(n) O(n) Searching in sorted data O(log n) O(n) Accessing n-th element O(1) O(n) Traversing O(n) O(n) Deleting entire Array/LinkedList O(1) O(n) / O(1)","title":"Time complexity (linked list vs array)"},{"location":"Data%20Structures%20And%20Algorithms/05-linked-lists/10-time-complexity-linkedlist-vs-array/#time-complexity-linked-list-vs-array","text":"Array LinkedList Create O(1) O(1) Insertion at 1st position O(1) O(1) Insertion at last position O(1) O(1) Insertion at n-th position O(1) O(n) Deletion from 1st position O(1) O(1) Deletion from last position O(1) O(n) / O(1) Deletion from n-th position O(1) O(n) Searhing in unsorted data O(n) O(n) Searching in sorted data O(log n) O(n) Accessing n-th element O(1) O(n) Traversing O(n) O(n) Deleting entire Array/LinkedList O(1) O(n) / O(1)","title":"Time complexity (linked list vs array)"},{"location":"Data%20Structures%20And%20Algorithms/06-stack/01-what-and-why-of-stack/","text":"What and why of stack \u00b6 Now we are taking a look at our fist logical data structure - stack. What is stack \u00b6 Stack data structure is a list of elements that are pushed into a stack. It has two methods - push and pop. The stack follows LIFO (last in First out) methodology, which means that the last element you pushed out of the stack, will be the first one to pop. (image taken from https://www.thecrazyprogrammer.com/2018/01/stack-vs-heap.html) Why whould we learn / use stack? \u00b6 When we need to create an application that uses last incoming data first. An example for this would be using your browser's history navigation. When you visit a page it pushes the previous one on the history stack, when you press a button to navigate back, it will pop the last site you went. Common operations in a stack \u00b6 Create stack Create a stack structure. Push Add an element to the stack Pop Get an element from the stack, remove it from it. Peek Get an element from the stack without removing it isEmpty Check if the stack is empty isFull Check if the stack is full deleteStack Delete the stack structure","title":"What and why of stack"},{"location":"Data%20Structures%20And%20Algorithms/06-stack/01-what-and-why-of-stack/#what-and-why-of-stack","text":"Now we are taking a look at our fist logical data structure - stack.","title":"What and why of stack"},{"location":"Data%20Structures%20And%20Algorithms/06-stack/01-what-and-why-of-stack/#what-is-stack","text":"Stack data structure is a list of elements that are pushed into a stack. It has two methods - push and pop. The stack follows LIFO (last in First out) methodology, which means that the last element you pushed out of the stack, will be the first one to pop. (image taken from https://www.thecrazyprogrammer.com/2018/01/stack-vs-heap.html)","title":"What is stack"},{"location":"Data%20Structures%20And%20Algorithms/06-stack/01-what-and-why-of-stack/#why-whould-we-learn-use-stack","text":"When we need to create an application that uses last incoming data first. An example for this would be using your browser's history navigation. When you visit a page it pushes the previous one on the history stack, when you press a button to navigate back, it will pop the last site you went.","title":"Why whould we learn / use stack?"},{"location":"Data%20Structures%20And%20Algorithms/06-stack/01-what-and-why-of-stack/#common-operations-in-a-stack","text":"Create stack Create a stack structure. Push Add an element to the stack Pop Get an element from the stack, remove it from it. Peek Get an element from the stack without removing it isEmpty Check if the stack is empty isFull Check if the stack is full deleteStack Delete the stack structure","title":"Common operations in a stack"},{"location":"Data%20Structures%20And%20Algorithms/06-stack/02-array-stack/","text":"Stack structure using arrays \u00b6 When using an array to build a stack, the advantage is that it is easy to implement, but however, the stack can only be fixed size. createStack(int size): create blank array of `size` initialize variable topOfStack to -1 Time complexity - O(1) Space complexity - O(n) push(value): if stack is full return error else insert value at the top of the array topofStack++ Time complexity - O(1) Space complexity - O(1) pop() if stackIsEmpty() return error else print top of stack topOfStack-- Time complexity - O(1) Space complexity - O(1) peek() if stackIsEmpty() return error else print topOfStack Time complexity - O(1) Space complexity - O(1) isEmpty(): if (topOfStack == -1) return true return false Time complexity - O(1) Space complexity - O(1) isFull(): if(topOfStack === arr.size) return true return false Time complexity - O(1) Space complexity - O(1) deleteStack() arr = null Time complexity - O(1) Space complexity - O(1) Time and space complexity when using array \u00b6 Time complexity Space complexity create stack O(1) O(n) push O(1) O(1) pop O(1) O(1) peek O(1) O(1) isEmpty O(1) O(1) isFull O(1) O(1) delete stack O(1) O(1)","title":"Stack structure using arrays"},{"location":"Data%20Structures%20And%20Algorithms/06-stack/02-array-stack/#stack-structure-using-arrays","text":"When using an array to build a stack, the advantage is that it is easy to implement, but however, the stack can only be fixed size. createStack(int size): create blank array of `size` initialize variable topOfStack to -1 Time complexity - O(1) Space complexity - O(n) push(value): if stack is full return error else insert value at the top of the array topofStack++ Time complexity - O(1) Space complexity - O(1) pop() if stackIsEmpty() return error else print top of stack topOfStack-- Time complexity - O(1) Space complexity - O(1) peek() if stackIsEmpty() return error else print topOfStack Time complexity - O(1) Space complexity - O(1) isEmpty(): if (topOfStack == -1) return true return false Time complexity - O(1) Space complexity - O(1) isFull(): if(topOfStack === arr.size) return true return false Time complexity - O(1) Space complexity - O(1) deleteStack() arr = null Time complexity - O(1) Space complexity - O(1)","title":"Stack structure using arrays"},{"location":"Data%20Structures%20And%20Algorithms/06-stack/02-array-stack/#time-and-space-complexity-when-using-array","text":"Time complexity Space complexity create stack O(1) O(n) push O(1) O(1) pop O(1) O(1) peek O(1) O(1) isEmpty O(1) O(1) isFull O(1) O(1) delete stack O(1) O(1)","title":"Time and space complexity when using array"},{"location":"Data%20Structures%20And%20Algorithms/06-stack/03-linked-list-stack/","text":"Linked list stack \u00b6 createStack() create an object of SingleLinkedList class Time complexity - O(1) Space complexity - O(1) push(nodeValue) // insert an element between head and first element create a node node.value = nodeValue node.next = head head = node Time complexity - O(1) Space complexity - O(1) pop(): // return and remove the element head points to if isEmpty() return error tmpNode = head head = head.next return tmpNode.value Time complexity - O(1) Space complexity - O(1) peek() return head.value Time complexity - O(1) Space complexity - O(1) isEmpty() if head == null return true return false Time complexity - O(1) Space complexity - O(1) deleteStack() head = null Time complexity - O(1) Space complexity - O(1) Time and space complexity \u00b6 Time complexity Space complexity create stack O(1) O(n) push O(1) O(1) pop O(1) O(1) peek O(1) O(1) isEmpty O(1) O(1) isFull N/A N/A delete stack O(1) O(1) Note that the isFull method is not implemented, since the linked list can have as many elements as we want, provided our memory can hold it. Apart from that, the only difference is that when using linked list, the stack creation method uses space complexity of O(1) instead of O(n) since we create only one node not reserve the whole array.","title":"Linked list stack"},{"location":"Data%20Structures%20And%20Algorithms/06-stack/03-linked-list-stack/#linked-list-stack","text":"createStack() create an object of SingleLinkedList class Time complexity - O(1) Space complexity - O(1) push(nodeValue) // insert an element between head and first element create a node node.value = nodeValue node.next = head head = node Time complexity - O(1) Space complexity - O(1) pop(): // return and remove the element head points to if isEmpty() return error tmpNode = head head = head.next return tmpNode.value Time complexity - O(1) Space complexity - O(1) peek() return head.value Time complexity - O(1) Space complexity - O(1) isEmpty() if head == null return true return false Time complexity - O(1) Space complexity - O(1) deleteStack() head = null Time complexity - O(1) Space complexity - O(1)","title":"Linked list stack"},{"location":"Data%20Structures%20And%20Algorithms/06-stack/03-linked-list-stack/#time-and-space-complexity","text":"Time complexity Space complexity create stack O(1) O(n) push O(1) O(1) pop O(1) O(1) peek O(1) O(1) isEmpty O(1) O(1) isFull N/A N/A delete stack O(1) O(1) Note that the isFull method is not implemented, since the linked list can have as many elements as we want, provided our memory can hold it. Apart from that, the only difference is that when using linked list, the stack creation method uses space complexity of O(1) instead of O(n) since we create only one node not reserve the whole array.","title":"Time and space complexity"},{"location":"Data%20Structures%20And%20Algorithms/06-stack/04-when-to-use-and-when-to-avoid-stack/","text":"When to use and when to avoid stack \u00b6 When to use \u00b6 When we need to manage data in LIFO manner Cannot be easily corrupted (no one can insert data at the middle) When to avoid \u00b6 Random access is not possible - if we have done a mistake, it is costly to rectify it.","title":"When to use and when to avoid stack"},{"location":"Data%20Structures%20And%20Algorithms/06-stack/04-when-to-use-and-when-to-avoid-stack/#when-to-use-and-when-to-avoid-stack","text":"","title":"When to use and when to avoid stack"},{"location":"Data%20Structures%20And%20Algorithms/06-stack/04-when-to-use-and-when-to-avoid-stack/#when-to-use","text":"When we need to manage data in LIFO manner Cannot be easily corrupted (no one can insert data at the middle)","title":"When to use"},{"location":"Data%20Structures%20And%20Algorithms/06-stack/04-when-to-use-and-when-to-avoid-stack/#when-to-avoid","text":"Random access is not possible - if we have done a mistake, it is costly to rectify it.","title":"When to avoid"},{"location":"Data%20Structures%20And%20Algorithms/06-stack/05-time-and-space-complexity-array-vs-linked-list/","text":"Time and Space complexity on array vs linked list \u00b6 Time complexity (array) Space complexity (array) Time complexity (LL) Space complexity (LL) create queue O(1) O(n) O(1) O(1) enqueue O(1) O(1) O(1) O(1) dequeue O(1) O(1) O(1) O(1) peek O(1) O(1) O(1) O(1) isEmpty O(1) O(1) O(1) O(1) isFull O(1) O(1) N/A N/A delete queue O(1) O(1) O(1) O(1) Space efficient? NO YES When we compare both implementations of arrays and linked lists, we can say that both implementations are relatively the same, but the linked lists are more space efficient.","title":"Time and Space complexity on array vs linked list"},{"location":"Data%20Structures%20And%20Algorithms/06-stack/05-time-and-space-complexity-array-vs-linked-list/#time-and-space-complexity-on-array-vs-linked-list","text":"Time complexity (array) Space complexity (array) Time complexity (LL) Space complexity (LL) create queue O(1) O(n) O(1) O(1) enqueue O(1) O(1) O(1) O(1) dequeue O(1) O(1) O(1) O(1) peek O(1) O(1) O(1) O(1) isEmpty O(1) O(1) O(1) O(1) isFull O(1) O(1) N/A N/A delete queue O(1) O(1) O(1) O(1) Space efficient? NO YES When we compare both implementations of arrays and linked lists, we can say that both implementations are relatively the same, but the linked lists are more space efficient.","title":"Time and Space complexity on array vs linked list"},{"location":"Data%20Structures%20And%20Algorithms/07-queue/01-what-is-a-queue-and-why-use-it/","text":"What is a queue and why use it \u00b6 A queue is similar to stack, but when adding new elements to it, it adds them to the end. When removing and element, we will remove the first element from the queue. This is called a FIFO method - first in, first out. (image taken from http://pakitgroup.blogspot.com/p/data-structure.html) When should we use queues? \u00b6 When we need to create an application which utilizes first incoming data gets processed first (FIFO) priciples. Examples: - A reception counter When to avoid queues? \u00b6 When we might need random access to other values other than the first one. Correcting mistakes will be painful. Common operations in queues \u00b6 Create queue enQueue - insert data in queue deQueue - extract data from queue peekInQueue - get an element from queue isEmpty - check if queue is empty isFull - check if queue is full deleteQueue - delete the queue Implementation options of queue \u00b6 A queue can be implemented both in array and linked list. In array we can implement two types of queues - linear queue and circular queue. In linked lists - we can only implement linear queues.","title":"What is a queue and why use it"},{"location":"Data%20Structures%20And%20Algorithms/07-queue/01-what-is-a-queue-and-why-use-it/#what-is-a-queue-and-why-use-it","text":"A queue is similar to stack, but when adding new elements to it, it adds them to the end. When removing and element, we will remove the first element from the queue. This is called a FIFO method - first in, first out. (image taken from http://pakitgroup.blogspot.com/p/data-structure.html)","title":"What is a queue and why use it"},{"location":"Data%20Structures%20And%20Algorithms/07-queue/01-what-is-a-queue-and-why-use-it/#when-should-we-use-queues","text":"When we need to create an application which utilizes first incoming data gets processed first (FIFO) priciples. Examples: - A reception counter","title":"When should we use queues?"},{"location":"Data%20Structures%20And%20Algorithms/07-queue/01-what-is-a-queue-and-why-use-it/#when-to-avoid-queues","text":"When we might need random access to other values other than the first one. Correcting mistakes will be painful.","title":"When to avoid queues?"},{"location":"Data%20Structures%20And%20Algorithms/07-queue/01-what-is-a-queue-and-why-use-it/#common-operations-in-queues","text":"Create queue enQueue - insert data in queue deQueue - extract data from queue peekInQueue - get an element from queue isEmpty - check if queue is empty isFull - check if queue is full deleteQueue - delete the queue","title":"Common operations in queues"},{"location":"Data%20Structures%20And%20Algorithms/07-queue/01-what-is-a-queue-and-why-use-it/#implementation-options-of-queue","text":"A queue can be implemented both in array and linked list. In array we can implement two types of queues - linear queue and circular queue. In linked lists - we can only implement linear queues.","title":"Implementation options of queue"},{"location":"Data%20Structures%20And%20Algorithms/07-queue/02-linear-queue-array/","text":"Linear queue on array \u00b6 createQueue(size) create a blank array of `size` initialize topOfQueue, beginingOfQueue to -1 Time complexity O(1) Space complexity O(n) enQueue(value) if queue is full return error arr[topOfQueue + 1] = value topOfQueue++ Time complexity O(1) Space complexity O(1) deQueue() if queue is empty return error print arr[beginningOfQueue] beginningOfQueue++ if( beginningOfQueue > endOfQueue ) // all elements dequeued beginningOfQueue = topOfQueue = -1 Time complexity - O(1) Space complexity - O(1) peek() if queue empty return error print arr[beginningOfQueue] Time complexity - O(1) Space complexity - O(1) isQueueEmpty() return beginningOfQueue == -1 Time complexity - O(1) Space complexity - O(1) isQueueFull() return (beginningOfQueue == arr.length - 1) Time complexity - O(1) Space complexity - O(1) deleteQueue() array = null Time and space complexity of lineart queue \u00b6 Time complexity Space complexity create queue O(1) O(n) enqueue O(1) O(1) dequeue O(1) O(1) peek O(1) O(1) isEmpty O(1) O(1) isFull O(1) O(1) delete queue O(1) O(1)","title":"Linear queue on array"},{"location":"Data%20Structures%20And%20Algorithms/07-queue/02-linear-queue-array/#linear-queue-on-array","text":"createQueue(size) create a blank array of `size` initialize topOfQueue, beginingOfQueue to -1 Time complexity O(1) Space complexity O(n) enQueue(value) if queue is full return error arr[topOfQueue + 1] = value topOfQueue++ Time complexity O(1) Space complexity O(1) deQueue() if queue is empty return error print arr[beginningOfQueue] beginningOfQueue++ if( beginningOfQueue > endOfQueue ) // all elements dequeued beginningOfQueue = topOfQueue = -1 Time complexity - O(1) Space complexity - O(1) peek() if queue empty return error print arr[beginningOfQueue] Time complexity - O(1) Space complexity - O(1) isQueueEmpty() return beginningOfQueue == -1 Time complexity - O(1) Space complexity - O(1) isQueueFull() return (beginningOfQueue == arr.length - 1) Time complexity - O(1) Space complexity - O(1) deleteQueue() array = null","title":"Linear queue on array"},{"location":"Data%20Structures%20And%20Algorithms/07-queue/02-linear-queue-array/#time-and-space-complexity-of-lineart-queue","text":"Time complexity Space complexity create queue O(1) O(n) enqueue O(1) O(1) dequeue O(1) O(1) peek O(1) O(1) isEmpty O(1) O(1) isFull O(1) O(1) delete queue O(1) O(1)","title":"Time and space complexity of lineart queue"},{"location":"Data%20Structures%20And%20Algorithms/07-queue/03-circular-queue/","text":"Circular queues \u00b6 There is a small problem with the linear queues, so a circular queue was created. The problem arises when using deQueue method leaves a blank cell when it extracts an element, which then becomes unused. When we have a limited space, this is not an optimal usage of the space. The fix for that is to move all the elements to the start of the array, after executing the deQueue . The tradeoff is that when we start moving the elements to the start of the array, the time complexity moves from O(1) to O(n) . To manage this in O(1) time complexity, we can use circular queues, which basically checks, whenever we want to add an element to the queue, and we are already at the end, if there are any elements at the start that are empty and add them there. createQueue(size) create a blank array of `size` initialize topOfQueue, beginingOfQueue to -1 Time complexity O(1) Space complexity O(n) enQueue(value) if queue is full return error else if topOfQueue+1 == size // top at the last cell of array, reset it to first cell topOfQueue = 0 else topOfQueue++ arr[topOfQueue] = value Time complexity O(1) Space complexity O(1) deQueue() if queue is empty return error print arr[beginningOfQueue] if start == topOfQueue // only 1 element start = topOfQueue = -1 else if start + 1 == size // start has reached end of array start = 0 else start ++ Time complexity - O(1) Space complexity - O(1) peek() if queue empty return error print arr[beginningOfQueue] Time complexity - O(1) Space complexity - O(1) isQueueEmpty() if topOfQueue == -1 return true return false Time complexity - O(1) Space complexity - O(1) isQueueFull() if topOfQueue + 1 == start // completed a circle, queue is full return true else if start == 0 && topOfQueue == size return true return false Time complexity - O(1) Space complexity - O(1) deleteQueue() array = null Time and space complexity of lineart queue \u00b6 Time complexity Space complexity create queue O(1) O(n) enqueue O(1) O(1) dequeue O(1) O(1) peek O(1) O(1) isEmpty O(1) O(1) isFull O(1) O(1) delete queue O(1) O(1)","title":"Circular queues"},{"location":"Data%20Structures%20And%20Algorithms/07-queue/03-circular-queue/#circular-queues","text":"There is a small problem with the linear queues, so a circular queue was created. The problem arises when using deQueue method leaves a blank cell when it extracts an element, which then becomes unused. When we have a limited space, this is not an optimal usage of the space. The fix for that is to move all the elements to the start of the array, after executing the deQueue . The tradeoff is that when we start moving the elements to the start of the array, the time complexity moves from O(1) to O(n) . To manage this in O(1) time complexity, we can use circular queues, which basically checks, whenever we want to add an element to the queue, and we are already at the end, if there are any elements at the start that are empty and add them there. createQueue(size) create a blank array of `size` initialize topOfQueue, beginingOfQueue to -1 Time complexity O(1) Space complexity O(n) enQueue(value) if queue is full return error else if topOfQueue+1 == size // top at the last cell of array, reset it to first cell topOfQueue = 0 else topOfQueue++ arr[topOfQueue] = value Time complexity O(1) Space complexity O(1) deQueue() if queue is empty return error print arr[beginningOfQueue] if start == topOfQueue // only 1 element start = topOfQueue = -1 else if start + 1 == size // start has reached end of array start = 0 else start ++ Time complexity - O(1) Space complexity - O(1) peek() if queue empty return error print arr[beginningOfQueue] Time complexity - O(1) Space complexity - O(1) isQueueEmpty() if topOfQueue == -1 return true return false Time complexity - O(1) Space complexity - O(1) isQueueFull() if topOfQueue + 1 == start // completed a circle, queue is full return true else if start == 0 && topOfQueue == size return true return false Time complexity - O(1) Space complexity - O(1) deleteQueue() array = null","title":"Circular queues"},{"location":"Data%20Structures%20And%20Algorithms/07-queue/03-circular-queue/#time-and-space-complexity-of-lineart-queue","text":"Time complexity Space complexity create queue O(1) O(n) enqueue O(1) O(1) dequeue O(1) O(1) peek O(1) O(1) isEmpty O(1) O(1) isFull O(1) O(1) delete queue O(1) O(1)","title":"Time and space complexity of lineart queue"},{"location":"Data%20Structures%20And%20Algorithms/07-queue/05-linear-queue-linked-list/","text":"Linear queue implementation with LinkedList \u00b6 createQueue() create a blank SingleLinkedList Time Complexity - O(1) Space Complexity - O(1) enqueue(nodeValue): create a node node.value = nodeValue node.next = null if tail == null // queue empty tail = node else tail.next = node tail = node Time Complexity - O(1) Space Complexity - O(1) deQueue() if head == null return error tmpNode = head head = head.next return tmpNode.value Time Complexity - O(1) Space Complexity - O(1) peek() if head == null return error return head.value Time Complexity - O(1) Space Complexity - O(1) isQueueEmpty() if head == null return true return false Time Complexity - O(1) Space Complexity - O(1) deleteQueue() head = tail = null Time Complexity - O(1) Space Complexity - O(1) Time and space complexity of lineart queue \u00b6 Time complexity Space complexity create queue O(1) O(n) enqueue O(1) O(1) dequeue O(1) O(1) peek O(1) O(1) isEmpty O(1) O(1) isFull N/A N/A delete queue O(1) O(1) Again, isFull is not implemented because linked list can dinamically change it's size.","title":"Linear queue implementation with LinkedList"},{"location":"Data%20Structures%20And%20Algorithms/07-queue/05-linear-queue-linked-list/#linear-queue-implementation-with-linkedlist","text":"createQueue() create a blank SingleLinkedList Time Complexity - O(1) Space Complexity - O(1) enqueue(nodeValue): create a node node.value = nodeValue node.next = null if tail == null // queue empty tail = node else tail.next = node tail = node Time Complexity - O(1) Space Complexity - O(1) deQueue() if head == null return error tmpNode = head head = head.next return tmpNode.value Time Complexity - O(1) Space Complexity - O(1) peek() if head == null return error return head.value Time Complexity - O(1) Space Complexity - O(1) isQueueEmpty() if head == null return true return false Time Complexity - O(1) Space Complexity - O(1) deleteQueue() head = tail = null Time Complexity - O(1) Space Complexity - O(1)","title":"Linear queue implementation with LinkedList"},{"location":"Data%20Structures%20And%20Algorithms/07-queue/05-linear-queue-linked-list/#time-and-space-complexity-of-lineart-queue","text":"Time complexity Space complexity create queue O(1) O(n) enqueue O(1) O(1) dequeue O(1) O(1) peek O(1) O(1) isEmpty O(1) O(1) isFull N/A N/A delete queue O(1) O(1) Again, isFull is not implemented because linked list can dinamically change it's size.","title":"Time and space complexity of lineart queue"},{"location":"Data%20Structures%20And%20Algorithms/07-queue/05-time-and-space-complexity-array-vs-linked-list/","text":"Time and Space complexity on array vs linked list \u00b6 Time complexity (array) Space complexity (array) Time complexity (LL) Space complexity (LL) create queue O(1) O(n) O(1) O(1) enqueue O(1) O(1) O(1) O(1) dequeue O(1) O(1) O(1) O(1) peek O(1) O(1) O(1) O(1) isEmpty O(1) O(1) O(1) O(1) isFull O(1) O(1) N/A N/A delete queue O(1) O(1) O(1) O(1) Space efficient? NO YES When we compare both implementations of arrays and linked lists, we can say that both implementations are relatively the same, but the linked lists are more space efficient.","title":"Time and Space complexity on array vs linked list"},{"location":"Data%20Structures%20And%20Algorithms/07-queue/05-time-and-space-complexity-array-vs-linked-list/#time-and-space-complexity-on-array-vs-linked-list","text":"Time complexity (array) Space complexity (array) Time complexity (LL) Space complexity (LL) create queue O(1) O(n) O(1) O(1) enqueue O(1) O(1) O(1) O(1) dequeue O(1) O(1) O(1) O(1) peek O(1) O(1) O(1) O(1) isEmpty O(1) O(1) O(1) O(1) isFull O(1) O(1) N/A N/A delete queue O(1) O(1) O(1) O(1) Space efficient? NO YES When we compare both implementations of arrays and linked lists, we can say that both implementations are relatively the same, but the linked lists are more space efficient.","title":"Time and Space complexity on array vs linked list"},{"location":"Data%20Structures%20And%20Algorithms/08-binary-tree/01-what-is-a-tree/","text":"What is a tree \u00b6 A tree is a data structure that is used to represent data in a hierarchical form. Every node has 2 components - data that it holds and reference to it's parent. At the very top it has a root node with a left sub-category and right sub-category under it. Tree terminology \u00b6 - Root - node with no parent - Edge - link from parent to child - Leaf - node with no children - Sibling - children of the same parent - Ancestor - parent, it's parents ... of a given node - Depth - length of the path from root to node - Height - Length of the path from node to the deepest node - Predecessor - Immediate previous node inorder traversal of the binary tree - Successor - immediate next node inorder traversal of the binary tree Predecessor example: Starts with the left subtree, then root node, then right subtree. What is a binary tree \u00b6 A tree is called a binary tree if each node has zero, one or two child nodes. It is a family of data structure (BST, Heap tree, AVL, Red-Black, Syntax tree, Huffman cofing tree etc.) Used to solve specific problems like: - Huffman coding - Heap (priority queue) - Expression parsing Types of binary trees \u00b6 Strict binary tree - if each node has either 2 children or none Full binary tree - if each non leaf node has 2 children and all leaf nodes are at same level Complete binary tree - if all levels are completely filled except possibly the last level and the last level has all keys as left as possible. Common operations \u00b6 Creating a tree Inserting a node Deleting a node Searching for a value Traversal of all nodes Deletion of the tree","title":"What is a tree"},{"location":"Data%20Structures%20And%20Algorithms/08-binary-tree/01-what-is-a-tree/#what-is-a-tree","text":"A tree is a data structure that is used to represent data in a hierarchical form. Every node has 2 components - data that it holds and reference to it's parent. At the very top it has a root node with a left sub-category and right sub-category under it.","title":"What is a tree"},{"location":"Data%20Structures%20And%20Algorithms/08-binary-tree/01-what-is-a-tree/#tree-terminology","text":"- Root - node with no parent - Edge - link from parent to child - Leaf - node with no children - Sibling - children of the same parent - Ancestor - parent, it's parents ... of a given node - Depth - length of the path from root to node - Height - Length of the path from node to the deepest node - Predecessor - Immediate previous node inorder traversal of the binary tree - Successor - immediate next node inorder traversal of the binary tree Predecessor example: Starts with the left subtree, then root node, then right subtree.","title":"Tree terminology"},{"location":"Data%20Structures%20And%20Algorithms/08-binary-tree/01-what-is-a-tree/#what-is-a-binary-tree","text":"A tree is called a binary tree if each node has zero, one or two child nodes. It is a family of data structure (BST, Heap tree, AVL, Red-Black, Syntax tree, Huffman cofing tree etc.) Used to solve specific problems like: - Huffman coding - Heap (priority queue) - Expression parsing","title":"What is a binary tree"},{"location":"Data%20Structures%20And%20Algorithms/08-binary-tree/01-what-is-a-tree/#types-of-binary-trees","text":"Strict binary tree - if each node has either 2 children or none Full binary tree - if each non leaf node has 2 children and all leaf nodes are at same level Complete binary tree - if all levels are completely filled except possibly the last level and the last level has all keys as left as possible.","title":"Types of binary trees"},{"location":"Data%20Structures%20And%20Algorithms/08-binary-tree/01-what-is-a-tree/#common-operations","text":"Creating a tree Inserting a node Deleting a node Searching for a value Traversal of all nodes Deletion of the tree","title":"Common operations"},{"location":"Data%20Structures%20And%20Algorithms/08-binary-tree/02-how-tree-is-represented-in-code/","text":"How tree is represented in code \u00b6 We can implement trees by using arrays or linked lists. At the logical level, a tree looks like this: At the LinkedList implementation, it looks like this: When implemented with array, a tree will look like this: Left child - cell[2x] Right child - cell[2x+1]","title":"How tree is represented in code"},{"location":"Data%20Structures%20And%20Algorithms/08-binary-tree/02-how-tree-is-represented-in-code/#how-tree-is-represented-in-code","text":"We can implement trees by using arrays or linked lists. At the logical level, a tree looks like this: At the LinkedList implementation, it looks like this: When implemented with array, a tree will look like this: Left child - cell[2x] Right child - cell[2x+1]","title":"How tree is represented in code"},{"location":"Data%20Structures%20And%20Algorithms/08-binary-tree/03-binary-tree-linked-list/","text":"Implementing binary tree with a Linked List \u00b6 createBinaryTree() Create an object of Binary Tree class Time complexity - O(1) Space complexity - O(1) preOrderTraversal(root) ---------------- T(n) if(root == null) -------------- O(1) return error -------------- O(1) print root -------------------- O(1) preOrderTraversal(root.left) -- T(n/2) preOrderTraversal(root.right) - T(n/2) Time complexity - O(n) Space complexity - O(n) inOrderTraversal(root) ---------------- T(n) if root == null -------------- O(1) return error ------------- O(1) inOrderTraversal(root.left) -- T(n/2) print root ------------------- O(1) inOrderTraversal(root.right) - T(n/2) Time complexity - O(n) Space complexity - O(n) postOrderTraversal(root) ---------------- T(n) if root == null ---------------- O(1) return error --------------- O(1) postOrderTraversal(root.left) -- T(n/2) postOrderTraversal(root.right) - T(n/2) print root --------------------- O(1) Time complexity - O(n) Space complexity - O(n) levelOrderTraversal(root) create a queue(Q) -------------------- O(1) enqueue(root) ------------------------ O(1) while(queue is not empty) ------------ O(n) enqueue child of first element --- O(1) dequeue and print ---------------- O(1) Time complexity - O(n) Space complexity - O(n) searchTree(value) if root == null ----------------- O(1) return error ---------------- O(1) do a level order traversal ------ O(n) if value found -------------- O(1) return true ------------- O(1) return false -------------------- O(1) Time complexity - O(n) Space complexity - O(n) insertNode(): if root is blank insert new node at root else do a level order traversal and find the first blank space insert that blank space Time complexity - O(n) Space complexity - O(n) deleteNode() search for the node to be deleted find deepest node in the tree copy deepest node's data in current node delete deepest node Time complexity - O(n) Space complexity - O(n) DeleteBinaryTree() root = null Time complexity - O(1) Space complexity - O(1) Time and space complexity \u00b6 Time complexity Space complexity create tree O(1) O(n) insert value O(n) O(n) delete value O(n) O(n) search O(n) O(n) traverse O(n) O(n) delete tree O(1) O(1)","title":"Implementing binary tree with a Linked List"},{"location":"Data%20Structures%20And%20Algorithms/08-binary-tree/03-binary-tree-linked-list/#implementing-binary-tree-with-a-linked-list","text":"createBinaryTree() Create an object of Binary Tree class Time complexity - O(1) Space complexity - O(1) preOrderTraversal(root) ---------------- T(n) if(root == null) -------------- O(1) return error -------------- O(1) print root -------------------- O(1) preOrderTraversal(root.left) -- T(n/2) preOrderTraversal(root.right) - T(n/2) Time complexity - O(n) Space complexity - O(n) inOrderTraversal(root) ---------------- T(n) if root == null -------------- O(1) return error ------------- O(1) inOrderTraversal(root.left) -- T(n/2) print root ------------------- O(1) inOrderTraversal(root.right) - T(n/2) Time complexity - O(n) Space complexity - O(n) postOrderTraversal(root) ---------------- T(n) if root == null ---------------- O(1) return error --------------- O(1) postOrderTraversal(root.left) -- T(n/2) postOrderTraversal(root.right) - T(n/2) print root --------------------- O(1) Time complexity - O(n) Space complexity - O(n) levelOrderTraversal(root) create a queue(Q) -------------------- O(1) enqueue(root) ------------------------ O(1) while(queue is not empty) ------------ O(n) enqueue child of first element --- O(1) dequeue and print ---------------- O(1) Time complexity - O(n) Space complexity - O(n) searchTree(value) if root == null ----------------- O(1) return error ---------------- O(1) do a level order traversal ------ O(n) if value found -------------- O(1) return true ------------- O(1) return false -------------------- O(1) Time complexity - O(n) Space complexity - O(n) insertNode(): if root is blank insert new node at root else do a level order traversal and find the first blank space insert that blank space Time complexity - O(n) Space complexity - O(n) deleteNode() search for the node to be deleted find deepest node in the tree copy deepest node's data in current node delete deepest node Time complexity - O(n) Space complexity - O(n) DeleteBinaryTree() root = null Time complexity - O(1) Space complexity - O(1)","title":"Implementing binary tree with a Linked List"},{"location":"Data%20Structures%20And%20Algorithms/08-binary-tree/03-binary-tree-linked-list/#time-and-space-complexity","text":"Time complexity Space complexity create tree O(1) O(n) insert value O(n) O(n) delete value O(n) O(n) search O(n) O(n) traverse O(n) O(n) delete tree O(1) O(1)","title":"Time and space complexity"},{"location":"Data%20Structures%20And%20Algorithms/08-binary-tree/04-binary-tree-array/","text":"Binary tree with array \u00b6 createBinaryTree(size) create a blank array of `size` update lastUsedIndex to 0 Time complexity - O(1) Space complexity - O(n) insertValueInBinaryTree() if Tree is full return error else insert value in first unused cell of array update lastUsedIndex Time complexity - O(1) Space complexity - O(1) searchValueInBinaryTree() traverse the entire array from 1 to lastUsedIndex if value found return true return false Time complexity - O(n) Space complexity - O(1) inOrderTraversal(index) if index > lastUsedIndex return inOrderTraversal(index * 2) print current_index.value inOrderTraversal(index * 2 + 1) Time complexity - O(n) Space complexity - O(n) --- each recursion call is pushed to stack preOrderTraversal(index) if index > lastUsedIndex return print current_index.value preOrderTraversal(index * 2) preOrderTraversal(index * 2 + 1) Time complexity - O(n) Space complexity - O(n) --- each recursion call is pushed to stack postOrderTraversal(index) if index > lastUsedIndex return postOrderTraversal(index * 2) postOrderTraversal(index * 2 + 1) print current_index.value Time complexity - O(n) Space complexity - O(n) --- each recursion call is pushed to stack levelOrderTraversal() loop 1 to lastUsedIndex print current_index.value Time complexity - O(n) Space complexity - O(1) deleteNodeFromBinaryTree() search for desired value in array ------- O(n) if value found replace cell with last cell and update lastUpdatedIndex return error Time complexity - O(n) Space complexity - O(1) deleteBinaryTree() set array as null Time complexity - O(1) Space complexity - O(1) Time and space complexity \u00b6 Time complexity Space complexity create tree O(1) O(n) insert value O(1) O(1) delete value O(n) O(1) search O(n) O(1) traverse O(n) O(1) delete tree O(1) O(1)","title":"Binary tree with array"},{"location":"Data%20Structures%20And%20Algorithms/08-binary-tree/04-binary-tree-array/#binary-tree-with-array","text":"createBinaryTree(size) create a blank array of `size` update lastUsedIndex to 0 Time complexity - O(1) Space complexity - O(n) insertValueInBinaryTree() if Tree is full return error else insert value in first unused cell of array update lastUsedIndex Time complexity - O(1) Space complexity - O(1) searchValueInBinaryTree() traverse the entire array from 1 to lastUsedIndex if value found return true return false Time complexity - O(n) Space complexity - O(1) inOrderTraversal(index) if index > lastUsedIndex return inOrderTraversal(index * 2) print current_index.value inOrderTraversal(index * 2 + 1) Time complexity - O(n) Space complexity - O(n) --- each recursion call is pushed to stack preOrderTraversal(index) if index > lastUsedIndex return print current_index.value preOrderTraversal(index * 2) preOrderTraversal(index * 2 + 1) Time complexity - O(n) Space complexity - O(n) --- each recursion call is pushed to stack postOrderTraversal(index) if index > lastUsedIndex return postOrderTraversal(index * 2) postOrderTraversal(index * 2 + 1) print current_index.value Time complexity - O(n) Space complexity - O(n) --- each recursion call is pushed to stack levelOrderTraversal() loop 1 to lastUsedIndex print current_index.value Time complexity - O(n) Space complexity - O(1) deleteNodeFromBinaryTree() search for desired value in array ------- O(n) if value found replace cell with last cell and update lastUpdatedIndex return error Time complexity - O(n) Space complexity - O(1) deleteBinaryTree() set array as null Time complexity - O(1) Space complexity - O(1)","title":"Binary tree with array"},{"location":"Data%20Structures%20And%20Algorithms/08-binary-tree/04-binary-tree-array/#time-and-space-complexity","text":"Time complexity Space complexity create tree O(1) O(n) insert value O(1) O(1) delete value O(n) O(1) search O(n) O(1) traverse O(n) O(1) delete tree O(1) O(1)","title":"Time and space complexity"},{"location":"Data%20Structures%20And%20Algorithms/08-binary-tree/05-time-and-space-complexity-array-vs-linked-list/","text":"Time and space complexity on array vs linked list \u00b6 Time complexity (array) Space complexity (array) Time complexity (LL) Space complexity (LL) create tree O(1) O(n) O(1) O(1) insert O(1) O(1) O(n) O(n) delete O(n) O(1) O(n) O(n) search O(n) O(1) O(n) O(n) traverse O(n) O(1) O(n) O(n) delete O(1) O(1) O(1) O(1) space efficient? NO YES In this case where neither of the implementations are better. Using arrays is faster, but it has a limited size. Where LinkedLists are more space efficient but is slower.","title":"Time and space complexity on array vs linked list"},{"location":"Data%20Structures%20And%20Algorithms/08-binary-tree/05-time-and-space-complexity-array-vs-linked-list/#time-and-space-complexity-on-array-vs-linked-list","text":"Time complexity (array) Space complexity (array) Time complexity (LL) Space complexity (LL) create tree O(1) O(n) O(1) O(1) insert O(1) O(1) O(n) O(n) delete O(n) O(1) O(n) O(n) search O(n) O(1) O(n) O(n) traverse O(n) O(1) O(n) O(n) delete O(1) O(1) O(1) O(1) space efficient? NO YES In this case where neither of the implementations are better. Using arrays is faster, but it has a limited size. Where LinkedLists are more space efficient but is slower.","title":"Time and space complexity on array vs linked list"},{"location":"Data%20Structures%20And%20Algorithms/09-binary-search-tree/01-what-is-a-binary-search-tree/","text":"What is a binary search tree \u00b6 A binary search tree is basically a binary tree that follows additional rules: - The left sub-tree of a node has a key less tan or equal to it's parent node's key. - The right sub-tree of a node has a key greater to it's parent node's key. Example: Common operations of BST \u00b6 Creation of BST Searching for a value Traversal of all nodes Insertion of a node Deletion of a node Deletion of BST","title":"What is a binary search tree"},{"location":"Data%20Structures%20And%20Algorithms/09-binary-search-tree/01-what-is-a-binary-search-tree/#what-is-a-binary-search-tree","text":"A binary search tree is basically a binary tree that follows additional rules: - The left sub-tree of a node has a key less tan or equal to it's parent node's key. - The right sub-tree of a node has a key greater to it's parent node's key. Example:","title":"What is a binary search tree"},{"location":"Data%20Structures%20And%20Algorithms/09-binary-search-tree/01-what-is-a-binary-search-tree/#common-operations-of-bst","text":"Creation of BST Searching for a value Traversal of all nodes Insertion of a node Deletion of a node Deletion of BST","title":"Common operations of BST"},{"location":"Data%20Structures%20And%20Algorithms/09-binary-search-tree/02-bst-operations/","text":"Binary search tree operations \u00b6 createBST() initialize root with null Time complexity - O(1) Space complexity - O(1) searchBST(root, value): ----------------- T(n) if root == null --------------------- O(1) return null --------------------- O(1) else if root == value --------------- O(1) return root --------------------- O(1) else if value < root ---------------- O(1) searchBST(root.left, value) ----- T(n/2) else if value > root ---------------- O(1) searchBST(root.right, value) ---- T(n/2) Time complexity - O(log n) Space complexity - O(log n) preOrderTraversal(root) ----------------- T(n) if root == null --------------------- O(1) return error -------------------- O(1) print root -------------------------- O(1) preOrderTraversal(root.left) -------- T(n/2) preOrderTraversal(root.right) ------- T(n/2) Time complexity - O(n) Space complexity - O(n) inOrderTraversal(root) ----------------- T(n) if root == null --------------------- O(1) return error -------------------- O(1) inOrderTraversal(root.left) -------- T(n/2) print root -------------------------- O(1) inOrderTraversal(root.right) ------- T(n/2) Time complexity - O(n) Space complexity - O(n) postOrderTraversal(root) ----------------- T(n) if root == null --------------------- O(1) return error -------------------- O(1) postOrderTraversal(root.left) -------- T(n/2) postOrderTraversal(root.right) ------- T(n/2) print root -------------------------- O(1) Time complexity - O(n) Space complexity - O(n) levelOrderTraversal(root) create a queue(Q) enqueue(root) while(queue not empty) dequeue() and print enqueue the child of dequeued element Time complexity - O(n) Space complexity - O(n) insertBST(currentNode, valueToInsert) if currentNode == null create a node, insert valueToInsert in it else if (valueToInsert <= currentNode.value) currentNode.left = insertBST(currentNode.left, valueToInsert) else currentNode.right = insertBST(currentNode.right, valueToInsert) return currentNode Time complexity - O(log n) Space complexity - O(log n) deleteBST(root, valueToBeDeleted) if root == null return null if valueToBeDeleted < root.value root.left = deleteBST(root.left, valueToBeDeleted) else if valueToBeDeleted > root.value root.right = deleteBST(root.right, valueToBeDeleted) else // if current node is the node to be deleted if root have both children, then find minimum element from right subtree replace current node with minimum node from right subtree and delete minimum node from right else if nodeToBeDeleted has only right child root = root.right else // if nodeToBeDeleted does not have children root = null return root Time complexity - O(log n) Space complexity - O(log n) deleteBST() root = null Time complexity - O(1) Space complexity - O(1) Time and space complexity \u00b6 Time complexity Space complexity create tree O(1) O(1) insert value O(log n) O(log n) delete value O(n) O(n) search O(log n) O(log n) traverse O(log n) O(log n) delete tree O(1) O(1)","title":"Binary search tree operations"},{"location":"Data%20Structures%20And%20Algorithms/09-binary-search-tree/02-bst-operations/#binary-search-tree-operations","text":"createBST() initialize root with null Time complexity - O(1) Space complexity - O(1) searchBST(root, value): ----------------- T(n) if root == null --------------------- O(1) return null --------------------- O(1) else if root == value --------------- O(1) return root --------------------- O(1) else if value < root ---------------- O(1) searchBST(root.left, value) ----- T(n/2) else if value > root ---------------- O(1) searchBST(root.right, value) ---- T(n/2) Time complexity - O(log n) Space complexity - O(log n) preOrderTraversal(root) ----------------- T(n) if root == null --------------------- O(1) return error -------------------- O(1) print root -------------------------- O(1) preOrderTraversal(root.left) -------- T(n/2) preOrderTraversal(root.right) ------- T(n/2) Time complexity - O(n) Space complexity - O(n) inOrderTraversal(root) ----------------- T(n) if root == null --------------------- O(1) return error -------------------- O(1) inOrderTraversal(root.left) -------- T(n/2) print root -------------------------- O(1) inOrderTraversal(root.right) ------- T(n/2) Time complexity - O(n) Space complexity - O(n) postOrderTraversal(root) ----------------- T(n) if root == null --------------------- O(1) return error -------------------- O(1) postOrderTraversal(root.left) -------- T(n/2) postOrderTraversal(root.right) ------- T(n/2) print root -------------------------- O(1) Time complexity - O(n) Space complexity - O(n) levelOrderTraversal(root) create a queue(Q) enqueue(root) while(queue not empty) dequeue() and print enqueue the child of dequeued element Time complexity - O(n) Space complexity - O(n) insertBST(currentNode, valueToInsert) if currentNode == null create a node, insert valueToInsert in it else if (valueToInsert <= currentNode.value) currentNode.left = insertBST(currentNode.left, valueToInsert) else currentNode.right = insertBST(currentNode.right, valueToInsert) return currentNode Time complexity - O(log n) Space complexity - O(log n) deleteBST(root, valueToBeDeleted) if root == null return null if valueToBeDeleted < root.value root.left = deleteBST(root.left, valueToBeDeleted) else if valueToBeDeleted > root.value root.right = deleteBST(root.right, valueToBeDeleted) else // if current node is the node to be deleted if root have both children, then find minimum element from right subtree replace current node with minimum node from right subtree and delete minimum node from right else if nodeToBeDeleted has only right child root = root.right else // if nodeToBeDeleted does not have children root = null return root Time complexity - O(log n) Space complexity - O(log n) deleteBST() root = null Time complexity - O(1) Space complexity - O(1)","title":"Binary search tree operations"},{"location":"Data%20Structures%20And%20Algorithms/09-binary-search-tree/02-bst-operations/#time-and-space-complexity","text":"Time complexity Space complexity create tree O(1) O(1) insert value O(log n) O(log n) delete value O(n) O(n) search O(log n) O(log n) traverse O(log n) O(log n) delete tree O(1) O(1)","title":"Time and space complexity"},{"location":"Data%20Structures%20And%20Algorithms/10-AVL-tree/01-what-is-AVL-tree/","text":"What is an AVL tree \u00b6 Why AVL tree? \u00b6 When we would need to insert numbers like 10,20,30,40,50,60,70 in BST, only the the right side will be occupied of each child. When searching for 60, it will traverse to the right and find the node we are looking for, which is fine. However, the problem is that we traversed 6 objects until we found it. If we would represent the tree in a more balanced way, it would save time. This tree would take us only 2 iterations. So, the lesson is that depending on incoming data, a BST can get skewed and hence it's performance starts going down. Instead of O(log n) it can go up to O(n). The AVL tree attempts to solve this problem of skewing by introducting concept called Rotation . What is AVL tree? \u00b6 An AVL tree is a balanced Binary Search Tree where the height of immediate subrtrees of any node differs by at most one (also called balance factor). If at any time heights differ by more than one, rebalancing is done to restore this property (called rotation). Empty height is always considered -1. Common operations of AVL \u00b6 Create an AVL tree Search a node Traverse all nodes Insert a node Delete a node Delete AVL tree","title":"What is an AVL tree"},{"location":"Data%20Structures%20And%20Algorithms/10-AVL-tree/01-what-is-AVL-tree/#what-is-an-avl-tree","text":"","title":"What is an AVL tree"},{"location":"Data%20Structures%20And%20Algorithms/10-AVL-tree/01-what-is-AVL-tree/#why-avl-tree","text":"When we would need to insert numbers like 10,20,30,40,50,60,70 in BST, only the the right side will be occupied of each child. When searching for 60, it will traverse to the right and find the node we are looking for, which is fine. However, the problem is that we traversed 6 objects until we found it. If we would represent the tree in a more balanced way, it would save time. This tree would take us only 2 iterations. So, the lesson is that depending on incoming data, a BST can get skewed and hence it's performance starts going down. Instead of O(log n) it can go up to O(n). The AVL tree attempts to solve this problem of skewing by introducting concept called Rotation .","title":"Why AVL tree?"},{"location":"Data%20Structures%20And%20Algorithms/10-AVL-tree/01-what-is-AVL-tree/#what-is-avl-tree","text":"An AVL tree is a balanced Binary Search Tree where the height of immediate subrtrees of any node differs by at most one (also called balance factor). If at any time heights differ by more than one, rebalancing is done to restore this property (called rotation). Empty height is always considered -1.","title":"What is AVL tree?"},{"location":"Data%20Structures%20And%20Algorithms/10-AVL-tree/01-what-is-AVL-tree/#common-operations-of-avl","text":"Create an AVL tree Search a node Traverse all nodes Insert a node Delete a node Delete AVL tree","title":"Common operations of AVL"},{"location":"Data%20Structures%20And%20Algorithms/10-AVL-tree/02-AVL-implementation/","text":"AVL implementation \u00b6 createAVL() root = null Time complexity - O(1) Space complexity - O(1) searchAVL(root, value) if( root is null ) return null else if root == value return root else if value < root searchAVL(root.left, value) else if value > root searchAVL(root.right, value) Time complexity - O(log n) Space complexity - O(log n) preOrderTraversal(root) if root == null return error print root preOrderTraversal(root.left) preOrderTraversal(root.right) Time complexity - O(n) Space complexity - O(n) inOrderTraversal(root) if root == null return error inOrderTraversal(root.left) print root inOrderTraversal(root.right) Time complexity - O(n) Space complexity - O(n) postOrderTraversal(root) if root == null return error postOrderTraversal(root.left) postOrderTraversal(root.right) print root Time complexity - O(n) Space complexity - O(n) levelOrderTraversal(root) create a queue enqueue(root) while(queue not empty) dequeue and print enqueue children Time complexity - O(n) Space complexity - O(n) Now, the insertion and deletion operations are a little bit more complicated in case of AVL trees. When we want to insert a node in AVL tree, there can be 2 cases: 1. When rotation is not required 2. When rotation is required (LL, LR, RR, RL) We can determine whether we need rotation by checking if any nodes left tree and right tree height differs by more than 1 level. When we want to do a rotation, there are there can be 4 conditions. 1. Left Left condition \u00b6 Disbalanced node's grandchild is located on left, left subtree. We do a right rotation: rightRotate(currentDisbalancedNode) newRoot = currentDisbalancedNode.left currentDisbalancedNode.left = currentDisbalancedNode.left.right newRoot.right = currentDisbalancedNode currentDisbalancedNode.height = calculateHeight(currentDisbalancedNode) newRoot.height = calculateHeight(newRoot) Time complexity - O(1) Space complexity - O(1) 2. Left Right condition (LR) \u00b6 Disbalanced node's grandchild is located in Left-Right position. We do a left rotation, but on the disbalanced node's child position. Then we can do a right rotation . leftRotate(currentDisbalancedNodesLeftChild) newRoot = currentDisbalancedNodesLeftChild.right currentDisbalancedNodesLeftChild.right = currentDisbalancedNodesLeftChild.right.left newRoot.left = currentDisbalancedNodesLeftChild currentDisbalancedNodesLeftChild.height = calculateHeight(currentDisbalancedNodesLeftChild) newRoot.height = calculateHeight(newRoot) return newRoot Time complexity - O(1) Space complexity - O(1) 3. Right Right contidition (RR) \u00b6 It is exactly opposite of the LL condition. We apply leftRotate, but on the disbalancedNode instead of it's left child like in the LR condifition. leftRotate(currentDisbalancedNode) newRoot = currentDisbalancedNode.right currentDisbalancedNode.right = currentDisbalancedNode.right.left newRoot.left = currrentDisbalancedNode CurrentDisbalancedNode.height = calculateHeight(currentDisbalancedNode) newRoot.height = calculateHeight(newRoot) return newRoot Time complexity - O(1) Space complexity - O(1) 4. Right Left condition (RL) \u00b6 We do a right rotation on disbalanced node's child. Then left rotation on the disbalanced node. rightRotate(currentDisbalancedNodesRightChild) newRoot = currentDisbalancedNodesRightChild.left currentDisbalancedNodesRightChild = currentDisbalancedNodesRightChild.left.right newRoot.right = currentDisbalancednodesRightChild currentDisbalancedNodesRightChild.height = calculateHeight(currentDisbalancedNodesRightChild) newRoot.height = calculateHeight(newRoot) return newRoot Time complexity - O(1) Space complexity - O(1) leftRotate(currentDisbalancedNode) newRoot = currentDisbalancedNode.right currentDisbalancedNode.right = currentDisbalancedNode.right.left newRoot.left = currrentDisbalancedNode CurrentDisbalancedNode.height = calculateHeight(currentDisbalancedNode) newRoot.height = calculateHeight(newRoot) return newRoot Time complexity - O(1) Space complexity - O(1) Insert, end-to-end case \u00b6 We are going to construct an example where all of the previously mentioned cases will be called. Node insert( Node root, int data ) if(root == null) return new Node(data) // BST condition else if (data <= root.data ) root.left = insert(root.left, data) // BST condition else root.right = insert(root.right, data) // BST condition int balance = height(root.left) - height(root.right) if(balance > 1) // if left subree is overloaded if( height root.left.left >= height(root.left.right) ) RightRotation(root) // LL condition else // LR condition LeftRotation(root.left) RightRotation(root) else if (balance < -1) // if right subtree is overloaded if height(root.right.right) >= height(root.right.left) LeftRotation(root) // PR condition else // RL condition return RightRotation(root.right) LeftRotation(root) root.height = max(root.left, root.right) + 1 return root Time complexity - O(log n) Space complexity - O(log n) Deletion of a node \u00b6 There can be 3 cases: 1. Tree does not exist 2. Rotation is not required (BST conditions) 3. Rotation is required deleteNode(currentNode, valueToBeDeleted): if(currentNode === null) return null if(valueToBeDeleted < currentNode.value) currentNode.left = deleteNode(currentNode.left, valueToBeDeleted) else if (valueToBeDeleted > currentNode.value) currentNode.right = deleteNode(currentNode.right, valueToBeDeleted) else if current node have both children then find minimum element from right subtree (case 3) retplace current node with minimum node from right subree and delete minimum node from right else if nodeToBeDeleted has only left child (case 2) currentNode = currentNode.left else if nodeToBeDeleted has only right child (case 2) currentNode = currentNode.right else // if nodeToBeDeleted do not have any children (case 1) currentNode = null int balance = checkBalance(currentNode.left, currentNode.right) if(balance > 1) if(checkBalance(currentNode.left.left, currentNode.left.right) > 0) currentNode = rightRotate(currentNode) // LL condition else currentNode.left = leftRotate(currentNode.left) // LR currentNode = rightRotate(currentNode) else if balance < -1 if(checkBalance(currentNode.right.right, currentNode.right.left) > 0) currentNode = leftRotate(currentNode) // RR else currentNode.right = rightRotate(currentNode.right) // RL currentNode = leftRotate(currentNode) if(currentNode.left !== null) currentNode.left.setHeight(calculateHeight(currentNode.left) if(currentNode.right !== null) currentNode.right.setHeight(calculateHeight(currentNode.right) currentNode.setHeight(calculateHeight(currentNode)); return currentNode Time complexity - O(log n) Space complexity - O(log n) Deletion of entire AVL tree \u00b6 delete() root = null Time complexity - O(1) Space complexity - O(1) When we set the root to null, automatically it's children are orphaned and garbage collector deletes the entire tree. Time & Space complexity in AVL tree \u00b6 Time complexity Space complexity create tree O(1) O(1) insert value O(log n) O(log n) delete value O(log n) O(log n) search O(log n) O(log n) traverse O(n) O(n) delete tree O(1) O(1)","title":"AVL implementation"},{"location":"Data%20Structures%20And%20Algorithms/10-AVL-tree/02-AVL-implementation/#avl-implementation","text":"createAVL() root = null Time complexity - O(1) Space complexity - O(1) searchAVL(root, value) if( root is null ) return null else if root == value return root else if value < root searchAVL(root.left, value) else if value > root searchAVL(root.right, value) Time complexity - O(log n) Space complexity - O(log n) preOrderTraversal(root) if root == null return error print root preOrderTraversal(root.left) preOrderTraversal(root.right) Time complexity - O(n) Space complexity - O(n) inOrderTraversal(root) if root == null return error inOrderTraversal(root.left) print root inOrderTraversal(root.right) Time complexity - O(n) Space complexity - O(n) postOrderTraversal(root) if root == null return error postOrderTraversal(root.left) postOrderTraversal(root.right) print root Time complexity - O(n) Space complexity - O(n) levelOrderTraversal(root) create a queue enqueue(root) while(queue not empty) dequeue and print enqueue children Time complexity - O(n) Space complexity - O(n) Now, the insertion and deletion operations are a little bit more complicated in case of AVL trees. When we want to insert a node in AVL tree, there can be 2 cases: 1. When rotation is not required 2. When rotation is required (LL, LR, RR, RL) We can determine whether we need rotation by checking if any nodes left tree and right tree height differs by more than 1 level. When we want to do a rotation, there are there can be 4 conditions.","title":"AVL implementation"},{"location":"Data%20Structures%20And%20Algorithms/10-AVL-tree/02-AVL-implementation/#1-left-left-condition","text":"Disbalanced node's grandchild is located on left, left subtree. We do a right rotation: rightRotate(currentDisbalancedNode) newRoot = currentDisbalancedNode.left currentDisbalancedNode.left = currentDisbalancedNode.left.right newRoot.right = currentDisbalancedNode currentDisbalancedNode.height = calculateHeight(currentDisbalancedNode) newRoot.height = calculateHeight(newRoot) Time complexity - O(1) Space complexity - O(1)","title":"1. Left Left condition"},{"location":"Data%20Structures%20And%20Algorithms/10-AVL-tree/02-AVL-implementation/#2-left-right-condition-lr","text":"Disbalanced node's grandchild is located in Left-Right position. We do a left rotation, but on the disbalanced node's child position. Then we can do a right rotation . leftRotate(currentDisbalancedNodesLeftChild) newRoot = currentDisbalancedNodesLeftChild.right currentDisbalancedNodesLeftChild.right = currentDisbalancedNodesLeftChild.right.left newRoot.left = currentDisbalancedNodesLeftChild currentDisbalancedNodesLeftChild.height = calculateHeight(currentDisbalancedNodesLeftChild) newRoot.height = calculateHeight(newRoot) return newRoot Time complexity - O(1) Space complexity - O(1)","title":"2. Left Right condition (LR)"},{"location":"Data%20Structures%20And%20Algorithms/10-AVL-tree/02-AVL-implementation/#3-right-right-contidition-rr","text":"It is exactly opposite of the LL condition. We apply leftRotate, but on the disbalancedNode instead of it's left child like in the LR condifition. leftRotate(currentDisbalancedNode) newRoot = currentDisbalancedNode.right currentDisbalancedNode.right = currentDisbalancedNode.right.left newRoot.left = currrentDisbalancedNode CurrentDisbalancedNode.height = calculateHeight(currentDisbalancedNode) newRoot.height = calculateHeight(newRoot) return newRoot Time complexity - O(1) Space complexity - O(1)","title":"3. Right Right contidition (RR)"},{"location":"Data%20Structures%20And%20Algorithms/10-AVL-tree/02-AVL-implementation/#4-right-left-condition-rl","text":"We do a right rotation on disbalanced node's child. Then left rotation on the disbalanced node. rightRotate(currentDisbalancedNodesRightChild) newRoot = currentDisbalancedNodesRightChild.left currentDisbalancedNodesRightChild = currentDisbalancedNodesRightChild.left.right newRoot.right = currentDisbalancednodesRightChild currentDisbalancedNodesRightChild.height = calculateHeight(currentDisbalancedNodesRightChild) newRoot.height = calculateHeight(newRoot) return newRoot Time complexity - O(1) Space complexity - O(1) leftRotate(currentDisbalancedNode) newRoot = currentDisbalancedNode.right currentDisbalancedNode.right = currentDisbalancedNode.right.left newRoot.left = currrentDisbalancedNode CurrentDisbalancedNode.height = calculateHeight(currentDisbalancedNode) newRoot.height = calculateHeight(newRoot) return newRoot Time complexity - O(1) Space complexity - O(1)","title":"4. Right Left condition (RL)"},{"location":"Data%20Structures%20And%20Algorithms/10-AVL-tree/02-AVL-implementation/#insert-end-to-end-case","text":"We are going to construct an example where all of the previously mentioned cases will be called. Node insert( Node root, int data ) if(root == null) return new Node(data) // BST condition else if (data <= root.data ) root.left = insert(root.left, data) // BST condition else root.right = insert(root.right, data) // BST condition int balance = height(root.left) - height(root.right) if(balance > 1) // if left subree is overloaded if( height root.left.left >= height(root.left.right) ) RightRotation(root) // LL condition else // LR condition LeftRotation(root.left) RightRotation(root) else if (balance < -1) // if right subtree is overloaded if height(root.right.right) >= height(root.right.left) LeftRotation(root) // PR condition else // RL condition return RightRotation(root.right) LeftRotation(root) root.height = max(root.left, root.right) + 1 return root Time complexity - O(log n) Space complexity - O(log n)","title":"Insert, end-to-end case"},{"location":"Data%20Structures%20And%20Algorithms/10-AVL-tree/02-AVL-implementation/#deletion-of-a-node","text":"There can be 3 cases: 1. Tree does not exist 2. Rotation is not required (BST conditions) 3. Rotation is required deleteNode(currentNode, valueToBeDeleted): if(currentNode === null) return null if(valueToBeDeleted < currentNode.value) currentNode.left = deleteNode(currentNode.left, valueToBeDeleted) else if (valueToBeDeleted > currentNode.value) currentNode.right = deleteNode(currentNode.right, valueToBeDeleted) else if current node have both children then find minimum element from right subtree (case 3) retplace current node with minimum node from right subree and delete minimum node from right else if nodeToBeDeleted has only left child (case 2) currentNode = currentNode.left else if nodeToBeDeleted has only right child (case 2) currentNode = currentNode.right else // if nodeToBeDeleted do not have any children (case 1) currentNode = null int balance = checkBalance(currentNode.left, currentNode.right) if(balance > 1) if(checkBalance(currentNode.left.left, currentNode.left.right) > 0) currentNode = rightRotate(currentNode) // LL condition else currentNode.left = leftRotate(currentNode.left) // LR currentNode = rightRotate(currentNode) else if balance < -1 if(checkBalance(currentNode.right.right, currentNode.right.left) > 0) currentNode = leftRotate(currentNode) // RR else currentNode.right = rightRotate(currentNode.right) // RL currentNode = leftRotate(currentNode) if(currentNode.left !== null) currentNode.left.setHeight(calculateHeight(currentNode.left) if(currentNode.right !== null) currentNode.right.setHeight(calculateHeight(currentNode.right) currentNode.setHeight(calculateHeight(currentNode)); return currentNode Time complexity - O(log n) Space complexity - O(log n)","title":"Deletion of a node"},{"location":"Data%20Structures%20And%20Algorithms/10-AVL-tree/02-AVL-implementation/#deletion-of-entire-avl-tree","text":"delete() root = null Time complexity - O(1) Space complexity - O(1) When we set the root to null, automatically it's children are orphaned and garbage collector deletes the entire tree.","title":"Deletion of entire AVL tree"},{"location":"Data%20Structures%20And%20Algorithms/10-AVL-tree/02-AVL-implementation/#time-space-complexity-in-avl-tree","text":"Time complexity Space complexity create tree O(1) O(1) insert value O(log n) O(log n) delete value O(log n) O(log n) search O(log n) O(log n) traverse O(n) O(n) delete tree O(1) O(1)","title":"Time &amp; Space complexity in AVL tree"},{"location":"Data%20Structures%20And%20Algorithms/11-binary-heap/01-what-is-a-binary-heap/","text":"What is a binary heap \u00b6 Binary heap is a binary tree, but with some special properties: - Heap property: - value of any given node must be <= value of it's children (min-heap property) - value of any given node must be >= value of it's children (max heap) - Complete tree - All levels are completely filled except possibly the last level and the last level has all keys as left as possible. - This makes binary heap ideal candidate for array implementation. Why learn Binary Heap? \u00b6 There are cases when we want to find min/max number along set of numbers in log(n) time. Also, we want to make sure that inserting additional numbers does not take more than O(log n) time. Possible solutions: 1. Store numbers in a sorted array. But, when we are going to insert a new number, it will take O(n) time to do that. 2. Store the numbers in a linked list, in a sorted manner, but it will also take O(n) time. Types of binary heap \u00b6 There are 2 types of binary heap: Min heap - if the value of each node is less than or equal to value of both it's children Max heap - if the value of each node is more than or equal to value of both it's children Common operations \u00b6 Create Heap - creates a blank Array to be used for storing heap Peek top of heap - returns min/max from heap Extract min/max - extracts min/max from heap, we can extract only this node. Size of heap - returns size of the heap insert value in heap - insert value in heap delete heap - deletes the entire heap","title":"What is a binary heap"},{"location":"Data%20Structures%20And%20Algorithms/11-binary-heap/01-what-is-a-binary-heap/#what-is-a-binary-heap","text":"Binary heap is a binary tree, but with some special properties: - Heap property: - value of any given node must be <= value of it's children (min-heap property) - value of any given node must be >= value of it's children (max heap) - Complete tree - All levels are completely filled except possibly the last level and the last level has all keys as left as possible. - This makes binary heap ideal candidate for array implementation.","title":"What is a binary heap"},{"location":"Data%20Structures%20And%20Algorithms/11-binary-heap/01-what-is-a-binary-heap/#why-learn-binary-heap","text":"There are cases when we want to find min/max number along set of numbers in log(n) time. Also, we want to make sure that inserting additional numbers does not take more than O(log n) time. Possible solutions: 1. Store numbers in a sorted array. But, when we are going to insert a new number, it will take O(n) time to do that. 2. Store the numbers in a linked list, in a sorted manner, but it will also take O(n) time.","title":"Why learn Binary Heap?"},{"location":"Data%20Structures%20And%20Algorithms/11-binary-heap/01-what-is-a-binary-heap/#types-of-binary-heap","text":"There are 2 types of binary heap: Min heap - if the value of each node is less than or equal to value of both it's children Max heap - if the value of each node is more than or equal to value of both it's children","title":"Types of binary heap"},{"location":"Data%20Structures%20And%20Algorithms/11-binary-heap/01-what-is-a-binary-heap/#common-operations","text":"Create Heap - creates a blank Array to be used for storing heap Peek top of heap - returns min/max from heap Extract min/max - extracts min/max from heap, we can extract only this node. Size of heap - returns size of the heap insert value in heap - insert value in heap delete heap - deletes the entire heap","title":"Common operations"},{"location":"Data%20Structures%20And%20Algorithms/11-binary-heap/02-binary-heap-implementation/","text":"Binary heap operations \u00b6 Implementation options \u00b6 Array based Reference / pointer based implementation Array representation \u00b6 At the logical level, the Binary Heap looks like this: The binary heaps looks like this in the array implementation: Left child - cell[2x] Right child - cell[2x+1] createHeap(size) create a blank array of size+1 initialize sizeOfHeap with 0 Time complexity - O(1) Space complexity - O(n) peekOfTop() if tree does not exist return error return first cell of array Time complexity - O(1) Space complexity - O(1) sizeOfHeap() return sizeOfHeap // number of cells used in heap array insertValueInHeap(value) if tree does not exist return error else insert value in first unused cell of array sizeOfHeap++ heapifyBottomToTop(sizeOfHeap) Time complexity - O(log n) Space complexity - O(log n) extractMin() if tree does not exist return error extract 1st cell of array promote last element to first sizeOfHeap-- heapifyTopToBottom(1) Time complexity - O(log n) Space complexity - O(log n) deleteHeap() set array to null Time complexity - O(1) Space complexity - O(1) Why avoid using reference based implementation? \u00b6 When we extract from the heap, we delete the first note, get the last node and bring it to the top. When using references, there is no way we can find out the last element of the tree in O(log n) time.","title":"Binary heap operations"},{"location":"Data%20Structures%20And%20Algorithms/11-binary-heap/02-binary-heap-implementation/#binary-heap-operations","text":"","title":"Binary heap operations"},{"location":"Data%20Structures%20And%20Algorithms/11-binary-heap/02-binary-heap-implementation/#implementation-options","text":"Array based Reference / pointer based implementation","title":"Implementation options"},{"location":"Data%20Structures%20And%20Algorithms/11-binary-heap/02-binary-heap-implementation/#array-representation","text":"At the logical level, the Binary Heap looks like this: The binary heaps looks like this in the array implementation: Left child - cell[2x] Right child - cell[2x+1] createHeap(size) create a blank array of size+1 initialize sizeOfHeap with 0 Time complexity - O(1) Space complexity - O(n) peekOfTop() if tree does not exist return error return first cell of array Time complexity - O(1) Space complexity - O(1) sizeOfHeap() return sizeOfHeap // number of cells used in heap array insertValueInHeap(value) if tree does not exist return error else insert value in first unused cell of array sizeOfHeap++ heapifyBottomToTop(sizeOfHeap) Time complexity - O(log n) Space complexity - O(log n) extractMin() if tree does not exist return error extract 1st cell of array promote last element to first sizeOfHeap-- heapifyTopToBottom(1) Time complexity - O(log n) Space complexity - O(log n) deleteHeap() set array to null Time complexity - O(1) Space complexity - O(1)","title":"Array representation"},{"location":"Data%20Structures%20And%20Algorithms/11-binary-heap/02-binary-heap-implementation/#why-avoid-using-reference-based-implementation","text":"When we extract from the heap, we delete the first note, get the last node and bring it to the top. When using references, there is no way we can find out the last element of the tree in O(log n) time.","title":"Why avoid using reference based implementation?"},{"location":"Data%20Structures%20And%20Algorithms/12-trie/01-what-is-a-trie/","text":"What is a Trie? \u00b6 It is a search tree which is typically used to store/search strings in space/time efficient way. In it, any node can store non repetitive multiple characters Also, every node stores link of next character of the string. Also, every node keeps a track of end of string . Common usages \u00b6 Spelling checker Auto complete string etc Common operations \u00b6 Creating a Trie Inserting a String in Trie Searching a String in Trie Deleting a String from Trie","title":"What is a Trie?"},{"location":"Data%20Structures%20And%20Algorithms/12-trie/01-what-is-a-trie/#what-is-a-trie","text":"It is a search tree which is typically used to store/search strings in space/time efficient way. In it, any node can store non repetitive multiple characters Also, every node stores link of next character of the string. Also, every node keeps a track of end of string .","title":"What is a Trie?"},{"location":"Data%20Structures%20And%20Algorithms/12-trie/01-what-is-a-trie/#common-usages","text":"Spelling checker Auto complete string etc","title":"Common usages"},{"location":"Data%20Structures%20And%20Algorithms/12-trie/01-what-is-a-trie/#common-operations","text":"Creating a Trie Inserting a String in Trie Searching a String in Trie Deleting a String from Trie","title":"Common operations"},{"location":"Data%20Structures%20And%20Algorithms/12-trie/02-trie-implementation/","text":"Implementation of Trie \u00b6 Creating a Trie \u00b6 createTrie() create a blank root node Inserting a string in trie \u00b6 There can be 4 cases: 1. The Trie is blank 2. New String's prefix is common with another strings prefix 3. New Strings prefix is already present as a complete string 4. String to be inserted is already present in the Trie Searching a string in Trie \u00b6 There can be 3 cases: 1. String does not exist in Trie 2. String exists in Trie 3. Current string is a prefix of another String. But this string does not exist in Trie. Delete String from Trie \u00b6 Some other words prefix is same as prefix of this word (BCDE, BCKG) Whe go back from the last node and check if no other string is dependent on it, if not, we delete the node. The current word is a prefix of some other word (BCDE, BCDEF) In such cases we do not delete the string, just update the end of the word marker. Some other word is a prefix of this word (BCDE, BC) No one is dependent on this Word","title":"Implementation of Trie"},{"location":"Data%20Structures%20And%20Algorithms/12-trie/02-trie-implementation/#implementation-of-trie","text":"","title":"Implementation of Trie"},{"location":"Data%20Structures%20And%20Algorithms/12-trie/02-trie-implementation/#creating-a-trie","text":"createTrie() create a blank root node","title":"Creating a Trie"},{"location":"Data%20Structures%20And%20Algorithms/12-trie/02-trie-implementation/#inserting-a-string-in-trie","text":"There can be 4 cases: 1. The Trie is blank 2. New String's prefix is common with another strings prefix 3. New Strings prefix is already present as a complete string 4. String to be inserted is already present in the Trie","title":"Inserting a string in trie"},{"location":"Data%20Structures%20And%20Algorithms/12-trie/02-trie-implementation/#searching-a-string-in-trie","text":"There can be 3 cases: 1. String does not exist in Trie 2. String exists in Trie 3. Current string is a prefix of another String. But this string does not exist in Trie.","title":"Searching a string in Trie"},{"location":"Data%20Structures%20And%20Algorithms/12-trie/02-trie-implementation/#delete-string-from-trie","text":"Some other words prefix is same as prefix of this word (BCDE, BCKG) Whe go back from the last node and check if no other string is dependent on it, if not, we delete the node. The current word is a prefix of some other word (BCDE, BCDEF) In such cases we do not delete the string, just update the end of the word marker. Some other word is a prefix of this word (BCDE, BC) No one is dependent on this Word","title":"Delete String from Trie"},{"location":"Data%20Structures%20And%20Algorithms/13-hashing/01-what-is-hashing/","text":"What is Hashing \u00b6 Hashing is a method of sorting and indexing data. The idea behind hashing is to allow large amounts of data to be indexed using keys commonly created by formulas. Who do we need Hashing? \u00b6 It's time efficient: Data structures Time complexity for search operation Array O(log n) Linked List O(n) Tree O(log n) Hashing O(1) best case / O(n) worst case Some terminologies \u00b6 Hash function - a hash function is any function that can be used to map data of arbitrary size to data of fixed size. Key - input data given by user Hash value - the values returned by a hash function are called hash values, hash codes, digests, or simply hashes. Hash table - it is a data structure which implements an associative array abstract data type, a structure that can map keys to values. Collision - a collision occurs when to different keys produce the same output as the hash value. Characteristics of a good Hash function \u00b6 It distributes hash values uniformly accross the hash table The hash function uses all the input data Types of collision resulution techniques \u00b6 There are two types of collision resolution techniques: Direct Chaining Our hash table is not an array of hash table, it is an array of references as a linked list. When we add a value in the hash table, we add a reference to it's node. When there is another node that should go in the same index of the hash table, we update the next reference in the previously added node to the new node. Open addressing Linear probling We compute a hash function and see that the index in the hash table is already taken. We go to the next index and see if it's open. Quadratic probing In case of quadratic probing, every time we have a collision, we add the collision number squared. First time x + 1^2 Second time x + 2^2 ... Double hashing We have 2 hash functions - one primary, one secondary. We use the primary hash function, whenever a collision is detected, we calculate the secondary hash value and add it to the primary hash value. The result is used as the hash table index. If the collision is still there, we multiply the secondary hash value with the collision number and add it to the primary hash value. PrimaryHashFunction + (1 * secondaryHashFunction()) PrimaryHashFunction + (2 * secondaryHashFunction()) PrimaryHashFunction + (3 * secondaryHashFunction()) ... What happens when the hash table is full? \u00b6 Direct chaining: Situation will never arise Open addressing: Need to create 2x size of current hash table and redo hashing for existing keys. Pros & Cons of collision resolution techniques \u00b6 Direct chaining No fear of exhausting hash table buckets Fear of big linked lists (can effect performance big time) Open addressing Easy implementation Fear of exhausting hash table buckets If input size is known then always use \"open addressing\", else can use any of two. If deletion is very high, we should always go with direct chaining. Practical uses of hashing \u00b6 Password verification File systems - mapping files to a physical location on disk Pros & Cons of hashing \u00b6 Pros: - On an average insertion/deletion/search operation takes O(1) time Cons: - In the worst case insertion/deletion/search might take O(n) time (when hash functions are not good enough).","title":"What is Hashing"},{"location":"Data%20Structures%20And%20Algorithms/13-hashing/01-what-is-hashing/#what-is-hashing","text":"Hashing is a method of sorting and indexing data. The idea behind hashing is to allow large amounts of data to be indexed using keys commonly created by formulas.","title":"What is Hashing"},{"location":"Data%20Structures%20And%20Algorithms/13-hashing/01-what-is-hashing/#who-do-we-need-hashing","text":"It's time efficient: Data structures Time complexity for search operation Array O(log n) Linked List O(n) Tree O(log n) Hashing O(1) best case / O(n) worst case","title":"Who do we need Hashing?"},{"location":"Data%20Structures%20And%20Algorithms/13-hashing/01-what-is-hashing/#some-terminologies","text":"Hash function - a hash function is any function that can be used to map data of arbitrary size to data of fixed size. Key - input data given by user Hash value - the values returned by a hash function are called hash values, hash codes, digests, or simply hashes. Hash table - it is a data structure which implements an associative array abstract data type, a structure that can map keys to values. Collision - a collision occurs when to different keys produce the same output as the hash value.","title":"Some terminologies"},{"location":"Data%20Structures%20And%20Algorithms/13-hashing/01-what-is-hashing/#characteristics-of-a-good-hash-function","text":"It distributes hash values uniformly accross the hash table The hash function uses all the input data","title":"Characteristics of a good Hash function"},{"location":"Data%20Structures%20And%20Algorithms/13-hashing/01-what-is-hashing/#types-of-collision-resulution-techniques","text":"There are two types of collision resolution techniques: Direct Chaining Our hash table is not an array of hash table, it is an array of references as a linked list. When we add a value in the hash table, we add a reference to it's node. When there is another node that should go in the same index of the hash table, we update the next reference in the previously added node to the new node. Open addressing Linear probling We compute a hash function and see that the index in the hash table is already taken. We go to the next index and see if it's open. Quadratic probing In case of quadratic probing, every time we have a collision, we add the collision number squared. First time x + 1^2 Second time x + 2^2 ... Double hashing We have 2 hash functions - one primary, one secondary. We use the primary hash function, whenever a collision is detected, we calculate the secondary hash value and add it to the primary hash value. The result is used as the hash table index. If the collision is still there, we multiply the secondary hash value with the collision number and add it to the primary hash value. PrimaryHashFunction + (1 * secondaryHashFunction()) PrimaryHashFunction + (2 * secondaryHashFunction()) PrimaryHashFunction + (3 * secondaryHashFunction()) ...","title":"Types of collision resulution techniques"},{"location":"Data%20Structures%20And%20Algorithms/13-hashing/01-what-is-hashing/#what-happens-when-the-hash-table-is-full","text":"Direct chaining: Situation will never arise Open addressing: Need to create 2x size of current hash table and redo hashing for existing keys.","title":"What happens when the hash table is full?"},{"location":"Data%20Structures%20And%20Algorithms/13-hashing/01-what-is-hashing/#pros-cons-of-collision-resolution-techniques","text":"Direct chaining No fear of exhausting hash table buckets Fear of big linked lists (can effect performance big time) Open addressing Easy implementation Fear of exhausting hash table buckets If input size is known then always use \"open addressing\", else can use any of two. If deletion is very high, we should always go with direct chaining.","title":"Pros &amp; Cons of collision resolution techniques"},{"location":"Data%20Structures%20And%20Algorithms/13-hashing/01-what-is-hashing/#practical-uses-of-hashing","text":"Password verification File systems - mapping files to a physical location on disk","title":"Practical uses of hashing"},{"location":"Data%20Structures%20And%20Algorithms/13-hashing/01-what-is-hashing/#pros-cons-of-hashing","text":"Pros: - On an average insertion/deletion/search operation takes O(1) time Cons: - In the worst case insertion/deletion/search might take O(n) time (when hash functions are not good enough).","title":"Pros &amp; Cons of hashing"},{"location":"Data%20Structures%20And%20Algorithms/14-sorting/01-what-and-why-sorting/","text":"What is sorting? \u00b6 Sorting refers to arranging data in a particular format: either ascending or descending order. Types of sorting \u00b6 Every sorting algorithm can be divided into following categories: On the basis of space used in-place out-of-place On the basis of stability stable un-stable In-place vs Out-place sorting \u00b6 In-place Sort: Sorting algorithms which does not require any extra space for sorting. Example - bubble sort. Out-of-place Sort: Requires extra space for sorting Example - Merge Sort Stable vs Unstable sorting \u00b6 Stable Sort: If a Sorting algorithm after sorting the contents does not change the sequence of similar content in which they appear, is called stable sorting. Example - insertion sort Unstable Sort: If a sorting algorithm after sorting the contents, changes the sequence of similar content in which they appearm it is called unstable sort. Example: quick sort Sorting terminologies \u00b6 Increasing order - if successive element is greater than previous one. Decresing order - if successive element is less than the current one. Non-increasing order - if successive element is less than or equal to it's previous element in the sequence. This order occurs when the sequence contains duplicate values. Non-decreasing order - if the successive element is greater than or equal to it's previous element in the sequence. This order occurs when there are duplicate values.","title":"What is sorting?"},{"location":"Data%20Structures%20And%20Algorithms/14-sorting/01-what-and-why-sorting/#what-is-sorting","text":"Sorting refers to arranging data in a particular format: either ascending or descending order.","title":"What is sorting?"},{"location":"Data%20Structures%20And%20Algorithms/14-sorting/01-what-and-why-sorting/#types-of-sorting","text":"Every sorting algorithm can be divided into following categories: On the basis of space used in-place out-of-place On the basis of stability stable un-stable","title":"Types of sorting"},{"location":"Data%20Structures%20And%20Algorithms/14-sorting/01-what-and-why-sorting/#in-place-vs-out-place-sorting","text":"In-place Sort: Sorting algorithms which does not require any extra space for sorting. Example - bubble sort. Out-of-place Sort: Requires extra space for sorting Example - Merge Sort","title":"In-place vs Out-place sorting"},{"location":"Data%20Structures%20And%20Algorithms/14-sorting/01-what-and-why-sorting/#stable-vs-unstable-sorting","text":"Stable Sort: If a Sorting algorithm after sorting the contents does not change the sequence of similar content in which they appear, is called stable sorting. Example - insertion sort Unstable Sort: If a sorting algorithm after sorting the contents, changes the sequence of similar content in which they appearm it is called unstable sort. Example: quick sort","title":"Stable vs Unstable sorting"},{"location":"Data%20Structures%20And%20Algorithms/14-sorting/01-what-and-why-sorting/#sorting-terminologies","text":"Increasing order - if successive element is greater than previous one. Decresing order - if successive element is less than the current one. Non-increasing order - if successive element is less than or equal to it's previous element in the sequence. This order occurs when the sequence contains duplicate values. Non-decreasing order - if the successive element is greater than or equal to it's previous element in the sequence. This order occurs when there are duplicate values.","title":"Sorting terminologies"},{"location":"Data%20Structures%20And%20Algorithms/14-sorting/02-bubble-sort/","text":"Bubble sort \u00b6 This is the simples of all the sorting algorithms. Sometimes referred as sinking sort Repeadly steps through the list to be sorted, compares each pair of adjacent items and swaps them if they are in the wrong order. bubbleSort(int arr[]) int n = arr.length for(int i = 0; i < n -1; i++) // run from first cell to last cell for( int j = 0; j < n - i - 1; j++ ) // run from first cell to last cell - iteration if(arr[j] > arr[j + 1]) swap(arr[j], arr[j+1]) Time complexity O(n^2) Space complexity O(1) When to use/avoid bubble sort: \u00b6 When to use: When input is already sorted Space is a concern Easy to implement When to avoid Average case time complexity is poor","title":"Bubble sort"},{"location":"Data%20Structures%20And%20Algorithms/14-sorting/02-bubble-sort/#bubble-sort","text":"This is the simples of all the sorting algorithms. Sometimes referred as sinking sort Repeadly steps through the list to be sorted, compares each pair of adjacent items and swaps them if they are in the wrong order. bubbleSort(int arr[]) int n = arr.length for(int i = 0; i < n -1; i++) // run from first cell to last cell for( int j = 0; j < n - i - 1; j++ ) // run from first cell to last cell - iteration if(arr[j] > arr[j + 1]) swap(arr[j], arr[j+1]) Time complexity O(n^2) Space complexity O(1)","title":"Bubble sort"},{"location":"Data%20Structures%20And%20Algorithms/14-sorting/02-bubble-sort/#when-to-useavoid-bubble-sort","text":"When to use: When input is already sorted Space is a concern Easy to implement When to avoid Average case time complexity is poor","title":"When to use/avoid bubble sort:"},{"location":"Data%20Structures%20And%20Algorithms/14-sorting/02-selection-sort/","text":"Selection sort \u00b6 The selection sort algorithm is based on the idea of finding the minimum or maximum element in an unsorted array and then putting in it's correct position in a sorted array. selectionSort(A) loop: j = 0 to n - 1 int iMin = j loop: i = J + 1 to n - 1 if(a[i] < a[iMin]) iMin = i if(iMin != j) swap(a[j], a[iMin]) Time complexity - O(n^2) Space complexity - O(1) When to use / avoid \u00b6 When to use: When we don't have additional memory Want easy implementation When to avoid When time complexity is a concern","title":"Selection sort"},{"location":"Data%20Structures%20And%20Algorithms/14-sorting/02-selection-sort/#selection-sort","text":"The selection sort algorithm is based on the idea of finding the minimum or maximum element in an unsorted array and then putting in it's correct position in a sorted array. selectionSort(A) loop: j = 0 to n - 1 int iMin = j loop: i = J + 1 to n - 1 if(a[i] < a[iMin]) iMin = i if(iMin != j) swap(a[j], a[iMin]) Time complexity - O(n^2) Space complexity - O(1)","title":"Selection sort"},{"location":"Data%20Structures%20And%20Algorithms/14-sorting/02-selection-sort/#when-to-use-avoid","text":"When to use: When we don't have additional memory Want easy implementation When to avoid When time complexity is a concern","title":"When to use / avoid"},{"location":"Data%20Structures%20And%20Algorithms/14-sorting/03-insertion-sort/","text":"Insertion sort \u00b6 In insertion sort algorithm we: 1. Divide the given array into 2 parts - sorted and unsorted. 2. Then from unsorted array we pick the first element and find it's correct position in sorted array. 3. Repeat while unsorted array is not empty insertionSort(A) loop: i = 1 to n currentNumber = A[i], j=i while(A[j-1] > currentNumber && j > 0) J[j] = A[j-1] j-- A[j] = currentNumber Time complexity - O(n^2) Space complexity - O(1) When to use / avoid \u00b6 When to use No extra space Simple implementation Best when we have continuous inflow of numbers and we want to keep the list sorted. When not to use Average time complexity is bad","title":"Insertion sort"},{"location":"Data%20Structures%20And%20Algorithms/14-sorting/03-insertion-sort/#insertion-sort","text":"In insertion sort algorithm we: 1. Divide the given array into 2 parts - sorted and unsorted. 2. Then from unsorted array we pick the first element and find it's correct position in sorted array. 3. Repeat while unsorted array is not empty insertionSort(A) loop: i = 1 to n currentNumber = A[i], j=i while(A[j-1] > currentNumber && j > 0) J[j] = A[j-1] j-- A[j] = currentNumber Time complexity - O(n^2) Space complexity - O(1)","title":"Insertion sort"},{"location":"Data%20Structures%20And%20Algorithms/14-sorting/03-insertion-sort/#when-to-use-avoid","text":"When to use No extra space Simple implementation Best when we have continuous inflow of numbers and we want to keep the list sorted. When not to use Average time complexity is bad","title":"When to use / avoid"},{"location":"Data%20Structures%20And%20Algorithms/14-sorting/04-bucket-sort/","text":"Bucket sort \u00b6 Bucket sort is a sorting algorithm that works by: 1. distributing the elements of an array into a number of buckets. ``` Create number of buckets = ceil/floor(sqrt(total number of items)) iterate through each number and place it in approriate bucket Approriate bucket = ceil((value * number of buckets) / max value in array) ``` Sort all buckets Merge all the buckets BucketSort(A) find no of buckets to be created and create those buckets find divisor value loop: i = 0 to n-1 insert A[i] into array B[] using divisor and bucket# sort B[] with insertion sort (or any sort) concatenate B1[], B2[], B3[]... Time Complexity - O(n log n) Space Complexity - O(n) When to use \u00b6 When input is uniformly distributed over a range Biggest advantage of bucket sort (compared to other sorting algorithms) is that each bucket can be sorted independently, which makes it great for distributed computing. When to avoid \u00b6 When space is a concern","title":"Bucket sort"},{"location":"Data%20Structures%20And%20Algorithms/14-sorting/04-bucket-sort/#bucket-sort","text":"Bucket sort is a sorting algorithm that works by: 1. distributing the elements of an array into a number of buckets. ``` Create number of buckets = ceil/floor(sqrt(total number of items)) iterate through each number and place it in approriate bucket Approriate bucket = ceil((value * number of buckets) / max value in array) ``` Sort all buckets Merge all the buckets BucketSort(A) find no of buckets to be created and create those buckets find divisor value loop: i = 0 to n-1 insert A[i] into array B[] using divisor and bucket# sort B[] with insertion sort (or any sort) concatenate B1[], B2[], B3[]... Time Complexity - O(n log n) Space Complexity - O(n)","title":"Bucket sort"},{"location":"Data%20Structures%20And%20Algorithms/14-sorting/04-bucket-sort/#when-to-use","text":"When input is uniformly distributed over a range Biggest advantage of bucket sort (compared to other sorting algorithms) is that each bucket can be sorted independently, which makes it great for distributed computing.","title":"When to use"},{"location":"Data%20Structures%20And%20Algorithms/14-sorting/04-bucket-sort/#when-to-avoid","text":"When space is a concern","title":"When to avoid"},{"location":"Data%20Structures%20And%20Algorithms/14-sorting/05-merge-sort/","text":"Merge sort \u00b6 Merge sort is basically a divide and conquer algorithm. It divides inpout array into two haves, keep breaking those 2 halves recursively, until they become too small to be broken further. Then each of the broken pieces are merged together into the final answer. mergeSort(A,l,r) if r > l middle m = (l+r)/2 mergeSort(A,l, m) mergeSort(A,l+1, r) merge(A, l, m, r) merge(A, p, m, r): create tmp arrays L & R and copy A, p, m into L & A, M+1 r, into R. i=j=0 loop: k = p to r if L[i] < R[j] A[k] = L[i]; i++ else A[k] = R[j]; j++ Time complexity - O(n log n) Space complexity - O(n) When to use \u00b6 When tou need a stable sort When Average expected time is O(n log n) When to avoid \u00b6 When space is a concern like embedded systems","title":"Merge sort"},{"location":"Data%20Structures%20And%20Algorithms/14-sorting/05-merge-sort/#merge-sort","text":"Merge sort is basically a divide and conquer algorithm. It divides inpout array into two haves, keep breaking those 2 halves recursively, until they become too small to be broken further. Then each of the broken pieces are merged together into the final answer. mergeSort(A,l,r) if r > l middle m = (l+r)/2 mergeSort(A,l, m) mergeSort(A,l+1, r) merge(A, l, m, r) merge(A, p, m, r): create tmp arrays L & R and copy A, p, m into L & A, M+1 r, into R. i=j=0 loop: k = p to r if L[i] < R[j] A[k] = L[i]; i++ else A[k] = R[j]; j++ Time complexity - O(n log n) Space complexity - O(n)","title":"Merge sort"},{"location":"Data%20Structures%20And%20Algorithms/14-sorting/05-merge-sort/#when-to-use","text":"When tou need a stable sort When Average expected time is O(n log n)","title":"When to use"},{"location":"Data%20Structures%20And%20Algorithms/14-sorting/05-merge-sort/#when-to-avoid","text":"When space is a concern like embedded systems","title":"When to avoid"},{"location":"Data%20Structures%20And%20Algorithms/14-sorting/06-quick-sort/","text":"Quick sort \u00b6 Quick sort is a divide and conquer algorithm. At each step it finds Pivot and then makes sure that all the smaller elements are left of Pivot and all bigger elements are right of it. It does this recursively until the entire array is sorted. Unlike merge sort, it does not require any external space. QuckSort(A, p, q) if(p < q) r = partition(A, p, q) QuickSort(A,p,r-1) QuickSort(A, r+1, p) Partition(A, p, q) pivot = q i = p -1 for (j = p to q) if(A[i] < A[pivot]) increment i and then swap(A[i], [j]) Time complexity - O(n log n) Space complexity - O(n) When to use \u00b6 When average case is desired to be O(n log n) When to avoid \u00b6 When space is a concern When stable sort is required","title":"Quick sort"},{"location":"Data%20Structures%20And%20Algorithms/14-sorting/06-quick-sort/#quick-sort","text":"Quick sort is a divide and conquer algorithm. At each step it finds Pivot and then makes sure that all the smaller elements are left of Pivot and all bigger elements are right of it. It does this recursively until the entire array is sorted. Unlike merge sort, it does not require any external space. QuckSort(A, p, q) if(p < q) r = partition(A, p, q) QuickSort(A,p,r-1) QuickSort(A, r+1, p) Partition(A, p, q) pivot = q i = p -1 for (j = p to q) if(A[i] < A[pivot]) increment i and then swap(A[i], [j]) Time complexity - O(n log n) Space complexity - O(n)","title":"Quick sort"},{"location":"Data%20Structures%20And%20Algorithms/14-sorting/06-quick-sort/#when-to-use","text":"When average case is desired to be O(n log n)","title":"When to use"},{"location":"Data%20Structures%20And%20Algorithms/14-sorting/06-quick-sort/#when-to-avoid","text":"When space is a concern When stable sort is required","title":"When to avoid"},{"location":"Data%20Structures%20And%20Algorithms/14-sorting/07-heap-sort/","text":"Heap Sort \u00b6 Heap Sort works by first organizing the data to be sorted into a spacial type of binary tree called heap. Then it removes the topmost item (the larges/smallest) and inserts into current array. It keeps until Binary Heap is not empty. Is best suited with arrays. Does not work best with Linked Lists. HeapSort(A) for i = 0 to A.length -1 insertInHeap(A[i]) for i = 0 to A.length -1 extractFromHeap(A[i]) Time complexity - O(n logn) Space complexity - O(1) When to use \u00b6 When space is a concern When to avoid \u00b6 When we need our sort to be stable","title":"Heap Sort"},{"location":"Data%20Structures%20And%20Algorithms/14-sorting/07-heap-sort/#heap-sort","text":"Heap Sort works by first organizing the data to be sorted into a spacial type of binary tree called heap. Then it removes the topmost item (the larges/smallest) and inserts into current array. It keeps until Binary Heap is not empty. Is best suited with arrays. Does not work best with Linked Lists. HeapSort(A) for i = 0 to A.length -1 insertInHeap(A[i]) for i = 0 to A.length -1 extractFromHeap(A[i]) Time complexity - O(n logn) Space complexity - O(1)","title":"Heap Sort"},{"location":"Data%20Structures%20And%20Algorithms/14-sorting/07-heap-sort/#when-to-use","text":"When space is a concern","title":"When to use"},{"location":"Data%20Structures%20And%20Algorithms/14-sorting/07-heap-sort/#when-to-avoid","text":"When we need our sort to be stable","title":"When to avoid"},{"location":"Data%20Structures%20And%20Algorithms/14-sorting/08-sorting-algorithms-compared/","text":"Sorting algorithms compared \u00b6 Algorithm Time Complexity Space complexity Stable Bubble Sort O(n^2) O(1) Yes Selection Sort O(n^2) O(1) No Insertion Sort O(n^2) O(1) Yes Bucket Sort O(n logn) O(n) Yes* Merge Sort O(n logn) O(n) Yes Quick Sort O(n logn) O(n)* No Heap Sort O(n logn) O(1) No","title":"Sorting algorithms compared"},{"location":"Data%20Structures%20And%20Algorithms/14-sorting/08-sorting-algorithms-compared/#sorting-algorithms-compared","text":"Algorithm Time Complexity Space complexity Stable Bubble Sort O(n^2) O(1) Yes Selection Sort O(n^2) O(1) No Insertion Sort O(n^2) O(1) Yes Bucket Sort O(n logn) O(n) Yes* Merge Sort O(n logn) O(n) Yes Quick Sort O(n logn) O(n)* No Heap Sort O(n logn) O(1) No","title":"Sorting algorithms compared"},{"location":"Data%20Structures%20And%20Algorithms/15-graphs/01-what-are-graphs/","text":"What is a graph \u00b6 A graph is a pair of sets (V, E) where V is the set of vertices and E is the set of Edgets, connecting the pairs of vertices. V = {v1, v2, v3, v4, v5,v6, v7, v8, v9, v10} E = {v1v2, v2v3, v1v4, v4v7, v7v8, v2v5, v5v8, v3v6, v6v9, v8v9, v3v10, v9v10} One of the use cases of graphs can be that we can use graphs to find shortest paths between points in the graph. Terminologies \u00b6 Vertices - nodes of the graph Edges - the arcs that connect pairs of vertices Unweighted graph - a graph not having a weight associated with any edge Weighted graph - graph having weight associated to each edge Undirected graph - graph that is set of vertices connected by edgets, where the edges don't have a direction associated with them. Directed graph - the edges have direction associated with them Cyclic graph - a graph having at least one loop Acyclic graph - without any loops Tree - A special case of directed acyclick graph (DAG) Types of graphs \u00b6 Directed Weighted Positive Negative Unweighted Undirected Weighted Positive Negative Unweighted","title":"What is a graph"},{"location":"Data%20Structures%20And%20Algorithms/15-graphs/01-what-are-graphs/#what-is-a-graph","text":"A graph is a pair of sets (V, E) where V is the set of vertices and E is the set of Edgets, connecting the pairs of vertices. V = {v1, v2, v3, v4, v5,v6, v7, v8, v9, v10} E = {v1v2, v2v3, v1v4, v4v7, v7v8, v2v5, v5v8, v3v6, v6v9, v8v9, v3v10, v9v10} One of the use cases of graphs can be that we can use graphs to find shortest paths between points in the graph.","title":"What is a graph"},{"location":"Data%20Structures%20And%20Algorithms/15-graphs/01-what-are-graphs/#terminologies","text":"Vertices - nodes of the graph Edges - the arcs that connect pairs of vertices Unweighted graph - a graph not having a weight associated with any edge Weighted graph - graph having weight associated to each edge Undirected graph - graph that is set of vertices connected by edgets, where the edges don't have a direction associated with them. Directed graph - the edges have direction associated with them Cyclic graph - a graph having at least one loop Acyclic graph - without any loops Tree - A special case of directed acyclick graph (DAG)","title":"Terminologies"},{"location":"Data%20Structures%20And%20Algorithms/15-graphs/01-what-are-graphs/#types-of-graphs","text":"Directed Weighted Positive Negative Unweighted Undirected Weighted Positive Negative Unweighted","title":"Types of graphs"},{"location":"Data%20Structures%20And%20Algorithms/15-graphs/02-graph-representation-in-code/","text":"Graph representation in code \u00b6 We can use adjacency matrix to represent the graph in code. An adjacency matrix is a square matrix used to represend a finite graph. The elements of the matrix indicate whether pairs of vertices are adjacent or not in the graph. We can also use an adjacency list, which is a collection of unordered lists used to represent a finite graph. Each list describes a set of neighbors of a vertex in the graph. When to use each representation \u00b6 If the graph is a complete or near to complete graph, then we should use adjacency matrix. If the number of edges are few, then we should use adjacency list.","title":"Graph representation in code"},{"location":"Data%20Structures%20And%20Algorithms/15-graphs/02-graph-representation-in-code/#graph-representation-in-code","text":"We can use adjacency matrix to represent the graph in code. An adjacency matrix is a square matrix used to represend a finite graph. The elements of the matrix indicate whether pairs of vertices are adjacent or not in the graph. We can also use an adjacency list, which is a collection of unordered lists used to represent a finite graph. Each list describes a set of neighbors of a vertex in the graph.","title":"Graph representation in code"},{"location":"Data%20Structures%20And%20Algorithms/15-graphs/02-graph-representation-in-code/#when-to-use-each-representation","text":"If the graph is a complete or near to complete graph, then we should use adjacency matrix. If the number of edges are few, then we should use adjacency list.","title":"When to use each representation"},{"location":"Data%20Structures%20And%20Algorithms/15-graphs/03-traversal-bfs-dfs/","text":"Graph traversal \u00b6 Graph traversal refers to the process of visiting each vertex in a graph. There can be 2 types of traversal techniques: - Breadth First Search (BFS) - Depth First Search (DFS) Breadth First Search (BFS) \u00b6 BFS is an algorithm for traversing Graph Data Structures. It starts at some arbitrary node of a graph and explores the neighbor nodes (which are at current level) first, before moving to the next neighbors. BFS(G) while all the vertices are not explored, do: enqueue(any evertex) while Q is not empty p = Dequeue() if p is unvisited print p and mark p as visited enqueue(all adjacent unvisited vertices of p) Time complexity - O(V + E) Space complexity - O(V + E) One corner case of the BFS is - disconnected graph. When there are vertices that are not connected to any other vertices - they will not be traversed. Depth First Search (DFS) \u00b6 Depth First Search (DFS) is an algorithm for traversing graph data structures. It starts selecting some arbitrary node and explores as far as possible along each edge before backtracing. DFS() while all the vertices are not explored, do: push(any vertex) while Stack is not empty p = pop() if p is unvisited print p and mark p as visited push(all unvisited adjacent vertices of p) Time complexity - O(E + V) Space complexity - O(E + V) Same thing as the BFS - the DFS method will not look at any vertices that are disconnected from the graph. DFS vs BFS \u00b6 DFS BFS How it works internally It goes in depth first It goes in breadth first Internally uses Stack Queue Time Complexity O(E + V) O(E + V) Space Complexity O(E + V) O(E + V) When to use which? If we already know that target vertex is buried very deep. If we know that target vertex is close to starting point.","title":"Graph traversal"},{"location":"Data%20Structures%20And%20Algorithms/15-graphs/03-traversal-bfs-dfs/#graph-traversal","text":"Graph traversal refers to the process of visiting each vertex in a graph. There can be 2 types of traversal techniques: - Breadth First Search (BFS) - Depth First Search (DFS)","title":"Graph traversal"},{"location":"Data%20Structures%20And%20Algorithms/15-graphs/03-traversal-bfs-dfs/#breadth-first-search-bfs","text":"BFS is an algorithm for traversing Graph Data Structures. It starts at some arbitrary node of a graph and explores the neighbor nodes (which are at current level) first, before moving to the next neighbors. BFS(G) while all the vertices are not explored, do: enqueue(any evertex) while Q is not empty p = Dequeue() if p is unvisited print p and mark p as visited enqueue(all adjacent unvisited vertices of p) Time complexity - O(V + E) Space complexity - O(V + E) One corner case of the BFS is - disconnected graph. When there are vertices that are not connected to any other vertices - they will not be traversed.","title":"Breadth First Search (BFS)"},{"location":"Data%20Structures%20And%20Algorithms/15-graphs/03-traversal-bfs-dfs/#depth-first-search-dfs","text":"Depth First Search (DFS) is an algorithm for traversing graph data structures. It starts selecting some arbitrary node and explores as far as possible along each edge before backtracing. DFS() while all the vertices are not explored, do: push(any vertex) while Stack is not empty p = pop() if p is unvisited print p and mark p as visited push(all unvisited adjacent vertices of p) Time complexity - O(E + V) Space complexity - O(E + V) Same thing as the BFS - the DFS method will not look at any vertices that are disconnected from the graph.","title":"Depth First Search (DFS)"},{"location":"Data%20Structures%20And%20Algorithms/15-graphs/03-traversal-bfs-dfs/#dfs-vs-bfs","text":"DFS BFS How it works internally It goes in depth first It goes in breadth first Internally uses Stack Queue Time Complexity O(E + V) O(E + V) Space Complexity O(E + V) O(E + V) When to use which? If we already know that target vertex is buried very deep. If we know that target vertex is close to starting point.","title":"DFS vs BFS"},{"location":"Data%20Structures%20And%20Algorithms/15-graphs/04-topological-sort/","text":"Topological Sort \u00b6 Topological Sort sorts given actions in such way that if there is a dependency of one action on another,then the dependent action always comes later that it's parent action. topologicalSort(G) for all the nodes if this Vertex is not visited topologicalVisit(node) pop stack topologicalVisit(currentNode) for each neighbour of currentNode if neighbour is not visited topologicalVisit(neighbour) mark currentNode as visited and push node in stack Time Complexity - O(E + V) Space Complexity - O(E + V)","title":"Topological Sort"},{"location":"Data%20Structures%20And%20Algorithms/15-graphs/04-topological-sort/#topological-sort","text":"Topological Sort sorts given actions in such way that if there is a dependency of one action on another,then the dependent action always comes later that it's parent action. topologicalSort(G) for all the nodes if this Vertex is not visited topologicalVisit(node) pop stack topologicalVisit(currentNode) for each neighbour of currentNode if neighbour is not visited topologicalVisit(neighbour) mark currentNode as visited and push node in stack Time Complexity - O(E + V) Space Complexity - O(E + V)","title":"Topological Sort"},{"location":"Data%20Structures%20And%20Algorithms/15-graphs/05-single-source-shortest-path-problem/","text":"Single Source Shortest Path Problem (SSSP) \u00b6 Single source shortest path problem is about finding a path between a given vertex (called source) to all other vertices in a graph such that, the total distance between them (source / destination) is minimum. We can use several methods to do this: - Breadth First Search (BFS) - Dijkstra - Bellman Ford Breadth First Search (BFS) \u00b6 BFS can be modified to find a Single source shortest path. We need an extra variable parent to keep track of the path. bfsForSSSP(any starting vertex) initialize Queue Create a parent reference in each node Enqueue (Source Vertex) do while queue is not empty currentVertex = Dequeue(vertex) for each adjacent vertices if adjacent vertex is not visited enqueue adjacent vertex and update their parent as currentVertex. Also mark currentVertex as visited. Time Complexity - O(E) Space Complexity - O(E) BFS explores a graph only in breadth-way , but there can always be a better route which is not breadth-way . This will not work for weighted graphs. Why DFS does not work for SSSP \u00b6 The DFS is used for going depth-first and then backtrack. Which basically will find the path with the most depth. Dijkstras algorithm \u00b6 Dijkstra(G) Set the distance of all the vertices as infinite and source vertex as 0. Save all the vertices in minHeap do while minHeap is not empty currentVertex = extract from minHeap for each neighbour of currentVertex if current vertex's distance + currentEdge < neighbours distance update neighbours distance and parent Time Complexity - O(V^2) Space Complexity - O(V) The Dijkstra's algorithm will not work for negative cycles. A path is called a negative cycle if: - There is a cycle ( A cycle is a path of edges and vertices wherein a vertex is reachable from itself). - Total weight of cycle should be a negative number. This is because we can nver find minimum distance in a negative cycle. Bellman Ford algorithm \u00b6 The advantage of Bellman Ford's algorithm is that it will work with negative cycles. If a graph contains a negative cycle that is reachable from the source, then there is no cheapest path: any path that has a point on the negative cycle can be made cheaper by one more walt around the negative cycle. In such a case, the Bellman-Ford algorithm can detect negative cycles and report their existence. BellmannFord(G) set all vertex distances to infinite and source as 0 for 1 to v-1: for each edge(u,v) ifd(V) > d(u) + w(u, v) // if current weight of V is greater than current weight of U + current edge d(V) = d(u) + w(u,v) update parent of `V` for each edge(u, v) if d(V) != d(u) + w(u, v) then report existance of negative-weight cycle print all vertices with distances and parents Time complexity - O(VE) Space complexity - O(V) The algorithm basically does: 1. It is trying to see if any node achieved a better distance in the previous iteration 2. Then in the current step it tries to use that better distance and improve it of other vertices 3. Now the issue can happen in case - The graph is skewed/biased (what can be the max distance from source to destination?) - Processing of edges are happening in unfavourable way - resulting in deffering calculation of distance for farthest vertex What can be farthest vertex from source V-1. Comparison of algorithms \u00b6","title":"Single Source Shortest Path Problem (SSSP)"},{"location":"Data%20Structures%20And%20Algorithms/15-graphs/05-single-source-shortest-path-problem/#single-source-shortest-path-problem-sssp","text":"Single source shortest path problem is about finding a path between a given vertex (called source) to all other vertices in a graph such that, the total distance between them (source / destination) is minimum. We can use several methods to do this: - Breadth First Search (BFS) - Dijkstra - Bellman Ford","title":"Single Source Shortest Path Problem (SSSP)"},{"location":"Data%20Structures%20And%20Algorithms/15-graphs/05-single-source-shortest-path-problem/#breadth-first-search-bfs","text":"BFS can be modified to find a Single source shortest path. We need an extra variable parent to keep track of the path. bfsForSSSP(any starting vertex) initialize Queue Create a parent reference in each node Enqueue (Source Vertex) do while queue is not empty currentVertex = Dequeue(vertex) for each adjacent vertices if adjacent vertex is not visited enqueue adjacent vertex and update their parent as currentVertex. Also mark currentVertex as visited. Time Complexity - O(E) Space Complexity - O(E) BFS explores a graph only in breadth-way , but there can always be a better route which is not breadth-way . This will not work for weighted graphs.","title":"Breadth First Search (BFS)"},{"location":"Data%20Structures%20And%20Algorithms/15-graphs/05-single-source-shortest-path-problem/#why-dfs-does-not-work-for-sssp","text":"The DFS is used for going depth-first and then backtrack. Which basically will find the path with the most depth.","title":"Why DFS does not work for SSSP"},{"location":"Data%20Structures%20And%20Algorithms/15-graphs/05-single-source-shortest-path-problem/#dijkstras-algorithm","text":"Dijkstra(G) Set the distance of all the vertices as infinite and source vertex as 0. Save all the vertices in minHeap do while minHeap is not empty currentVertex = extract from minHeap for each neighbour of currentVertex if current vertex's distance + currentEdge < neighbours distance update neighbours distance and parent Time Complexity - O(V^2) Space Complexity - O(V) The Dijkstra's algorithm will not work for negative cycles. A path is called a negative cycle if: - There is a cycle ( A cycle is a path of edges and vertices wherein a vertex is reachable from itself). - Total weight of cycle should be a negative number. This is because we can nver find minimum distance in a negative cycle.","title":"Dijkstras algorithm"},{"location":"Data%20Structures%20And%20Algorithms/15-graphs/05-single-source-shortest-path-problem/#bellman-ford-algorithm","text":"The advantage of Bellman Ford's algorithm is that it will work with negative cycles. If a graph contains a negative cycle that is reachable from the source, then there is no cheapest path: any path that has a point on the negative cycle can be made cheaper by one more walt around the negative cycle. In such a case, the Bellman-Ford algorithm can detect negative cycles and report their existence. BellmannFord(G) set all vertex distances to infinite and source as 0 for 1 to v-1: for each edge(u,v) ifd(V) > d(u) + w(u, v) // if current weight of V is greater than current weight of U + current edge d(V) = d(u) + w(u,v) update parent of `V` for each edge(u, v) if d(V) != d(u) + w(u, v) then report existance of negative-weight cycle print all vertices with distances and parents Time complexity - O(VE) Space complexity - O(V) The algorithm basically does: 1. It is trying to see if any node achieved a better distance in the previous iteration 2. Then in the current step it tries to use that better distance and improve it of other vertices 3. Now the issue can happen in case - The graph is skewed/biased (what can be the max distance from source to destination?) - Processing of edges are happening in unfavourable way - resulting in deffering calculation of distance for farthest vertex What can be farthest vertex from source V-1.","title":"Bellman Ford algorithm"},{"location":"Data%20Structures%20And%20Algorithms/15-graphs/05-single-source-shortest-path-problem/#comparison-of-algorithms","text":"","title":"Comparison of algorithms"},{"location":"Data%20Structures%20And%20Algorithms/15-graphs/06-all-pair-shortest-path-problem/","text":"All Pair Shortest Path (APSP) Problem \u00b6 All pair shortest path problem is about finding a path between every vertex to all other vertices in a graph such that the total distance between them is minimum. We basically do the SSSP problem for each vertex in our graph. We can use following methods to do this: - Breadth First Search (BFS) - Dijkstra algorithm - Bellman Ford algorithm - Floyd Warshall algorithm Floyd Warshall Algorithm \u00b6 FloydWarshall(G) initialize a table of size VxV: D with infinity copy D from G for k = 0 to n - 1 // run the loop as many times as number of vertices for i = 0 to n - 1 // run the loop such that we visit cell in 2D array in row wise fashion for j = 0 to n - 1 if D[i][j] > D[i][k] + D[k][j] D[i][j] = D[i][k] + D[k][j] return D Time Complexity - O(V^3) Space Complexity - O(V^2) The Floyd Warshall algorithm will not work with negative cycles. That is because if we go through a cycle, we need to go via negative cycle participating vertex at least twice. Since we never run the loop twice via the same vertex, Floyd algorithm can never detect a negative cycle. Algorithm comparison \u00b6","title":"All Pair Shortest Path (APSP) Problem"},{"location":"Data%20Structures%20And%20Algorithms/15-graphs/06-all-pair-shortest-path-problem/#all-pair-shortest-path-apsp-problem","text":"All pair shortest path problem is about finding a path between every vertex to all other vertices in a graph such that the total distance between them is minimum. We basically do the SSSP problem for each vertex in our graph. We can use following methods to do this: - Breadth First Search (BFS) - Dijkstra algorithm - Bellman Ford algorithm - Floyd Warshall algorithm","title":"All Pair Shortest Path (APSP) Problem"},{"location":"Data%20Structures%20And%20Algorithms/15-graphs/06-all-pair-shortest-path-problem/#floyd-warshall-algorithm","text":"FloydWarshall(G) initialize a table of size VxV: D with infinity copy D from G for k = 0 to n - 1 // run the loop as many times as number of vertices for i = 0 to n - 1 // run the loop such that we visit cell in 2D array in row wise fashion for j = 0 to n - 1 if D[i][j] > D[i][k] + D[k][j] D[i][j] = D[i][k] + D[k][j] return D Time Complexity - O(V^3) Space Complexity - O(V^2) The Floyd Warshall algorithm will not work with negative cycles. That is because if we go through a cycle, we need to go via negative cycle participating vertex at least twice. Since we never run the loop twice via the same vertex, Floyd algorithm can never detect a negative cycle.","title":"Floyd Warshall Algorithm"},{"location":"Data%20Structures%20And%20Algorithms/15-graphs/06-all-pair-shortest-path-problem/#algorithm-comparison","text":"","title":"Algorithm comparison"},{"location":"Data%20Structures%20And%20Algorithms/15-graphs/07-mimimum-spanning-tree/","text":"Minimum Spanning Tree (MST) \u00b6 A minimum spanning tree (MST) is a subset of the edges of connected, weighted, undirected graph that: - Connects all the vertices together - Without any cycles - With a minimum total edge weight. Example problem: - We have 5 islands that we need to connect with bridges - Cost of each bridge varies depending on different factors - Which bridge to be constructed and which not? How to find? Disjoint set \u00b6 A Disjoint set is a data structure that keeps track of set of elements which are partitioned into a number of disjoint and non-overlapping sets. Each set has a representative, which helps in identifying that set. This One of which is Kruskal Algorithm where it is used to detect cycle in underected graph. Common operations in Disjoint Set \u00b6 makeSet(n) - used only once to create initial set union(x,y) - used to merge both of the sets findSet(x) - returns the set name in which element is there createSet(N) create n sets for `N` elements Time complexity - O(n) Space complexity - O(n) union(s1, s2) if s1 and s2 are in the same set then return if s1 is bigger then merge s2 into s1 else s2 into s1 return merged set Time complexity - O(n) Space complexity - O(1) findSet(x) return representative of set in which element x is there Time complexity - O(1) Space complexity - O(1) Kruskals Algorithm \u00b6 Kruskals algorithm is a greedy algorithm. It finds a minimum spanning tree for a connected weighted graph by: - Adding increasing arcs at each step - Also avoiding cycle in every step mstKruskal(G) for each vertex: makeSet(X) sort each edge in non-decresing order by weight for each edge(u,v) do: if findSet(u) != findSet(v) Union(u,v) cost = cost + edge(u,v) Time complexity - O(E logE) Space complexity - O(E + V) Prims Algorithm \u00b6 Prims algorithm is a greedy algorithm. Take any Vertex as Source and mark weight of all the vertices as infinite and source as 0. For every adjacent unvisited vertex of current vertex if current weight of this adjacent vertex is more than current edge, then update adjacent vertex's weight. Mark currentVertex as visited. Do above steps for all the vertices in increasing order of weights. mstPrims(G) create a PriorityQueue(Q) insert all the vertices into Q such that key value of starting vertex is 0 and others are infinite while Q is not empty: currentVertex = dequeue(Q) for every adjacent unvisited Vertex of currentVertex if current weight of this adjacent vertex is more than current edge update adjacent vertex's distance and parent mark currentVertex as visited print all the vertices with Weights Time complexity - O(E logV) Space complexity - O(V) Algorithm comparison \u00b6 The Kruskal's algorithm concentrates on edges, finalizes edge in every iteration. The Prim's algorithm concentrates on vertices, finalizes vertices in every iteration.","title":"Minimum Spanning Tree (MST)"},{"location":"Data%20Structures%20And%20Algorithms/15-graphs/07-mimimum-spanning-tree/#minimum-spanning-tree-mst","text":"A minimum spanning tree (MST) is a subset of the edges of connected, weighted, undirected graph that: - Connects all the vertices together - Without any cycles - With a minimum total edge weight. Example problem: - We have 5 islands that we need to connect with bridges - Cost of each bridge varies depending on different factors - Which bridge to be constructed and which not? How to find?","title":"Minimum Spanning Tree (MST)"},{"location":"Data%20Structures%20And%20Algorithms/15-graphs/07-mimimum-spanning-tree/#disjoint-set","text":"A Disjoint set is a data structure that keeps track of set of elements which are partitioned into a number of disjoint and non-overlapping sets. Each set has a representative, which helps in identifying that set. This One of which is Kruskal Algorithm where it is used to detect cycle in underected graph.","title":"Disjoint set"},{"location":"Data%20Structures%20And%20Algorithms/15-graphs/07-mimimum-spanning-tree/#common-operations-in-disjoint-set","text":"makeSet(n) - used only once to create initial set union(x,y) - used to merge both of the sets findSet(x) - returns the set name in which element is there createSet(N) create n sets for `N` elements Time complexity - O(n) Space complexity - O(n) union(s1, s2) if s1 and s2 are in the same set then return if s1 is bigger then merge s2 into s1 else s2 into s1 return merged set Time complexity - O(n) Space complexity - O(1) findSet(x) return representative of set in which element x is there Time complexity - O(1) Space complexity - O(1)","title":"Common operations in Disjoint Set"},{"location":"Data%20Structures%20And%20Algorithms/15-graphs/07-mimimum-spanning-tree/#kruskals-algorithm","text":"Kruskals algorithm is a greedy algorithm. It finds a minimum spanning tree for a connected weighted graph by: - Adding increasing arcs at each step - Also avoiding cycle in every step mstKruskal(G) for each vertex: makeSet(X) sort each edge in non-decresing order by weight for each edge(u,v) do: if findSet(u) != findSet(v) Union(u,v) cost = cost + edge(u,v) Time complexity - O(E logE) Space complexity - O(E + V)","title":"Kruskals Algorithm"},{"location":"Data%20Structures%20And%20Algorithms/15-graphs/07-mimimum-spanning-tree/#prims-algorithm","text":"Prims algorithm is a greedy algorithm. Take any Vertex as Source and mark weight of all the vertices as infinite and source as 0. For every adjacent unvisited vertex of current vertex if current weight of this adjacent vertex is more than current edge, then update adjacent vertex's weight. Mark currentVertex as visited. Do above steps for all the vertices in increasing order of weights. mstPrims(G) create a PriorityQueue(Q) insert all the vertices into Q such that key value of starting vertex is 0 and others are infinite while Q is not empty: currentVertex = dequeue(Q) for every adjacent unvisited Vertex of currentVertex if current weight of this adjacent vertex is more than current edge update adjacent vertex's distance and parent mark currentVertex as visited print all the vertices with Weights Time complexity - O(E logV) Space complexity - O(V)","title":"Prims Algorithm"},{"location":"Data%20Structures%20And%20Algorithms/15-graphs/07-mimimum-spanning-tree/#algorithm-comparison","text":"The Kruskal's algorithm concentrates on edges, finalizes edge in every iteration. The Prim's algorithm concentrates on vertices, finalizes vertices in every iteration.","title":"Algorithm comparison"},{"location":"Data%20Structures%20And%20Algorithms/16-magic-framework/01-what-is-a-magic-framework/","text":"What is a Magic Framework? \u00b6 The Magic Framework is basically a flow charart which helps us on taking decisions on how to solve problems.","title":"What is a Magic Framework?"},{"location":"Data%20Structures%20And%20Algorithms/16-magic-framework/01-what-is-a-magic-framework/#what-is-a-magic-framework","text":"The Magic Framework is basically a flow charart which helps us on taking decisions on how to solve problems.","title":"What is a Magic Framework?"},{"location":"Data%20Structures%20And%20Algorithms/17-greedy-algorithm/01-what-is-greedy-algorithm/","text":"What is greedy algorithm \u00b6 Greedy algorithm is an algorithmic paradigm that builds up a solution piece by piece. It always chooses the next piece that offers the most obvious and immediate benefit. Greedy fits perfectly for those solutions in which choosing a locally optimal solution also leads to global optimal solution (aka greedy choice ). Common greedy algorithms \u00b6 Already taught algoritms: Insertion Sort Selection Sort Topological Sort Prims Kruskal Activity Selection Problem Coin Change Problem Fractional Knapsack","title":"What is greedy algorithm"},{"location":"Data%20Structures%20And%20Algorithms/17-greedy-algorithm/01-what-is-greedy-algorithm/#what-is-greedy-algorithm","text":"Greedy algorithm is an algorithmic paradigm that builds up a solution piece by piece. It always chooses the next piece that offers the most obvious and immediate benefit. Greedy fits perfectly for those solutions in which choosing a locally optimal solution also leads to global optimal solution (aka greedy choice ).","title":"What is greedy algorithm"},{"location":"Data%20Structures%20And%20Algorithms/17-greedy-algorithm/01-what-is-greedy-algorithm/#common-greedy-algorithms","text":"Already taught algoritms: Insertion Sort Selection Sort Topological Sort Prims Kruskal Activity Selection Problem Coin Change Problem Fractional Knapsack","title":"Common greedy algorithms"},{"location":"Data%20Structures%20And%20Algorithms/17-greedy-algorithm/02-already-known-algorithms/","text":"Already known algorithms \u00b6 In this section we will go through all of the previously mentioned algorithms and see why are they classified as greedy algorithms. Insertion Sort \u00b6 We divide array into 2 parts - sorted and unsorted array. We take the first element from the unsorted subarray and place it in the sorted array in it's correct place. It follows Greedy Choice , as it aims at finding local optimum solution at each step. Selection Sort \u00b6 We divide the array into 2 parts - sorted and unsorted. We take the minimum element from the unsorted subarray and place it at the end of the sorted array. It follows Greedy Choice , as it aims at finding local optimum solution at each step. Topological Sort \u00b6 We take each nodes as they come by. If they do not fulfil the criteria of greedy choice i.e. if the node does not lead to local optimum solution, we defer their push operation in stack. Prim's algorithm \u00b6 We take the best age for now and then come back to see if better edge is available. Let's say we start from B and then later find that C has a better edge to D. Kruskal algorithm \u00b6 We take the best edge (as it is smallest) and see if it is creating a loop. If loop is created, we ignore that edge, else we take it.","title":"Already known algorithms"},{"location":"Data%20Structures%20And%20Algorithms/17-greedy-algorithm/02-already-known-algorithms/#already-known-algorithms","text":"In this section we will go through all of the previously mentioned algorithms and see why are they classified as greedy algorithms.","title":"Already known algorithms"},{"location":"Data%20Structures%20And%20Algorithms/17-greedy-algorithm/02-already-known-algorithms/#insertion-sort","text":"We divide array into 2 parts - sorted and unsorted array. We take the first element from the unsorted subarray and place it in the sorted array in it's correct place. It follows Greedy Choice , as it aims at finding local optimum solution at each step.","title":"Insertion Sort"},{"location":"Data%20Structures%20And%20Algorithms/17-greedy-algorithm/02-already-known-algorithms/#selection-sort","text":"We divide the array into 2 parts - sorted and unsorted. We take the minimum element from the unsorted subarray and place it at the end of the sorted array. It follows Greedy Choice , as it aims at finding local optimum solution at each step.","title":"Selection Sort"},{"location":"Data%20Structures%20And%20Algorithms/17-greedy-algorithm/02-already-known-algorithms/#topological-sort","text":"We take each nodes as they come by. If they do not fulfil the criteria of greedy choice i.e. if the node does not lead to local optimum solution, we defer their push operation in stack.","title":"Topological Sort"},{"location":"Data%20Structures%20And%20Algorithms/17-greedy-algorithm/02-already-known-algorithms/#prims-algorithm","text":"We take the best age for now and then come back to see if better edge is available. Let's say we start from B and then later find that C has a better edge to D.","title":"Prim's algorithm"},{"location":"Data%20Structures%20And%20Algorithms/17-greedy-algorithm/02-already-known-algorithms/#kruskal-algorithm","text":"We take the best edge (as it is smallest) and see if it is creating a loop. If loop is created, we ignore that edge, else we take it.","title":"Kruskal algorithm"},{"location":"Data%20Structures%20And%20Algorithms/17-greedy-algorithm/03-activity-selection-problem/","text":"Activity Selection Problem \u00b6 We are given n activities with their start and finish times. Select the maximum number of activities that can be performed by a single person, assuming that a person can only work on a single activity at a time. We can sort the table by the finish time. Now we can go through the elements and see if we can take it or not, by simply checking if the next element's starting time is more or equal the previous element's starting time. The answer is: A3, A2, A5, A6 . So the algorithm for this problem is as follows: 1. Sort the activities according to their finish time 2. Select the first activity from the sorted array and print it 3. Do following for remainint activities in the sorted array: - If the start time of the activity is greated or equal to the finish time of previously selected activity then select this activity and print it. ActivitySelectionProblem(A) sort(A,finishTime) --------------------------------------------- O(n log n) previousActivity = first activity ------------------------------ O(1) print first activity ------------------------------------------- O(1) loop: i = 1 to n - 1 ------------------------------------------- O(n) if A[startTime] of i >= A[finishTime] of PreviousActivity -- O(1) print A[startTime], A[finishTime] ---------------------- O(1) Time complexity - O(n log n) + O(n) = O(n log n) Space complexity - O(1)","title":"Activity Selection Problem"},{"location":"Data%20Structures%20And%20Algorithms/17-greedy-algorithm/03-activity-selection-problem/#activity-selection-problem","text":"We are given n activities with their start and finish times. Select the maximum number of activities that can be performed by a single person, assuming that a person can only work on a single activity at a time. We can sort the table by the finish time. Now we can go through the elements and see if we can take it or not, by simply checking if the next element's starting time is more or equal the previous element's starting time. The answer is: A3, A2, A5, A6 . So the algorithm for this problem is as follows: 1. Sort the activities according to their finish time 2. Select the first activity from the sorted array and print it 3. Do following for remainint activities in the sorted array: - If the start time of the activity is greated or equal to the finish time of previously selected activity then select this activity and print it. ActivitySelectionProblem(A) sort(A,finishTime) --------------------------------------------- O(n log n) previousActivity = first activity ------------------------------ O(1) print first activity ------------------------------------------- O(1) loop: i = 1 to n - 1 ------------------------------------------- O(n) if A[startTime] of i >= A[finishTime] of PreviousActivity -- O(1) print A[startTime], A[finishTime] ---------------------- O(1) Time complexity - O(n log n) + O(n) = O(n log n) Space complexity - O(1)","title":"Activity Selection Problem"},{"location":"Data%20Structures%20And%20Algorithms/17-greedy-algorithm/04-coin-change-problem/","text":"Coin change problem \u00b6 Given a value V, we want to make changes for V dollars. We have infinite supply of each of the denominations in US currency. i.e. we have infinite supply of {1,2,5,10,20,50,100,500,1000} valued coins/notes, which is the miniumum number of coins and/or notes needed to make the changes for $V? Example \u00b6 Input value V=70 Output = 2 Breakdown - $50 * 1 + $20 * 1 Example 2 \u00b6 Input value V=121 Output - 3 Breakdown - $100 * 1 + $20 * 1 + $1 * 1 Solution \u00b6 Initialize results as empty Find the largest denomination that is smaller than V Add found denomination to result. Substract value of found denomination from V. If V becomes 0, then print result. Else go to step 2 CoinChangeProblem(N) initialize result as empty loop: until it self breaks MaxDenomination = FindLargestDenominationLesserThan(N) Insert(result, MaxDenomination) N = N - MaxDenomination if N == 0 print result and exit Time Complexity - O(n) Space Complexity - O(V) (length of the array to return)","title":"Coin change problem"},{"location":"Data%20Structures%20And%20Algorithms/17-greedy-algorithm/04-coin-change-problem/#coin-change-problem","text":"Given a value V, we want to make changes for V dollars. We have infinite supply of each of the denominations in US currency. i.e. we have infinite supply of {1,2,5,10,20,50,100,500,1000} valued coins/notes, which is the miniumum number of coins and/or notes needed to make the changes for $V?","title":"Coin change problem"},{"location":"Data%20Structures%20And%20Algorithms/17-greedy-algorithm/04-coin-change-problem/#example","text":"Input value V=70 Output = 2 Breakdown - $50 * 1 + $20 * 1","title":"Example"},{"location":"Data%20Structures%20And%20Algorithms/17-greedy-algorithm/04-coin-change-problem/#example-2","text":"Input value V=121 Output - 3 Breakdown - $100 * 1 + $20 * 1 + $1 * 1","title":"Example 2"},{"location":"Data%20Structures%20And%20Algorithms/17-greedy-algorithm/04-coin-change-problem/#solution","text":"Initialize results as empty Find the largest denomination that is smaller than V Add found denomination to result. Substract value of found denomination from V. If V becomes 0, then print result. Else go to step 2 CoinChangeProblem(N) initialize result as empty loop: until it self breaks MaxDenomination = FindLargestDenominationLesserThan(N) Insert(result, MaxDenomination) N = N - MaxDenomination if N == 0 print result and exit Time Complexity - O(n) Space Complexity - O(V) (length of the array to return)","title":"Solution"},{"location":"Data%20Structures%20And%20Algorithms/17-greedy-algorithm/05-fractional-knapstack-problem/","text":"Fractional Knapstack Problem \u00b6 Fill the knapstack such that the value is maximum and total weight is atmost W. Items can be broken down to maximize knapsack value. Assumtions \u00b6 Every type of itam has only 1 qty available Items can be broken down into fractions Example \u00b6 So we have 3 types of items: - value: $100, weight: 20kg (A) - value: $120, weight: 30kg (B) - value: $60, weight: 10kg (C) Max knapsack capacity is 50kg. So, we take C, A and 2/3 of B Total weight - 10 + 20 + 30 2/3 = 50 Total value - 60+100+120 2/3 = 240 Algorithm \u00b6 Calculate the ration (value/weight) for each item Sort the items based on this ration Take the items with the highes ration sequentially as long as the weight allows At the end add the next item as much (fractional) as we can FractionalKnapsackProblem(CapactityOfKnsapsack, A[][]) initialize ratio[] ratio = CalculateRatio(A) SortDescending(ratio[]) loop: i = 0 to n - 1 if( totalWeight + currentItemWeight < knapsackCapacity ) consume (A[i][0]) totalWeight = totalWeight + currentItemWeight else consumeFraction(A[i][0]); break; Time complexity - O(n log n) Space complexity - O(n)","title":"Fractional Knapstack Problem"},{"location":"Data%20Structures%20And%20Algorithms/17-greedy-algorithm/05-fractional-knapstack-problem/#fractional-knapstack-problem","text":"Fill the knapstack such that the value is maximum and total weight is atmost W. Items can be broken down to maximize knapsack value.","title":"Fractional Knapstack Problem"},{"location":"Data%20Structures%20And%20Algorithms/17-greedy-algorithm/05-fractional-knapstack-problem/#assumtions","text":"Every type of itam has only 1 qty available Items can be broken down into fractions","title":"Assumtions"},{"location":"Data%20Structures%20And%20Algorithms/17-greedy-algorithm/05-fractional-knapstack-problem/#example","text":"So we have 3 types of items: - value: $100, weight: 20kg (A) - value: $120, weight: 30kg (B) - value: $60, weight: 10kg (C) Max knapsack capacity is 50kg. So, we take C, A and 2/3 of B Total weight - 10 + 20 + 30 2/3 = 50 Total value - 60+100+120 2/3 = 240","title":"Example"},{"location":"Data%20Structures%20And%20Algorithms/17-greedy-algorithm/05-fractional-knapstack-problem/#algorithm","text":"Calculate the ration (value/weight) for each item Sort the items based on this ration Take the items with the highes ration sequentially as long as the weight allows At the end add the next item as much (fractional) as we can FractionalKnapsackProblem(CapactityOfKnsapsack, A[][]) initialize ratio[] ratio = CalculateRatio(A) SortDescending(ratio[]) loop: i = 0 to n - 1 if( totalWeight + currentItemWeight < knapsackCapacity ) consume (A[i][0]) totalWeight = totalWeight + currentItemWeight else consumeFraction(A[i][0]); break; Time complexity - O(n log n) Space complexity - O(n)","title":"Algorithm"},{"location":"Data%20Structures%20And%20Algorithms/18-divide-and-conquer/01-what-is-divide-and-conquer/","text":"What is Divide and Conquer \u00b6 Divide & conquer is an algorithm design paradigm which works by recursively breaking down a problem into sub-problems of similar type, until these become simple enough to be solved directly. The solutions to add the sub-problems are then combined to give a solution to the original problem. Properties of Divide and Conquer \u00b6 Optimal substructure: Any problem has optimal substructure property if it's overall optimal solution can be constructed from the optimal solutions of it's subproblem. Example: Fib(n) = Fib(n - 1) + Fib(n - 2). Divide and conquer is most effective when problem has optimal substructure property like it is in Merge Sort, Quick Sort and Binary Search algorithms.","title":"What is Divide and Conquer"},{"location":"Data%20Structures%20And%20Algorithms/18-divide-and-conquer/01-what-is-divide-and-conquer/#what-is-divide-and-conquer","text":"Divide & conquer is an algorithm design paradigm which works by recursively breaking down a problem into sub-problems of similar type, until these become simple enough to be solved directly. The solutions to add the sub-problems are then combined to give a solution to the original problem.","title":"What is Divide and Conquer"},{"location":"Data%20Structures%20And%20Algorithms/18-divide-and-conquer/01-what-is-divide-and-conquer/#properties-of-divide-and-conquer","text":"Optimal substructure: Any problem has optimal substructure property if it's overall optimal solution can be constructed from the optimal solutions of it's subproblem. Example: Fib(n) = Fib(n - 1) + Fib(n - 2). Divide and conquer is most effective when problem has optimal substructure property like it is in Merge Sort, Quick Sort and Binary Search algorithms.","title":"Properties of Divide and Conquer"},{"location":"Data%20Structures%20And%20Algorithms/18-divide-and-conquer/02-already-known-algorithms/","text":"Already known algorithms \u00b6 Binary Search \u00b6 BinarySearch(int findNumber, int arr[], start, end) mid = FindMid(arr[], start, end) if mid > findNumber BinarySearch(int findNumber, int arr[], start, mid) else if mid < findNumber BinarySearch(int findNumber, int arr[], mid, end) else if mid == findNumber return mid Merge Sort algorithm \u00b6 mergeSort(A, l, r) if r > l middle m = (l+r)/2 mergeSort(A,l,m) mergeSort(A,m+1,r) merge(A,I,m,r) merge(A,p,m,r) create tmp arrays L & R and copy A,p,m into L & A,m+1,r into R i = j = 0 loop: k = p to r if L[i] < R[j] A[k] = L[i]; i++ else A[k] = R[j]; j++ Quick Sort \u00b6 QuickSort(A, p, q) if(p < q) r = partition(A,p,q) QuickSort(A,p,r-1) QuickSort(A,r+1,p) partition(A,p, q) pivot = q i = p - 1 for(j = p to q) if A[i] <= A[pivot] increment i and swap(A[i], [j])","title":"Already known algorithms"},{"location":"Data%20Structures%20And%20Algorithms/18-divide-and-conquer/02-already-known-algorithms/#already-known-algorithms","text":"","title":"Already known algorithms"},{"location":"Data%20Structures%20And%20Algorithms/18-divide-and-conquer/02-already-known-algorithms/#binary-search","text":"BinarySearch(int findNumber, int arr[], start, end) mid = FindMid(arr[], start, end) if mid > findNumber BinarySearch(int findNumber, int arr[], start, mid) else if mid < findNumber BinarySearch(int findNumber, int arr[], mid, end) else if mid == findNumber return mid","title":"Binary Search"},{"location":"Data%20Structures%20And%20Algorithms/18-divide-and-conquer/02-already-known-algorithms/#merge-sort-algorithm","text":"mergeSort(A, l, r) if r > l middle m = (l+r)/2 mergeSort(A,l,m) mergeSort(A,m+1,r) merge(A,I,m,r) merge(A,p,m,r) create tmp arrays L & R and copy A,p,m into L & A,m+1,r into R i = j = 0 loop: k = p to r if L[i] < R[j] A[k] = L[i]; i++ else A[k] = R[j]; j++","title":"Merge Sort algorithm"},{"location":"Data%20Structures%20And%20Algorithms/18-divide-and-conquer/02-already-known-algorithms/#quick-sort","text":"QuickSort(A, p, q) if(p < q) r = partition(A,p,q) QuickSort(A,p,r-1) QuickSort(A,r+1,p) partition(A,p, q) pivot = q i = p - 1 for(j = p to q) if A[i] <= A[pivot] increment i and swap(A[i], [j])","title":"Quick Sort"},{"location":"Data%20Structures%20And%20Algorithms/18-divide-and-conquer/03-fibonacci-series/","text":"Fibonacci series \u00b6 A series of numbers in which each number (fibonacci number) is the sum of the two preceding numbers. First 2 numbers by definition are 0 and 1. Example: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, ... FibonaccyRecursive(n) if(n < 1) return error // base case else if (n == 1) return 0 else if (n == 2) return 1 else return FibonacciRecursive(n - 1) + FibonnaciRecursive(n - 2) So, when we need to find a value of Fib(6) we need to calculate all these values:","title":"Fibonacci series"},{"location":"Data%20Structures%20And%20Algorithms/18-divide-and-conquer/03-fibonacci-series/#fibonacci-series","text":"A series of numbers in which each number (fibonacci number) is the sum of the two preceding numbers. First 2 numbers by definition are 0 and 1. Example: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, ... FibonaccyRecursive(n) if(n < 1) return error // base case else if (n == 1) return 0 else if (n == 2) return 1 else return FibonacciRecursive(n - 1) + FibonnaciRecursive(n - 2) So, when we need to find a value of Fib(6) we need to calculate all these values:","title":"Fibonacci series"},{"location":"Data%20Structures%20And%20Algorithms/18-divide-and-conquer/04-number-factor/","text":"Nubmer factor \u00b6 Problem statement: given N, count the number of ways to express N as sum of 1, 3 and 4. Example: \u00b6 n = 4 Number of ways = 4 Explanation: Following are the four ways we can express n: {4}, {1,3}, {3,1}, {1,1,1,1} Example 2 \u00b6 n = 5 Number of ways = 6 {4,1}, {1,4}, {3,1,1}, {1,1,3}, {1,1,1,1,1} Applying magic framework \u00b6 If we would apply the magic framework to this problem, we would see that: The greedy choice is not applicable here. That is because we can't really tell if we have taken the best sub-solution result - we can't tell if we pick number 1 that is the best pick of all the numbers. It does have an optimal substructure. We can break down the problem in smaller, similar sub-problems. The algorithm \u00b6 public int waysToGetN(int n) if(n == 0 || n == 1 || n == 2) // {}, {1}, {1,1} base cases return 1 if(n == 3) return 2 //{1,1,1}, {3} int subtract1 = waysToGetN(n-1) int subtract1 = waysToGetN(n-3) int subtract1 = waysToGetN(n-4) return subtract1 + subtract3 + subtract4","title":"Nubmer factor"},{"location":"Data%20Structures%20And%20Algorithms/18-divide-and-conquer/04-number-factor/#nubmer-factor","text":"Problem statement: given N, count the number of ways to express N as sum of 1, 3 and 4.","title":"Nubmer factor"},{"location":"Data%20Structures%20And%20Algorithms/18-divide-and-conquer/04-number-factor/#example","text":"n = 4 Number of ways = 4 Explanation: Following are the four ways we can express n: {4}, {1,3}, {3,1}, {1,1,1,1}","title":"Example:"},{"location":"Data%20Structures%20And%20Algorithms/18-divide-and-conquer/04-number-factor/#example-2","text":"n = 5 Number of ways = 6 {4,1}, {1,4}, {3,1,1}, {1,1,3}, {1,1,1,1,1}","title":"Example 2"},{"location":"Data%20Structures%20And%20Algorithms/18-divide-and-conquer/04-number-factor/#applying-magic-framework","text":"If we would apply the magic framework to this problem, we would see that: The greedy choice is not applicable here. That is because we can't really tell if we have taken the best sub-solution result - we can't tell if we pick number 1 that is the best pick of all the numbers. It does have an optimal substructure. We can break down the problem in smaller, similar sub-problems.","title":"Applying magic framework"},{"location":"Data%20Structures%20And%20Algorithms/18-divide-and-conquer/04-number-factor/#the-algorithm","text":"public int waysToGetN(int n) if(n == 0 || n == 1 || n == 2) // {}, {1}, {1,1} base cases return 1 if(n == 3) return 2 //{1,1,1}, {3} int subtract1 = waysToGetN(n-1) int subtract1 = waysToGetN(n-3) int subtract1 = waysToGetN(n-4) return subtract1 + subtract3 + subtract4","title":"The algorithm"},{"location":"Data%20Structures%20And%20Algorithms/18-divide-and-conquer/05-house-thief-problem/","text":"House Thief Problem \u00b6 There are N houses built in a line, each of which contains some value in it. A thief is going to steal the maximal value of these houses. But he can't steal in to adjacent houses. What is the maximum stolen value? Example: Input: {6, 7, 1, 30, 8, 2, 4} Output: 41 Thief will steal: House #7, 30, 4 Example 2: Input: {20, 5, 1, 13, 6, 11, 40} Output: 73 Thief will steal: House #20, 13, 40 Applying Magic Framework \u00b6 Is Greedy Choice applicable? We cannot guarantee that we take a number and it will be part of the best combination to pick. We can break it into similar subproblems Algorithm \u00b6 private int maxMoneyRecursive( int[] HouseNetWorth, int currentIndex ) int stealCurrentHouse = HouseNetWorth[currentIndex] + maxMoneyRecursive(HouseNetWorth, currentIndex + 2) int skipCurrentHouse = maxMoneyRecursive(HouseNetWorth, currentIndex + 1) return Math.max(stealCurrentHouse, skipCurrentHouse)","title":"House Thief Problem"},{"location":"Data%20Structures%20And%20Algorithms/18-divide-and-conquer/05-house-thief-problem/#house-thief-problem","text":"There are N houses built in a line, each of which contains some value in it. A thief is going to steal the maximal value of these houses. But he can't steal in to adjacent houses. What is the maximum stolen value? Example: Input: {6, 7, 1, 30, 8, 2, 4} Output: 41 Thief will steal: House #7, 30, 4 Example 2: Input: {20, 5, 1, 13, 6, 11, 40} Output: 73 Thief will steal: House #20, 13, 40","title":"House Thief Problem"},{"location":"Data%20Structures%20And%20Algorithms/18-divide-and-conquer/05-house-thief-problem/#applying-magic-framework","text":"Is Greedy Choice applicable? We cannot guarantee that we take a number and it will be part of the best combination to pick. We can break it into similar subproblems","title":"Applying Magic Framework"},{"location":"Data%20Structures%20And%20Algorithms/18-divide-and-conquer/05-house-thief-problem/#algorithm","text":"private int maxMoneyRecursive( int[] HouseNetWorth, int currentIndex ) int stealCurrentHouse = HouseNetWorth[currentIndex] + maxMoneyRecursive(HouseNetWorth, currentIndex + 2) int skipCurrentHouse = maxMoneyRecursive(HouseNetWorth, currentIndex + 1) return Math.max(stealCurrentHouse, skipCurrentHouse)","title":"Algorithm"},{"location":"Data%20Structures%20And%20Algorithms/18-divide-and-conquer/06-convert-one-string-to-another/","text":"Convert one string to another \u00b6 We are given strings s1 and s2. We need to convert s2 into s1 by deleting, inserting or replacing characters. Write a function to calculate the count of the minimum number of edit operations. Example: s1 = \"catch\" s2 = \"carch\" Output: 1 Explanation: We just need to replace r. s1 = \"table\" s2 = \"tbres\" Output: 3 Explanation: We need to insert a at second position, replace r with l. Delete s. We can divide the problem into sub-problems by firstly considering the whole string, then a substring, then a substring of that string and so on. private int findMinOperationsAux(String s1, String s2, int i1, int i2) if i1 == s1.length // if we have reached the end of s1, so rest of the characters of s2 needs to be deleted. return s2.length - i2 if i2 == s2.length // we have reached the end of s2, so rest of the characters needs to be inserted in s2 return s1.length - i1 if s1.charAt(i1) == s2.charAt(i2) // if string have a matching character, recursively match for the remaining lengths return findMinOperationsAux(s1, s2, i1+1, i2+1) int c1 = 1 + findMinOperationsAux(s1,s2,i1 + 1, i2) // insertion int c2 = 1 + findMinOperationsAux(s1,s2,i1, i2+1) // deletion int c3 = 1 + findMinOperationsAux(s1,s2,i1 + 1, i2 + 1) // replacement return Min(c1, c2, c3)","title":"Convert one string to another"},{"location":"Data%20Structures%20And%20Algorithms/18-divide-and-conquer/06-convert-one-string-to-another/#convert-one-string-to-another","text":"We are given strings s1 and s2. We need to convert s2 into s1 by deleting, inserting or replacing characters. Write a function to calculate the count of the minimum number of edit operations. Example: s1 = \"catch\" s2 = \"carch\" Output: 1 Explanation: We just need to replace r. s1 = \"table\" s2 = \"tbres\" Output: 3 Explanation: We need to insert a at second position, replace r with l. Delete s. We can divide the problem into sub-problems by firstly considering the whole string, then a substring, then a substring of that string and so on. private int findMinOperationsAux(String s1, String s2, int i1, int i2) if i1 == s1.length // if we have reached the end of s1, so rest of the characters of s2 needs to be deleted. return s2.length - i2 if i2 == s2.length // we have reached the end of s2, so rest of the characters needs to be inserted in s2 return s1.length - i1 if s1.charAt(i1) == s2.charAt(i2) // if string have a matching character, recursively match for the remaining lengths return findMinOperationsAux(s1, s2, i1+1, i2+1) int c1 = 1 + findMinOperationsAux(s1,s2,i1 + 1, i2) // insertion int c2 = 1 + findMinOperationsAux(s1,s2,i1, i2+1) // deletion int c3 = 1 + findMinOperationsAux(s1,s2,i1 + 1, i2 + 1) // replacement return Min(c1, c2, c3)","title":"Convert one string to another"},{"location":"Data%20Structures%20And%20Algorithms/18-divide-and-conquer/07-zero-one-knapsack/","text":"Zero/One knapsack \u00b6 Given the weights and profits of N items. We are asked to put these items in a knapsack which has capacity C. Restriction is, that we cannot break the item into smaller units. Challenge is to find the maximum profit from the items in the knapsack. Example: Items: {Mango, Apple, Banana, Orange} Profits: {31, 26, 72, 17} Weights: {3, 1, 5, 2} Knapsack capacity: 7 Now we will try to find max profit by collection different combinations of fruits in the knapsack, such that their total weight is not more than 7: Mango + Apply + Orange (total weight 6) => 74 Orange + Banana (total weight 7) => 89 Apple + Banana (total weight 6) => 98 int knapsackAux(int[] profits, int[] weights, int capacity, int currentIndex) if capacity <= 0 || currentIndex < 0 || currentIndex >= profits.length return 0 // base case int profit1 = 0 if (weights[currentIndex] <= capacity) profit1 = profits[currentIndex] + knapsackAux(profits, weights, capacity - weights[currentIndex], currentIndex + 1) int profit2 = knapsacAux(profits, weights, capacity, currentIndex + 1) // not taking the current element return Math.max(profit1, profit2)","title":"Zero/One knapsack"},{"location":"Data%20Structures%20And%20Algorithms/18-divide-and-conquer/07-zero-one-knapsack/#zeroone-knapsack","text":"Given the weights and profits of N items. We are asked to put these items in a knapsack which has capacity C. Restriction is, that we cannot break the item into smaller units. Challenge is to find the maximum profit from the items in the knapsack. Example: Items: {Mango, Apple, Banana, Orange} Profits: {31, 26, 72, 17} Weights: {3, 1, 5, 2} Knapsack capacity: 7 Now we will try to find max profit by collection different combinations of fruits in the knapsack, such that their total weight is not more than 7: Mango + Apply + Orange (total weight 6) => 74 Orange + Banana (total weight 7) => 89 Apple + Banana (total weight 6) => 98 int knapsackAux(int[] profits, int[] weights, int capacity, int currentIndex) if capacity <= 0 || currentIndex < 0 || currentIndex >= profits.length return 0 // base case int profit1 = 0 if (weights[currentIndex] <= capacity) profit1 = profits[currentIndex] + knapsackAux(profits, weights, capacity - weights[currentIndex], currentIndex + 1) int profit2 = knapsacAux(profits, weights, capacity, currentIndex + 1) // not taking the current element return Math.max(profit1, profit2)","title":"Zero/One knapsack"},{"location":"Data%20Structures%20And%20Algorithms/18-divide-and-conquer/08-longest-common-subsequence/","text":"Longest common subsequence \u00b6 We are given two strings s1 and s2. We need to find the length of the longes subsequence which is common in both of the strings. Subsequence is a sequence that can be derived from another sequence by deleting some elements without changing the order of the remaining elements. Example: s1 = elephant s2 = eretpat Output: 5 Explanation: The longest substring is eepat s1 = houdini s2 = hdupti Output: 3 Explanation: The longest substring is hui int findLCSLengthAux(String s1, String s2, int i1, int i2) if(i1 == s1.length || i2 == s2.length) // base case return 0 int c3 = 0 if s1.charAt(i1) == s2.charAt(i2) // current char matches c3 = 1 + findLCSLengthAux(s1, s2, i1 + 1, i2 + 1) int c1 = findLCSLengthAux(s1, s2, i1, i2 + 1) int c2 = findLCSLengthAux(s1, s2, i1 + 1, i2) return Max(c1, c2, c3)","title":"Longest common subsequence"},{"location":"Data%20Structures%20And%20Algorithms/18-divide-and-conquer/08-longest-common-subsequence/#longest-common-subsequence","text":"We are given two strings s1 and s2. We need to find the length of the longes subsequence which is common in both of the strings. Subsequence is a sequence that can be derived from another sequence by deleting some elements without changing the order of the remaining elements. Example: s1 = elephant s2 = eretpat Output: 5 Explanation: The longest substring is eepat s1 = houdini s2 = hdupti Output: 3 Explanation: The longest substring is hui int findLCSLengthAux(String s1, String s2, int i1, int i2) if(i1 == s1.length || i2 == s2.length) // base case return 0 int c3 = 0 if s1.charAt(i1) == s2.charAt(i2) // current char matches c3 = 1 + findLCSLengthAux(s1, s2, i1 + 1, i2 + 1) int c1 = findLCSLengthAux(s1, s2, i1, i2 + 1) int c2 = findLCSLengthAux(s1, s2, i1 + 1, i2) return Max(c1, c2, c3)","title":"Longest common subsequence"},{"location":"Data%20Structures%20And%20Algorithms/18-divide-and-conquer/09-longest-palindromic-subsequence/","text":"Longest palindromic subsequence \u00b6 We are given a string S. We need to find the length of it's palindromic subsequence LPS in the given S string. Palindrome is a string that reads the same backwards as well as forwards and can be of odd or even length. Example: INPUT: ELRMENMET Output: 5 LPS is EMEME INPUT AMEEMWMEA Output: 6 LPS IS AMEEMA int LPSAux( String st, int startIndex, int endIndex ) if startIndex > endIndex // dont need to traverse more than 1/2 of the string return 0 if startIndex == endIndex // there is only 1 character return 1 int count1 = 0 if st.charAt(startIndex == st.charAt(endIndex)) count1 = 2+ LPSAux(st, startIndex + 1, endIndex - 1) int count2 = LPSAux(st, startIndex + 1, endIndex) int count3 = LPSAUx(st, startIndex, endIndex - 1) return Max(count1, count2, count3)","title":"Longest palindromic subsequence"},{"location":"Data%20Structures%20And%20Algorithms/18-divide-and-conquer/09-longest-palindromic-subsequence/#longest-palindromic-subsequence","text":"We are given a string S. We need to find the length of it's palindromic subsequence LPS in the given S string. Palindrome is a string that reads the same backwards as well as forwards and can be of odd or even length. Example: INPUT: ELRMENMET Output: 5 LPS is EMEME INPUT AMEEMWMEA Output: 6 LPS IS AMEEMA int LPSAux( String st, int startIndex, int endIndex ) if startIndex > endIndex // dont need to traverse more than 1/2 of the string return 0 if startIndex == endIndex // there is only 1 character return 1 int count1 = 0 if st.charAt(startIndex == st.charAt(endIndex)) count1 = 2+ LPSAux(st, startIndex + 1, endIndex - 1) int count2 = LPSAux(st, startIndex + 1, endIndex) int count3 = LPSAUx(st, startIndex, endIndex - 1) return Max(count1, count2, count3)","title":"Longest palindromic subsequence"},{"location":"Data%20Structures%20And%20Algorithms/18-divide-and-conquer/10-longest-palindromic-substring/","text":"Longes palindromic substring \u00b6 We are given a string S. We need to find the length of it's Longes Palindromic Sunstring (LPS). Example: Input: ABCYRCFBTUA Palindromic subsequence: ABCYCBA Palindromic substring: A Input: ABCCBUA Output: 4 Explanation: LPS is BCCB int lps_aux(String string, int startIndex, int endIndex) if startIndex > endIndex // don't need to traverse more than 1/2 string return 0 if startIndex == endIndex // only one character return 1 int c1 = 0 if(string.charAt(startIndex) == string.charAt(endIndex)) // add 2 to the existing known palindrome length only if remaining string is a palindrom too int remainingLength = endIndex - startIndex - 1 if remainingLength == lpsaux(string, startIndex + 1, endIndex - 1) c1 = remainingLength + 2 int c2 = lps_aux(string, startIndex + 1, endIndex) int c3 = lps_aux(string, startIndex, endIndex - 1) return Math.max(c1, c2, c3)","title":"Longes palindromic substring"},{"location":"Data%20Structures%20And%20Algorithms/18-divide-and-conquer/10-longest-palindromic-substring/#longes-palindromic-substring","text":"We are given a string S. We need to find the length of it's Longes Palindromic Sunstring (LPS). Example: Input: ABCYRCFBTUA Palindromic subsequence: ABCYCBA Palindromic substring: A Input: ABCCBUA Output: 4 Explanation: LPS is BCCB int lps_aux(String string, int startIndex, int endIndex) if startIndex > endIndex // don't need to traverse more than 1/2 string return 0 if startIndex == endIndex // only one character return 1 int c1 = 0 if(string.charAt(startIndex) == string.charAt(endIndex)) // add 2 to the existing known palindrome length only if remaining string is a palindrom too int remainingLength = endIndex - startIndex - 1 if remainingLength == lpsaux(string, startIndex + 1, endIndex - 1) c1 = remainingLength + 2 int c2 = lps_aux(string, startIndex + 1, endIndex) int c3 = lps_aux(string, startIndex, endIndex - 1) return Math.max(c1, c2, c3)","title":"Longes palindromic substring"},{"location":"Data%20Structures%20And%20Algorithms/18-divide-and-conquer/11-min-cost-to-reach-last-cell-in-2d-array/","text":"Min cost to reach last cell in 2d array \u00b6 We are given a 2D matrix, accessing each cell have a cost associated with it. We need to start from (0,0) cell and go till (n-1, n-1) cell. Challenge is to do the traversal in minimum cost. Example: int findMinCost(int[][] cost, int row, int col) if row == -1 || col == -1 return Integer.MAX_VALUE if row == 0 && col == 0 return cost[0][0] int minCost1 = findMinCost(cost, row-1, col) // get min cost if we go up int minCost2 = findMinCost(cost, row, col -1) // left int minCost = integer.min(minCost1, minCost2) int currentCollsCost = cost[row][col] return minCost + currentCellsCost","title":"Min cost to reach last cell in 2d array"},{"location":"Data%20Structures%20And%20Algorithms/18-divide-and-conquer/11-min-cost-to-reach-last-cell-in-2d-array/#min-cost-to-reach-last-cell-in-2d-array","text":"We are given a 2D matrix, accessing each cell have a cost associated with it. We need to start from (0,0) cell and go till (n-1, n-1) cell. Challenge is to do the traversal in minimum cost. Example: int findMinCost(int[][] cost, int row, int col) if row == -1 || col == -1 return Integer.MAX_VALUE if row == 0 && col == 0 return cost[0][0] int minCost1 = findMinCost(cost, row-1, col) // get min cost if we go up int minCost2 = findMinCost(cost, row, col -1) // left int minCost = integer.min(minCost1, minCost2) int currentCollsCost = cost[row][col] return minCost + currentCellsCost","title":"Min cost to reach last cell in 2d array"},{"location":"Data%20Structures%20And%20Algorithms/18-divide-and-conquer/12-number-of-paths-to-last-cell-with-given-cost/","text":"Number of paths to last cell with given cost \u00b6 We are given a 2D matrix. Accessing each cell has a cost associated to it. We need to start from (0,0) and go to (n-1, n-1) cell. We are given total cost to reach the end of the array. We can go only right or down cell from current cell. Challenge is to find the number of ways to reach end of matrix given the total cost. int numberOfPaths(int array[][], int row, int cost) if(cost < 0) return 0 if row == 0 && col == 0 return array[0][0] - cost == 0 ? 1 : 0 if row == 0 // at first row, we can only go left return NumberOfPaths(array, 0, col - 1, cost - array[row][col]) if col == 0 return NumberOfPaths(array, row - 1, 0, cost - array[row][col]) int noOfPathsFromPreviousRow = numberOfPaths(array, row - 1, col, cost - array[row][col]) int noOfPathsFromPreviousCol = numberOfPaths(array, row, col - 1, cost - array[row][col]) return noOfPathsFromPreviousRow + noOfPathsFromPreviousCol","title":"Number of paths to last cell with given cost"},{"location":"Data%20Structures%20And%20Algorithms/18-divide-and-conquer/12-number-of-paths-to-last-cell-with-given-cost/#number-of-paths-to-last-cell-with-given-cost","text":"We are given a 2D matrix. Accessing each cell has a cost associated to it. We need to start from (0,0) and go to (n-1, n-1) cell. We are given total cost to reach the end of the array. We can go only right or down cell from current cell. Challenge is to find the number of ways to reach end of matrix given the total cost. int numberOfPaths(int array[][], int row, int cost) if(cost < 0) return 0 if row == 0 && col == 0 return array[0][0] - cost == 0 ? 1 : 0 if row == 0 // at first row, we can only go left return NumberOfPaths(array, 0, col - 1, cost - array[row][col]) if col == 0 return NumberOfPaths(array, row - 1, 0, cost - array[row][col]) int noOfPathsFromPreviousRow = numberOfPaths(array, row - 1, col, cost - array[row][col]) int noOfPathsFromPreviousCol = numberOfPaths(array, row, col - 1, cost - array[row][col]) return noOfPathsFromPreviousRow + noOfPathsFromPreviousCol","title":"Number of paths to last cell with given cost"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/01-what-is-dynamic-programming/","text":"What is dynamic programming \u00b6 Dynamic programming is a method for solving a complex problem by breaking it down into a collection of simpler subproblems, solving each of those subproblems just once, and storing their solutions. The next time the same subproblem occurs, instead of recomputing it's solution, one simply looks up the previously computed solution, thereby saving computation time at the expense of a modest expenditure in storage space. Dynamic programming is mainly and optimization over plain divide and conquer. Top Down approach \u00b6 We can use Fibonacci series for an example. FibonacciRecursive(n) if(n < 1) return error else if (n == 1) return 0 else if (n == 2) return 1 return FibonacciRecursive(n-1) + FibonacciRecursive(n-2) Time complexity - O(1.6180)^n Space complexity - O(n) We can optimize this algorithm to memoize the results and increase the performance of the algorithm. int FibonacciAux(int[] memoize, int n) if n < 1 return error else if n == 1 return 0 else if n == 2 return 1 if memoize[n] == 0 memoize[n] = FibonacciAux(memoize, n-1) + FibonacciAux(memoize, n-2) return memoize[n] The top down approach breaks the problem into smaller subproblems. Then, these subproblems are solved and stored into memory if any of them ever comes up again. Bottom up approach \u00b6 In bottom up approach, we evaluate the function starting with the smallest possible input arguement value and then we work our way up. While computing the values we store all the computed values in memory. As larger arguments are evaluated, precomputed values for smaller arguments can be reused. Int this case all the subproblems that we might need in future are precomputed and then they are used to solve a bigger problem. int FibonacciBottomUp(int n) create a new arrayTable[] of size n if n < 1 return error table[0] = 0 table[1] = 1 for(int i = 3; i < n; i++) table[i] = table[i-2] + table[i-3] return table[n] Time Complexity - O(n) Space Complexity - O(n) Divide&Conquer vs Top Down vs Bottom Up \u00b6 Problem Divide & Conquer Top Down Bottom Up Fibonacci numbers O(1.6180)^n O(n) O(n) Top down vs Bottom up methods \u00b6 Top Down Bottom Up Ease of algorithm Easy to come up with solution as it is an extension of Divide & Conquer Not so easy to come up with a solution Run time of Algorithm Slow Fast Space efficiency Unnecessary use of extra Stack space. Can run out of stack space if computation is huge. No stack is used When to use Need a quick solution Need an efficient solution","title":"What is dynamic programming"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/01-what-is-dynamic-programming/#what-is-dynamic-programming","text":"Dynamic programming is a method for solving a complex problem by breaking it down into a collection of simpler subproblems, solving each of those subproblems just once, and storing their solutions. The next time the same subproblem occurs, instead of recomputing it's solution, one simply looks up the previously computed solution, thereby saving computation time at the expense of a modest expenditure in storage space. Dynamic programming is mainly and optimization over plain divide and conquer.","title":"What is dynamic programming"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/01-what-is-dynamic-programming/#top-down-approach","text":"We can use Fibonacci series for an example. FibonacciRecursive(n) if(n < 1) return error else if (n == 1) return 0 else if (n == 2) return 1 return FibonacciRecursive(n-1) + FibonacciRecursive(n-2) Time complexity - O(1.6180)^n Space complexity - O(n) We can optimize this algorithm to memoize the results and increase the performance of the algorithm. int FibonacciAux(int[] memoize, int n) if n < 1 return error else if n == 1 return 0 else if n == 2 return 1 if memoize[n] == 0 memoize[n] = FibonacciAux(memoize, n-1) + FibonacciAux(memoize, n-2) return memoize[n] The top down approach breaks the problem into smaller subproblems. Then, these subproblems are solved and stored into memory if any of them ever comes up again.","title":"Top Down approach"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/01-what-is-dynamic-programming/#bottom-up-approach","text":"In bottom up approach, we evaluate the function starting with the smallest possible input arguement value and then we work our way up. While computing the values we store all the computed values in memory. As larger arguments are evaluated, precomputed values for smaller arguments can be reused. Int this case all the subproblems that we might need in future are precomputed and then they are used to solve a bigger problem. int FibonacciBottomUp(int n) create a new arrayTable[] of size n if n < 1 return error table[0] = 0 table[1] = 1 for(int i = 3; i < n; i++) table[i] = table[i-2] + table[i-3] return table[n] Time Complexity - O(n) Space Complexity - O(n)","title":"Bottom up approach"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/01-what-is-dynamic-programming/#divideconquer-vs-top-down-vs-bottom-up","text":"Problem Divide & Conquer Top Down Bottom Up Fibonacci numbers O(1.6180)^n O(n) O(n)","title":"Divide&amp;Conquer vs Top Down vs Bottom Up"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/01-what-is-dynamic-programming/#top-down-vs-bottom-up-methods","text":"Top Down Bottom Up Ease of algorithm Easy to come up with solution as it is an extension of Divide & Conquer Not so easy to come up with a solution Run time of Algorithm Slow Fast Space efficiency Unnecessary use of extra Stack space. Can run out of stack space if computation is huge. No stack is used When to use Need a quick solution Need an efficient solution","title":"Top down vs Bottom up methods"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/02-number-factor-problem/","text":"Number factor problem \u00b6 We have a function public int waysToGetN(int n) if(n == 0 || n == 1 || n == 2) // {}, {1}, {1,1} base cases return 1 if(n == 3) return 2 //{1,1,1}, {3} int subtract1 = waysToGetN(n-1) int subtract3 = waysToGetN(n-3) int subtract4 = waysToGetN(n-4) return subtract1 + subtract3 + subtract4 We see that there are problems that we are solving more than once. like waysToGetN(4) . We can use dynamic programming to optimize it. public int waysToGetN(int n) return waysToGetN_TopDown(dp, n) public int waysToGetN_TopDown(int[] dp, int n) if(n == 0 || n == 1 || n == 2) // {}, {1}, {1,1} base cases return 1 if(n == 3) return 2 //{1,1,1}, {3} if dp[n] == 0 int subtract1 = waysToGetN_TopDown(dp, n - 1) int subtract3 = waysToGetN_TopDown(dp, n - 3) int subtract4 = waysToGetN_TopDown(dp, n - 4) dp[n] = subtract1 + subtract3 + subtract4 return dp[n]","title":"Number factor problem"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/02-number-factor-problem/#number-factor-problem","text":"We have a function public int waysToGetN(int n) if(n == 0 || n == 1 || n == 2) // {}, {1}, {1,1} base cases return 1 if(n == 3) return 2 //{1,1,1}, {3} int subtract1 = waysToGetN(n-1) int subtract3 = waysToGetN(n-3) int subtract4 = waysToGetN(n-4) return subtract1 + subtract3 + subtract4 We see that there are problems that we are solving more than once. like waysToGetN(4) . We can use dynamic programming to optimize it. public int waysToGetN(int n) return waysToGetN_TopDown(dp, n) public int waysToGetN_TopDown(int[] dp, int n) if(n == 0 || n == 1 || n == 2) // {}, {1}, {1,1} base cases return 1 if(n == 3) return 2 //{1,1,1}, {3} if dp[n] == 0 int subtract1 = waysToGetN_TopDown(dp, n - 1) int subtract3 = waysToGetN_TopDown(dp, n - 3) int subtract4 = waysToGetN_TopDown(dp, n - 4) dp[n] = subtract1 + subtract3 + subtract4 return dp[n]","title":"Number factor problem"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/03-house-thief-problem/","text":"House Thief Problem \u00b6 private int maxMoneyRecursive( int[] HouseNetWorth, int currentIndex ) int stealCurrentHouse = HouseNetWorth[currentIndex] + maxMoneyRecursive(HouseNetWorth, currentIndex + 2) int skipCurrentHouse = maxMoneyRecursive(HouseNetWorth, currentIndex + 1) return Math.max(stealCurrentHouse, skipCurrentHouse) Also here we can see that we are solving the same problems again and again like MaxMoney(3), MaxMoney(4), MaxMoney(5). We can optimize it using dynamic programming. Top Down approach \u00b6 int maxMoney_TopDown(int[] dp, int[] HouseNetWorth, int currentIndex) if( currentIndex >= HouseNetWorth.length) return 0 if dp[currentIndex] == 0 int stealCurrent = HouseNetWorth[currentIndex] + maxMoney_TopDown(dp, HouseNetWorth, currentIndex + 2) int skipCurrent = maxMoney_TopDown(dp, HouseNetWorth, currentIndex + 1) dp[currentIndex] = Math.max(stealCurrent, skipCurrent) return dp[currentIndex] Bottom Up approach \u00b6 int maxMoney_BottomUp(int wealth) int dp[] = new int[wealth.length + 2] dp[wealth.length] = 0 for int = wealth.length - 1; i >= 0; i-- dp[i] Math.max(wealth[i] + dp[i+2], dp[i+1]) return dp[0]","title":"House Thief Problem"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/03-house-thief-problem/#house-thief-problem","text":"private int maxMoneyRecursive( int[] HouseNetWorth, int currentIndex ) int stealCurrentHouse = HouseNetWorth[currentIndex] + maxMoneyRecursive(HouseNetWorth, currentIndex + 2) int skipCurrentHouse = maxMoneyRecursive(HouseNetWorth, currentIndex + 1) return Math.max(stealCurrentHouse, skipCurrentHouse) Also here we can see that we are solving the same problems again and again like MaxMoney(3), MaxMoney(4), MaxMoney(5). We can optimize it using dynamic programming.","title":"House Thief Problem"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/03-house-thief-problem/#top-down-approach","text":"int maxMoney_TopDown(int[] dp, int[] HouseNetWorth, int currentIndex) if( currentIndex >= HouseNetWorth.length) return 0 if dp[currentIndex] == 0 int stealCurrent = HouseNetWorth[currentIndex] + maxMoney_TopDown(dp, HouseNetWorth, currentIndex + 2) int skipCurrent = maxMoney_TopDown(dp, HouseNetWorth, currentIndex + 1) dp[currentIndex] = Math.max(stealCurrent, skipCurrent) return dp[currentIndex]","title":"Top Down approach"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/03-house-thief-problem/#bottom-up-approach","text":"int maxMoney_BottomUp(int wealth) int dp[] = new int[wealth.length + 2] dp[wealth.length] = 0 for int = wealth.length - 1; i >= 0; i-- dp[i] Math.max(wealth[i] + dp[i+2], dp[i+1]) return dp[0]","title":"Bottom Up approach"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/04-convert-one-string-to-another/","text":"Convert one string to another \u00b6 private int findMinOperationsAux(String s1, String s2, int i1, int i2) if i1 == s1.length // if we have reached the end of s1, so rest of the characters of s2 needs to be deleted. return s2.length - i2 if i2 == s2.length // we have reached the end of s2, so rest of the characters needs to be inserted in s2 return s1.length - i1 if s1.charAt(i1) == s2.charAt(i2) // if string have a matching character, recursively match for the remaining lengths return findMinOperationsAux(s1, s2, i1+1, i2+1) int c1 = 1 + findMinOperationsAux(s1,s2,i1 + 1, i2) // insertion int c2 = 1 + findMinOperationsAux(s1,s2,i1, i2+1) // deletion int c3 = 1 + findMinOperationsAux(s1,s2,i1 + 1, i2 + 1) // replacement return Min(c1, c2, c3) Top Down approach \u00b6 private int findMinOperationsRecursive(integer[][] dp, String s1, String s2, int i1, int i2) if dp[i1][i2] == null if i1 == s1.length // if we have reached the end of s1, so rest of the characters of s2 needs to be deleted. return s2.length - i2 if i2 == s2.length // we have reached the end of s2, so rest of the characters needs to be inserted in s2 return s1.length - i1 if s1.charAt(i1) == s2.charAt(i2) // if string have a matching character, recursively match for the remaining lengths return findMinOperationsAux(s1, s2, i1+1, i2+1) int c1 = 1 + findMinOperationsAux(s1,s2,i1 + 1, i2) // insertion int c2 = 1 + findMinOperationsAux(s1,s2,i1, i2+1) // deletion int c3 = 1 + findMinOperationsAux(s1,s2,i1 + 1, i2 + 1) // replacement dp[i1][i2] = 1+ Math.min(c1, c2, c3) return dp[i1][i2] Bottom Up approach \u00b6 int findMinOperations(String s1, String s2) int[][] dp = new int[s1.length +1][s2.length + 1]; for int i1 = 0; i1 < s1.length; i++ dp[i1][0] = i1 for int i2 = 0; i2 < s2.length; i2++ dp[i2][0] = i2 for int i1 = 1; i1 <= s1.length; i1++ for int i2 = 1; i2 <= s2.length; i2++ if s1.charAt(i1 - 1) == s2.charAt(i2 - 1) dp[i1][i2] = dp[i1 - 1][i2 - 1] else dp[i1][i2] = 1 + Math.min(dp[i1 - 1][i2], dp[i1][i2-1], dp[i1-1][i2-1]) return dp[0][0]","title":"Convert one string to another"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/04-convert-one-string-to-another/#convert-one-string-to-another","text":"private int findMinOperationsAux(String s1, String s2, int i1, int i2) if i1 == s1.length // if we have reached the end of s1, so rest of the characters of s2 needs to be deleted. return s2.length - i2 if i2 == s2.length // we have reached the end of s2, so rest of the characters needs to be inserted in s2 return s1.length - i1 if s1.charAt(i1) == s2.charAt(i2) // if string have a matching character, recursively match for the remaining lengths return findMinOperationsAux(s1, s2, i1+1, i2+1) int c1 = 1 + findMinOperationsAux(s1,s2,i1 + 1, i2) // insertion int c2 = 1 + findMinOperationsAux(s1,s2,i1, i2+1) // deletion int c3 = 1 + findMinOperationsAux(s1,s2,i1 + 1, i2 + 1) // replacement return Min(c1, c2, c3)","title":"Convert one string to another"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/04-convert-one-string-to-another/#top-down-approach","text":"private int findMinOperationsRecursive(integer[][] dp, String s1, String s2, int i1, int i2) if dp[i1][i2] == null if i1 == s1.length // if we have reached the end of s1, so rest of the characters of s2 needs to be deleted. return s2.length - i2 if i2 == s2.length // we have reached the end of s2, so rest of the characters needs to be inserted in s2 return s1.length - i1 if s1.charAt(i1) == s2.charAt(i2) // if string have a matching character, recursively match for the remaining lengths return findMinOperationsAux(s1, s2, i1+1, i2+1) int c1 = 1 + findMinOperationsAux(s1,s2,i1 + 1, i2) // insertion int c2 = 1 + findMinOperationsAux(s1,s2,i1, i2+1) // deletion int c3 = 1 + findMinOperationsAux(s1,s2,i1 + 1, i2 + 1) // replacement dp[i1][i2] = 1+ Math.min(c1, c2, c3) return dp[i1][i2]","title":"Top Down approach"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/04-convert-one-string-to-another/#bottom-up-approach","text":"int findMinOperations(String s1, String s2) int[][] dp = new int[s1.length +1][s2.length + 1]; for int i1 = 0; i1 < s1.length; i++ dp[i1][0] = i1 for int i2 = 0; i2 < s2.length; i2++ dp[i2][0] = i2 for int i1 = 1; i1 <= s1.length; i1++ for int i2 = 1; i2 <= s2.length; i2++ if s1.charAt(i1 - 1) == s2.charAt(i2 - 1) dp[i1][i2] = dp[i1 - 1][i2 - 1] else dp[i1][i2] = 1 + Math.min(dp[i1 - 1][i2], dp[i1][i2-1], dp[i1-1][i2-1]) return dp[0][0]","title":"Bottom Up approach"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/05-zero-one-knapsack-problem/","text":"Zero/One knapsack problem \u00b6 int knapsackAux(int[] profits, int[] weights, int capacity, int currentIndex) if capacity <= 0 || currentIndex < 0 || currentIndex >= profits.length return 0 // base case int profit1 = 0 if (weights[currentIndex] <= capacity) profit1 = profits[currentIndex] + knapsackAux(profits, weights, capacity - weights[currentIndex], currentIndex + 1) int profit2 = knapsacAux(profits, weights, capacity, currentIndex + 1) // not taking the current element return Math.max(profit1, profit2) Top Down appoach \u00b6 int knapsackAux(Integer[][] dp, int[] profits, int weights, int capacity, int currentIndex) if capacity <= 0 || currentIndex < 0 || currentIndex >= profits.length return 0 if dp[currentIndex][capacity] != null return dp[currentIndex][capacity] int profit1 = 0 if weights[currentIndex] <= capacity profit1 = profits[currentIndex] + knapsackAux(dp, profits, weights, capacity - weights[currentIndex], currentIndex + 1) int profit2 = knapsackAux(dp, profits, weights, capacity, currentIndex + 1) dp[currentIndex][capacity] = Math.max(profit1, profit2) return dp[currentIndex][capacity] Bottom Up approach \u00b6 int solveKnapsack( int[] profits, int[] weights, int capacity) if capacity <= 0 || profits.length == 0 || weights.length != profits.length return 0 int[][] dp = new int[numberOfRows][capacity + 1] for int row = numberOfRows - 2; row >= 0; row -- for int column = 1; column <= capacity; column++ int profit1 = profit2 = 0 if weights[row] <= column profit1 = profits[row] + dp[row + 1][column - weights[row]] profit2 = dp[row + 1][column] dp[row][column] = Math.max(profit1, profit2) return dp[0][capacity]","title":"Zero/One knapsack problem"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/05-zero-one-knapsack-problem/#zeroone-knapsack-problem","text":"int knapsackAux(int[] profits, int[] weights, int capacity, int currentIndex) if capacity <= 0 || currentIndex < 0 || currentIndex >= profits.length return 0 // base case int profit1 = 0 if (weights[currentIndex] <= capacity) profit1 = profits[currentIndex] + knapsackAux(profits, weights, capacity - weights[currentIndex], currentIndex + 1) int profit2 = knapsacAux(profits, weights, capacity, currentIndex + 1) // not taking the current element return Math.max(profit1, profit2)","title":"Zero/One knapsack problem"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/05-zero-one-knapsack-problem/#top-down-appoach","text":"int knapsackAux(Integer[][] dp, int[] profits, int weights, int capacity, int currentIndex) if capacity <= 0 || currentIndex < 0 || currentIndex >= profits.length return 0 if dp[currentIndex][capacity] != null return dp[currentIndex][capacity] int profit1 = 0 if weights[currentIndex] <= capacity profit1 = profits[currentIndex] + knapsackAux(dp, profits, weights, capacity - weights[currentIndex], currentIndex + 1) int profit2 = knapsackAux(dp, profits, weights, capacity, currentIndex + 1) dp[currentIndex][capacity] = Math.max(profit1, profit2) return dp[currentIndex][capacity]","title":"Top Down appoach"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/05-zero-one-knapsack-problem/#bottom-up-approach","text":"int solveKnapsack( int[] profits, int[] weights, int capacity) if capacity <= 0 || profits.length == 0 || weights.length != profits.length return 0 int[][] dp = new int[numberOfRows][capacity + 1] for int row = numberOfRows - 2; row >= 0; row -- for int column = 1; column <= capacity; column++ int profit1 = profit2 = 0 if weights[row] <= column profit1 = profits[row] + dp[row + 1][column - weights[row]] profit2 = dp[row + 1][column] dp[row][column] = Math.max(profit1, profit2) return dp[0][capacity]","title":"Bottom Up approach"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/06-longest-common-subsequence/","text":"Longest Common Subsequence \u00b6 int findLCSLengthAux(String s1, String s2, int i1, int i2) if(i1 == s1.length || i2 == s2.length) // base case return 0 int c3 = 0 if s1.charAt(i1) == s2.charAt(i2) // current char matches c3 = 1 + findLCSLengthAux(s1, s2, i1 + 1, i2 + 1) int c1 = findLCSLengthAux(s1, s2, i1, i2 + 1) int c2 = findLCSLengthAux(s1, s2, i1 + 1, i2) return Max(c1, c2, c3) Top Down approach \u00b6 int findLCSLengthAux(int[][] dp, String s1, String s2, int i1, int i2) if i1 == s1.length || i2 == s2.length return 0 int c3 = 0 if dp[i1][i2] == -1 if s1.charAt(i1) == s2.charAt(i2) c3 = 1 + findLCSLengthAux(dp, s1, s2, i1 + 1, i2 + 1) c1 = findLCSLengthAux(dp, s1, s2, i1, i2 + 1) c2 = findLCSLengthAux(dp, s1, s2, i1 + 1, i2) dp[i1][i2] = Math.max(c1,c2,c3) return dp[i1][i2] Bottom Up approach \u00b6 int findLCSLength(String s1, String s2) int[][] dp = new int[s1.length + 1][s2.length + 1] for int i = s1.length; i >= 1; i-- for int j = s2.length; j >= 1; j-- if s1.charAt(i - 1) == s2.charAt(j - 1) dp[i][j] = Math.max(1 + dp[i-1][j-1], dp[i][j+1], dp[i+1][j]) else dp[i][j] - Math.max(dp[i][j+1], dp[i+1][j]) return dp[0][0]","title":"Longest Common Subsequence"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/06-longest-common-subsequence/#longest-common-subsequence","text":"int findLCSLengthAux(String s1, String s2, int i1, int i2) if(i1 == s1.length || i2 == s2.length) // base case return 0 int c3 = 0 if s1.charAt(i1) == s2.charAt(i2) // current char matches c3 = 1 + findLCSLengthAux(s1, s2, i1 + 1, i2 + 1) int c1 = findLCSLengthAux(s1, s2, i1, i2 + 1) int c2 = findLCSLengthAux(s1, s2, i1 + 1, i2) return Max(c1, c2, c3)","title":"Longest Common Subsequence"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/06-longest-common-subsequence/#top-down-approach","text":"int findLCSLengthAux(int[][] dp, String s1, String s2, int i1, int i2) if i1 == s1.length || i2 == s2.length return 0 int c3 = 0 if dp[i1][i2] == -1 if s1.charAt(i1) == s2.charAt(i2) c3 = 1 + findLCSLengthAux(dp, s1, s2, i1 + 1, i2 + 1) c1 = findLCSLengthAux(dp, s1, s2, i1, i2 + 1) c2 = findLCSLengthAux(dp, s1, s2, i1 + 1, i2) dp[i1][i2] = Math.max(c1,c2,c3) return dp[i1][i2]","title":"Top Down approach"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/06-longest-common-subsequence/#bottom-up-approach","text":"int findLCSLength(String s1, String s2) int[][] dp = new int[s1.length + 1][s2.length + 1] for int i = s1.length; i >= 1; i-- for int j = s2.length; j >= 1; j-- if s1.charAt(i - 1) == s2.charAt(j - 1) dp[i][j] = Math.max(1 + dp[i-1][j-1], dp[i][j+1], dp[i+1][j]) else dp[i][j] - Math.max(dp[i][j+1], dp[i+1][j]) return dp[0][0]","title":"Bottom Up approach"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/07-longest-palindromic-subsequence/","text":"Longest Palindromic subsequence \u00b6 int LPSAux( String st, int startIndex, int endIndex ) if startIndex > endIndex // dont need to traverse more than 1/2 of the string return 0 if startIndex == endIndex // there is only 1 character return 1 int count1 = 0 if st.charAt(startIndex == st.charAt(endIndex)) count1 = 2+ LPSAux(st, startIndex + 1, endIndex - 1) int count2 = LPSAux(st, startIndex + 1, endIndex) int count3 = LPSAUx(st, startIndex, endIndex - 1) return Max(count1, count2, count3) Top Down approach \u00b6 int lps_Aux(int[][] dp, String string, int startIndex, int endIndex) if startIndex > endIndex return 0 if startIndex == endIndex return 1 if(dp[startIndex][endIndex] == null) int c3 = 0 if string.charAt(startIndex) == string.charAt(endIndex) c3 = 2 + lps_Aux(dp, string, startIndex + 1, endIndex -1) int c1 = lps_Aux(dp, string, startIndex + 1, endIndex) int c1 = lps_Aux(dp, string, startIndex, endIndex - 1) dp[startIndex][endIndex] = Math.max(c1,c2,c3) return dp[startIndex][endIndex] Bottom up approach \u00b6 int findLPSLength(String st) int[][] dp = new int[st.length][st.length] for int col = 0; col < st.length; col++ for int row = st.length; row >= 0; row-- if row > col dp[row][col] = 0 else if row == col dp[row][col] = 1 else if st.charAt(row) == st.charAt(col) dp[row][col] == Math.max( 2+dp[row+1][col-1], dp[row][col-1], dp[row+1][col] ) else dp[row][col] = Math.max(dp[row][col-1], dp[row+1][col]) return dp[0][st.length-1]","title":"Longest Palindromic subsequence"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/07-longest-palindromic-subsequence/#longest-palindromic-subsequence","text":"int LPSAux( String st, int startIndex, int endIndex ) if startIndex > endIndex // dont need to traverse more than 1/2 of the string return 0 if startIndex == endIndex // there is only 1 character return 1 int count1 = 0 if st.charAt(startIndex == st.charAt(endIndex)) count1 = 2+ LPSAux(st, startIndex + 1, endIndex - 1) int count2 = LPSAux(st, startIndex + 1, endIndex) int count3 = LPSAUx(st, startIndex, endIndex - 1) return Max(count1, count2, count3)","title":"Longest Palindromic subsequence"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/07-longest-palindromic-subsequence/#top-down-approach","text":"int lps_Aux(int[][] dp, String string, int startIndex, int endIndex) if startIndex > endIndex return 0 if startIndex == endIndex return 1 if(dp[startIndex][endIndex] == null) int c3 = 0 if string.charAt(startIndex) == string.charAt(endIndex) c3 = 2 + lps_Aux(dp, string, startIndex + 1, endIndex -1) int c1 = lps_Aux(dp, string, startIndex + 1, endIndex) int c1 = lps_Aux(dp, string, startIndex, endIndex - 1) dp[startIndex][endIndex] = Math.max(c1,c2,c3) return dp[startIndex][endIndex]","title":"Top Down approach"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/07-longest-palindromic-subsequence/#bottom-up-approach","text":"int findLPSLength(String st) int[][] dp = new int[st.length][st.length] for int col = 0; col < st.length; col++ for int row = st.length; row >= 0; row-- if row > col dp[row][col] = 0 else if row == col dp[row][col] = 1 else if st.charAt(row) == st.charAt(col) dp[row][col] == Math.max( 2+dp[row+1][col-1], dp[row][col-1], dp[row+1][col] ) else dp[row][col] = Math.max(dp[row][col-1], dp[row+1][col]) return dp[0][st.length-1]","title":"Bottom up approach"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/08-longest-palindromic-substring/","text":"Longest palindromic substring \u00b6 int lps_aux(String string, int startIndex, int endIndex) if startIndex > endIndex // don't need to traverse more than 1/2 string return 0 if startIndex == endIndex // only one character return 1 int c1 = 0 if(string.charAt(startIndex) == string.charAt(endIndex)) // add 2 to the existing known palindrome length only if remaining string is a palindrom too int remainingLength = endIndex - startIndex - 1 if remainingLength == lpsaux(string, startIndex + 1, endIndex - 1) c1 = remainingLength + 2 int c2 = lps_aux(string, startIndex + 1, endIndex) int c3 = lps_aux(string, startIndex, endIndex - 1) return Math.max(c1, c2, c3) Top down approach \u00b6 int lps_aux(int[][] dp, String string, int startIndex, int endIndex) if startIndex > endIndex return 0 if startIndex == endIndex return 1 if dp[startIndex][endIndex] == null int c1 = 0 if string.charAt(startIndex == string.charAt(endIndex)) int remainingLength = endIndex - startIndex - 1 if remainingLength == lpsaux(dp, string, startIndex + 1, endIndex - 1) c1 = remainingLength + 2 int c2 = lps_aux(dp, string, startIndex + 1, endIndex) int c3 = lps_aux(dp, string, startIndex, endIndex - 1) dp[startIndex][endIndex] = Math.max(c1,c2,c3) return dp[startIndex][endIndex] ## Bottom up approach int findLPSLength(String st) int [][] dp = new int[st.length][st.length] for int col = 0; col < st.length; col++ for int row = st.length; row >= 0; row-- if row > col dp[row][col] = 0 else if row == col dp[row][col] = 1 else if st.charAt(row) == st.charAt(col) int expectedSubstringLength = col - row - 1 int stringLengthToBeUsed = dp[row + 1][col-1] == expectedSubstringLength ? dp[row + 1][col] else dp[row][col]= Math.max(dp[row][col - 1], dp[row + 1][col]) return dp[0][st.length - 1] ```","title":"Longest palindromic substring"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/08-longest-palindromic-substring/#longest-palindromic-substring","text":"int lps_aux(String string, int startIndex, int endIndex) if startIndex > endIndex // don't need to traverse more than 1/2 string return 0 if startIndex == endIndex // only one character return 1 int c1 = 0 if(string.charAt(startIndex) == string.charAt(endIndex)) // add 2 to the existing known palindrome length only if remaining string is a palindrom too int remainingLength = endIndex - startIndex - 1 if remainingLength == lpsaux(string, startIndex + 1, endIndex - 1) c1 = remainingLength + 2 int c2 = lps_aux(string, startIndex + 1, endIndex) int c3 = lps_aux(string, startIndex, endIndex - 1) return Math.max(c1, c2, c3)","title":"Longest palindromic substring"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/08-longest-palindromic-substring/#top-down-approach","text":"int lps_aux(int[][] dp, String string, int startIndex, int endIndex) if startIndex > endIndex return 0 if startIndex == endIndex return 1 if dp[startIndex][endIndex] == null int c1 = 0 if string.charAt(startIndex == string.charAt(endIndex)) int remainingLength = endIndex - startIndex - 1 if remainingLength == lpsaux(dp, string, startIndex + 1, endIndex - 1) c1 = remainingLength + 2 int c2 = lps_aux(dp, string, startIndex + 1, endIndex) int c3 = lps_aux(dp, string, startIndex, endIndex - 1) dp[startIndex][endIndex] = Math.max(c1,c2,c3) return dp[startIndex][endIndex] ## Bottom up approach int findLPSLength(String st) int [][] dp = new int[st.length][st.length] for int col = 0; col < st.length; col++ for int row = st.length; row >= 0; row-- if row > col dp[row][col] = 0 else if row == col dp[row][col] = 1 else if st.charAt(row) == st.charAt(col) int expectedSubstringLength = col - row - 1 int stringLengthToBeUsed = dp[row + 1][col-1] == expectedSubstringLength ? dp[row + 1][col] else dp[row][col]= Math.max(dp[row][col - 1], dp[row + 1][col]) return dp[0][st.length - 1] ```","title":"Top down approach"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/09-min-cost-to-reach-last-cell-in-2d-array/","text":"Min cost to reach last cell in 2D array \u00b6 int findMinCost(int[][] cost, int row, int col) if row == -1 || col == -1 return Integer.MAX_VALUE if row == 0 && col == 0 return cost[0][0] int minCost1 = findMinCost(cost, row-1, col) // get min cost if we go up int minCost2 = findMinCost(cost, row, col -1) // left int minCost = integer.min(minCost1, minCost2) int currentCollsCost = cost[row][col] return minCost + currentCellsCost Top Down approach \u00b6 int findMinCost_aux(int[][] dp, int[][] array, int row, int col) if row == -1 || col == -1 return Integer.MAX_VALUE if row == 0 && col == 0 return array[0][0] if dp[row][col] == 0 int minCost1 = findMinCost_aux(dp, array, row - 1, col) int minCost2 = findMinCost_aux(dp, array, row, col - 1) int minCost = Integer.min(minCost1, minCost2) int currentCellsCost = array[row][col] dp[row][col] = minCost + currentCellsCost return dp[row][col] Bottom up \u00b6 int findMinCost(int[][] array, int row, int col) int[][] dpo = new int[row + 1][col + 1] for int i = 0; i <= col; i++ dp[0][i] = Integer.MAX_VALUE for int i = 0; i <= row; i++ dp[i][0] = Integer.MAX_VALUE dp[0][1] = 0 for int i = 1; i <= row; i ++ for int j = 1; j <= col; j++ dp[i][j] = Integer.min(dp[i-1][j], dp[i][j-1] + array[i-1][j-1]) return dp[row][col]","title":"Min cost to reach last cell in 2D array"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/09-min-cost-to-reach-last-cell-in-2d-array/#min-cost-to-reach-last-cell-in-2d-array","text":"int findMinCost(int[][] cost, int row, int col) if row == -1 || col == -1 return Integer.MAX_VALUE if row == 0 && col == 0 return cost[0][0] int minCost1 = findMinCost(cost, row-1, col) // get min cost if we go up int minCost2 = findMinCost(cost, row, col -1) // left int minCost = integer.min(minCost1, minCost2) int currentCollsCost = cost[row][col] return minCost + currentCellsCost","title":"Min cost to reach last cell in 2D array"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/09-min-cost-to-reach-last-cell-in-2d-array/#top-down-approach","text":"int findMinCost_aux(int[][] dp, int[][] array, int row, int col) if row == -1 || col == -1 return Integer.MAX_VALUE if row == 0 && col == 0 return array[0][0] if dp[row][col] == 0 int minCost1 = findMinCost_aux(dp, array, row - 1, col) int minCost2 = findMinCost_aux(dp, array, row, col - 1) int minCost = Integer.min(minCost1, minCost2) int currentCellsCost = array[row][col] dp[row][col] = minCost + currentCellsCost return dp[row][col]","title":"Top Down approach"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/09-min-cost-to-reach-last-cell-in-2d-array/#bottom-up","text":"int findMinCost(int[][] array, int row, int col) int[][] dpo = new int[row + 1][col + 1] for int i = 0; i <= col; i++ dp[0][i] = Integer.MAX_VALUE for int i = 0; i <= row; i++ dp[i][0] = Integer.MAX_VALUE dp[0][1] = 0 for int i = 1; i <= row; i ++ for int j = 1; j <= col; j++ dp[i][j] = Integer.min(dp[i-1][j], dp[i][j-1] + array[i-1][j-1]) return dp[row][col]","title":"Bottom up"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/10-number-of-paths-to-last-cell-with-given-cost/","text":"Number of paths to last cell given cost \u00b6 int numberOfPaths(int array[][], int row, int cost) if(cost < 0) return 0 if row == 0 && col == 0 return array[0][0] - cost == 0 ? 1 : 0 if row == 0 // at first row, we can only go left return NumberOfPaths(array, 0, col - 1, cost - array[row][col]) if col == 0 return NumberOfPaths(array, row - 1, 0, cost - array[row][col]) int noOfPathsFromPreviousRow = numberOfPaths(array, row - 1, col, cost - array[row][col]) int noOfPathsFromPreviousCol = numberOfPaths(array, row, col - 1, cost - array[row][col]) return noOfPathsFromPreviousRow + noOfPathsFromPreviousCol Top Down approach \u00b6 int numberOfPathsAux(int dp[][], int array[][], int row, int col, int cost) if cost < 0 return 0 if row == 0 && col == 0 return (array[0][0] - cost == 0) ? 1 : 0 if dp[row][col] == 0 if row == 0 dp[row][col] = numberOfPaths(array, 0, col - 1, cost - array[row][ col]) else if (col == 0) dp[row][col] = numberOfPaths(array, row - 1, 0, cost - array[row][col]) else int noOfPathsFromPreviousRow = numberOfPaths(array, row - 1, col, cost - array[row][col]) int noOfPathsFromPreviousCol = numberOfPaths(array, row, col - 1, cost - array[row][col]) dp[row][col] = noOfPathsFromPreviousRow + noOfPathsFrompreviousCol return dp[row][col] Bottom up approach \u00b6 NumberOfPathsToReachLastCell(arr,cost) for int row = numberOfRow - 2; row >= 0; row -- for int col = numberOfCol - 2; col >= 0; col -- NumberOfPathsNode tmp = new NumberOfPathsNode(array[row][col], dp[row][col+1], dp[row+1][col] costToReachLastCell) tmp.calculateNumberOfWaysSatisfyinfRightCell() tmp.calculateNumberOfWaysSatisfyinfDownCell() tmp.setNumberOfWaysToComeHereFromRightOrDown() dp[row][col] = tmp print dp[0][0]","title":"Number of paths to last cell given cost"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/10-number-of-paths-to-last-cell-with-given-cost/#number-of-paths-to-last-cell-given-cost","text":"int numberOfPaths(int array[][], int row, int cost) if(cost < 0) return 0 if row == 0 && col == 0 return array[0][0] - cost == 0 ? 1 : 0 if row == 0 // at first row, we can only go left return NumberOfPaths(array, 0, col - 1, cost - array[row][col]) if col == 0 return NumberOfPaths(array, row - 1, 0, cost - array[row][col]) int noOfPathsFromPreviousRow = numberOfPaths(array, row - 1, col, cost - array[row][col]) int noOfPathsFromPreviousCol = numberOfPaths(array, row, col - 1, cost - array[row][col]) return noOfPathsFromPreviousRow + noOfPathsFromPreviousCol","title":"Number of paths to last cell given cost"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/10-number-of-paths-to-last-cell-with-given-cost/#top-down-approach","text":"int numberOfPathsAux(int dp[][], int array[][], int row, int col, int cost) if cost < 0 return 0 if row == 0 && col == 0 return (array[0][0] - cost == 0) ? 1 : 0 if dp[row][col] == 0 if row == 0 dp[row][col] = numberOfPaths(array, 0, col - 1, cost - array[row][ col]) else if (col == 0) dp[row][col] = numberOfPaths(array, row - 1, 0, cost - array[row][col]) else int noOfPathsFromPreviousRow = numberOfPaths(array, row - 1, col, cost - array[row][col]) int noOfPathsFromPreviousCol = numberOfPaths(array, row, col - 1, cost - array[row][col]) dp[row][col] = noOfPathsFromPreviousRow + noOfPathsFrompreviousCol return dp[row][col]","title":"Top Down approach"},{"location":"Data%20Structures%20And%20Algorithms/19-dynamic-programming/10-number-of-paths-to-last-cell-with-given-cost/#bottom-up-approach","text":"NumberOfPathsToReachLastCell(arr,cost) for int row = numberOfRow - 2; row >= 0; row -- for int col = numberOfCol - 2; col >= 0; col -- NumberOfPathsNode tmp = new NumberOfPathsNode(array[row][col], dp[row][col+1], dp[row+1][col] costToReachLastCell) tmp.calculateNumberOfWaysSatisfyinfRightCell() tmp.calculateNumberOfWaysSatisfyinfDownCell() tmp.setNumberOfWaysToComeHereFromRightOrDown() dp[row][col] = tmp print dp[0][0]","title":"Bottom up approach"},{"location":"Databases/MariaDB/JSONOBJECTAGG%20alternative/","text":"JSONOBJECTAGG() alternative in MariaDB \u00b6 While the function is supported in MySQL, it is not in MariaDB. The workaround is to use something like this: NOTE: this will not work with strings that have quotes in them, it will render a broken JSON. SELECT CONCAT(\"{\", GROUP_CONCAT(DISTINCT CONCAT('\"', 'key', '\": \"', 'value', '\"') SEPARATOR ', '), \"}\") json","title":"JSONOBJECTAGG() alternative in MariaDB"},{"location":"Databases/MariaDB/JSONOBJECTAGG%20alternative/#jsonobjectagg-alternative-in-mariadb","text":"While the function is supported in MySQL, it is not in MariaDB. The workaround is to use something like this: NOTE: this will not work with strings that have quotes in them, it will render a broken JSON. SELECT CONCAT(\"{\", GROUP_CONCAT(DISTINCT CONCAT('\"', 'key', '\": \"', 'value', '\"') SEPARATOR ', '), \"}\") json","title":"JSONOBJECTAGG() alternative in MariaDB"},{"location":"Databases/MySQL/","text":"Learning MySQL \u00b6 Sources: mysqltutorial.org https://medium.com/@avishwakarma/mysql-scaling-and-how-to-do-it-d7f5b8dbd5b9","title":"Learning MySQL"},{"location":"Databases/MySQL/#learning-mysql","text":"Sources: mysqltutorial.org https://medium.com/@avishwakarma/mysql-scaling-and-how-to-do-it-d7f5b8dbd5b9","title":"Learning MySQL"},{"location":"Databases/MySQL/Export%20all%20databases%20into%20separate%20.gz%20files/","text":"Export all databases into separate .gz files \u00b6 dbs_export() { MYSQL_HOST='localhost' MYSQL_USER='root' MYSQL_PASS='' MYSQL_DB='' while getopts \":h:u:p:d:\" opt; do case $opt in h) MYSQL_HOST=\"$OPTARG\" ;; u) MYSQL_USER=\"$OPTARG\" ;; p) MYSQL_PASS=\"$OPTARG\" ;; d) MYSQL_DB=\"$OPTARG\" ;; \\?) echo \"Invalid option -$OPTARG\" >&2 ;; esac done echo \"-- START --\" echo \"SET autocommit=0;SET unique_checks=0;SET foreign_key_checks=0;\" > tmp_sqlhead.sql echo \"SET autocommit=1;SET unique_checks=1;SET foreign_key_checks=1;\" > tmp_sqlend.sql if [ -z \"$MYSQL_DB\" ] then echo \"-- Dumping all DB ...\" for I in $(mysql -h $MYSQL_HOST -u $MYSQL_USER --password=$MYSQL_PASS -e 'show databases' -s --skip-column-names); do if [ \"$I\" = information_schema ] || [ \"$I\" = mysql ] || [ \"$I\" = phpmyadmin ] || [ \"$I\" = performance_schema ] # exclude this DB then echo \"-- Skip $I ...\" continue fi echo \"-- Dumping $I ...\" # Pipe compress and concat the head/end with the stoutput of mysqlump ( '-' cat argument) mysqldump -h $MYSQL_HOST -u $MYSQL_USER --password=$MYSQL_PASS $I | cat tmp_sqlhead.sql - tmp_sqlend.sql | gzip -fc > \"$I.sql.gz\" done else I=$MYSQL_DB; echo \"-- Dumping $I ...\" # Pipe compress and concat the head/end with the stoutput of mysqlump ( '-' cat argument) mysqldump -h $MYSQL_HOST -u $MYSQL_USER --password=$MYSQL_PASS $I | cat tmp_sqlhead.sql - tmp_sqlend.sql | gzip -fc > \"$I.sql.gz\" fi # remove tmp files rm tmp_sqlhead.sql rm tmp_sqlend.sql echo \"-- FINISH --\" }","title":"Export all databases into separate .gz files"},{"location":"Databases/MySQL/Export%20all%20databases%20into%20separate%20.gz%20files/#export-all-databases-into-separate-gz-files","text":"dbs_export() { MYSQL_HOST='localhost' MYSQL_USER='root' MYSQL_PASS='' MYSQL_DB='' while getopts \":h:u:p:d:\" opt; do case $opt in h) MYSQL_HOST=\"$OPTARG\" ;; u) MYSQL_USER=\"$OPTARG\" ;; p) MYSQL_PASS=\"$OPTARG\" ;; d) MYSQL_DB=\"$OPTARG\" ;; \\?) echo \"Invalid option -$OPTARG\" >&2 ;; esac done echo \"-- START --\" echo \"SET autocommit=0;SET unique_checks=0;SET foreign_key_checks=0;\" > tmp_sqlhead.sql echo \"SET autocommit=1;SET unique_checks=1;SET foreign_key_checks=1;\" > tmp_sqlend.sql if [ -z \"$MYSQL_DB\" ] then echo \"-- Dumping all DB ...\" for I in $(mysql -h $MYSQL_HOST -u $MYSQL_USER --password=$MYSQL_PASS -e 'show databases' -s --skip-column-names); do if [ \"$I\" = information_schema ] || [ \"$I\" = mysql ] || [ \"$I\" = phpmyadmin ] || [ \"$I\" = performance_schema ] # exclude this DB then echo \"-- Skip $I ...\" continue fi echo \"-- Dumping $I ...\" # Pipe compress and concat the head/end with the stoutput of mysqlump ( '-' cat argument) mysqldump -h $MYSQL_HOST -u $MYSQL_USER --password=$MYSQL_PASS $I | cat tmp_sqlhead.sql - tmp_sqlend.sql | gzip -fc > \"$I.sql.gz\" done else I=$MYSQL_DB; echo \"-- Dumping $I ...\" # Pipe compress and concat the head/end with the stoutput of mysqlump ( '-' cat argument) mysqldump -h $MYSQL_HOST -u $MYSQL_USER --password=$MYSQL_PASS $I | cat tmp_sqlhead.sql - tmp_sqlend.sql | gzip -fc > \"$I.sql.gz\" fi # remove tmp files rm tmp_sqlhead.sql rm tmp_sqlend.sql echo \"-- FINISH --\" }","title":"Export all databases into separate .gz files"},{"location":"Databases/MySQL/GROUP_CONCAT%20separator/","text":"GROUP_CONCAT separator \u00b6 When using GROUP_CONCAT , you can specify a custom separator SELECT GROUP_CONCAT( a.`field` SEPARATOR '\u241f' ) AS `fields`;","title":"GROUP_CONCAT separator"},{"location":"Databases/MySQL/GROUP_CONCAT%20separator/#group_concat-separator","text":"When using GROUP_CONCAT , you can specify a custom separator SELECT GROUP_CONCAT( a.`field` SEPARATOR '\u241f' ) AS `fields`;","title":"GROUP_CONCAT separator"},{"location":"Databases/MySQL/Import%20.sql.gz%20files%20into%20MySQL/","text":"Import .sql.gz files into MySQL \u00b6 zcat < filename.sql.gz | mysql -u root database","title":"Import .sql.gz files into MySQL"},{"location":"Databases/MySQL/Import%20.sql.gz%20files%20into%20MySQL/#import-sqlgz-files-into-mysql","text":"zcat < filename.sql.gz | mysql -u root database","title":"Import .sql.gz files into MySQL"},{"location":"Databases/MySQL/MySQL%20Performance%20boosts/","text":"Performance boosts \u00b6 InnoDB Buffer Pool Size \u00b6 Holds the data and indexes of tables in memory. Bigger buffer results in faster row lookups. The bigger the better, default is 8MB. Use InnoDB where it requires. Query Cache \u00b6 Keeps the result of queries in memory until invalidated by writes. query_cache_size - total size of memory available to query caching, the default is 128 MB. query_cache_limit - the maximum number of kilobytes one query can take up in the cache, default is 8MB. Writing Queries \u00b6 When writing bad queries, the performance will be bad no matter the scale. Use EXPLAIN to profile the query execution plan Use DISTINCT not GROUP BY Never use an indexed column with a function Avoid using functions in where clause SSD \u00b6 Use a Solid State Drives (SSD) for database servers as they are better for latency and access time than regular HDDs.","title":"Performance boosts"},{"location":"Databases/MySQL/MySQL%20Performance%20boosts/#performance-boosts","text":"","title":"Performance boosts"},{"location":"Databases/MySQL/MySQL%20Performance%20boosts/#innodb-buffer-pool-size","text":"Holds the data and indexes of tables in memory. Bigger buffer results in faster row lookups. The bigger the better, default is 8MB. Use InnoDB where it requires.","title":"InnoDB Buffer Pool Size"},{"location":"Databases/MySQL/MySQL%20Performance%20boosts/#query-cache","text":"Keeps the result of queries in memory until invalidated by writes. query_cache_size - total size of memory available to query caching, the default is 128 MB. query_cache_limit - the maximum number of kilobytes one query can take up in the cache, default is 8MB.","title":"Query Cache"},{"location":"Databases/MySQL/MySQL%20Performance%20boosts/#writing-queries","text":"When writing bad queries, the performance will be bad no matter the scale. Use EXPLAIN to profile the query execution plan Use DISTINCT not GROUP BY Never use an indexed column with a function Avoid using functions in where clause","title":"Writing Queries"},{"location":"Databases/MySQL/MySQL%20Performance%20boosts/#ssd","text":"Use a Solid State Drives (SSD) for database servers as they are better for latency and access time than regular HDDs.","title":"SSD"},{"location":"Databases/MySQL/MySQL%20functions/","text":"MySQL functions \u00b6 Aggregate functions \u00b6 The data that you need is not always directly stored in the datables. However, you can get it by performing the calculations of the stored data when you select it. For example, you cannot get the total amount of each order by simply querying from the orderdetails table bcause the orderdetails table stores only quentity and price of each item, you have to select the quentity and price of an item for each order and calculate the order's total. To perform such caulculations in a query, you use aggregate functions. By definition, an aggregate function performs a calculation on a set of values and returns a signle value. MySQL provides many aggregate functions that include AVG , COUNT , SUM , MIN , MAX etc. An aggregate function ignores NULL values when it performs calculation except for the COUNT function. AVG function \u00b6 The AVG function calculates the average value of a set of values. It ignores NULL values in the calculation. SELECT AVG(buyPrice) average_buy_price FROM products You use the DISTINCT operator in the AVG function to calculate the average value of the distinct values. For example, if you have a set of values 1,1,2,3, the AVG function with DISTINCT operator will return two i.e., (1 + 2 + 3) / 2. We often use the AVG() function in conjunction with the GROUP BY clause to calculate the average value for each group of rows in a table. For example, to calculate the average buy price of products for each product line, you use the AVG() function with the GROUP BY clause as the following query: COUNT function \u00b6 The COUNT function returns the number of the rows in a table. SELECT COUNT(*) AS Total FROM products The COUNT function has several forms such as COUNT(*) and COUNT(DISTINCT expression) . SUM function \u00b6 The SUM function returns the sum of a set of values. The SUM function ignores NULL values. If no matching row found, the SUM function returns a NULL value. SELECT productCode,sum(priceEach * quantityOrdered) total FROM orderdetails GROUP by productCode MAX function \u00b6 The MAX function returns the maximum value in a set of values. SELECT MAX(buyPrice) highest_price, FROM Products MIN function \u00b6 The MIN function returns the minimum value in a set of values. SELECT MIN(buyPrice) lowest_price, FROM Products INSTR function \u00b6 Sometimes, you want to locate a substring in a string or to check if a substring exists in a string. In this case, you can use a string built-in function called INSTR. The INSTR function returns the position of the first occurrence of a substring in a string. If the substring is not found in the str, the INSTR function returns zero (0). The INSTR function accepts two arguments: The str is the string that you want to search in. The substr is the substring that you want to search for. The INSTR function is not case sensitive. It means that it does not matter if you pass the lowercase, uppercase, title case, etc., the results are always the same. If you want the INSTR function to perform searches in case-sensitive manner on a non-binary string, you use the BINARY operator to cast a one the argument of the INSTR function from a non-binary string to a binary string. SELECT INSTR('MySQL INSTR', 'MySQL'); SELECT INSTR('MySQL INSTR', 'mysql'); SELECT INSTR('MySQL INSTR', BINARY 'mysql'); SELECT productName FROM products WHERE INSTR(productname,'Car') > 0; GROUP_CONCAT function \u00b6 The MySQL GROUP_CONCAT function concatenates strings from a group into a single string with various options. SELECT GROUP_CONCAT(country) FROM customers; SELECT GROUP_CONCAT(DISTINCT country ORDER BY country SEPARATOR ';') FROM customers; Standard deviation function \u00b6 To calculate population starndard deviation, you use one of following functions: STD(expression) - returns population standard deviation ofthe expression. the STD function returns NULL if there was no matching row. STDDEV(expression) - is equivalent to the STD function. It is provided to be compatible with Oracle database only. STDEV_POP - is equivalent to the STD function. To calculate the sample standard deviation, you use the STDDEV_SAMP function. MySQL also provides some functions for population vaiance and sample variance calculation VAR_POP - calculate the population standard variance of the expression VARIANCE - equivalent to VAR_POP VAR_SAMP - calculates the sample standard variance of the expression SELECT FORMAT(STD(orderCount),2) FROM (SELECT customerNumber, count(*) orderCount FROM orders GROUP BY customerNumber) t; String functions \u00b6 CONCAT function \u00b6 To concatenate two or more quoted string values, you place the string next to each other as the following syntax: SELECT concat(contactFirstName,' ',contactLastName) Fullname FROM customers; LENGTH & CHAR_LENGTH \u00b6 To get the length of a string measured in bytes, you use the LENGTH function as follows: You use the CHAR_LENGTH function to get the length of a string measured in characters as follows: SET @s = CONVERT('MySQL String Length' USING ucs2); SELECT CHAR_LENGTH(@s), LENGTH(@s); LEFT \u00b6 The LEFT function is a string function that returns the left part of a string with a specified length. SELECT LEFT('MySQL LEFT', 5); Will return MySQL REPLACE \u00b6 MySQL provides you with a useful string function called REPLACE that allows you to replace a string in a column of a table by a new string. UPDATE tbl_name SET field_name = REPLACE(field_name, string_to_find, string_to_replace) WHERE conditions; SUBSTRING \u00b6 The SUBSTRING function returns a substring with a given length from a string starting at a specific position. MySQL provides various forms of the substring function. For example, to get the \u201d SUBSTRING\u201d out of the \u201d MySQL SUBSTRING\u201d string, the position of the substring must be 7 as the following SELECT statement: SELECT SUBSTRING('MYSQL SUBSTRING', 7); TRIM \u00b6 MySQL provides a very useful string function named TRIM to help you clean up the data. The following illustrates the syntax of the TRIM function. MySQL provides a very useful string function named TRIM to help you clean up the data. The following illustrates the syntax of the TRIM function. SELECT TRIM(' MySQL TRIM Function '); FIND_IN_SET \u00b6 MySQL provides a built-in string function called FIND_IN_SET that allows you to find the position of a string within a comma-separated list of strings. SELECT FIND_IN_SET('y','x,y,z'); -- 2 FORMAT \u00b6 If the result of the expression is a decimal with many decimal places, you can format those numbers, you use the FORMAT function with the following syntax: SELECT FORMAT(12500.2015, 2,'de_DE'); SELECT productname, CONCAT('$', FORMAT(quantityInStock * buyPrice, 2)) stock_value FROM products; Control flow functions \u00b6 CASE \u00b6 MySQL CASE expression is a flow control structure that allows you to construct conditions inside a query such as SELECT or WHERE clause. MySQL provides you with two forms of the CASE expressions. CASE value WHEN compare_value_1 THEN result_1 WHEN compare_value_2 THEN result_2 \u2026 ELSE result END IF \u00b6 The MySQL IF statement allows you to execute a set of SQL statements based on a certain condition or value of an expression. To form an expression in MySQL, you can combine literals, variables, operators, and even functions. An expression can return TRUE FALSE, or NULL. IF expression THEN statements; ELSEIF elseif-expression THEN elseif-statements; ... ELSE else-statements; END IF; IFNULL \u00b6 MySQL IFNULL function is one of the MySQL control flow functions that accepts two arguments and returns the first argument if it is not NULL. Otherwise, the IFNULL function returns the second argument. SELECT IFNULL(NULL,'IFNULL function'); -- returns IFNULL function NULLIF \u00b6 The NULLIF function is one of the control flow functions that accepts 2 arguments. The NULLIF function returns NULL if the first argument is equal to the second argument, otherwise it returns the first argument. SELECT NULLIF(1,1); -- return NULL SELECT NULLIF(1,2); -- return 1 SELECT NULLIF('MySQL NULLIF','MySQL NULLIF'); -- return NULL SELECT NULLIF('MySQL NULLIF','MySQL IFNULL'); -- return MySQL NULLIF SELECT NULLIF(1,NULL); -- return 1 because 1 <=> NULL SELECT NULLIF(NULL,1); -- return NULL the first argument Date and time functions \u00b6 CURDATE \u00b6 The CURDATE() function returns the current date as a value in the 'YYYY-MM-DD' format if it is used in a string context or YYYMMDD format if it is used in a numeric context. mysql> SELECT CURDATE(); +------------+ | CURDATE() | +------------+ | 2017-07-13 | +------------+ 1 row in set (0.00 sec) DATEDIFF \u00b6 The MySQL DATEDIFF function calculates the number of days between two DATE, DATETIME, or TIMESTAMP values. SELECT DATEDIFF('2011-08-17','2011-08-17'); -- 0 day SELECT DATEDIFF('2011-08-17','2011-08-08'); -- 9 days SELECT DATEDIFF('2011-08-08','2011-08-17'); -- -9 days DAY \u00b6 The DAY function returns the day of the month of a given date. The following shows the syntax of the DAY function: mysql> SELECT DAY('2010-01-15'); +-------------------+ | DAY('2010-01-15') | +-------------------+ | 15 | +-------------------+ 1 row in set (0.00 sec) To get the number of days in a month based on a specified date, you combine the LAST_DAY and DAY functions as shown in the following example: mysql> SELECT DAY(LAST_DAY('2016-02-03')); +-----------------------------+ | DAY(LAST_DAY('2016-02-03')) | +-----------------------------+ | 29 | +-----------------------------+ 1 row in set (0.00 sec) DATE_ADD \u00b6 The DATE_ADD function adds an interval to a DATE or DATETIME value. The following illustrates the syntax of the DATE_ADD function: SELECT DATE_ADD('1999-12-31 23:59:59', INTERVAL 1 SECOND) result; +---------------------+ | result | +---------------------+ | 2000-01-01 00:00:00 | +---------------------+ 1 row in set (0.00 sec) DATE_SUB \u00b6 The DATE_SUB() function subtracts a time value (or an interval) from a DATE or DATETIME value. The following illustrates the DATE_SUB() function: SELECT DATE_SUB('2017-07-04',INTERVAL 1 DAY) result; +------------+ | result | +------------+ | 2017-07-03 | +------------+ 1 row in set (0.00 sec) DATE_FORMAT \u00b6 To format a date value to a specific format, you use the DATE_FORMAT function. The syntax of the DATE_FORMAT function is as follows: SELECT orderNumber, DATE_FORMAT(orderdate, '%Y-%m-%d') orderDate, DATE_FORMAT(requireddate, '%a %D %b %Y') requireddate, DATE_FORMAT(shippedDate, '%W %D %M %Y') shippedDate FROM orders; Specifier Meaning %a Three-characters abbreviated weekday name e.g., Mon, Tue, Wed, etc. %b Three-characters abbreviated month name e.g., Jan, Feb, Mar, etc. %c Month in numeric e.g., 1, 2, 3\u202612 %D Day of the month with English suffix e.g., 0th, 1st, 2nd, etc. %d Day of the month with leading zero if it is 1 number e.g., 00, 01,02, \u202631 %e Day of the month without leading zero e.g., 1,2,\u202631 %f Microseconds in the range of 000000..999999 %H Hour in 24-hour format with leading zero e.g., 00..23 %h Hour in 12-hour format with leading zero e.g., 01, 02\u202612 %I Same as %h %i Minutes with leading zero e.g., 00, 01,\u202659 %j Day of year with leading zero e.g., 001,002,\u2026366 %k Hour in 24-hour format without leading zero e.g., 0,1,2\u202623 %l Hour in 12-hour format without leading zero e.g., 1,2\u202612 %M Full month name e.g., January, February,\u2026December %m Month name with leading zero e.g., 00,01,02,\u202612 %p AM or PM, depending on other time specifiers %r Time in 12-hour format hh:mm:ss AM or PM %S Seconds with leading zero 00,01,\u202659 %s Same as %S %T Time in 24-hour format hh:mm:ss %U Week number with leading zero when the first day of week is Sunday e.g., 00,01,02\u202653 %u Week number with leading zero when the first day of week is Monday e.g., 00,01,02\u202653 %V Same as %U; it is used with %X %v Same as %u; it is used with %x %W Full name of weekday e.g., Sunday, Monday,\u2026, Saturday %w Weekday in number (0=Sunday, 1= Monday,etc.) %X Year for the week in four digits where the first day of the week is Sunday; often used with %V %x Year for the week, where the first day of the week is Monday, four digits; used with %v %Y Four digits year e.g., 2000 and 2001. %y Two digits year e.g., 10,11,and 12. %% Add percentage (%) character to the output DATE_FORMAT string Formatted date %Y-%m-%d 7/4/2013 %e/%c/%Y 4/7/2013 %c/%e/%Y 7/4/2013 %d/%m/%Y 4/7/2013 %m/%d/%Y 7/4/2013 %e/%c/%Y %H:%i 4/7/2013 11:20 %c/%e/%Y %H:%i 7/4/2013 11:20 %d/%m/%Y %H:%i 4/7/2013 11:20 %m/%d/%Y %H:%i 7/4/2013 11:20 %e/%c/%Y %T 4/7/2013 11:20 %c/%e/%Y %T 7/4/2013 11:20 %d/%m/%Y %T 4/7/2013 11:20 %m/%d/%Y %T 7/4/2013 11:20 %a %D %b %Y Thu 4th Jul 2013 %a %D %b %Y %H:%i Thu 4th Jul 2013 11:20 %a %D %b %Y %T Thu 4th Jul 2013 11:20:05 %a %b %e %Y Thu Jul 4 2013 %a %b %e %Y %H:%i Thu Jul 4 2013 11:20 %a %b %e %Y %T Thu Jul 4 2013 11:20:05 %W %D %M %Y Thursday 4th July 2013 %W %D %M %Y %H:%i Thursday 4th July 2013 11:20 %W %D %M %Y %T Thursday 4th July 2013 11:20:05 %l:%i %p %b %e, %Y 7/4/2013 11:20 %M %e, %Y 4-Jul-13 %a, %d %b %Y %T Thu, 04 Jul 2013 11:20:05 DAYNAME \u00b6 MySQL DAYNAME function returns the name of a weekday for a specified date. The following illustrates the syntax of the DAYNAME function: mysql> SELECT DAYNAME('2000-01-01') dayname; +----------+ | dayname | +----------+ | Saturday | +----------+ 1 row in set (0.00 sec) mysql> SET @@lc_time_names = 'fr_FR'; Query OK, 0 rows affected (0.00 sec) mysql> SELECT DAYNAME('2000-01-01') dayname; +---------+ | dayname | +---------+ | samedi | +---------+ 1 row in set (0.00 sec) DAYOFWEEK \u00b6 The DAYOFWEEK function returns the weekday index for a date i.e., 1 for Sunday, 2 for Monday, \u2026 7 for Saturday. These index values correspond to the ODBC standard. mysql> SELECT DAYNAME('2012-12-01'), DAYOFWEEK('2012-12-01'); +-----------------------+-------------------------+ | DAYNAME('2012-12-01') | DAYOFWEEK('2012-12-01') | +-----------------------+-------------------------+ | Saturday | 7 | +-----------------------+-------------------------+ 1 row in set (0.00 sec) EXTRACT \u00b6 The EXTRACT() function extracts part of a date. The following illustrates the syntax of the EXTRACT() function. mysql> SELECT EXTRACT(DAY FROM '2017-07-14 09:04:44') DAY; +------+ | DAY | +------+ | 14 | +------+ 1 row in set (0.00 sec) The unit is the interval that you want to extract from the date. The following are the valid intervals for the unit argument. DAY, DAY_HOUR, DAY_MICROSECOND, DAY_MINUTE, DAY_SECOND, HOUR, HOUR_MICROSECOND, HOUR_MINUTE, HOUR_SECOND, MICROSECOND, MINUTE, MINUTE_MICROSECOND, MINUTE_SECOND, MONTH, QUARTER, SECOND, SECOND_MICROSECOND, WEEK, YEAR, YEAR_MONTH NOW \u00b6 The MySQL NOW() function returns the current date and time in the configured time zone as a string or a number in the 'YYYY-MM-DD HH:MM:DD' or 'YYYYMMDDHHMMSS.uuuuuu' format. SELECT NOW(); MONTH \u00b6 The MONTH function returns an integer that represents the month of a specified date value. The following illustrates the syntax of the MONTH function: mysql> SELECT MONTH('2010-01-01'); +---------------------+ | MONTH('2010-01-01') | +---------------------+ | 1 | +---------------------+ 1 row in set (0.00 sec) STR_TO_DATE \u00b6 The STR_TO_DATE() converts the str string into a date value based on the fmt format string. The STR_TO_DATE() function may return a DATE , TIME, or DATETIME value based on the input and format strings. If the input string is illegal, the STR_TO_DATE() function returns NULL. SELECT STR_TO_DATE('21,5,2013','%d,%m,%Y'); > 2013-05-21 SYSDATE \u00b6 The SYSDATE() function returns the current date and time as a value in 'YYYY-MM-DD HH:MM:SS' format if the function is used in a string context or YYYYMMDDHHMMSS format in case the function is used in a numeric context. mysql> SELECT SYSDATE(); +---------------------+ | SYSDATE() | +---------------------+ | 2017-07-13 17:42:37 | +---------------------+ 1 row in set (0.00 sec) TIMEDIFF \u00b6 The TIMEDIFF returns the difference between two TIME or DATETIME values. See the following syntax of TIMEDIFF function. mysql> SELECT TIMEDIFF('12:00:00','10:00:00') diff; +----------+ | diff | +----------+ | 02:00:00 | +----------+ 1 row in set (0.00 sec) TIMESTAMPDIFF \u00b6 The TIMESTAMPDIFF function returns the result of begin - end, where begin and end are DATE or DATETIME expressions. The unit argument determines the unit of the result of (end - begin) represented as an integer. The following are valid units: MICROSECOND, SECOND, MINUTE, HOUR, DAY, WEEK, MONTH, QUARTER, YEAR WEEK \u00b6 To check whether a particular date belongs to which week number, you use the WEEK function as follows: SELECT WEEK(date, mode); The WEEK function accepts two arguments: date is the date that you want to get a week number. mode is an optional argument that determines the logic of week number calculation. It allows you to specify whether the week should start on Monday or Sunday and the returned week number should be between 0 and 52 or 0 and 53. If you ignore the mode argument, the WEEK function will use the value of the default_week_format system variable by default. WEEKDAY \u00b6 The WEEKDAY function returns a weekday index for a date i.e., 0 for Monday, 1 for Tuesday, \u2026 6 for Sunday. mysql> SELECT DAYNAME('2010-01-01'), WEEKDAY('2010-01-01'); +-----------------------+-----------------------+ | DAYNAME('2010-01-01') | WEEKDAY('2010-01-01') | +-----------------------+-----------------------+ | Friday | 4 | +-----------------------+-----------------------+ 1 row in set (0.00 sec) YEAR \u00b6 The YEAR() function takes a date argument and returns the year of the date. See the syntax of the YEAR() function: SELECT YEAR('2017-01-01'); +--------------------+ | YEAR('2017-01-01') | +--------------------+ | 2017 | +--------------------+ 1 row in set (0.00 sec) Comparison functions \u00b6 COALESCE \u00b6 The COALESCE function takes a number of arguments and returns the first non-NULL argument. In case all arguments are NULL, the COALESCE function returns NULL. SELECT COALESCE(NULL, 0); -- 0 SELECT COALESCE(NULL, NULL); -- NULL; GREATEST & LEAST \u00b6 Both GREATEST and LEAST functions take N arguments and return the greatest and smallest values respectively. The following illustrates the syntax of the GREATEST and LEAST function: SELECT GREATEST(10, 20, 30), -- 30 LEAST(10, 20, 30); -- 10 SELECT GREATEST(10, null, 30), -- null LEAST(10, null , 30); -- null ISNULL \u00b6 The ISNULL function takes one argument and tests whether that argument is NULL or not. The ISNULL function returns 1 if the argument is NULL, otherwise, it returns 0. SELECT ISNULL(NULL); -- 1 SELECT ISNULL(1); -- 0 SELECT ISNULL(1 + NULL); -- 1; SELECT ISNULL(1 / 0 ); -- 1; MATH functions \u00b6 ABS \u00b6 The ABS() function is a mathematical function that returns the absolute (positive) value of a number. SELECT ABS(-10), ABS(0), ABS(10); > 10, 0, 10 CEIL \u00b6 The CEIL() function takes an input number and returns the smallest integer greater than or equal to that number. SELECT CEIL(1.59); -- 2 SELECT CEIL(-1.59) -- -1 FLOOR \u00b6 The FLOOR() function accepts one argument which can be number or numeric expression and returns the largest integer number less than or equal to the argument. SELECT FLOOR(1.59); -- 1 SELECT FLOOR(-1.59) -- -2 MOD \u00b6 The MOD() function returns the remainder of one number divided by another. The following shows the syntax of the MOD() function: SELECT MOD(11, 3); -- 2 ROUND \u00b6 The ROUND() is a mathematical function that allows you to round a number to a specified number of decimal places. SELECT ROUND(20.5); -- 21 SELECT ROUND(20.5, 0); -- 21 SELECT ROUND(121.55,-2) -- 100 TRUNCATE \u00b6 MySQL TRUNCATE() function truncates a number to a specified number of decimal places as shown below: SELECT TRUNCATE(1.555,1); --- 1.5 Other functions \u00b6 LAST_INSERT_ID \u00b6 In database design, you often use a surrogate key to generate unique integer values for the primary key column using the AUTO_INCREMENT attribute. INSERT INTO tbl(description) VALUES('MySQL last_insert_id'); SELECT LAST_INSERT_ID(); CAST \u00b6 The CAST() function converts a value of any type into a value that has a specified type. The target type can be any one of the following types: BINARY, CHAR, DATE, DATETIME, TIME,DECIMAL, SIGNED, UNSIGNED . SELECT (1 + CAST('1' AS UNSIGNED))/2; SELECT CONCAT('MySQL CAST example #',CAST(2 AS CHAR));","title":"MySQL functions"},{"location":"Databases/MySQL/MySQL%20functions/#mysql-functions","text":"","title":"MySQL functions"},{"location":"Databases/MySQL/MySQL%20functions/#aggregate-functions","text":"The data that you need is not always directly stored in the datables. However, you can get it by performing the calculations of the stored data when you select it. For example, you cannot get the total amount of each order by simply querying from the orderdetails table bcause the orderdetails table stores only quentity and price of each item, you have to select the quentity and price of an item for each order and calculate the order's total. To perform such caulculations in a query, you use aggregate functions. By definition, an aggregate function performs a calculation on a set of values and returns a signle value. MySQL provides many aggregate functions that include AVG , COUNT , SUM , MIN , MAX etc. An aggregate function ignores NULL values when it performs calculation except for the COUNT function.","title":"Aggregate functions"},{"location":"Databases/MySQL/MySQL%20functions/#avg-function","text":"The AVG function calculates the average value of a set of values. It ignores NULL values in the calculation. SELECT AVG(buyPrice) average_buy_price FROM products You use the DISTINCT operator in the AVG function to calculate the average value of the distinct values. For example, if you have a set of values 1,1,2,3, the AVG function with DISTINCT operator will return two i.e., (1 + 2 + 3) / 2. We often use the AVG() function in conjunction with the GROUP BY clause to calculate the average value for each group of rows in a table. For example, to calculate the average buy price of products for each product line, you use the AVG() function with the GROUP BY clause as the following query:","title":"AVG function"},{"location":"Databases/MySQL/MySQL%20functions/#count-function","text":"The COUNT function returns the number of the rows in a table. SELECT COUNT(*) AS Total FROM products The COUNT function has several forms such as COUNT(*) and COUNT(DISTINCT expression) .","title":"COUNT function"},{"location":"Databases/MySQL/MySQL%20functions/#sum-function","text":"The SUM function returns the sum of a set of values. The SUM function ignores NULL values. If no matching row found, the SUM function returns a NULL value. SELECT productCode,sum(priceEach * quantityOrdered) total FROM orderdetails GROUP by productCode","title":"SUM function"},{"location":"Databases/MySQL/MySQL%20functions/#max-function","text":"The MAX function returns the maximum value in a set of values. SELECT MAX(buyPrice) highest_price, FROM Products","title":"MAX function"},{"location":"Databases/MySQL/MySQL%20functions/#min-function","text":"The MIN function returns the minimum value in a set of values. SELECT MIN(buyPrice) lowest_price, FROM Products","title":"MIN function"},{"location":"Databases/MySQL/MySQL%20functions/#instr-function","text":"Sometimes, you want to locate a substring in a string or to check if a substring exists in a string. In this case, you can use a string built-in function called INSTR. The INSTR function returns the position of the first occurrence of a substring in a string. If the substring is not found in the str, the INSTR function returns zero (0). The INSTR function accepts two arguments: The str is the string that you want to search in. The substr is the substring that you want to search for. The INSTR function is not case sensitive. It means that it does not matter if you pass the lowercase, uppercase, title case, etc., the results are always the same. If you want the INSTR function to perform searches in case-sensitive manner on a non-binary string, you use the BINARY operator to cast a one the argument of the INSTR function from a non-binary string to a binary string. SELECT INSTR('MySQL INSTR', 'MySQL'); SELECT INSTR('MySQL INSTR', 'mysql'); SELECT INSTR('MySQL INSTR', BINARY 'mysql'); SELECT productName FROM products WHERE INSTR(productname,'Car') > 0;","title":"INSTR function"},{"location":"Databases/MySQL/MySQL%20functions/#group_concat-function","text":"The MySQL GROUP_CONCAT function concatenates strings from a group into a single string with various options. SELECT GROUP_CONCAT(country) FROM customers; SELECT GROUP_CONCAT(DISTINCT country ORDER BY country SEPARATOR ';') FROM customers;","title":"GROUP_CONCAT function"},{"location":"Databases/MySQL/MySQL%20functions/#standard-deviation-function","text":"To calculate population starndard deviation, you use one of following functions: STD(expression) - returns population standard deviation ofthe expression. the STD function returns NULL if there was no matching row. STDDEV(expression) - is equivalent to the STD function. It is provided to be compatible with Oracle database only. STDEV_POP - is equivalent to the STD function. To calculate the sample standard deviation, you use the STDDEV_SAMP function. MySQL also provides some functions for population vaiance and sample variance calculation VAR_POP - calculate the population standard variance of the expression VARIANCE - equivalent to VAR_POP VAR_SAMP - calculates the sample standard variance of the expression SELECT FORMAT(STD(orderCount),2) FROM (SELECT customerNumber, count(*) orderCount FROM orders GROUP BY customerNumber) t;","title":"Standard deviation function"},{"location":"Databases/MySQL/MySQL%20functions/#string-functions","text":"","title":"String functions"},{"location":"Databases/MySQL/MySQL%20functions/#concat-function","text":"To concatenate two or more quoted string values, you place the string next to each other as the following syntax: SELECT concat(contactFirstName,' ',contactLastName) Fullname FROM customers;","title":"CONCAT function"},{"location":"Databases/MySQL/MySQL%20functions/#length-char_length","text":"To get the length of a string measured in bytes, you use the LENGTH function as follows: You use the CHAR_LENGTH function to get the length of a string measured in characters as follows: SET @s = CONVERT('MySQL String Length' USING ucs2); SELECT CHAR_LENGTH(@s), LENGTH(@s);","title":"LENGTH &amp; CHAR_LENGTH"},{"location":"Databases/MySQL/MySQL%20functions/#left","text":"The LEFT function is a string function that returns the left part of a string with a specified length. SELECT LEFT('MySQL LEFT', 5); Will return MySQL","title":"LEFT"},{"location":"Databases/MySQL/MySQL%20functions/#replace","text":"MySQL provides you with a useful string function called REPLACE that allows you to replace a string in a column of a table by a new string. UPDATE tbl_name SET field_name = REPLACE(field_name, string_to_find, string_to_replace) WHERE conditions;","title":"REPLACE"},{"location":"Databases/MySQL/MySQL%20functions/#substring","text":"The SUBSTRING function returns a substring with a given length from a string starting at a specific position. MySQL provides various forms of the substring function. For example, to get the \u201d SUBSTRING\u201d out of the \u201d MySQL SUBSTRING\u201d string, the position of the substring must be 7 as the following SELECT statement: SELECT SUBSTRING('MYSQL SUBSTRING', 7);","title":"SUBSTRING"},{"location":"Databases/MySQL/MySQL%20functions/#trim","text":"MySQL provides a very useful string function named TRIM to help you clean up the data. The following illustrates the syntax of the TRIM function. MySQL provides a very useful string function named TRIM to help you clean up the data. The following illustrates the syntax of the TRIM function. SELECT TRIM(' MySQL TRIM Function ');","title":"TRIM"},{"location":"Databases/MySQL/MySQL%20functions/#find_in_set","text":"MySQL provides a built-in string function called FIND_IN_SET that allows you to find the position of a string within a comma-separated list of strings. SELECT FIND_IN_SET('y','x,y,z'); -- 2","title":"FIND_IN_SET"},{"location":"Databases/MySQL/MySQL%20functions/#format","text":"If the result of the expression is a decimal with many decimal places, you can format those numbers, you use the FORMAT function with the following syntax: SELECT FORMAT(12500.2015, 2,'de_DE'); SELECT productname, CONCAT('$', FORMAT(quantityInStock * buyPrice, 2)) stock_value FROM products;","title":"FORMAT"},{"location":"Databases/MySQL/MySQL%20functions/#control-flow-functions","text":"","title":"Control flow functions"},{"location":"Databases/MySQL/MySQL%20functions/#case","text":"MySQL CASE expression is a flow control structure that allows you to construct conditions inside a query such as SELECT or WHERE clause. MySQL provides you with two forms of the CASE expressions. CASE value WHEN compare_value_1 THEN result_1 WHEN compare_value_2 THEN result_2 \u2026 ELSE result END","title":"CASE"},{"location":"Databases/MySQL/MySQL%20functions/#if","text":"The MySQL IF statement allows you to execute a set of SQL statements based on a certain condition or value of an expression. To form an expression in MySQL, you can combine literals, variables, operators, and even functions. An expression can return TRUE FALSE, or NULL. IF expression THEN statements; ELSEIF elseif-expression THEN elseif-statements; ... ELSE else-statements; END IF;","title":"IF"},{"location":"Databases/MySQL/MySQL%20functions/#ifnull","text":"MySQL IFNULL function is one of the MySQL control flow functions that accepts two arguments and returns the first argument if it is not NULL. Otherwise, the IFNULL function returns the second argument. SELECT IFNULL(NULL,'IFNULL function'); -- returns IFNULL function","title":"IFNULL"},{"location":"Databases/MySQL/MySQL%20functions/#nullif","text":"The NULLIF function is one of the control flow functions that accepts 2 arguments. The NULLIF function returns NULL if the first argument is equal to the second argument, otherwise it returns the first argument. SELECT NULLIF(1,1); -- return NULL SELECT NULLIF(1,2); -- return 1 SELECT NULLIF('MySQL NULLIF','MySQL NULLIF'); -- return NULL SELECT NULLIF('MySQL NULLIF','MySQL IFNULL'); -- return MySQL NULLIF SELECT NULLIF(1,NULL); -- return 1 because 1 <=> NULL SELECT NULLIF(NULL,1); -- return NULL the first argument","title":"NULLIF"},{"location":"Databases/MySQL/MySQL%20functions/#date-and-time-functions","text":"","title":"Date and time functions"},{"location":"Databases/MySQL/MySQL%20functions/#curdate","text":"The CURDATE() function returns the current date as a value in the 'YYYY-MM-DD' format if it is used in a string context or YYYMMDD format if it is used in a numeric context. mysql> SELECT CURDATE(); +------------+ | CURDATE() | +------------+ | 2017-07-13 | +------------+ 1 row in set (0.00 sec)","title":"CURDATE"},{"location":"Databases/MySQL/MySQL%20functions/#datediff","text":"The MySQL DATEDIFF function calculates the number of days between two DATE, DATETIME, or TIMESTAMP values. SELECT DATEDIFF('2011-08-17','2011-08-17'); -- 0 day SELECT DATEDIFF('2011-08-17','2011-08-08'); -- 9 days SELECT DATEDIFF('2011-08-08','2011-08-17'); -- -9 days","title":"DATEDIFF"},{"location":"Databases/MySQL/MySQL%20functions/#day","text":"The DAY function returns the day of the month of a given date. The following shows the syntax of the DAY function: mysql> SELECT DAY('2010-01-15'); +-------------------+ | DAY('2010-01-15') | +-------------------+ | 15 | +-------------------+ 1 row in set (0.00 sec) To get the number of days in a month based on a specified date, you combine the LAST_DAY and DAY functions as shown in the following example: mysql> SELECT DAY(LAST_DAY('2016-02-03')); +-----------------------------+ | DAY(LAST_DAY('2016-02-03')) | +-----------------------------+ | 29 | +-----------------------------+ 1 row in set (0.00 sec)","title":"DAY"},{"location":"Databases/MySQL/MySQL%20functions/#date_add","text":"The DATE_ADD function adds an interval to a DATE or DATETIME value. The following illustrates the syntax of the DATE_ADD function: SELECT DATE_ADD('1999-12-31 23:59:59', INTERVAL 1 SECOND) result; +---------------------+ | result | +---------------------+ | 2000-01-01 00:00:00 | +---------------------+ 1 row in set (0.00 sec)","title":"DATE_ADD"},{"location":"Databases/MySQL/MySQL%20functions/#date_sub","text":"The DATE_SUB() function subtracts a time value (or an interval) from a DATE or DATETIME value. The following illustrates the DATE_SUB() function: SELECT DATE_SUB('2017-07-04',INTERVAL 1 DAY) result; +------------+ | result | +------------+ | 2017-07-03 | +------------+ 1 row in set (0.00 sec)","title":"DATE_SUB"},{"location":"Databases/MySQL/MySQL%20functions/#date_format","text":"To format a date value to a specific format, you use the DATE_FORMAT function. The syntax of the DATE_FORMAT function is as follows: SELECT orderNumber, DATE_FORMAT(orderdate, '%Y-%m-%d') orderDate, DATE_FORMAT(requireddate, '%a %D %b %Y') requireddate, DATE_FORMAT(shippedDate, '%W %D %M %Y') shippedDate FROM orders; Specifier Meaning %a Three-characters abbreviated weekday name e.g., Mon, Tue, Wed, etc. %b Three-characters abbreviated month name e.g., Jan, Feb, Mar, etc. %c Month in numeric e.g., 1, 2, 3\u202612 %D Day of the month with English suffix e.g., 0th, 1st, 2nd, etc. %d Day of the month with leading zero if it is 1 number e.g., 00, 01,02, \u202631 %e Day of the month without leading zero e.g., 1,2,\u202631 %f Microseconds in the range of 000000..999999 %H Hour in 24-hour format with leading zero e.g., 00..23 %h Hour in 12-hour format with leading zero e.g., 01, 02\u202612 %I Same as %h %i Minutes with leading zero e.g., 00, 01,\u202659 %j Day of year with leading zero e.g., 001,002,\u2026366 %k Hour in 24-hour format without leading zero e.g., 0,1,2\u202623 %l Hour in 12-hour format without leading zero e.g., 1,2\u202612 %M Full month name e.g., January, February,\u2026December %m Month name with leading zero e.g., 00,01,02,\u202612 %p AM or PM, depending on other time specifiers %r Time in 12-hour format hh:mm:ss AM or PM %S Seconds with leading zero 00,01,\u202659 %s Same as %S %T Time in 24-hour format hh:mm:ss %U Week number with leading zero when the first day of week is Sunday e.g., 00,01,02\u202653 %u Week number with leading zero when the first day of week is Monday e.g., 00,01,02\u202653 %V Same as %U; it is used with %X %v Same as %u; it is used with %x %W Full name of weekday e.g., Sunday, Monday,\u2026, Saturday %w Weekday in number (0=Sunday, 1= Monday,etc.) %X Year for the week in four digits where the first day of the week is Sunday; often used with %V %x Year for the week, where the first day of the week is Monday, four digits; used with %v %Y Four digits year e.g., 2000 and 2001. %y Two digits year e.g., 10,11,and 12. %% Add percentage (%) character to the output DATE_FORMAT string Formatted date %Y-%m-%d 7/4/2013 %e/%c/%Y 4/7/2013 %c/%e/%Y 7/4/2013 %d/%m/%Y 4/7/2013 %m/%d/%Y 7/4/2013 %e/%c/%Y %H:%i 4/7/2013 11:20 %c/%e/%Y %H:%i 7/4/2013 11:20 %d/%m/%Y %H:%i 4/7/2013 11:20 %m/%d/%Y %H:%i 7/4/2013 11:20 %e/%c/%Y %T 4/7/2013 11:20 %c/%e/%Y %T 7/4/2013 11:20 %d/%m/%Y %T 4/7/2013 11:20 %m/%d/%Y %T 7/4/2013 11:20 %a %D %b %Y Thu 4th Jul 2013 %a %D %b %Y %H:%i Thu 4th Jul 2013 11:20 %a %D %b %Y %T Thu 4th Jul 2013 11:20:05 %a %b %e %Y Thu Jul 4 2013 %a %b %e %Y %H:%i Thu Jul 4 2013 11:20 %a %b %e %Y %T Thu Jul 4 2013 11:20:05 %W %D %M %Y Thursday 4th July 2013 %W %D %M %Y %H:%i Thursday 4th July 2013 11:20 %W %D %M %Y %T Thursday 4th July 2013 11:20:05 %l:%i %p %b %e, %Y 7/4/2013 11:20 %M %e, %Y 4-Jul-13 %a, %d %b %Y %T Thu, 04 Jul 2013 11:20:05","title":"DATE_FORMAT"},{"location":"Databases/MySQL/MySQL%20functions/#dayname","text":"MySQL DAYNAME function returns the name of a weekday for a specified date. The following illustrates the syntax of the DAYNAME function: mysql> SELECT DAYNAME('2000-01-01') dayname; +----------+ | dayname | +----------+ | Saturday | +----------+ 1 row in set (0.00 sec) mysql> SET @@lc_time_names = 'fr_FR'; Query OK, 0 rows affected (0.00 sec) mysql> SELECT DAYNAME('2000-01-01') dayname; +---------+ | dayname | +---------+ | samedi | +---------+ 1 row in set (0.00 sec)","title":"DAYNAME"},{"location":"Databases/MySQL/MySQL%20functions/#dayofweek","text":"The DAYOFWEEK function returns the weekday index for a date i.e., 1 for Sunday, 2 for Monday, \u2026 7 for Saturday. These index values correspond to the ODBC standard. mysql> SELECT DAYNAME('2012-12-01'), DAYOFWEEK('2012-12-01'); +-----------------------+-------------------------+ | DAYNAME('2012-12-01') | DAYOFWEEK('2012-12-01') | +-----------------------+-------------------------+ | Saturday | 7 | +-----------------------+-------------------------+ 1 row in set (0.00 sec)","title":"DAYOFWEEK"},{"location":"Databases/MySQL/MySQL%20functions/#extract","text":"The EXTRACT() function extracts part of a date. The following illustrates the syntax of the EXTRACT() function. mysql> SELECT EXTRACT(DAY FROM '2017-07-14 09:04:44') DAY; +------+ | DAY | +------+ | 14 | +------+ 1 row in set (0.00 sec) The unit is the interval that you want to extract from the date. The following are the valid intervals for the unit argument. DAY, DAY_HOUR, DAY_MICROSECOND, DAY_MINUTE, DAY_SECOND, HOUR, HOUR_MICROSECOND, HOUR_MINUTE, HOUR_SECOND, MICROSECOND, MINUTE, MINUTE_MICROSECOND, MINUTE_SECOND, MONTH, QUARTER, SECOND, SECOND_MICROSECOND, WEEK, YEAR, YEAR_MONTH","title":"EXTRACT"},{"location":"Databases/MySQL/MySQL%20functions/#now","text":"The MySQL NOW() function returns the current date and time in the configured time zone as a string or a number in the 'YYYY-MM-DD HH:MM:DD' or 'YYYYMMDDHHMMSS.uuuuuu' format. SELECT NOW();","title":"NOW"},{"location":"Databases/MySQL/MySQL%20functions/#month","text":"The MONTH function returns an integer that represents the month of a specified date value. The following illustrates the syntax of the MONTH function: mysql> SELECT MONTH('2010-01-01'); +---------------------+ | MONTH('2010-01-01') | +---------------------+ | 1 | +---------------------+ 1 row in set (0.00 sec)","title":"MONTH"},{"location":"Databases/MySQL/MySQL%20functions/#str_to_date","text":"The STR_TO_DATE() converts the str string into a date value based on the fmt format string. The STR_TO_DATE() function may return a DATE , TIME, or DATETIME value based on the input and format strings. If the input string is illegal, the STR_TO_DATE() function returns NULL. SELECT STR_TO_DATE('21,5,2013','%d,%m,%Y'); > 2013-05-21","title":"STR_TO_DATE"},{"location":"Databases/MySQL/MySQL%20functions/#sysdate","text":"The SYSDATE() function returns the current date and time as a value in 'YYYY-MM-DD HH:MM:SS' format if the function is used in a string context or YYYYMMDDHHMMSS format in case the function is used in a numeric context. mysql> SELECT SYSDATE(); +---------------------+ | SYSDATE() | +---------------------+ | 2017-07-13 17:42:37 | +---------------------+ 1 row in set (0.00 sec)","title":"SYSDATE"},{"location":"Databases/MySQL/MySQL%20functions/#timediff","text":"The TIMEDIFF returns the difference between two TIME or DATETIME values. See the following syntax of TIMEDIFF function. mysql> SELECT TIMEDIFF('12:00:00','10:00:00') diff; +----------+ | diff | +----------+ | 02:00:00 | +----------+ 1 row in set (0.00 sec)","title":"TIMEDIFF"},{"location":"Databases/MySQL/MySQL%20functions/#timestampdiff","text":"The TIMESTAMPDIFF function returns the result of begin - end, where begin and end are DATE or DATETIME expressions. The unit argument determines the unit of the result of (end - begin) represented as an integer. The following are valid units: MICROSECOND, SECOND, MINUTE, HOUR, DAY, WEEK, MONTH, QUARTER, YEAR","title":"TIMESTAMPDIFF"},{"location":"Databases/MySQL/MySQL%20functions/#week","text":"To check whether a particular date belongs to which week number, you use the WEEK function as follows: SELECT WEEK(date, mode); The WEEK function accepts two arguments: date is the date that you want to get a week number. mode is an optional argument that determines the logic of week number calculation. It allows you to specify whether the week should start on Monday or Sunday and the returned week number should be between 0 and 52 or 0 and 53. If you ignore the mode argument, the WEEK function will use the value of the default_week_format system variable by default.","title":"WEEK"},{"location":"Databases/MySQL/MySQL%20functions/#weekday","text":"The WEEKDAY function returns a weekday index for a date i.e., 0 for Monday, 1 for Tuesday, \u2026 6 for Sunday. mysql> SELECT DAYNAME('2010-01-01'), WEEKDAY('2010-01-01'); +-----------------------+-----------------------+ | DAYNAME('2010-01-01') | WEEKDAY('2010-01-01') | +-----------------------+-----------------------+ | Friday | 4 | +-----------------------+-----------------------+ 1 row in set (0.00 sec)","title":"WEEKDAY"},{"location":"Databases/MySQL/MySQL%20functions/#year","text":"The YEAR() function takes a date argument and returns the year of the date. See the syntax of the YEAR() function: SELECT YEAR('2017-01-01'); +--------------------+ | YEAR('2017-01-01') | +--------------------+ | 2017 | +--------------------+ 1 row in set (0.00 sec)","title":"YEAR"},{"location":"Databases/MySQL/MySQL%20functions/#comparison-functions","text":"","title":"Comparison functions"},{"location":"Databases/MySQL/MySQL%20functions/#coalesce","text":"The COALESCE function takes a number of arguments and returns the first non-NULL argument. In case all arguments are NULL, the COALESCE function returns NULL. SELECT COALESCE(NULL, 0); -- 0 SELECT COALESCE(NULL, NULL); -- NULL;","title":"COALESCE"},{"location":"Databases/MySQL/MySQL%20functions/#greatest-least","text":"Both GREATEST and LEAST functions take N arguments and return the greatest and smallest values respectively. The following illustrates the syntax of the GREATEST and LEAST function: SELECT GREATEST(10, 20, 30), -- 30 LEAST(10, 20, 30); -- 10 SELECT GREATEST(10, null, 30), -- null LEAST(10, null , 30); -- null","title":"GREATEST &amp; LEAST"},{"location":"Databases/MySQL/MySQL%20functions/#isnull","text":"The ISNULL function takes one argument and tests whether that argument is NULL or not. The ISNULL function returns 1 if the argument is NULL, otherwise, it returns 0. SELECT ISNULL(NULL); -- 1 SELECT ISNULL(1); -- 0 SELECT ISNULL(1 + NULL); -- 1; SELECT ISNULL(1 / 0 ); -- 1;","title":"ISNULL"},{"location":"Databases/MySQL/MySQL%20functions/#math-functions","text":"","title":"MATH functions"},{"location":"Databases/MySQL/MySQL%20functions/#abs","text":"The ABS() function is a mathematical function that returns the absolute (positive) value of a number. SELECT ABS(-10), ABS(0), ABS(10); > 10, 0, 10","title":"ABS"},{"location":"Databases/MySQL/MySQL%20functions/#ceil","text":"The CEIL() function takes an input number and returns the smallest integer greater than or equal to that number. SELECT CEIL(1.59); -- 2 SELECT CEIL(-1.59) -- -1","title":"CEIL"},{"location":"Databases/MySQL/MySQL%20functions/#floor","text":"The FLOOR() function accepts one argument which can be number or numeric expression and returns the largest integer number less than or equal to the argument. SELECT FLOOR(1.59); -- 1 SELECT FLOOR(-1.59) -- -2","title":"FLOOR"},{"location":"Databases/MySQL/MySQL%20functions/#mod","text":"The MOD() function returns the remainder of one number divided by another. The following shows the syntax of the MOD() function: SELECT MOD(11, 3); -- 2","title":"MOD"},{"location":"Databases/MySQL/MySQL%20functions/#round","text":"The ROUND() is a mathematical function that allows you to round a number to a specified number of decimal places. SELECT ROUND(20.5); -- 21 SELECT ROUND(20.5, 0); -- 21 SELECT ROUND(121.55,-2) -- 100","title":"ROUND"},{"location":"Databases/MySQL/MySQL%20functions/#truncate","text":"MySQL TRUNCATE() function truncates a number to a specified number of decimal places as shown below: SELECT TRUNCATE(1.555,1); --- 1.5","title":"TRUNCATE"},{"location":"Databases/MySQL/MySQL%20functions/#other-functions","text":"","title":"Other functions"},{"location":"Databases/MySQL/MySQL%20functions/#last_insert_id","text":"In database design, you often use a surrogate key to generate unique integer values for the primary key column using the AUTO_INCREMENT attribute. INSERT INTO tbl(description) VALUES('MySQL last_insert_id'); SELECT LAST_INSERT_ID();","title":"LAST_INSERT_ID"},{"location":"Databases/MySQL/MySQL%20functions/#cast","text":"The CAST() function converts a value of any type into a value that has a specified type. The target type can be any one of the following types: BINARY, CHAR, DATE, DATETIME, TIME,DECIMAL, SIGNED, UNSIGNED . SELECT (1 + CAST('1' AS UNSIGNED))/2; SELECT CONCAT('MySQL CAST example #',CAST(2 AS CHAR));","title":"CAST"},{"location":"Databases/MySQL/MySQL%20scaling/","text":"MySQL scaling \u00b6 Methods \u00b6 The process and methods of scaling are categorized into several categories: Vertical Scaling or Scale-up \u00b6 In vertical scaling, we increase the CPU, RAM and Storage or but a more robust server in order to support more traffic and load on our database server. Pros Less power consumption than running multiple servers Colling costs are less than scaling horizontally Generally less challenges to implement Less licensing costs (if using paid databases) Cons It's way more costly There is a limit to increasing hardware on a single system Greater risk of hardware failure causing bigger outages Horizontal Scaling or Scaling-out \u00b6 In horizontal scaling we add more systems with smaller configuration in terms of CPU, RAM and storage to distribute the traffic or load across these systems. Pros Much cheaper than scaling vertically Easier to run fault-tolerance Easy to upgrade Cons Bigger footprint on data centre Higher utility cost More licensing fees (if using paid databases) When we use the Vertical and Horizontal scaling at the same time, we call it the Hybrid Scaling approach. Scaling MySQL database \u00b6 By default, the MySQL can be scaled either by using vertical or hybrid approaches, but not fully horizontal approach. Master-slave approach \u00b6 Generally, we tend to create a Master-Slave architecture and route all the write queries on the master instance and all the read queries on the slave instances which are replicated from the Master . We can have multiple Slave instances running at one and scale our read operations horizontally. But the Master can only be scaled Vertically. Problems with the Master-Slave approach Every record inserted in MySQL tables have one primary key to index the records for faster select operations. Generally, we have mysql_insert_id as our primary key on the table, when having multiple masters, this key will be duplicated accross the databases and the data will be redundant to use. In order to solve the duplicate ID issue, we can have one ID generator which generates the unique IDs and we can use the same ID as our primary key, but this will add one extra point of failure in the system due to concurrency. If the ID generator is down or slow, the whole database will fail. Solutions and approaches \u00b6 ID Generator \u00b6 To solve the duplicate ID issue in multiple master architecture you can use an ID generator server which generates unique time-based serial hashes which can be used as a primary key in mysql table. Concurrency \u00b6 To solve the concurrency issue you can use a pool of IDs stored in the cache of the ID generator. When a write request occurs - it takes a generated ID from the cache and uses it as the primary key.","title":"MySQL scaling"},{"location":"Databases/MySQL/MySQL%20scaling/#mysql-scaling","text":"","title":"MySQL scaling"},{"location":"Databases/MySQL/MySQL%20scaling/#methods","text":"The process and methods of scaling are categorized into several categories:","title":"Methods"},{"location":"Databases/MySQL/MySQL%20scaling/#vertical-scaling-or-scale-up","text":"In vertical scaling, we increase the CPU, RAM and Storage or but a more robust server in order to support more traffic and load on our database server. Pros Less power consumption than running multiple servers Colling costs are less than scaling horizontally Generally less challenges to implement Less licensing costs (if using paid databases) Cons It's way more costly There is a limit to increasing hardware on a single system Greater risk of hardware failure causing bigger outages","title":"Vertical Scaling or Scale-up"},{"location":"Databases/MySQL/MySQL%20scaling/#horizontal-scaling-or-scaling-out","text":"In horizontal scaling we add more systems with smaller configuration in terms of CPU, RAM and storage to distribute the traffic or load across these systems. Pros Much cheaper than scaling vertically Easier to run fault-tolerance Easy to upgrade Cons Bigger footprint on data centre Higher utility cost More licensing fees (if using paid databases) When we use the Vertical and Horizontal scaling at the same time, we call it the Hybrid Scaling approach.","title":"Horizontal Scaling or Scaling-out"},{"location":"Databases/MySQL/MySQL%20scaling/#scaling-mysql-database","text":"By default, the MySQL can be scaled either by using vertical or hybrid approaches, but not fully horizontal approach.","title":"Scaling MySQL database"},{"location":"Databases/MySQL/MySQL%20scaling/#master-slave-approach","text":"Generally, we tend to create a Master-Slave architecture and route all the write queries on the master instance and all the read queries on the slave instances which are replicated from the Master . We can have multiple Slave instances running at one and scale our read operations horizontally. But the Master can only be scaled Vertically. Problems with the Master-Slave approach Every record inserted in MySQL tables have one primary key to index the records for faster select operations. Generally, we have mysql_insert_id as our primary key on the table, when having multiple masters, this key will be duplicated accross the databases and the data will be redundant to use. In order to solve the duplicate ID issue, we can have one ID generator which generates the unique IDs and we can use the same ID as our primary key, but this will add one extra point of failure in the system due to concurrency. If the ID generator is down or slow, the whole database will fail.","title":"Master-slave approach"},{"location":"Databases/MySQL/MySQL%20scaling/#solutions-and-approaches","text":"","title":"Solutions and approaches"},{"location":"Databases/MySQL/MySQL%20scaling/#id-generator","text":"To solve the duplicate ID issue in multiple master architecture you can use an ID generator server which generates unique time-based serial hashes which can be used as a primary key in mysql table.","title":"ID Generator"},{"location":"Databases/MySQL/MySQL%20scaling/#concurrency","text":"To solve the concurrency issue you can use a pool of IDs stored in the cache of the ID generator. When a write request occurs - it takes a generated ID from the cache and uses it as the primary key.","title":"Concurrency"},{"location":"Databases/MySQL/Rename%20MySQL%20database%20from%20bash/","text":"You can use a following command to rename databases in bash. mysql -u USER -pPASSWORD OLDDB -sNe 'show tables' | while read table; do mysql -u USER -pPASSWORD NEWDB -sNe \"RENAME TABLE \\`OLDDB\\`.$table TO \\`NEWDB\\`.$table\"; done","title":"Rename MySQL database from bash"},{"location":"Databases/MySQL/advanced/","text":"Notes are based on materials in http://www.mysqltutorial.org/ .","title":"Index"},{"location":"Databases/MySQL/advanced/01_stored_procedures/","text":"Stored procedures \u00b6 Stored procedure is a segment of declarative SQL statements stored inside the database catalog. A stored procedure can be invoked by triggers, other stored procedures and applications. A stored procedure that calls itself is known as a recursive stored procedure. Most database management systems support recursive stored procedures. However, MySQL does not support it very well. You should check your version of MySQL database before implementing them. Advantages of MySQL stored procedures \u00b6 Typically, stored procedures help increase performance of the applications. Once created, stored procedures are compiled and stored in the database. However, MySQL implements the stored procedures slightly different. Stored procedures are compiled on demand and then they are stored in procedure cache for every single connection. If an application uses a stored procedure multiple times in a single connection, the compiled version is used, otherwise, the stored procedure works like a query. Stored procedures help reduce the traffic between application and database server because instead of sending multiple lengthy SQL statements, the application has to send only the name and parameters of the stored procedure. Stored procedures are reusable and transparent to any applications. Stored procedures expose the database interface to all applications so that developers do not have to develop functions that already are supported in stored procedures. Stored procedures are secure. The database administrator can grant appropriate permissions to applications that access stored procedures in the database without giving any permissions on the underlying database tables. Disadvantages of MySQL stored procedures \u00b6 If you use many stored procedures, the memory usage of every connection that is using those stored procedures will increase substantially. In addition, if you overuse a large number of logical operations inside stored procedures, the CPU usage will increase because the database server is not well-designed for logical operations. Stored procedure's constructs are not designed for developing complex and flexible business logic. It is difficult to debug stored procedures. Only a few database management systems allow you to debug stored procedures. Unfortunetely, MySQL does not provide facilities for debugging stored procedures. It is not easy to develop and maintain stored procedures. Developing and maintaining stored procedures are often required a specialized skill set that not all application developers possess. This may lead to problems in both application development and maintenance phases. Writting and calling a stored procedure \u00b6 DROP procedure IF EXISTS `GetAllProducts`; DELIMITER // CREATE PROCEDURE GetAllProducts() BEGIN SELECT * FROM products; END // DELIMITER ; The first command DELIMITER // is not related to the stored procedure syntax. It changes the standard delimiter from ; to another. This is done in order to pass the stored procedure to the server as a whole rather than letting the mysql tool interpret each statement at a time. The END keyword ends the stored procedure. The DELIMITER ; reverts the default delimiter. To call the stored procedure: CALL GetAllProducts(); Variables in stored procedures \u00b6 A variable is a named data object whose value can change during the stored procedure execution. These variables are local to the stored procedure, must be declared before using it. DECLARE variable_name datatype(size) DEFAULT default_value; DECLARE x,y INT DEFAULT 0; Assigning variables DECLARE total_count INT DEFAULT 0; SET total_count = 10; Besides the SET statement, a SELECT INTO can be used to assign the result of a query. DECLARE total_products INT DEFAULT 0; SELECT COUNT(*) INTO total_products FROM products; Variable scope \u00b6 A variable has it's own scope that defines it's lifetime. If you declare a variable inside a stored procedure, it will be out of scope when the END statement of stored procedure reaches. If you declare a variable inside BEGIN END block, it will be out of scope if the END is reached. You can declare two or more variables with the same name in different scopes. A variable whose name begines with the @ sign is a session variable. It is available and accessible until the session ends. Stored procedure parameters \u00b6 In MySQL, a parameter has one of 3 modes: IN , OUT or INOUT . IN - is the default mode. When you define an IN parameter in a stored procedure, the calling program has to pass an argument to the stored procedure. In addition, the value of an IN parameter is protected. It means that event the value of the IN parameter is changed inside the stored procedure, it's original value is retained after the procedure ends. OUT - the value of an OUT parameter can be changed inside the stored procedure and it's new value is passed back to the calling program. Notice that stored procedure cannot access the initial value of the OUT parameter when it starts. INOUT - a combination of IN and OUT parameters. It means that the calling program may pass the argument, and the stored procedure can modify the parameter, then pass the new value back to the calling program. MODE param_name param_type(param_size) Each parameter is separated by a comm , if the stored procedure has more than one parameter. DELIMITER // CREATE PROCEDURE GetOfficeByCountry(IN countryName VARCHAR(255)) BEGIN SELECT * FROM offices WHERE country = countryName; END // DELIMITER ; CALL GetOfficeByCountry('USA'); DELIMITER $$ CREATE PROCEDURE CountOrderByStatus( IN orderStatus VARCHAR(25), OUT total INT) BEGIN SELECT count(orderNumber) INTO total FROM orders WHERE status = orderStatus; END$$ DELIMITER ; CALL CountOrderByStatus('Shipped',@total); SELECT @total; DELIMITER $$ CREATE PROCEDURE set_counter(INOUT count INT(4),IN inc INT(4)) BEGIN SET count = count + inc; END$$ DELIMITER ; SET @counter = 1; CALL set_counter(@counter,1); -- 2 CALL set_counter(@counter,1); -- 3 CALL set_counter(@counter,5); -- 8 SELECT @counter; -- 8 Returning multiple values \u00b6 DELIMITER $$ CREATE PROCEDURE get_order_by_cust( IN cust_no INT, OUT shipped INT, OUT canceled INT, OUT resolved INT, OUT disputed INT) BEGIN -- shipped SELECT count(*) INTO shipped FROM orders WHERE customerNumber = cust_no AND status = 'Shipped'; -- canceled SELECT count(*) INTO canceled FROM orders WHERE customerNumber = cust_no AND status = 'Canceled'; -- resolved SELECT count(*) INTO resolved FROM orders WHERE customerNumber = cust_no AND status = 'Resolved'; -- disputed SELECT count(*) INTO disputed FROM orders WHERE customerNumber = cust_no AND status = 'Disputed'; END CALL get_order_by_cust(141,@shipped,@canceled,@resolved,@disputed); SELECT @shipped,@canceled,@resolved,@disputed;","title":"Stored procedures"},{"location":"Databases/MySQL/advanced/01_stored_procedures/#stored-procedures","text":"Stored procedure is a segment of declarative SQL statements stored inside the database catalog. A stored procedure can be invoked by triggers, other stored procedures and applications. A stored procedure that calls itself is known as a recursive stored procedure. Most database management systems support recursive stored procedures. However, MySQL does not support it very well. You should check your version of MySQL database before implementing them.","title":"Stored procedures"},{"location":"Databases/MySQL/advanced/01_stored_procedures/#advantages-of-mysql-stored-procedures","text":"Typically, stored procedures help increase performance of the applications. Once created, stored procedures are compiled and stored in the database. However, MySQL implements the stored procedures slightly different. Stored procedures are compiled on demand and then they are stored in procedure cache for every single connection. If an application uses a stored procedure multiple times in a single connection, the compiled version is used, otherwise, the stored procedure works like a query. Stored procedures help reduce the traffic between application and database server because instead of sending multiple lengthy SQL statements, the application has to send only the name and parameters of the stored procedure. Stored procedures are reusable and transparent to any applications. Stored procedures expose the database interface to all applications so that developers do not have to develop functions that already are supported in stored procedures. Stored procedures are secure. The database administrator can grant appropriate permissions to applications that access stored procedures in the database without giving any permissions on the underlying database tables.","title":"Advantages of MySQL stored procedures"},{"location":"Databases/MySQL/advanced/01_stored_procedures/#disadvantages-of-mysql-stored-procedures","text":"If you use many stored procedures, the memory usage of every connection that is using those stored procedures will increase substantially. In addition, if you overuse a large number of logical operations inside stored procedures, the CPU usage will increase because the database server is not well-designed for logical operations. Stored procedure's constructs are not designed for developing complex and flexible business logic. It is difficult to debug stored procedures. Only a few database management systems allow you to debug stored procedures. Unfortunetely, MySQL does not provide facilities for debugging stored procedures. It is not easy to develop and maintain stored procedures. Developing and maintaining stored procedures are often required a specialized skill set that not all application developers possess. This may lead to problems in both application development and maintenance phases.","title":"Disadvantages of MySQL stored procedures"},{"location":"Databases/MySQL/advanced/01_stored_procedures/#writting-and-calling-a-stored-procedure","text":"DROP procedure IF EXISTS `GetAllProducts`; DELIMITER // CREATE PROCEDURE GetAllProducts() BEGIN SELECT * FROM products; END // DELIMITER ; The first command DELIMITER // is not related to the stored procedure syntax. It changes the standard delimiter from ; to another. This is done in order to pass the stored procedure to the server as a whole rather than letting the mysql tool interpret each statement at a time. The END keyword ends the stored procedure. The DELIMITER ; reverts the default delimiter. To call the stored procedure: CALL GetAllProducts();","title":"Writting and calling a stored procedure"},{"location":"Databases/MySQL/advanced/01_stored_procedures/#variables-in-stored-procedures","text":"A variable is a named data object whose value can change during the stored procedure execution. These variables are local to the stored procedure, must be declared before using it. DECLARE variable_name datatype(size) DEFAULT default_value; DECLARE x,y INT DEFAULT 0; Assigning variables DECLARE total_count INT DEFAULT 0; SET total_count = 10; Besides the SET statement, a SELECT INTO can be used to assign the result of a query. DECLARE total_products INT DEFAULT 0; SELECT COUNT(*) INTO total_products FROM products;","title":"Variables in stored procedures"},{"location":"Databases/MySQL/advanced/01_stored_procedures/#variable-scope","text":"A variable has it's own scope that defines it's lifetime. If you declare a variable inside a stored procedure, it will be out of scope when the END statement of stored procedure reaches. If you declare a variable inside BEGIN END block, it will be out of scope if the END is reached. You can declare two or more variables with the same name in different scopes. A variable whose name begines with the @ sign is a session variable. It is available and accessible until the session ends.","title":"Variable scope"},{"location":"Databases/MySQL/advanced/01_stored_procedures/#stored-procedure-parameters","text":"In MySQL, a parameter has one of 3 modes: IN , OUT or INOUT . IN - is the default mode. When you define an IN parameter in a stored procedure, the calling program has to pass an argument to the stored procedure. In addition, the value of an IN parameter is protected. It means that event the value of the IN parameter is changed inside the stored procedure, it's original value is retained after the procedure ends. OUT - the value of an OUT parameter can be changed inside the stored procedure and it's new value is passed back to the calling program. Notice that stored procedure cannot access the initial value of the OUT parameter when it starts. INOUT - a combination of IN and OUT parameters. It means that the calling program may pass the argument, and the stored procedure can modify the parameter, then pass the new value back to the calling program. MODE param_name param_type(param_size) Each parameter is separated by a comm , if the stored procedure has more than one parameter. DELIMITER // CREATE PROCEDURE GetOfficeByCountry(IN countryName VARCHAR(255)) BEGIN SELECT * FROM offices WHERE country = countryName; END // DELIMITER ; CALL GetOfficeByCountry('USA'); DELIMITER $$ CREATE PROCEDURE CountOrderByStatus( IN orderStatus VARCHAR(25), OUT total INT) BEGIN SELECT count(orderNumber) INTO total FROM orders WHERE status = orderStatus; END$$ DELIMITER ; CALL CountOrderByStatus('Shipped',@total); SELECT @total; DELIMITER $$ CREATE PROCEDURE set_counter(INOUT count INT(4),IN inc INT(4)) BEGIN SET count = count + inc; END$$ DELIMITER ; SET @counter = 1; CALL set_counter(@counter,1); -- 2 CALL set_counter(@counter,1); -- 3 CALL set_counter(@counter,5); -- 8 SELECT @counter; -- 8","title":"Stored procedure parameters"},{"location":"Databases/MySQL/advanced/01_stored_procedures/#returning-multiple-values","text":"DELIMITER $$ CREATE PROCEDURE get_order_by_cust( IN cust_no INT, OUT shipped INT, OUT canceled INT, OUT resolved INT, OUT disputed INT) BEGIN -- shipped SELECT count(*) INTO shipped FROM orders WHERE customerNumber = cust_no AND status = 'Shipped'; -- canceled SELECT count(*) INTO canceled FROM orders WHERE customerNumber = cust_no AND status = 'Canceled'; -- resolved SELECT count(*) INTO resolved FROM orders WHERE customerNumber = cust_no AND status = 'Resolved'; -- disputed SELECT count(*) INTO disputed FROM orders WHERE customerNumber = cust_no AND status = 'Disputed'; END CALL get_order_by_cust(141,@shipped,@canceled,@resolved,@disputed); SELECT @shipped,@canceled,@resolved,@disputed;","title":"Returning multiple values"},{"location":"Databases/MySQL/advanced/02_if_statements/","text":"MySQL IF Statement \u00b6 The MySQL allows you to execute a set of SQL statements based on a certain condiftion or value of an expression. IF expression THEN statements; END IF; If the expression evaluates to TRUE , then the statements will be executed. MySQL IF-ELSE \u00b6 IF expression THEN statements; ELSE else-statements; END IF; MySQL IF-ELSEIF-ELSE \u00b6 IF expression THEN statements; ELSEIF elseif-expression THEN elseif-statements; ... ELSE else-statements; END IF; DELIMITER $$ CREATE PROCEDURE GetCustomerLevel( in p_customerNumber int(11), out p_customerLevel varchar(10)) BEGIN DECLARE creditlim double; SELECT creditlimit INTO creditlim FROM customers WHERE customerNumber = p_customerNumber; IF creditlim > 50000 THEN SET p_customerLevel = 'PLATINUM'; ELSEIF (creditlim <= 50000 AND creditlim >= 10000) THEN SET p_customerLevel = 'GOLD'; ELSEIF creditlim < 10000 THEN SET p_customerLevel = 'SILVER'; END IF; END$$","title":"MySQL IF Statement"},{"location":"Databases/MySQL/advanced/02_if_statements/#mysql-if-statement","text":"The MySQL allows you to execute a set of SQL statements based on a certain condiftion or value of an expression. IF expression THEN statements; END IF; If the expression evaluates to TRUE , then the statements will be executed.","title":"MySQL IF Statement"},{"location":"Databases/MySQL/advanced/02_if_statements/#mysql-if-else","text":"IF expression THEN statements; ELSE else-statements; END IF;","title":"MySQL IF-ELSE"},{"location":"Databases/MySQL/advanced/02_if_statements/#mysql-if-elseif-else","text":"IF expression THEN statements; ELSEIF elseif-expression THEN elseif-statements; ... ELSE else-statements; END IF; DELIMITER $$ CREATE PROCEDURE GetCustomerLevel( in p_customerNumber int(11), out p_customerLevel varchar(10)) BEGIN DECLARE creditlim double; SELECT creditlimit INTO creditlim FROM customers WHERE customerNumber = p_customerNumber; IF creditlim > 50000 THEN SET p_customerLevel = 'PLATINUM'; ELSEIF (creditlim <= 50000 AND creditlim >= 10000) THEN SET p_customerLevel = 'GOLD'; ELSEIF creditlim < 10000 THEN SET p_customerLevel = 'SILVER'; END IF; END$$","title":"MySQL IF-ELSEIF-ELSE"},{"location":"Databases/MySQL/advanced/03_case_statements/","text":"MySQL CASE statement \u00b6 There are two forms of the CASE statements - simple and searched CASE statements. Simple CASE statement \u00b6 CASE case_expression WHEN when_expression_1 THEN commands WHEN when_expression_2 THEN commands ... ELSE commands END CASE; You use it to check the value of an expression agains a set of unique values. DELIMITER $$ CREATE PROCEDURE GetCustomerShipping( in p_customerNumber int(11), out p_shiping varchar(50)) BEGIN DECLARE customerCountry varchar(50); SELECT country INTO customerCountry FROM customers WHERE customerNumber = p_customerNumber; CASE customerCountry WHEN 'USA' THEN SET p_shiping = '2-day Shipping'; WHEN 'Canada' THEN SET p_shiping = '3-day Shipping'; ELSE SET p_shiping = '5-day Shipping'; END CASE; END$$ SET @customerNo = 112; SELECT country into @country FROM customers WHERE customernumber = @customerNo; CALL GetCustomerShipping(@customerNo,@shipping); SELECT @customerNo AS Customer, @country AS Country, @shipping AS Shipping; Searched CASE statement \u00b6 The simple CASE statement only allows you match a value of an expression against a set of distinct values. In order to perform more complex matches such as ranges, you use the searched CASE statement. It is equivalent to the IF statement, however it's construct is much more readable. CASE WHEN condition_1 THEN commands WHEN condition_2 THEN commands ... ELSE commands END CASE; DELIMITER $$ CREATE PROCEDURE GetCustomerLevel( in p_customerNumber int(11), out p_customerLevel varchar(10)) BEGIN DECLARE creditlim double; SELECT creditlimit INTO creditlim FROM customers WHERE customerNumber = p_customerNumber; CASE WHEN creditlim > 50000 THEN SET p_customerLevel = 'PLATINUM'; WHEN (creditlim <= 50000 AND creditlim >= 10000) THEN SET p_customerLevel = 'GOLD'; WHEN creditlim < 10000 THEN SET p_customerLevel = 'SILVER'; END CASE; END$$ CALL GetCustomerLevel(112,@level); SELECT @level AS 'Customer Level';","title":"MySQL CASE statement"},{"location":"Databases/MySQL/advanced/03_case_statements/#mysql-case-statement","text":"There are two forms of the CASE statements - simple and searched CASE statements.","title":"MySQL CASE statement"},{"location":"Databases/MySQL/advanced/03_case_statements/#simple-case-statement","text":"CASE case_expression WHEN when_expression_1 THEN commands WHEN when_expression_2 THEN commands ... ELSE commands END CASE; You use it to check the value of an expression agains a set of unique values. DELIMITER $$ CREATE PROCEDURE GetCustomerShipping( in p_customerNumber int(11), out p_shiping varchar(50)) BEGIN DECLARE customerCountry varchar(50); SELECT country INTO customerCountry FROM customers WHERE customerNumber = p_customerNumber; CASE customerCountry WHEN 'USA' THEN SET p_shiping = '2-day Shipping'; WHEN 'Canada' THEN SET p_shiping = '3-day Shipping'; ELSE SET p_shiping = '5-day Shipping'; END CASE; END$$ SET @customerNo = 112; SELECT country into @country FROM customers WHERE customernumber = @customerNo; CALL GetCustomerShipping(@customerNo,@shipping); SELECT @customerNo AS Customer, @country AS Country, @shipping AS Shipping;","title":"Simple CASE statement"},{"location":"Databases/MySQL/advanced/03_case_statements/#searched-case-statement","text":"The simple CASE statement only allows you match a value of an expression against a set of distinct values. In order to perform more complex matches such as ranges, you use the searched CASE statement. It is equivalent to the IF statement, however it's construct is much more readable. CASE WHEN condition_1 THEN commands WHEN condition_2 THEN commands ... ELSE commands END CASE; DELIMITER $$ CREATE PROCEDURE GetCustomerLevel( in p_customerNumber int(11), out p_customerLevel varchar(10)) BEGIN DECLARE creditlim double; SELECT creditlimit INTO creditlim FROM customers WHERE customerNumber = p_customerNumber; CASE WHEN creditlim > 50000 THEN SET p_customerLevel = 'PLATINUM'; WHEN (creditlim <= 50000 AND creditlim >= 10000) THEN SET p_customerLevel = 'GOLD'; WHEN creditlim < 10000 THEN SET p_customerLevel = 'SILVER'; END CASE; END$$ CALL GetCustomerLevel(112,@level); SELECT @level AS 'Customer Level';","title":"Searched CASE statement"},{"location":"Databases/MySQL/advanced/04_choosing_between_if_and_case/","text":"Choosing bethween IF and CASE statements \u00b6 When deciding to use either IF or CASE statements, you should take following points into consideration: A simple CASE statement is much more readable than the IF statement when you compare a single expression against a range of unique values. In addition, the simple CASE statement is more efficient than the IF statement. When you check complex expressions based on multiple values, the IF statement is easier to understand. If you choose to use the CASE statement, you have to make sure that at least one of CASE conditions is matched. Otherwise, you need to define an error handler to catch the error. Recall that you do not have to do this in the IF statement.","title":"Choosing bethween IF and CASE statements"},{"location":"Databases/MySQL/advanced/04_choosing_between_if_and_case/#choosing-bethween-if-and-case-statements","text":"When deciding to use either IF or CASE statements, you should take following points into consideration: A simple CASE statement is much more readable than the IF statement when you compare a single expression against a range of unique values. In addition, the simple CASE statement is more efficient than the IF statement. When you check complex expressions based on multiple values, the IF statement is easier to understand. If you choose to use the CASE statement, you have to make sure that at least one of CASE conditions is matched. Otherwise, you need to define an error handler to catch the error. Recall that you do not have to do this in the IF statement.","title":"Choosing bethween IF and CASE statements"},{"location":"Databases/MySQL/advanced/05_loops/","text":"MySQL Lopp in stored procedures \u00b6 MySQL provides loop statements that allow you to execute a block of SQL code repeatedly based on a condition. There are three loop statements in MySQL - WHILE , REPEAT and LOOP . While Loop \u00b6 WHILE expression DO statements END WHILE DELIMITER $$ DROP PROCEDURE IF EXISTS test_mysql_while_loop$$ CREATE PROCEDURE test_mysql_while_loop() BEGIN DECLARE x INT; DECLARE str VARCHAR(255); SET x = 1; SET str = ''; WHILE x <= 5 DO SET str = CONCAT(str,x,','); SET x = x + 1; END WHILE; SELECT str; END$$ DELIMITER ; CALL test_mysql_while_loop(); REPEAT loop \u00b6 REPEAT statements; UNTIL expression END REPEAT ```sql DELIMITER $$ DROP PROCEDURE IF EXISTS mysql_test_repeat_loop$$ CREATE PROCEDURE mysql_test_repeat_loop() BEGIN DECLARE x INT; DECLARE str VARCHAR(255); SET x = 1; SET str = ''; REPEAT SET str = CONCAT(str,x,','); SET x = x + 1; UNTIL x > 5 END REPEAT; SELECT str; END$$ DELIMITER ; CALL mysql_test_repeat_loop(); ``` ## LOOP, LEAVE and ITERATE statements There are two statements that allow you to control the loop: The LEAVE statement allow you to exit the loop immediately without waiting for checking the condition. The LEAVE statement works like the break statement in other languages. The ITERATE statement allows you to skip the endire code under it and start a new iteration. Similar to continue in other languages. LOOP statement \u00b6 ```sql CREATE PROCEDURE test_mysql_loop() BEGIN DECLARE x INT; DECLARE str VARCHAR(255); SET x = 1; SET str = ''; loop_label: LOOP IF x > 10 THEN LEAVE loop_label; END IF; SET x = x + 1; IF (x mod 2) THEN ITERATE loop_label; ELSE SET str = CONCAT(str,x,','); END IF; END LOOP; SELECT str; END; ```","title":"MySQL Lopp in stored procedures"},{"location":"Databases/MySQL/advanced/05_loops/#mysql-lopp-in-stored-procedures","text":"MySQL provides loop statements that allow you to execute a block of SQL code repeatedly based on a condition. There are three loop statements in MySQL - WHILE , REPEAT and LOOP .","title":"MySQL Lopp in stored procedures"},{"location":"Databases/MySQL/advanced/05_loops/#while-loop","text":"WHILE expression DO statements END WHILE DELIMITER $$ DROP PROCEDURE IF EXISTS test_mysql_while_loop$$ CREATE PROCEDURE test_mysql_while_loop() BEGIN DECLARE x INT; DECLARE str VARCHAR(255); SET x = 1; SET str = ''; WHILE x <= 5 DO SET str = CONCAT(str,x,','); SET x = x + 1; END WHILE; SELECT str; END$$ DELIMITER ; CALL test_mysql_while_loop();","title":"While Loop"},{"location":"Databases/MySQL/advanced/05_loops/#repeat-loop","text":"REPEAT statements; UNTIL expression END REPEAT ```sql DELIMITER $$ DROP PROCEDURE IF EXISTS mysql_test_repeat_loop$$ CREATE PROCEDURE mysql_test_repeat_loop() BEGIN DECLARE x INT; DECLARE str VARCHAR(255); SET x = 1; SET str = ''; REPEAT SET str = CONCAT(str,x,','); SET x = x + 1; UNTIL x > 5 END REPEAT; SELECT str; END$$ DELIMITER ; CALL mysql_test_repeat_loop(); ``` ## LOOP, LEAVE and ITERATE statements There are two statements that allow you to control the loop: The LEAVE statement allow you to exit the loop immediately without waiting for checking the condition. The LEAVE statement works like the break statement in other languages. The ITERATE statement allows you to skip the endire code under it and start a new iteration. Similar to continue in other languages.","title":"REPEAT loop"},{"location":"Databases/MySQL/advanced/05_loops/#loop-statement","text":"```sql CREATE PROCEDURE test_mysql_loop() BEGIN DECLARE x INT; DECLARE str VARCHAR(255); SET x = 1; SET str = ''; loop_label: LOOP IF x > 10 THEN LEAVE loop_label; END IF; SET x = x + 1; IF (x mod 2) THEN ITERATE loop_label; ELSE SET str = CONCAT(str,x,','); END IF; END LOOP; SELECT str; END; ```","title":"LOOP statement"},{"location":"Databases/MySQL/advanced/06_cursors/","text":"MySQL Cursors \u00b6 To handle a result set inside a stored procedure, you use a cursor. A cursor allows you to iterate a set of rows returned by a query and process each row accordingly. MySQL cursor is read-only, non-scollable and asensitive. Read-only means that you cannot update data in the underlying table through the cursor. Non-scrollable means that you can only fetch rows in the order determined by the SELECT statement. You cannot fetch rows in the reversed order. You can't skip or jump to a specific row either. Asensitive means that there are two kinds of cursors - asensitive and insensitive cursors. Asensitive cursors point to the actual data, whereas an insensitive cursor uses a temporary copy of the data. An asensitive cursor performs faster than an insensitive cursor because it does not have to make a temporary copy of the data. However, any change that made to the data from other connections will affect the data. The MySQL cursors can be used in stored procedures, stored function and triggers. DECLARE cursor_name CURSOR FOR SELECT_statement; The cursor declaration must be after any variable declaration. If you declare a cursor before variables declaration, MySQL will issue an error. A cursor must be associated wit a SELECT statemtn. Next, you open the cursor by using the OPEN statemnt which initializes the result set for the cursor. OPEN cursor_name; Then you use FETCH statement to retrieve the next row pointed by the cursor and move the cursor to the next row in the result set. FETCH cursor_name INTO variables list; Finally, you call CLOSE statement to deactivate the cursor and release the memory. CLOSE cursor_name; When working with MySQL cursor, you must also declare a NOT FOUND handler to handle the situation when the cursor could not find any row. Because each time you call the FETCH statement, the cursor attempts to read the next row in the result set. DECLARE CONTINUE HANDLER FOR NOT FOUND SET finished = 1; Example \u00b6 DELIMITER $$ CREATE PROCEDURE build_email_list (INOUT email_list varchar(4000)) BEGIN DECLARE v_finished INTEGER DEFAULT 0; DECLARE v_email varchar(100) DEFAULT \"\"; -- declare cursor for employee email DEClARE email_cursor CURSOR FOR SELECT email FROM employees; -- declare NOT FOUND handler DECLARE CONTINUE HANDLER FOR NOT FOUND SET v_finished = 1; OPEN email_cursor; get_email: LOOP FETCH email_cursor INTO v_email; IF v_finished = 1 THEN LEAVE get_email; END IF; -- build email list SET email_list = CONCAT(v_email,\";\",email_list); END LOOP get_email; CLOSE email_cursor; END$$ DELIMITER ; SET @email_list = \"\"; CALL build_email_list(@email_list); SELECT @email_list;","title":"MySQL Cursors"},{"location":"Databases/MySQL/advanced/06_cursors/#mysql-cursors","text":"To handle a result set inside a stored procedure, you use a cursor. A cursor allows you to iterate a set of rows returned by a query and process each row accordingly. MySQL cursor is read-only, non-scollable and asensitive. Read-only means that you cannot update data in the underlying table through the cursor. Non-scrollable means that you can only fetch rows in the order determined by the SELECT statement. You cannot fetch rows in the reversed order. You can't skip or jump to a specific row either. Asensitive means that there are two kinds of cursors - asensitive and insensitive cursors. Asensitive cursors point to the actual data, whereas an insensitive cursor uses a temporary copy of the data. An asensitive cursor performs faster than an insensitive cursor because it does not have to make a temporary copy of the data. However, any change that made to the data from other connections will affect the data. The MySQL cursors can be used in stored procedures, stored function and triggers. DECLARE cursor_name CURSOR FOR SELECT_statement; The cursor declaration must be after any variable declaration. If you declare a cursor before variables declaration, MySQL will issue an error. A cursor must be associated wit a SELECT statemtn. Next, you open the cursor by using the OPEN statemnt which initializes the result set for the cursor. OPEN cursor_name; Then you use FETCH statement to retrieve the next row pointed by the cursor and move the cursor to the next row in the result set. FETCH cursor_name INTO variables list; Finally, you call CLOSE statement to deactivate the cursor and release the memory. CLOSE cursor_name; When working with MySQL cursor, you must also declare a NOT FOUND handler to handle the situation when the cursor could not find any row. Because each time you call the FETCH statement, the cursor attempts to read the next row in the result set. DECLARE CONTINUE HANDLER FOR NOT FOUND SET finished = 1;","title":"MySQL Cursors"},{"location":"Databases/MySQL/advanced/06_cursors/#example","text":"DELIMITER $$ CREATE PROCEDURE build_email_list (INOUT email_list varchar(4000)) BEGIN DECLARE v_finished INTEGER DEFAULT 0; DECLARE v_email varchar(100) DEFAULT \"\"; -- declare cursor for employee email DEClARE email_cursor CURSOR FOR SELECT email FROM employees; -- declare NOT FOUND handler DECLARE CONTINUE HANDLER FOR NOT FOUND SET v_finished = 1; OPEN email_cursor; get_email: LOOP FETCH email_cursor INTO v_email; IF v_finished = 1 THEN LEAVE get_email; END IF; -- build email list SET email_list = CONCAT(v_email,\";\",email_list); END LOOP get_email; CLOSE email_cursor; END$$ DELIMITER ; SET @email_list = \"\"; CALL build_email_list(@email_list); SELECT @email_list;","title":"Example"},{"location":"Databases/MySQL/advanced/07_listing_stored_procedures_in_database/","text":"Listing Stored procedures in database \u00b6 MySQL provides us with several useful statements that help manage stored procedures more effectively. Those statements include listing stored procedures and showing the stored procedure's source code. Displaying characteristics of stored procedures \u00b6 SHOW PROCEDURE STATUS [LIKE 'pattern' | WHERE expr] You can use LIKE or WHERE clause to filter out stored procedures based on various criteria. SHOW PROCEDURE STATUS WHERE db = 'classicmodels'; SHOW PROCEDURE STATUS WHERE name LIKE '%product%' Displaying stored procedure's source code \u00b6 SHOW CREATE PROCEDURE stored_procedure_name","title":"Listing Stored procedures in database"},{"location":"Databases/MySQL/advanced/07_listing_stored_procedures_in_database/#listing-stored-procedures-in-database","text":"MySQL provides us with several useful statements that help manage stored procedures more effectively. Those statements include listing stored procedures and showing the stored procedure's source code.","title":"Listing Stored procedures in database"},{"location":"Databases/MySQL/advanced/07_listing_stored_procedures_in_database/#displaying-characteristics-of-stored-procedures","text":"SHOW PROCEDURE STATUS [LIKE 'pattern' | WHERE expr] You can use LIKE or WHERE clause to filter out stored procedures based on various criteria. SHOW PROCEDURE STATUS WHERE db = 'classicmodels'; SHOW PROCEDURE STATUS WHERE name LIKE '%product%'","title":"Displaying characteristics of stored procedures"},{"location":"Databases/MySQL/advanced/07_listing_stored_procedures_in_database/#displaying-stored-procedures-source-code","text":"SHOW CREATE PROCEDURE stored_procedure_name","title":"Displaying stored procedure's source code"},{"location":"Databases/MySQL/advanced/08_error_handling_in_stored_procedures/","text":"Handling errors in Stored Procedures \u00b6 When an error occurs inside a stored procedure, it is important to handle it appropriately, such as continuing or exiting the current code block's execution, and issuing a meaningful error message. Declaring a handler \u00b6 DECLARE action HANDLER FOR condition_value statement; If a condition whose value matches the condition_value , MySQL will execute the statement and continue or exit the current code block based on the action . The action accepts one of following values: CONTINUE - the execution of enclosing code in block continues. EXIT - the execution of the enclosing block where the handler is declared is terminated. The condition_value specifies a particular condition or a class of condition that activate the handler. The condition_value accepts one of the following values: A MySQL error code. A standard SQLSTATE value. Or it can be an SQLWARNING , NOTFOUND or SQLEXCEPTION condition, which is shorthand for the class of SQLSTATE values. The NOTFOUND condition is used for a cursor or SELECT INTO variable_list statement. A named condition associated with either a MySQL error code or SQLSTATE value. The statement could be a simple statement or compound statement enclosing by the BEGIN and END keywords. Handler for SQLEXCEPTION DECLARE CONTINUE HANDLER FOR SQLEXCEPTION SET has_error = 1; DECLARE EXIT HANDLER FOR SQLEXCEPTION BEGIN ROLLBACK; SELECT 'An error has occurred, operation rollbacked and the stored procedure was terminated'; END; Handler for NOTFOUND DECLARE CONTINUE HANDLER FOR NOT FOUND SET no_row_found = 1; Handler for error code 1062 - duplicate key. DECLARE CONTINUE HANDLER FOR 1062 SELECT 'Error, duplicate key occurred'; Example in a stored procedure DELIMITER $$ CREATE PROCEDURE insert_article_tags(IN article_id INT, IN tag_id INT) BEGIN DECLARE CONTINUE HANDLER FOR 1062 SELECT CONCAT('duplicate keys (',article_id,',',tag_id,') found') AS msg; -- insert a new record into article_tags INSERT INTO article_tags(article_id,tag_id) VALUES(article_id,tag_id); -- return tag count for the article SELECT COUNT(*) FROM article_tags; END CALL insert_article_tags(1,1); CALL insert_article_tags(1,2); CALL insert_article_tags(1,3); CALL insert_article_tags(1,3); DELIMITER $$ CREATE PROCEDURE insert_article_tags_2(IN article_id INT, IN tag_id INT) BEGIN DECLARE EXIT HANDLER FOR SQLEXCEPTION SELECT 'SQLException invoked'; DECLARE EXIT HANDLER FOR 1062 SELECT 'MySQL error code 1062 invoked'; DECLARE EXIT HANDLER FOR SQLSTATE '23000' SELECT 'SQLSTATE 23000 invoked'; -- insert a new record into article_tags INSERT INTO article_tags(article_id,tag_id) VALUES(article_id,tag_id); -- return tag count for the article SELECT COUNT(*) FROM article_tags; END MySQL handler precedence \u00b6 In case there are multiple handlers that are eligible for handling an error, MySQL will call the most specific handler to handle the error first. DELIMITER $$ CREATE PROCEDURE insert_article_tags_3(IN article_id INT, IN tag_id INT) BEGIN DECLARE EXIT HANDLER FOR 1062 SELECT 'Duplicate keys error encountered'; DECLARE EXIT HANDLER FOR SQLEXCEPTION SELECT 'SQLException encountered'; DECLARE EXIT HANDLER FOR SQLSTATE '23000' SELECT 'SQLSTATE 23000'; -- insert a new record into article_tags INSERT INTO article_tags(article_id,tag_id) VALUES(article_id,tag_id); -- return tag count for the article SELECT COUNT(*) FROM article_tags; END CALL insert_article_tags_3(1,3); You will see that the MySQL error code handler will be called. Using a named error condition \u00b6 DECLARE table_not_found CONDITION for 1051; DECLARE EXIT HANDLER FOR table_not_found SELECT 'Please create table abc first'; SELECT * FROM abc; Notice that the condition declaration must appear before handler or cursor declarations.","title":"Handling errors in Stored Procedures"},{"location":"Databases/MySQL/advanced/08_error_handling_in_stored_procedures/#handling-errors-in-stored-procedures","text":"When an error occurs inside a stored procedure, it is important to handle it appropriately, such as continuing or exiting the current code block's execution, and issuing a meaningful error message.","title":"Handling errors in Stored Procedures"},{"location":"Databases/MySQL/advanced/08_error_handling_in_stored_procedures/#declaring-a-handler","text":"DECLARE action HANDLER FOR condition_value statement; If a condition whose value matches the condition_value , MySQL will execute the statement and continue or exit the current code block based on the action . The action accepts one of following values: CONTINUE - the execution of enclosing code in block continues. EXIT - the execution of the enclosing block where the handler is declared is terminated. The condition_value specifies a particular condition or a class of condition that activate the handler. The condition_value accepts one of the following values: A MySQL error code. A standard SQLSTATE value. Or it can be an SQLWARNING , NOTFOUND or SQLEXCEPTION condition, which is shorthand for the class of SQLSTATE values. The NOTFOUND condition is used for a cursor or SELECT INTO variable_list statement. A named condition associated with either a MySQL error code or SQLSTATE value. The statement could be a simple statement or compound statement enclosing by the BEGIN and END keywords. Handler for SQLEXCEPTION DECLARE CONTINUE HANDLER FOR SQLEXCEPTION SET has_error = 1; DECLARE EXIT HANDLER FOR SQLEXCEPTION BEGIN ROLLBACK; SELECT 'An error has occurred, operation rollbacked and the stored procedure was terminated'; END; Handler for NOTFOUND DECLARE CONTINUE HANDLER FOR NOT FOUND SET no_row_found = 1; Handler for error code 1062 - duplicate key. DECLARE CONTINUE HANDLER FOR 1062 SELECT 'Error, duplicate key occurred'; Example in a stored procedure DELIMITER $$ CREATE PROCEDURE insert_article_tags(IN article_id INT, IN tag_id INT) BEGIN DECLARE CONTINUE HANDLER FOR 1062 SELECT CONCAT('duplicate keys (',article_id,',',tag_id,') found') AS msg; -- insert a new record into article_tags INSERT INTO article_tags(article_id,tag_id) VALUES(article_id,tag_id); -- return tag count for the article SELECT COUNT(*) FROM article_tags; END CALL insert_article_tags(1,1); CALL insert_article_tags(1,2); CALL insert_article_tags(1,3); CALL insert_article_tags(1,3); DELIMITER $$ CREATE PROCEDURE insert_article_tags_2(IN article_id INT, IN tag_id INT) BEGIN DECLARE EXIT HANDLER FOR SQLEXCEPTION SELECT 'SQLException invoked'; DECLARE EXIT HANDLER FOR 1062 SELECT 'MySQL error code 1062 invoked'; DECLARE EXIT HANDLER FOR SQLSTATE '23000' SELECT 'SQLSTATE 23000 invoked'; -- insert a new record into article_tags INSERT INTO article_tags(article_id,tag_id) VALUES(article_id,tag_id); -- return tag count for the article SELECT COUNT(*) FROM article_tags; END","title":"Declaring a handler"},{"location":"Databases/MySQL/advanced/08_error_handling_in_stored_procedures/#mysql-handler-precedence","text":"In case there are multiple handlers that are eligible for handling an error, MySQL will call the most specific handler to handle the error first. DELIMITER $$ CREATE PROCEDURE insert_article_tags_3(IN article_id INT, IN tag_id INT) BEGIN DECLARE EXIT HANDLER FOR 1062 SELECT 'Duplicate keys error encountered'; DECLARE EXIT HANDLER FOR SQLEXCEPTION SELECT 'SQLException encountered'; DECLARE EXIT HANDLER FOR SQLSTATE '23000' SELECT 'SQLSTATE 23000'; -- insert a new record into article_tags INSERT INTO article_tags(article_id,tag_id) VALUES(article_id,tag_id); -- return tag count for the article SELECT COUNT(*) FROM article_tags; END CALL insert_article_tags_3(1,3); You will see that the MySQL error code handler will be called.","title":"MySQL handler precedence"},{"location":"Databases/MySQL/advanced/08_error_handling_in_stored_procedures/#using-a-named-error-condition","text":"DECLARE table_not_found CONDITION for 1051; DECLARE EXIT HANDLER FOR table_not_found SELECT 'Please create table abc first'; SELECT * FROM abc; Notice that the condition declaration must appear before handler or cursor declarations.","title":"Using a named error condition"},{"location":"Databases/MySQL/advanced/09_raising_error_conditions_with_signal_resignal/","text":"Raising Error condition with MySQL Signal / RESIGNAL statements \u00b6 MySQL SIGNAL Statements \u00b6 You use the SIGNAL statement to return an error or warning condition to the caller from a stored program e.g. stored procedure, stored function, trigger or event. The SIGNAL statement provides you with control over which information for returning such as value and message SQLSTATE . SIGNAL SQLSTATE | condition_name; SET condition_information_item_name_1 = value_1, condition_information_item_name_1 = value_2, etc; Following the SIGNAL keyword is a SQLSTATE value or a condition name declared by the DECLARE CONDITION statement. Notice that the SIGNAL statement must always specify a SQLSTATE value or a named condition that defined with an SQLSTATE value. To provide the caller with information, you use the SET clause. If you want tot return multiple condition information item names with values, you need to separate each name/value pair by a comma. The condition_information_item_name can be MESSAGE_TEXT , MYSQL_ERRORNO , CURSOR_NAME etc. DELIMITER $$ CREATE PROCEDURE AddOrderItem( in orderNo int, in productCode varchar(45), in qty int, in price double, in lineNo int ) BEGIN DECLARE C INT; SELECT COUNT(orderNumber) INTO C FROM orders WHERE orderNumber = orderNo; -- check if orderNumber exists IF(C != 1) THEN SIGNAL SQLSTATE '45000' SET MESSAGE_TEXT = 'Order No not found in orders table'; END IF; -- more code below -- ... END MySQL RESIGNAL statement \u00b6 Besides the SIGNAL statement, MySQL alsoprovides RESIGNAL statement used to raise a warning or error condition. The RESIGNAL statement is similar to SIGNAL statement in term of functionality and syntax, except that: You must use the RESIGNAL statement within an error or warning handler, otherwise, you will get an error message saying that RESIGNAL when handler is not active . Notice that you can use SIGNAL statement anywhere inside a stored procedure. You can omit all attributes of the RESIGNAL statement, event the SQLSTATE value. If you use the RESIGNAL statement, all attributes are the same as the ones passed to the condition handler. DELIMITER $$ CREATE PROCEDURE Divide(IN numerator INT, IN denominator INT, OUT result double) BEGIN DECLARE division_by_zero CONDITION FOR SQLSTATE '22012'; DECLARE CONTINUE HANDLER FOR division_by_zero RESIGNAL SET MESSAGE_TEXT = 'Division by zero / Denominator cannot be zero'; -- IF denominator = 0 THEN SIGNAL division_by_zero; ELSE SET result := numerator / denominator; END IF; END","title":"Raising Error condition with MySQL Signal / RESIGNAL statements"},{"location":"Databases/MySQL/advanced/09_raising_error_conditions_with_signal_resignal/#raising-error-condition-with-mysql-signal-resignal-statements","text":"","title":"Raising Error condition with MySQL Signal / RESIGNAL statements"},{"location":"Databases/MySQL/advanced/09_raising_error_conditions_with_signal_resignal/#mysql-signal-statements","text":"You use the SIGNAL statement to return an error or warning condition to the caller from a stored program e.g. stored procedure, stored function, trigger or event. The SIGNAL statement provides you with control over which information for returning such as value and message SQLSTATE . SIGNAL SQLSTATE | condition_name; SET condition_information_item_name_1 = value_1, condition_information_item_name_1 = value_2, etc; Following the SIGNAL keyword is a SQLSTATE value or a condition name declared by the DECLARE CONDITION statement. Notice that the SIGNAL statement must always specify a SQLSTATE value or a named condition that defined with an SQLSTATE value. To provide the caller with information, you use the SET clause. If you want tot return multiple condition information item names with values, you need to separate each name/value pair by a comma. The condition_information_item_name can be MESSAGE_TEXT , MYSQL_ERRORNO , CURSOR_NAME etc. DELIMITER $$ CREATE PROCEDURE AddOrderItem( in orderNo int, in productCode varchar(45), in qty int, in price double, in lineNo int ) BEGIN DECLARE C INT; SELECT COUNT(orderNumber) INTO C FROM orders WHERE orderNumber = orderNo; -- check if orderNumber exists IF(C != 1) THEN SIGNAL SQLSTATE '45000' SET MESSAGE_TEXT = 'Order No not found in orders table'; END IF; -- more code below -- ... END","title":"MySQL SIGNAL Statements"},{"location":"Databases/MySQL/advanced/09_raising_error_conditions_with_signal_resignal/#mysql-resignal-statement","text":"Besides the SIGNAL statement, MySQL alsoprovides RESIGNAL statement used to raise a warning or error condition. The RESIGNAL statement is similar to SIGNAL statement in term of functionality and syntax, except that: You must use the RESIGNAL statement within an error or warning handler, otherwise, you will get an error message saying that RESIGNAL when handler is not active . Notice that you can use SIGNAL statement anywhere inside a stored procedure. You can omit all attributes of the RESIGNAL statement, event the SQLSTATE value. If you use the RESIGNAL statement, all attributes are the same as the ones passed to the condition handler. DELIMITER $$ CREATE PROCEDURE Divide(IN numerator INT, IN denominator INT, OUT result double) BEGIN DECLARE division_by_zero CONDITION FOR SQLSTATE '22012'; DECLARE CONTINUE HANDLER FOR division_by_zero RESIGNAL SET MESSAGE_TEXT = 'Division by zero / Denominator cannot be zero'; -- IF denominator = 0 THEN SIGNAL division_by_zero; ELSE SET result := numerator / denominator; END IF; END","title":"MySQL RESIGNAL statement"},{"location":"Databases/MySQL/advanced/10_stored_function/","text":"MySQL Stored Function \u00b6 A stored function is a special kind of stored program that returns a single value. You use stored function to encapsulate common formulas or business rules that are reusable among SQL statements or stored programs. Different from a stored procedure, you can use a stored function in SQL statements whenever an expression is used. CREATE FUNCTION function_name(param1,param2,\u2026) RETURNS datatype [NOT] DETERMINISTIC statements The parameters are specified with IN , OUT and INOUT modifiers. DELIMITER $$ CREATE FUNCTION CustomerLevel(p_creditLimit double) RETURNS VARCHAR(10) DETERMINISTIC BEGIN DECLARE lvl varchar(10); IF p_creditLimit > 50000 THEN SET lvl = 'PLATINUM'; ELSEIF (p_creditLimit <= 50000 AND p_creditLimit >= 10000) THEN SET lvl = 'GOLD'; ELSEIF p_creditLimit < 10000 THEN SET lvl = 'SILVER'; END IF; RETURN (lvl); END SELECT customerName, CustomerLevel(creditLimit) FROM customers ORDER BY customerName; The function written as a stored procedure: DELIMITER $$ CREATE PROCEDURE GetCustomerLevel( IN p_customerNumber INT(11), OUT p_customerLevel varchar(10) ) BEGIN DECLARE creditlim DOUBLE; SELECT creditlimit INTO creditlim FROM customers WHERE customerNumber = p_customerNumber; SELECT CUSTOMERLEVEL(creditlim) INTO p_customerLevel; END","title":"MySQL Stored Function"},{"location":"Databases/MySQL/advanced/10_stored_function/#mysql-stored-function","text":"A stored function is a special kind of stored program that returns a single value. You use stored function to encapsulate common formulas or business rules that are reusable among SQL statements or stored programs. Different from a stored procedure, you can use a stored function in SQL statements whenever an expression is used. CREATE FUNCTION function_name(param1,param2,\u2026) RETURNS datatype [NOT] DETERMINISTIC statements The parameters are specified with IN , OUT and INOUT modifiers. DELIMITER $$ CREATE FUNCTION CustomerLevel(p_creditLimit double) RETURNS VARCHAR(10) DETERMINISTIC BEGIN DECLARE lvl varchar(10); IF p_creditLimit > 50000 THEN SET lvl = 'PLATINUM'; ELSEIF (p_creditLimit <= 50000 AND p_creditLimit >= 10000) THEN SET lvl = 'GOLD'; ELSEIF p_creditLimit < 10000 THEN SET lvl = 'SILVER'; END IF; RETURN (lvl); END SELECT customerName, CustomerLevel(creditLimit) FROM customers ORDER BY customerName; The function written as a stored procedure: DELIMITER $$ CREATE PROCEDURE GetCustomerLevel( IN p_customerNumber INT(11), OUT p_customerLevel varchar(10) ) BEGIN DECLARE creditlim DOUBLE; SELECT creditlimit INTO creditlim FROM customers WHERE customerNumber = p_customerNumber; SELECT CUSTOMERLEVEL(creditlim) INTO p_customerLevel; END","title":"MySQL Stored Function"},{"location":"Databases/MySQL/advanced/11_database_views/","text":"Database view \u00b6 A database view is a virtual table or logical table which is defined as a SQL SELECT query with joins. Because a database view is similar to a database table, which consits of rows and columns, so you can query data against it. Most DBMS including MySQL allow you to update data in the underlying tables through the database view with some prerequesites. Advantages of database view \u00b6 A database view allows you to simplify complex queries: a database view is defined by an SQL statement that associates with many underlying tables. You can use database view to hide the complexity of underlying tables to the end-users and external applications. Though a tdatabase view, you only have to use simple SQL statements instead of complex ones with many joins. A database view helps limit database access to specific users. You may not want to subset of sensitive data can be query queryable by all users. You can use a database view to expose only non-sensitive data to a specific group of users. A database view provides extra security layer. Security is a vital part of any relational DBMS. The database view offers additional protection for a DBMS. The database view allows you to create the read-only view to expose read-only data to specific users. Users can only retrieve data in read-only view but cannot update it. A database view enables computed columns. A database table should not have calculated columns however a database view should. When you query data from the database view, the data of the computed column is calculated on the fly. A database view enables backward compatibility. Disadvantages of database view \u00b6 Performance - querying data from a database view can be slow especially if the view is created on other views. Tables dependency - you can create a view based on underlying tables of the database. Whenever you change the structure of these tables that view is associated with, you have to change the view as well. MySQL processes query against the views in two ways: MySQL creates a temporary table based on the view definition statement and executes the incoming query on this temporary table. MySQL combines the incoming query with the query defined the view into one query and executes the combined query. MySQL supports versioning system for views. Each time when a view is altered or replaced, a copy of the view is back up in arc directory that resides in a specific database folder. The name of the backup file is view_name.frm-00001 . MySQL allows you to create a view based on other views. In other words, you can refer to other views in the SELECT statement which defines the view. View restrictions \u00b6 You cannot create an index on a view. MySQL uses indexes of the underlying tables when you query data against the views that use the merge algorithm. For the views that use the temptable algorithm, indexes are not utilized when you query data against the views. You cannot use subqueryies in the FROM clause of the SELECT statement. that defines the view before MySQL 5.7.7. If you drop or rename tables to which view references, MySQL does not issue any error. However, MySQL does invalidate the view. You can use the CHECK TABLE statement to check whether the view is valid or not. A simple view can be updatable. A view created based on a complex SELECT satement with join, subquery, etc. cannot be updatable. MySQL does not support materialized view like other DBMS like Oracle, PostgreSQL. Creating Views \u00b6 CREATE [ALGORITHM = {MERGE | TEMPTABLE | UNDEFINED}] VIEW view_name [(column_list)] AS select-statement; The algorithm attribute allows you to control which mechanism MySQL uses when creating the view. MySQL provides three algorithms: MERGE , TEMPTABLE , and UNDEFINED . Using MERGE algorithm, MySQL first combines the input query with the SELECT statement, which defines the view, into a single query. And then MySQL executes the combined query to return the result set. The MERGE algorithm is not allowed if the SELECT statement contains aggregate function like MIN, MAX, SUM, COUNT, AVG or DISTINCT, GROUP BY, HAVING, LIMIT, UNION, UNION ALL, subquery. In case the SELECT statement refers to no table, the MERGE algorithm is also not allowed. If the MERGE algorithm is not allowed, MySQL changes the algorithm to UNDEFINED . Note that the combination of input query and the query in the view definition into one query is reffered to as view resolution. Using TEMPTABLE algorithm, MySQL first creates a temporary table based on the SELECT statement that defines the view, and then it executes the input query against this temporary table. Because MySQL has to create a temporary table to store the result set and moves the data from the base tables to the temporary table, the TEMPTABLE algorithm is less efficient than the MERGE algorithm. In addition, a view that uses TEMPTABLE algorithm is not updatable. The UNDEFINED is the default algorithm when you create a view without specifying an explicit algorithm. The UNDEFINED algorithm lets MySQL make a choice of using MERGE or TEMPTABLE algorithm. MySQL prefers MERGE to TEMPTABLE algorithm because the MERGE is much more efficient. View name \u00b6 Within a database, views and tables share the same namespace, therefore, a view and a table cannot have the same name. In addition, the name of a view must follow the table's naming rules. SELECT statement \u00b6 In the SELECT statement, you can query data from any table or view that exists in the database. The SELECT statement can contain a subquery in WHERE clause but not in the FROM clause. The SELECT statement cannot refer to any variables including local variables, user variables, and session variables. The SELECT statement cannot refer to the parameters of prepared statements. Creating view examples \u00b6 CREATE VIEW SalePerOrder AS SELECT orderNumber, SUM(quantityOrdered * priceEach) total FROM orderDetails GROUP by orderNumber ORDER BY total DESC; SELECT * FROM salePerOrder; Creating view based on another view \u00b6 CREATE VIEW BigSalesOrder AS SELECT orderNumber, ROUND(total,2) as total FROM saleperorder WHERE total > 60000; SELECT orderNumber, total FROM BigSalesOrder; Creating views with joins \u00b6 CREATE VIEW customerOrders AS SELECT d.orderNumber, customerName, SUM(quantityOrdered * priceEach) total FROM orderDetails d INNER JOIN orders o ON o.orderNumber = d.orderNumber INNER JOIN customers c ON c.customerNumber = c.customerNumber GROUP BY d.orderNumber ORDER BY total DESC; SELECT * FROM customerOrders; Creating views with a subquery \u00b6 CREATE VIEW aboveAvgProducts AS SELECT productCode, productName, buyPrice FROM products WHERE buyPrice > (SELECT AVG(buyPrice) FROM products) ORDER BY buyPrice DESC; SELECT * FROM aboveAvgProducts; Creating updatable views \u00b6 In MySQL, views are not only query-able but also updatable. It means that you can use INSER or UPDATE statement to insert or update rows of the base table through the updatable view. In addition, you can use DELETE statement to remove rows of the underlying table through view. However, to create an updatable view, the SELECT statement that defines the view must not contain any of the following elements: Aggregate functions such as MIN, MAX, SUM, AVG and COUNT. DISTINCT GROUP BY HAVING UNION or UNION ALL Left join or Outer join Reference to non-updatable view in the FROM clause Reference only to literal values Multiple references to any column of the base table. If you create a view with the TEMPTABLE algorithmm you cannot update the view. CREATE VIEW officeInfo AS SELECT officeCode, phone, city FROM offices; SELECT * FROM officeInfo; UPDATE officeInfo SET phone = '+33 14 723 5555' WHERE officeCode = 4; Checking the updatable view information \u00b6 You can check if a view in a database is updatable by querying the is_updatable column from the views table in the information_schema database. SELECT table_name, is_updatable FROM information_schema.views WHERE table_schema = 'classicmodels'; Removing rows through a view \u00b6 -- create a new table named items CREATE TABLE items ( id INT AUTO_INCREMENT PRIMARY KEY, name VARCHAR(100) NOT NULL, price DECIMAL(11 , 2 ) NOT NULL ); -- insert data into the items table INSERT INTO items(name,price) VALUES('Laptop',700.56),('Desktop',699.99),('iPad',700.50) ; -- create a view based on items table CREATE VIEW LuxuryItems AS SELECT * FROM items WHERE price > 700; -- query data from the LuxuryItems view SELECT * FROM LuxuryItems; DELETE FROM LuxuryItems WHERE id = 3; Ensuring view consistency with the CHECK OPTION clause \u00b6 Somtimes, you create a view to reveal the partial data of a table only. However, a simple view is updatable therefore it is possible to update data which is not visible through the view. This update makes the view inconsistent. To ensure the consistency of the view, you use the WITH CHECK OPTION clause when you create or modify the view. It is an optional part of the CREATE VIEW statement. The WITH CHECK OPTION clause prevents you from updating or inserting rows that are not visible through the view. In other words, whether you update or insert a row of the base table through a view, MySQL ensures that the insert or update operation is conformed with the definition of the view. CREATE OR REPLACE VIEW view_name AS select_statement WITH CHECK OPTION; Understanding LOCAL & CASCADED in WITH CHECK OPTION Clause \u00b6 When you create a view with the WITH CHECK OPTION clause, MySQL checks every row that is being changed through the view e.g., insert, update, delete, to make it conformable to the view's definition. Because MySQL allows a view to be created based on another view, it also checks the rules in dependent views for consistency. To determine the scope of check, MySQL provides two options: LOCAL and CASCADED . By default it uses CASCADED scope. If a view uses a WITH LOCAL CHECK OPTION , MySQL checks the rules of views that have a WITH LOCAL CHECK OPTION and a WITH CASCADED CHECK OPTION . If a view uses WITH CASCADED CHECK OPTION , MySQL checks the rules of all dependent views. Managiung views \u00b6 Showing view definition \u00b6 SHOW CREATE VIEW [database_name].[view_ name]; CREATE VIEW organization AS SELECT CONCAT(E.lastname, E.firstname) AS Employee, CONCAT(M.lastname, M.firstname) AS Manager FROM employees AS E INNER JOIN employees AS M ON M.employeeNumber = E.ReportsTo ORDER BY Manager; SHOW CREATE VIEW organization; You can also display the definition of the view using any plain text editior to open the view definition file in the database folder. For example, to open the organization view definition, you can find the view definition file with the following path: \\data\\database\\organization.frm You should probably not modify the file. Modify the views \u00b6 ALTER [ALGORITHM = {MERGE | TEMPTABLE | UNDEFINED}] VIEW [database_name]. [view_name] AS [SELECT statement] ALTER VIEW organization AS SELECT CONCAT(E.lastname,E.firstname) AS Employee, E.email AS employeeEmail, CONCAT(M.lastname,M.firstname) AS Manager FROM employees AS E INNER JOIN employees AS M ON M.employeeNumber = E.ReportsTo ORDER BY Manager; CREATE OR REPLACE VIEW contacts AS SELECT firstName, lastName, extension, email FROM employees; CREATE OR REPLACE VIEW contacts AS SELECT firstName, lastName, extension, email, jobtitle FROM employees; Removing views \u00b6 DROP VIEW [IF EXISTS] [database_name].[view_name] DROP VIEW IF EXISTS organization; Each time you modify or remove a view, MySQL makes a backup of the view definition file to the /database_name/arc/ folder.","title":"Database view"},{"location":"Databases/MySQL/advanced/11_database_views/#database-view","text":"A database view is a virtual table or logical table which is defined as a SQL SELECT query with joins. Because a database view is similar to a database table, which consits of rows and columns, so you can query data against it. Most DBMS including MySQL allow you to update data in the underlying tables through the database view with some prerequesites.","title":"Database view"},{"location":"Databases/MySQL/advanced/11_database_views/#advantages-of-database-view","text":"A database view allows you to simplify complex queries: a database view is defined by an SQL statement that associates with many underlying tables. You can use database view to hide the complexity of underlying tables to the end-users and external applications. Though a tdatabase view, you only have to use simple SQL statements instead of complex ones with many joins. A database view helps limit database access to specific users. You may not want to subset of sensitive data can be query queryable by all users. You can use a database view to expose only non-sensitive data to a specific group of users. A database view provides extra security layer. Security is a vital part of any relational DBMS. The database view offers additional protection for a DBMS. The database view allows you to create the read-only view to expose read-only data to specific users. Users can only retrieve data in read-only view but cannot update it. A database view enables computed columns. A database table should not have calculated columns however a database view should. When you query data from the database view, the data of the computed column is calculated on the fly. A database view enables backward compatibility.","title":"Advantages of database view"},{"location":"Databases/MySQL/advanced/11_database_views/#disadvantages-of-database-view","text":"Performance - querying data from a database view can be slow especially if the view is created on other views. Tables dependency - you can create a view based on underlying tables of the database. Whenever you change the structure of these tables that view is associated with, you have to change the view as well. MySQL processes query against the views in two ways: MySQL creates a temporary table based on the view definition statement and executes the incoming query on this temporary table. MySQL combines the incoming query with the query defined the view into one query and executes the combined query. MySQL supports versioning system for views. Each time when a view is altered or replaced, a copy of the view is back up in arc directory that resides in a specific database folder. The name of the backup file is view_name.frm-00001 . MySQL allows you to create a view based on other views. In other words, you can refer to other views in the SELECT statement which defines the view.","title":"Disadvantages of database view"},{"location":"Databases/MySQL/advanced/11_database_views/#view-restrictions","text":"You cannot create an index on a view. MySQL uses indexes of the underlying tables when you query data against the views that use the merge algorithm. For the views that use the temptable algorithm, indexes are not utilized when you query data against the views. You cannot use subqueryies in the FROM clause of the SELECT statement. that defines the view before MySQL 5.7.7. If you drop or rename tables to which view references, MySQL does not issue any error. However, MySQL does invalidate the view. You can use the CHECK TABLE statement to check whether the view is valid or not. A simple view can be updatable. A view created based on a complex SELECT satement with join, subquery, etc. cannot be updatable. MySQL does not support materialized view like other DBMS like Oracle, PostgreSQL.","title":"View restrictions"},{"location":"Databases/MySQL/advanced/11_database_views/#creating-views","text":"CREATE [ALGORITHM = {MERGE | TEMPTABLE | UNDEFINED}] VIEW view_name [(column_list)] AS select-statement; The algorithm attribute allows you to control which mechanism MySQL uses when creating the view. MySQL provides three algorithms: MERGE , TEMPTABLE , and UNDEFINED . Using MERGE algorithm, MySQL first combines the input query with the SELECT statement, which defines the view, into a single query. And then MySQL executes the combined query to return the result set. The MERGE algorithm is not allowed if the SELECT statement contains aggregate function like MIN, MAX, SUM, COUNT, AVG or DISTINCT, GROUP BY, HAVING, LIMIT, UNION, UNION ALL, subquery. In case the SELECT statement refers to no table, the MERGE algorithm is also not allowed. If the MERGE algorithm is not allowed, MySQL changes the algorithm to UNDEFINED . Note that the combination of input query and the query in the view definition into one query is reffered to as view resolution. Using TEMPTABLE algorithm, MySQL first creates a temporary table based on the SELECT statement that defines the view, and then it executes the input query against this temporary table. Because MySQL has to create a temporary table to store the result set and moves the data from the base tables to the temporary table, the TEMPTABLE algorithm is less efficient than the MERGE algorithm. In addition, a view that uses TEMPTABLE algorithm is not updatable. The UNDEFINED is the default algorithm when you create a view without specifying an explicit algorithm. The UNDEFINED algorithm lets MySQL make a choice of using MERGE or TEMPTABLE algorithm. MySQL prefers MERGE to TEMPTABLE algorithm because the MERGE is much more efficient.","title":"Creating Views"},{"location":"Databases/MySQL/advanced/11_database_views/#view-name","text":"Within a database, views and tables share the same namespace, therefore, a view and a table cannot have the same name. In addition, the name of a view must follow the table's naming rules.","title":"View name"},{"location":"Databases/MySQL/advanced/11_database_views/#select-statement","text":"In the SELECT statement, you can query data from any table or view that exists in the database. The SELECT statement can contain a subquery in WHERE clause but not in the FROM clause. The SELECT statement cannot refer to any variables including local variables, user variables, and session variables. The SELECT statement cannot refer to the parameters of prepared statements.","title":"SELECT statement"},{"location":"Databases/MySQL/advanced/11_database_views/#creating-view-examples","text":"CREATE VIEW SalePerOrder AS SELECT orderNumber, SUM(quantityOrdered * priceEach) total FROM orderDetails GROUP by orderNumber ORDER BY total DESC; SELECT * FROM salePerOrder;","title":"Creating view examples"},{"location":"Databases/MySQL/advanced/11_database_views/#creating-view-based-on-another-view","text":"CREATE VIEW BigSalesOrder AS SELECT orderNumber, ROUND(total,2) as total FROM saleperorder WHERE total > 60000; SELECT orderNumber, total FROM BigSalesOrder;","title":"Creating view based on another view"},{"location":"Databases/MySQL/advanced/11_database_views/#creating-views-with-joins","text":"CREATE VIEW customerOrders AS SELECT d.orderNumber, customerName, SUM(quantityOrdered * priceEach) total FROM orderDetails d INNER JOIN orders o ON o.orderNumber = d.orderNumber INNER JOIN customers c ON c.customerNumber = c.customerNumber GROUP BY d.orderNumber ORDER BY total DESC; SELECT * FROM customerOrders;","title":"Creating views with joins"},{"location":"Databases/MySQL/advanced/11_database_views/#creating-views-with-a-subquery","text":"CREATE VIEW aboveAvgProducts AS SELECT productCode, productName, buyPrice FROM products WHERE buyPrice > (SELECT AVG(buyPrice) FROM products) ORDER BY buyPrice DESC; SELECT * FROM aboveAvgProducts;","title":"Creating views with a subquery"},{"location":"Databases/MySQL/advanced/11_database_views/#creating-updatable-views","text":"In MySQL, views are not only query-able but also updatable. It means that you can use INSER or UPDATE statement to insert or update rows of the base table through the updatable view. In addition, you can use DELETE statement to remove rows of the underlying table through view. However, to create an updatable view, the SELECT statement that defines the view must not contain any of the following elements: Aggregate functions such as MIN, MAX, SUM, AVG and COUNT. DISTINCT GROUP BY HAVING UNION or UNION ALL Left join or Outer join Reference to non-updatable view in the FROM clause Reference only to literal values Multiple references to any column of the base table. If you create a view with the TEMPTABLE algorithmm you cannot update the view. CREATE VIEW officeInfo AS SELECT officeCode, phone, city FROM offices; SELECT * FROM officeInfo; UPDATE officeInfo SET phone = '+33 14 723 5555' WHERE officeCode = 4;","title":"Creating updatable views"},{"location":"Databases/MySQL/advanced/11_database_views/#checking-the-updatable-view-information","text":"You can check if a view in a database is updatable by querying the is_updatable column from the views table in the information_schema database. SELECT table_name, is_updatable FROM information_schema.views WHERE table_schema = 'classicmodels';","title":"Checking the updatable view information"},{"location":"Databases/MySQL/advanced/11_database_views/#removing-rows-through-a-view","text":"-- create a new table named items CREATE TABLE items ( id INT AUTO_INCREMENT PRIMARY KEY, name VARCHAR(100) NOT NULL, price DECIMAL(11 , 2 ) NOT NULL ); -- insert data into the items table INSERT INTO items(name,price) VALUES('Laptop',700.56),('Desktop',699.99),('iPad',700.50) ; -- create a view based on items table CREATE VIEW LuxuryItems AS SELECT * FROM items WHERE price > 700; -- query data from the LuxuryItems view SELECT * FROM LuxuryItems; DELETE FROM LuxuryItems WHERE id = 3;","title":"Removing rows through a view"},{"location":"Databases/MySQL/advanced/11_database_views/#ensuring-view-consistency-with-the-check-option-clause","text":"Somtimes, you create a view to reveal the partial data of a table only. However, a simple view is updatable therefore it is possible to update data which is not visible through the view. This update makes the view inconsistent. To ensure the consistency of the view, you use the WITH CHECK OPTION clause when you create or modify the view. It is an optional part of the CREATE VIEW statement. The WITH CHECK OPTION clause prevents you from updating or inserting rows that are not visible through the view. In other words, whether you update or insert a row of the base table through a view, MySQL ensures that the insert or update operation is conformed with the definition of the view. CREATE OR REPLACE VIEW view_name AS select_statement WITH CHECK OPTION;","title":"Ensuring view consistency with the CHECK OPTION clause"},{"location":"Databases/MySQL/advanced/11_database_views/#understanding-local-cascaded-in-with-check-option-clause","text":"When you create a view with the WITH CHECK OPTION clause, MySQL checks every row that is being changed through the view e.g., insert, update, delete, to make it conformable to the view's definition. Because MySQL allows a view to be created based on another view, it also checks the rules in dependent views for consistency. To determine the scope of check, MySQL provides two options: LOCAL and CASCADED . By default it uses CASCADED scope. If a view uses a WITH LOCAL CHECK OPTION , MySQL checks the rules of views that have a WITH LOCAL CHECK OPTION and a WITH CASCADED CHECK OPTION . If a view uses WITH CASCADED CHECK OPTION , MySQL checks the rules of all dependent views.","title":"Understanding LOCAL &amp; CASCADED in WITH CHECK OPTION Clause"},{"location":"Databases/MySQL/advanced/11_database_views/#managiung-views","text":"","title":"Managiung views"},{"location":"Databases/MySQL/advanced/11_database_views/#showing-view-definition","text":"SHOW CREATE VIEW [database_name].[view_ name]; CREATE VIEW organization AS SELECT CONCAT(E.lastname, E.firstname) AS Employee, CONCAT(M.lastname, M.firstname) AS Manager FROM employees AS E INNER JOIN employees AS M ON M.employeeNumber = E.ReportsTo ORDER BY Manager; SHOW CREATE VIEW organization; You can also display the definition of the view using any plain text editior to open the view definition file in the database folder. For example, to open the organization view definition, you can find the view definition file with the following path: \\data\\database\\organization.frm You should probably not modify the file.","title":"Showing view definition"},{"location":"Databases/MySQL/advanced/11_database_views/#modify-the-views","text":"ALTER [ALGORITHM = {MERGE | TEMPTABLE | UNDEFINED}] VIEW [database_name]. [view_name] AS [SELECT statement] ALTER VIEW organization AS SELECT CONCAT(E.lastname,E.firstname) AS Employee, E.email AS employeeEmail, CONCAT(M.lastname,M.firstname) AS Manager FROM employees AS E INNER JOIN employees AS M ON M.employeeNumber = E.ReportsTo ORDER BY Manager; CREATE OR REPLACE VIEW contacts AS SELECT firstName, lastName, extension, email FROM employees; CREATE OR REPLACE VIEW contacts AS SELECT firstName, lastName, extension, email, jobtitle FROM employees;","title":"Modify the views"},{"location":"Databases/MySQL/advanced/11_database_views/#removing-views","text":"DROP VIEW [IF EXISTS] [database_name].[view_name] DROP VIEW IF EXISTS organization; Each time you modify or remove a view, MySQL makes a backup of the view definition file to the /database_name/arc/ folder.","title":"Removing views"},{"location":"Databases/MySQL/advanced/12_triggers/","text":"MySQL triggers \u00b6 A SQL trigger is a special type of a stored procedure. Special because it is not called directly like a stored procedure but instead it is triggered automatically when data modification event is made against a table. Advantages of SQL triggers \u00b6 SQL triggers provide an alternative way tto check the integrity of data. SQL triggers can catch errors in business logic in the database layer SQL triggers provide an alternative way to run scheduled tasks. By using SQL triggers, you don't have to wait to run the scheduled tasks because triggers are invoked automatically before or after a change is made to the data in the tables. SQL triggers are very useful to audit the changes of data in tables. DIsadvantages of SQL triggers \u00b6 SQL triggers only can provide an extended validation and they cannot replace all the validations. Some simple validations have to be done in the application layer. For example, you can validate user's inputs in the client side by using JavaScript. SQL triggers are invoked and executed invisible from the client applications, therefore, it is difficult to figure out what happens in the database layer. SQL triggers increase the overhead of the database server. MySQL triggers \u00b6 In MySQL, a trigger is a set of SQL statements that is invoked automatically when a change is made to the data on the associated table. A trigger can be defined to be invoded either before or after the data is changed by INSERT , UPDATE or DELETE statement. Before MySQL version 5.7.2, you can define maximum of 6 triggers for each table. BEFORE INSERT AFTER INSERT BEFORE UPDATE AFTER UPDATE BEFORE DELETE AFTER DELETE However, from MySQL version 5.7.2, you can define multiple triggers for the same trigger event and action time. When you use a statement that does not use INSER , DELETE or UPDATE statement to change data in a table, the triggers associated with the table are not invoked. For example, the TRUNCATE statement removes all data of a table but does not invoke the trigger associated with that table. There are some statements that use the INSERT statement behind the scenes such as REPLACE or LOAD DATA . If you use these statements, the correstponding triggers associated the tables are invoked. YOu must use a unique name for each trigger associated with a table. However, you can have the same trigger name defined for different tables through it is a good practice. You should name the triggers using the following convention: (BEFORE | AFTER)_tableName_(INSERT|UPDATE|DELETE) For example, before_order_update is a trigger invoked before a row in the order table is updated. tableName_(BEFORE|AFTER)_(INSERT|UPDATE|DELETE) MySQL trigger storage \u00b6 MySQL stores triggers in a data directory e.g. /data/classicmodels/ with the files named tablename.TRG and triggername.TRN : - The tablename.TRG file maps the trigger to corresponding table. - the triggername.TRN file contains the trigger definition. You can back up the MySQL triggers by copying trigger files to the backup folder. You can also backup the triggers using the mysqldumptool. MySQL trigger limitations \u00b6 MySQL triggers cover all features defined in the standard SQL. However, there are some limitations that you should know before using them in your applications. MySQL triggers cannot: Use SHOW , LOAD DATA , LOAD TABLE , BACKUP DATABASE , RESTORE , FLUSH and RETURN statements. Use statements that commit or rollback implicity or explicity such as COMMIT , ROLLBACK , START TRANSACTION , LOCK/UNLOCK TABLES , ALTER , CREATE , DROP , RENAME . Use prepared statements such as PREPARE and EXECUTE . Use dynamic SQL statements. From MySQL version 5.1.4. a trigger can call a stored procedure or stored function, which was a limitation in previous versions. Create a trigger in MySQL \u00b6 In order to create a new trigger, you use the CREATE TRIGGER statement. CREATE TRIGGER trigger_name trigger_time trigger_event ON table_name FOR EACH ROW BEGIN ... END You put the trigger name after the CREATE TRIGGER statement. The trigger name should follow the naming conventions defined previously. You must specify the activation time when you define a trigger. Trigger activation time can be BEFORE or AFTER . The trigger event can be INSERT , UPDATE , DELETE . A trigger must associated with a specific table. Without a table trigger would not exist therefore you have to specify the table name after the ON keyword. You place the SQL statements between BEGIN and END block. CREATE TABLE employees_audit ( id INT AUTO_INCREMENT PRIMARY KEY, employeeNumber INT NOT NULL, lastname VARCHAR(50) NOT NULL, changedat DATETIME DEFAULT NULL, action VARCHAR(50) DEFAULT NULL ); DELIMITER $$ CREATE TRIGGER before_employee_update BEFORE UPDATE ON employees FOR EACH ROW BEGIN INSERT INTO employees_audit SET action = 'update', employeeNumber = OLD.employeeNumber, lastname = OLD.lastname, changedat = NOW(); END$$ DELIMITER ; Inside the body of the trigger we used the OLD keyword to access empoloyeeNumber and lastname column of the row affected by trigger. Notice that in a trigger defined for INSERT you can use NEW keyword only. You cannot use the OLD keyword. Howver, in the trigger defined for DELETE , there is no new row so you can use the OLD keyword only. In the UPDATE trigger, OLD refers to the row before it is updated and NEW refers to the row after it is updated. Then, to view all triggers in the current database you can use the `SHOW TRIGGERS statement. SHOW TRIGGERS; Create multiple triggers for the same trigger event and action time \u00b6 Before MySQL version 5.7.2 you can only create one trigger for an event in a table e.g., you can only create one trigger for the BEFORE UPDATE or AFTER UPDATE event. MySQL 5.7.2+ lifts this limitation and allows you to create multiple triggers for the same event and action time in a table. The triggers will activate sequentially when an event occurs. The syntax for creating the first trigger remains the same. In case you have multiple triggers for the same event in a table, MySQL will invoke the triggers in the order that they were created. To change the order of the triggers you need to specify FOLLOWS or PRECEDES after the FOR EACH ROW clause. The FOLLOWS option allows the new trigger to activate after the existing trigger. The PRECEDES option allows the new trigger to activate before the existing trigger. The following is the syntax of createing a new additional trigger with explicit order: DELIMITER $$ CREATE TRIGGER trigger_name [BEFORE|AFTER] [INSERT|UPDATE|DELETE] ON table_name FOR EACH ROW [FOLLOWS|PRECEDES] existing_trigger_name BEGIN \u2026 END$$ DELIMITER ; You can view information on trigger order by using SELECT trigger_name, action_order FROM information_schema.triggers WHERE trigger_schema = 'classicmodels' ORDER BY event_object_table , action_timing , event_manipulation Managing triggers in MySQL \u00b6 After creating a trigger, you can display its definition in the data folder, which contains trigger definition file. A trigger is stored as a plain text file in the following data folder: /data_folder/database_name/table_name.trg MySQL provides you with an alternative way to display the trigger by querying the triggers table in the table information_schema database as follows: SELECT * FROM information_schema.triggers WHERE trigger_schema = 'database_name' AND trigger_name = 'trigger_name'; The statement allows you to view both content of the tirgger and it's metadata such as associated table name and definer, which is the name of the MySQL user who created the trigger. If you want to retrieve all triggers in a particular database, you need to query data from the triggers table in the information_schema database using the following SELECT statement. SELECT * FROM information_schema.triggers WHERE trigger_schema = 'database_name'; To find all triggers associated with a particular table, you use following query: SELECT * FROM information_schema.triggers WHERE trigger_schema = 'database_name' AND event_object_table = 'table_name'; For example, the following statement returns all triggers associated with the employees table in the classicmodels database. SELECT * FROM information_schema.triggers WHERE trigger_schema = 'classicmodels' AND event_object_table = 'employees'; Another quick way to diplsay triggers in a particular database is to use SHOW TRIGGERS statement as follows: SHOW TRIGGERS [FROM|IN] database_name [LIKE expr | WHERE expr]; SHOW TRIGGERS FROM classicmodels WHERE `table` = 'employees'; MySQL returns the following columns when you execute the SHOW TRIGGERS statement. Trigger: stores the name of the trigger e.g., before_employee_update trigger. Event: specifies the event e.g., INSER, UPDATE, or DELETE that invokes the trigger. Table: specifies the tyable where the trigger is associated with e.g. employees table. Statement: stores the statement or compound statement that is going to execute when the trigger is invoked. Timing: accepts two values - BEFORE and AFTER. It specifies the activation time of the trigger. Created: logs the created time when you created the trigger. sql_mode: specifies the SQL mode when the trigger executes. Definer: logs the account who created the trigger. Notice that to execute the SHOW TRIGGER statement, you must have the SUPER privilege. Removing trigger \u00b6 To remove an existing trigger, you use DROP TRIGGER statement as follows: DROP TRIGGER table_name.trigger_name; DROP TRIGGER employees.before_employees_update; To modify a trigger, you have to delete it first an recreate it with the new code. There is no such ALTER TRIGGER statement available in MySQL, therefore, you cannot modify an existing trigger like modifying other database objects such as tables, views, and stored procedures. Working with MySQL Scheduled Event \u00b6 A MySQL events is a task that runs based on a predefined schedule therefore sometimes it is referred to as a scheduled event. MySQL event is also known as temporal trigger because it is trigger by time, not by table update like a trigger. A MySQL event is similar to a cronjob in UNIX or a task scheduler in Windows. You can use MySQL events in many cases such as optimizing database tables, cleaning up logs, archiving data, or generate complex reports during off-peak-time. MySQL event scheduler configuration \u00b6 MySQL uses a special thread called event schedule thread to execute all scheduled events. You can see the status of event scheduler thread by executing: SHOW PROCESSLIST; By default, the event scheduler thread is not enabled. To enable and start the event scheduler thread, you need to execute the following command: SET GLOBAL event_scheduler = ON; Creating new MySQL events \u00b6 Creating an event is simar to creating other database objects such as stored procedures or triggers. An event is a named object that contains SQL statements. A stored procedure is only executed when it is invoked directly; a trigger is executed when an event associated with a table such as an inser, update or delete event occurs while an event can be executed once or more regular intervals. To create a schedule event, you use the CREATE EVENT statement as follows: CREATE EVENT [IF NOT EXIST] event_name ON SCHEDULE schedule DO event_body You put a schedule after the ON SCHEDULE clause. If the event is a one-time event, you use the syntax AT timestamp [+INTERVAL] . If the event is a recurring event, you use the EVERY interval STARTS timestamp [+interval] ENDS timestamp [+INTERVAL] . CREATE EVENT IF NOT EXISTS test_event_01 ON SCHEDULE AT CURRENT_TIMESTAMP DO INSERT INTO messages(message,created_at) VALUES('Test MySQL Event 1',NOW()); When event is expired, it is automatically dropped. To change the behaviour, you can use ON COMPLETION PRESERVE clause. The following statement creates another one-time event that is executed after its creation time 1 minute and not dropped after execution. CREATE EVENT test_event_02 ON SCHEDULE AT CURRENT_TIMESTAMP + INTERVAL 1 MINUTE ON COMPLETION PRESERVE DO INSERT INTO messages(message,created_at) VALUES('Test MySQL Event 2',NOW()); Recurring events CREATE EVENT test_event_03 ON SCHEDULE EVERY 1 MINUTE STARTS CURRENT_TIMESTAMP ENDS CURRENT_TIMESTAMP + INTERVAL 1 HOUR DO INSERT INTO messages(message,created_at) VALUES('Test MySQL recurring Event',NOW()); Drop MySQL Events DROP EVENT [IF EXIST] test_event_03; Modifying MySQL events \u00b6 MySQL allows you to change various attributes of an existing event. To change existing events, you use the ALTER EVENT statement as follows: ALTER EVENT event_name ON SCHEDULE schedule ON COMPLETION [NOT] PRESERVE RENAME TO new_event_name ENABLE | DISABLE DO event_body Notice that the ALTER EVENT statement is only applied to an existing event. if you try to modify a nonexistend event, MySQL will issue an error message therefore, you should always use the SHOW EVENTS statement to check the event for its existince before changing it. SHOW EVENTS FROM classicmodels Changing schedules ALTER EVENT test_event_04 ON SCHEDULE EVERY 2 MINUTE Changing event body ALTER EVENT test_event_04 DO INSERT INTO messages(message,created_at) VALUES('Message from event',NOW()); Disable events ALTER EVENT test_event_04 DISABLE; Enable events ALTER EVENT test_event_04 ENABLE; MySQL does not provide you with the RENAME EVENT statement. Fortunately, you can use the ALTER EVENT to rename an existing event as follows: ALTER EVENT test_event_04 RENAME TO test_event_05; Move events to another database ALTER EVENT classicmodels.test_event_05 RENAME TO newdb.test_event_05","title":"MySQL triggers"},{"location":"Databases/MySQL/advanced/12_triggers/#mysql-triggers","text":"A SQL trigger is a special type of a stored procedure. Special because it is not called directly like a stored procedure but instead it is triggered automatically when data modification event is made against a table.","title":"MySQL triggers"},{"location":"Databases/MySQL/advanced/12_triggers/#advantages-of-sql-triggers","text":"SQL triggers provide an alternative way tto check the integrity of data. SQL triggers can catch errors in business logic in the database layer SQL triggers provide an alternative way to run scheduled tasks. By using SQL triggers, you don't have to wait to run the scheduled tasks because triggers are invoked automatically before or after a change is made to the data in the tables. SQL triggers are very useful to audit the changes of data in tables.","title":"Advantages of SQL triggers"},{"location":"Databases/MySQL/advanced/12_triggers/#disadvantages-of-sql-triggers","text":"SQL triggers only can provide an extended validation and they cannot replace all the validations. Some simple validations have to be done in the application layer. For example, you can validate user's inputs in the client side by using JavaScript. SQL triggers are invoked and executed invisible from the client applications, therefore, it is difficult to figure out what happens in the database layer. SQL triggers increase the overhead of the database server.","title":"DIsadvantages of SQL triggers"},{"location":"Databases/MySQL/advanced/12_triggers/#mysql-triggers_1","text":"In MySQL, a trigger is a set of SQL statements that is invoked automatically when a change is made to the data on the associated table. A trigger can be defined to be invoded either before or after the data is changed by INSERT , UPDATE or DELETE statement. Before MySQL version 5.7.2, you can define maximum of 6 triggers for each table. BEFORE INSERT AFTER INSERT BEFORE UPDATE AFTER UPDATE BEFORE DELETE AFTER DELETE However, from MySQL version 5.7.2, you can define multiple triggers for the same trigger event and action time. When you use a statement that does not use INSER , DELETE or UPDATE statement to change data in a table, the triggers associated with the table are not invoked. For example, the TRUNCATE statement removes all data of a table but does not invoke the trigger associated with that table. There are some statements that use the INSERT statement behind the scenes such as REPLACE or LOAD DATA . If you use these statements, the correstponding triggers associated the tables are invoked. YOu must use a unique name for each trigger associated with a table. However, you can have the same trigger name defined for different tables through it is a good practice. You should name the triggers using the following convention: (BEFORE | AFTER)_tableName_(INSERT|UPDATE|DELETE) For example, before_order_update is a trigger invoked before a row in the order table is updated. tableName_(BEFORE|AFTER)_(INSERT|UPDATE|DELETE)","title":"MySQL triggers"},{"location":"Databases/MySQL/advanced/12_triggers/#mysql-trigger-storage","text":"MySQL stores triggers in a data directory e.g. /data/classicmodels/ with the files named tablename.TRG and triggername.TRN : - The tablename.TRG file maps the trigger to corresponding table. - the triggername.TRN file contains the trigger definition. You can back up the MySQL triggers by copying trigger files to the backup folder. You can also backup the triggers using the mysqldumptool.","title":"MySQL trigger storage"},{"location":"Databases/MySQL/advanced/12_triggers/#mysql-trigger-limitations","text":"MySQL triggers cover all features defined in the standard SQL. However, there are some limitations that you should know before using them in your applications. MySQL triggers cannot: Use SHOW , LOAD DATA , LOAD TABLE , BACKUP DATABASE , RESTORE , FLUSH and RETURN statements. Use statements that commit or rollback implicity or explicity such as COMMIT , ROLLBACK , START TRANSACTION , LOCK/UNLOCK TABLES , ALTER , CREATE , DROP , RENAME . Use prepared statements such as PREPARE and EXECUTE . Use dynamic SQL statements. From MySQL version 5.1.4. a trigger can call a stored procedure or stored function, which was a limitation in previous versions.","title":"MySQL trigger limitations"},{"location":"Databases/MySQL/advanced/12_triggers/#create-a-trigger-in-mysql","text":"In order to create a new trigger, you use the CREATE TRIGGER statement. CREATE TRIGGER trigger_name trigger_time trigger_event ON table_name FOR EACH ROW BEGIN ... END You put the trigger name after the CREATE TRIGGER statement. The trigger name should follow the naming conventions defined previously. You must specify the activation time when you define a trigger. Trigger activation time can be BEFORE or AFTER . The trigger event can be INSERT , UPDATE , DELETE . A trigger must associated with a specific table. Without a table trigger would not exist therefore you have to specify the table name after the ON keyword. You place the SQL statements between BEGIN and END block. CREATE TABLE employees_audit ( id INT AUTO_INCREMENT PRIMARY KEY, employeeNumber INT NOT NULL, lastname VARCHAR(50) NOT NULL, changedat DATETIME DEFAULT NULL, action VARCHAR(50) DEFAULT NULL ); DELIMITER $$ CREATE TRIGGER before_employee_update BEFORE UPDATE ON employees FOR EACH ROW BEGIN INSERT INTO employees_audit SET action = 'update', employeeNumber = OLD.employeeNumber, lastname = OLD.lastname, changedat = NOW(); END$$ DELIMITER ; Inside the body of the trigger we used the OLD keyword to access empoloyeeNumber and lastname column of the row affected by trigger. Notice that in a trigger defined for INSERT you can use NEW keyword only. You cannot use the OLD keyword. Howver, in the trigger defined for DELETE , there is no new row so you can use the OLD keyword only. In the UPDATE trigger, OLD refers to the row before it is updated and NEW refers to the row after it is updated. Then, to view all triggers in the current database you can use the `SHOW TRIGGERS statement. SHOW TRIGGERS;","title":"Create a trigger in MySQL"},{"location":"Databases/MySQL/advanced/12_triggers/#create-multiple-triggers-for-the-same-trigger-event-and-action-time","text":"Before MySQL version 5.7.2 you can only create one trigger for an event in a table e.g., you can only create one trigger for the BEFORE UPDATE or AFTER UPDATE event. MySQL 5.7.2+ lifts this limitation and allows you to create multiple triggers for the same event and action time in a table. The triggers will activate sequentially when an event occurs. The syntax for creating the first trigger remains the same. In case you have multiple triggers for the same event in a table, MySQL will invoke the triggers in the order that they were created. To change the order of the triggers you need to specify FOLLOWS or PRECEDES after the FOR EACH ROW clause. The FOLLOWS option allows the new trigger to activate after the existing trigger. The PRECEDES option allows the new trigger to activate before the existing trigger. The following is the syntax of createing a new additional trigger with explicit order: DELIMITER $$ CREATE TRIGGER trigger_name [BEFORE|AFTER] [INSERT|UPDATE|DELETE] ON table_name FOR EACH ROW [FOLLOWS|PRECEDES] existing_trigger_name BEGIN \u2026 END$$ DELIMITER ; You can view information on trigger order by using SELECT trigger_name, action_order FROM information_schema.triggers WHERE trigger_schema = 'classicmodels' ORDER BY event_object_table , action_timing , event_manipulation","title":"Create multiple triggers for the same trigger event and action time"},{"location":"Databases/MySQL/advanced/12_triggers/#managing-triggers-in-mysql","text":"After creating a trigger, you can display its definition in the data folder, which contains trigger definition file. A trigger is stored as a plain text file in the following data folder: /data_folder/database_name/table_name.trg MySQL provides you with an alternative way to display the trigger by querying the triggers table in the table information_schema database as follows: SELECT * FROM information_schema.triggers WHERE trigger_schema = 'database_name' AND trigger_name = 'trigger_name'; The statement allows you to view both content of the tirgger and it's metadata such as associated table name and definer, which is the name of the MySQL user who created the trigger. If you want to retrieve all triggers in a particular database, you need to query data from the triggers table in the information_schema database using the following SELECT statement. SELECT * FROM information_schema.triggers WHERE trigger_schema = 'database_name'; To find all triggers associated with a particular table, you use following query: SELECT * FROM information_schema.triggers WHERE trigger_schema = 'database_name' AND event_object_table = 'table_name'; For example, the following statement returns all triggers associated with the employees table in the classicmodels database. SELECT * FROM information_schema.triggers WHERE trigger_schema = 'classicmodels' AND event_object_table = 'employees'; Another quick way to diplsay triggers in a particular database is to use SHOW TRIGGERS statement as follows: SHOW TRIGGERS [FROM|IN] database_name [LIKE expr | WHERE expr]; SHOW TRIGGERS FROM classicmodels WHERE `table` = 'employees'; MySQL returns the following columns when you execute the SHOW TRIGGERS statement. Trigger: stores the name of the trigger e.g., before_employee_update trigger. Event: specifies the event e.g., INSER, UPDATE, or DELETE that invokes the trigger. Table: specifies the tyable where the trigger is associated with e.g. employees table. Statement: stores the statement or compound statement that is going to execute when the trigger is invoked. Timing: accepts two values - BEFORE and AFTER. It specifies the activation time of the trigger. Created: logs the created time when you created the trigger. sql_mode: specifies the SQL mode when the trigger executes. Definer: logs the account who created the trigger. Notice that to execute the SHOW TRIGGER statement, you must have the SUPER privilege.","title":"Managing triggers in MySQL"},{"location":"Databases/MySQL/advanced/12_triggers/#removing-trigger","text":"To remove an existing trigger, you use DROP TRIGGER statement as follows: DROP TRIGGER table_name.trigger_name; DROP TRIGGER employees.before_employees_update; To modify a trigger, you have to delete it first an recreate it with the new code. There is no such ALTER TRIGGER statement available in MySQL, therefore, you cannot modify an existing trigger like modifying other database objects such as tables, views, and stored procedures.","title":"Removing trigger"},{"location":"Databases/MySQL/advanced/12_triggers/#working-with-mysql-scheduled-event","text":"A MySQL events is a task that runs based on a predefined schedule therefore sometimes it is referred to as a scheduled event. MySQL event is also known as temporal trigger because it is trigger by time, not by table update like a trigger. A MySQL event is similar to a cronjob in UNIX or a task scheduler in Windows. You can use MySQL events in many cases such as optimizing database tables, cleaning up logs, archiving data, or generate complex reports during off-peak-time.","title":"Working with MySQL Scheduled Event"},{"location":"Databases/MySQL/advanced/12_triggers/#mysql-event-scheduler-configuration","text":"MySQL uses a special thread called event schedule thread to execute all scheduled events. You can see the status of event scheduler thread by executing: SHOW PROCESSLIST; By default, the event scheduler thread is not enabled. To enable and start the event scheduler thread, you need to execute the following command: SET GLOBAL event_scheduler = ON;","title":"MySQL event scheduler configuration"},{"location":"Databases/MySQL/advanced/12_triggers/#creating-new-mysql-events","text":"Creating an event is simar to creating other database objects such as stored procedures or triggers. An event is a named object that contains SQL statements. A stored procedure is only executed when it is invoked directly; a trigger is executed when an event associated with a table such as an inser, update or delete event occurs while an event can be executed once or more regular intervals. To create a schedule event, you use the CREATE EVENT statement as follows: CREATE EVENT [IF NOT EXIST] event_name ON SCHEDULE schedule DO event_body You put a schedule after the ON SCHEDULE clause. If the event is a one-time event, you use the syntax AT timestamp [+INTERVAL] . If the event is a recurring event, you use the EVERY interval STARTS timestamp [+interval] ENDS timestamp [+INTERVAL] . CREATE EVENT IF NOT EXISTS test_event_01 ON SCHEDULE AT CURRENT_TIMESTAMP DO INSERT INTO messages(message,created_at) VALUES('Test MySQL Event 1',NOW()); When event is expired, it is automatically dropped. To change the behaviour, you can use ON COMPLETION PRESERVE clause. The following statement creates another one-time event that is executed after its creation time 1 minute and not dropped after execution. CREATE EVENT test_event_02 ON SCHEDULE AT CURRENT_TIMESTAMP + INTERVAL 1 MINUTE ON COMPLETION PRESERVE DO INSERT INTO messages(message,created_at) VALUES('Test MySQL Event 2',NOW()); Recurring events CREATE EVENT test_event_03 ON SCHEDULE EVERY 1 MINUTE STARTS CURRENT_TIMESTAMP ENDS CURRENT_TIMESTAMP + INTERVAL 1 HOUR DO INSERT INTO messages(message,created_at) VALUES('Test MySQL recurring Event',NOW()); Drop MySQL Events DROP EVENT [IF EXIST] test_event_03;","title":"Creating new MySQL events"},{"location":"Databases/MySQL/advanced/12_triggers/#modifying-mysql-events","text":"MySQL allows you to change various attributes of an existing event. To change existing events, you use the ALTER EVENT statement as follows: ALTER EVENT event_name ON SCHEDULE schedule ON COMPLETION [NOT] PRESERVE RENAME TO new_event_name ENABLE | DISABLE DO event_body Notice that the ALTER EVENT statement is only applied to an existing event. if you try to modify a nonexistend event, MySQL will issue an error message therefore, you should always use the SHOW EVENTS statement to check the event for its existince before changing it. SHOW EVENTS FROM classicmodels Changing schedules ALTER EVENT test_event_04 ON SCHEDULE EVERY 2 MINUTE Changing event body ALTER EVENT test_event_04 DO INSERT INTO messages(message,created_at) VALUES('Message from event',NOW()); Disable events ALTER EVENT test_event_04 DISABLE; Enable events ALTER EVENT test_event_04 ENABLE; MySQL does not provide you with the RENAME EVENT statement. Fortunately, you can use the ALTER EVENT to rename an existing event as follows: ALTER EVENT test_event_04 RENAME TO test_event_05; Move events to another database ALTER EVENT classicmodels.test_event_05 RENAME TO newdb.test_event_05","title":"Modifying MySQL events"},{"location":"Databases/MySQL/advanced/13_indexes/","text":"MySQL indexes \u00b6 MySQL uses indexes to quickly find rows with specific column values. Without an index, MySQL must scan the whole table to locate the relevant rows. The larger table, the slower it searches. Creating indexes \u00b6 The phone book analogy \u00b6 Suppose you have a phone book that contains all the names and phone numbers of people in a city. Let's say you want to find Bob Cat's phone number. Knowing that the names are alphabetically ordered, you first look for the page where the last name is Cat, then you look for Bob and his phone number. Now, if the names in the phone book were not sorted alphabetically, you would need to go through all pages, reading every name in it ultil you find Bob Cat. This is called sequential searching. You go over all the entries until you find the person with the phone number that you are looking for. Relating the phone book to the database table, if you have a phonebook table and you have to find the phone number of Bob Cat, you would perform the following query: SELECT phone_number FROM phone_book WHERE first_name = 'Bob' AND last_name = 'Cat; It is pretty easy. Although thje query is fast, the database has to scan all the rows of the table until it finds the row. If the table has millions of rows, without an index, the data retrieval would take a lot of time to return the result. Introduction to index \u00b6 An index is a data structure such as B-Tree that improves the speed of data retrieval on a table at the cost of additional writes and storage to maintain it. The query optimizer may use indexes to quickly locate data without having to scan every row in a table for a given query. When you create a table with a primary key or unique key, MySQL automatically creates a special index named PRIMARY . This index is called the clustered index. The PRIMARY index is special because the index itself is stored together with the data in the same table. The clustered index enforces the order of rows in the table. Other indexes other than the PRIMARY index are called secindary indexes or non-clustered indexes. CREATE INDEX statement \u00b6 Typically, you create indexes for a table at the time of creation. For example, the following statement creates a new table with an index that consists of two columns c2 and c3. CREATE TABLE t( c1 INT PRIMARY KEY, c2 INT NOT NULL, c3 INT NOT NULL, c4 VARCHAR(10), INDEX (c2,c3) ); To add an index for a column or a set of columns you use CREATE INDEX statement as follows: CREATE INDEX index_name ON table_name (column_list) To create an index for a column or a list of columns, you specify the index name, the table to which the index belongs, and the column list. For example, to add a new index for the column c54, you use the following statement: CREATE INDEX idx_c4 ON t(c4); By default, MySQL creates the B-Tree index if you don't specify the index type. The following shows the permissible index type based on the storage engine of the table. Storage Engine Allowed Index Types InnoDB BTREE MyISAM BTREE MEMORY/HEAP HASH, BTREE MySQL CREATE INDEX example \u00b6 The following statement finds employees whose job title is Sales Rep : SELECT employeeNumber, lastName, firstName FROM employees WHERE jobTitle = 'Sales Rep'; To see how MySQL internally performed this query, you add the EXPLAIN clause at the beginning of the SELECT statement as follows: Now, let's create an index for the jobTitle column by using the CREATE INDEX statement: CREATE INDEX jobTitle on employees(jobTitle); To show the indexes of a table, you use the SHOW IDNEXES statement, for example SHOW INDEXES FROM employees MySQL DROP INDEX \u00b6 To remove an existing index from a table, you use DROP INDEX statement as follows: DROP INDEX index_name ON table_name [algorithm_option | lock_option] Algorithm \u00b6 The algorithm_option allows you to specify a specific algorithm used for the index removal. The following shows the syntax of the algorithm_option clause: ALGORITHM [=] {DEFAULT|INPLACE|COPY} For the index removal, the following algorithms are supported: COPY : The table is copied to the new table row by row, the DROP INDEX is them performed on the copy of the original table. The concurrent data manipulation statements such as INSERT and UPDATE are not permitted. INPLACE the table is rebuild in place instead of copied to the new one. MySQL issues an exclussive metadata lock on the table during the preparation and execution phases of the index removal operation. This algorithm allows concurrent data manupulation statements. Note that the ALGORITHM clause is optional. MySQL uses INPLACE . In case the INPLACE is not supported, MySQL uses COPY . Lock \u00b6 The lock_option controls the level of concurrent reads and writes on the table while the indewx is being removed. The following shows the syntax of the lock_option : LOCK [=] {DEFAULT|NONE\\SHARED|EXCLUSIVE} The following locking modes are supported: DEFAULT - this allows you to have the maximum level of concurrency for a given algorithm. First, it allows concurrent reads and writes if supported. If not, allow concurrent reads if supported. If not, eforce exclussive access. NONE if the NONE is supported, you can have concurrent reads and writes. Otherwise, MySQL issues an error. SHARED - if the SHARED is supported, you can have concurrent reads, but not writes. MySQL issues an error if the concurrent reads are not supported. EXCLUSIVE - this enforces exclusive access. DROP INDEX examples \u00b6 DROP INDEX name ON leads; DROP INDEX email ON leads ALGORITHM = INPLACE LOCK = DEFAULT; DROP INDEX `PRIMARY` ON table_name; DROP INDEX `PRIMARY` ON t; MySQL SHOW INDEXES \u00b6 To query the index information of a table, you use the SHOW INDEXES statement as follows: SHOW INDEXES FROM table_name To get the index of a table, you specify the table name after the FROM keyword. The statement will return the index information associated with the table in the current database. You can specify the database name if you are not connected to any database or you want to get the index information of a table in a different database: SHOW INDEXES FROM table_name IN database_name; SHOW INDEXES FROM database_name.table_name; The SHOW INDEXES will return following information table - name of the table non_unique - 1 if the index can contain duplicates, 0 if it can. key_name - the name of the index. The primary key index always has the name of PRIMARY . seq_in_index - The column sequence number in the index. The first column sequence number starts from 1. column_name - the column name collation - collation represents how the column is sorted in the index. A means ascending, B means descending, NULL means not sorted. cadinality - the cardinality returns an estimated number of unique values in the index. Note that the higher the cardinality, the greater the chance that the query optimizer uses the index for lookups. sub_part - The index prefix. It is null if the entire column is indexed. Otherwise, it shows the number of indexed charasteristics in case column is partially indexed. packed - indicates how the key is packed; NUL if it is not. null - YES - if the column may contain NULL values and blank if it does not. index_type prepresents the index method used such as BTREE , HASH , RTREE or FULLTEXT . comment - the information about the index not described in its own column. index_comment - shows the comment for the index specified when you create index with the COMMENT attribute. visible - whether the index is visible or invisible to the query optimizer or not; YES if is, NO if not. expression - if the index uses an expression rather than column or column prefix value, the expression indicates the expression for the key part and also the column_name column is NULL. To filter index information, you can use a WHERE clause as follows: SHOW INDEXES FROM table_name WHERE condition; You can use any information returned by the SHOW INDEXES statement to filter the index information. For example, the following statement returns only the invisible indexes of a table: SHOW INDEXES FROM table_name WHERE VISIBLE = 'NO'; Using MySQL UNIQUE Index to prevent duplicates \u00b6 To enforce the uniqueness value of one or more columns, you often use the PRIMARY KEY constraint. However, each table can have only one primary key. Hence, if you want to have a more than one column or a set of columns with unique values, you cannot use the primary key constraint. Luckily, MySQL provides another kind of index called UNIQUE index that allows you to enforce the uniqueness of values in one or more columns. Unlike the PRIMARY KEY index, you can have more that one UNIQUE index per table. To create a UNIQUE index, you use the CREATE UNIQUE INDEX statement as follows: CREATE UNIQUE INDEX index_name ON table_name(index_column_1,index_column_2,...); Another way to enforce the uniqueness of value in one or more columns is to use the UNIQUE constraing. When you create a UNIQUE constraint , MySQL creates an UNIQUE index behind the scenes. The following statement illustrates how to create a unique contraing when you create a table. CREATE TABLE table_name( ... UNIQUE KEY(index_column_,index_column_2,...) ); In this statement you can also use the UNIQUE INDEX instead of UNIQUE KEY because they are synonyms. If you want to add constraint to an existing table, you can use the ALTER TABLE statement as follows: ALTER TABLE table_name ADD CONSTRAINT constraint_name UNIQUE KEY(column_1,column_2,...); UNIQUE Index and NULL \u00b6 Unlike other database systems, MySQL considers NULL values as distinct values. Therefore, you can have multiple NULL values in the UNIQUE index. This is how MySQL was designed. It is not a bug even though it was reported as a bug. Another important point is that the UNIQUE constraint does not apply to NULL values except for the BDB storage engine. Prefix index \u00b6 When you create a secondary index for a column, MySQL stores the values of the columns in a separate data structure e.g., B-Tree and Hash. In case the columns are the string columns, the index will consume a lot of disk space and potentially slow down the INSERT operations. To address this issue, MySQL allows you to create an index for the leading part of the column values of the string columns using the following syntax: column_name(length) For example, the following statement creates the column prefix key part at the time of table creation: CREATE TABLE table_name( column_list, INDEX(column_name(length)) ) Or add an index to an existing table: CREATE INDEX index_name ON table_name(column_name(length)) In this syntax, the length is the number of characters for the non-binary string types such as CHAR , VARCHAR , and TEXT and the number of byters for binary string types e.g., BINARY , VARBINARY AND BLOB . MySQL allows you to optionally create column prefix key parts for CHAR , VARCHAR , BINARY and VARBINARY columns. If you create indexes for BLOB and TEXT columns, you must specify the column prefix key parts. Notice that the prefix support and lengths of prefixes it supported are storage engine dependent. For InnoDB tables with REDUNDAT or COMPACT row format, the maximum prefix length is 767 bytes. However, for the InnoDB tables with the DYNAMIC or COMPRESSED row format, the prefix lkength is 3072 bytes. MyISAM tables have the prefix length up to 1000 bytes. Prefix index example \u00b6 The following query finds the products whose names start with the string 1970: SELECT productName, buyPrice, msrp FROM products WHERE productName LIKE '1970%'; Because there is not index for the productName column, the query optimizer has to scan all rows to return the result as shown in the output of the EXPLAIN statement below: EXPLAIN SELECT productName, buyPrice, msrp FROM products WHERE productName LIKE '1970%'; If you often find the products by the product name, then you should create an index for this column because if will be more efficient for searches. The size of the product name column is 70 characters. We can use the column prefix key parts. The next question is how do you choose the length of the prefix? For doing this you can investigate the existing data. The goal is to maximize the uniqueness of the values in the column when you use the prefix. Yo do this you can follow these steps: Find the number of rows in table: SELECT COUNT(*) FROM products; Evaluate different prefix length until you can achgieve the reasonable uniqueness or rows: SELECT COUNT(DISTINCT LEFT(productName, 20)) unique_rows FROM products; From the output, 20 is a good prefix length in this case because if we use the first 20 characters of the product name for the index, all product names are unique. Let's create an index with the prefix length 20 for the productName column: CREATE INDEX idx_productname ON products(producName(20)); And execute the query that finds products whose name starts with the string 1970 again: EXPLAIN SELECT productName, buyPrice, msrp FROM products WHERE productName LIKE '1970%'; Now the query optimizer uses the newly created index which is much faster and more efficient than before. Invisible index \u00b6 The invisible indexes allow you to mark indexes as unavailable for the query optimizer. MySQL maintains the invisible indexes and keeps them up to date when the data in the columns associated with the indexes changes. By default, indexes are visible. To make them invisible, you have to explicitly declare its visiblility at the time of creating, or by using the ALTER TABLE command. MySQL provides us with the VISIBLE and INVISIBLE keywords to maintain index visibility. CREATE INDEX index_name ON table_name( c1, c2, ...) INVISIBLE; For example, the following statement creates an index on the extension column of the employees table in the sample database and marks it as an invisible index: CREATE INDEX extension ON employees(extension) INVISIBLE; To change the visibility of existing indexes, you use the following statement: ALTER TABLE table_name ALTER INDEX index_name [VISIBLE | INVISIBLE]; You can find the indexes and their visibility by querying the statistics table in the information_schema database: SELECT index_name, is_visible FROM information_schema.statistics WHERE table_schema = 'classicmodels' AND table_name = 'employees'; In addition, you can use the SHOW INDEXES command to display all indexes of table: SHOW INDEXES FROM employees; As mentioned earlier, the query optimizer does not use invisible index so why do you use the invisible index in the first place? Practically speaking, invisible indexes have a number of applications. For example, you can make an index invisible to see if has an impact to the performance and mark the index visible again if it does. Invisible index and primary key \u00b6 The index of the primary key column cannot be invisible. If you try to do so, it MySQL will issue an error. In addition, an implicit primary key index also cannot be invisible. When you define a UNIQUE index on a NOT NULL column of a table that does not have a primary key, MySQL implicitly understands that this column is the primary key column and does not allow you to make the index invisible. Consider the following example. Invisible index system variables \u00b6 To control indexes used by the query optimizer, MySQL uses the use_invisible_indexes flag of the optimizer_switch system variable. By default, the use_invisible_indexes is off: SELECT @@optimizer_switch; Descending index \u00b6 A descending index is an index that stores key values in a descending order. Before MySQL 8.0, you can specify the DESC in an index definition. However, MySQL ignored it. In the meantime, MySQL could scan the index in reverse order but it comes at a high cost. Starting from MySQL 8.0, the key values are stored in the descending order if you use the DESC keyword in the index definition. The query optimizer can take advantage of descending index when descending order is requested in the query. For example, we create t table with for indexes in different orders: DROP TABLE t; CREATE TABLE t ( a INT, b INT, INDEX a_asc_b_asc (a ASC , b ASC), INDEX a_asc_b_desc (a ASC , b DESC), INDEX a_desc_b_asc (a DESC , b ASC), INDEX a_desc_b_desc (a DESC , b DESC) ); Second, use the following the following stored procedure to insert rows into the t table: CREATE PROCEDURE insertSampleData( IN rowCount INT, IN low INT, IN high INT ) BEGIN DECLARE counter INT DEFAULT 0; REPEAT SET counter := counter + 1; -- insert data INSERT INTO t(a,b) VALUES( ROUND((RAND() * (high-low))+high), ROUND((RAND() * (high-low))+high) ); UNTIL counter >= rowCount END REPEAT; END$$ The stored procedure inserts a number or rows with the values between low and high into the a and b columns of the t table. CALL insertSampleData(10000,1,1000); EXPLAIN SELECT * FROM t ORDER BY a , b; -- use index a_asc_b_asc EXPLAIN SELECT * FROM t ORDER BY a , b DESC; -- use index a_asc_b_desc EXPLAIN SELECT * FROM t ORDER BY a DESC , b; -- use index a_desc_b_asc EXPLAIN SELECT * FROM t ORDER BY a DESC , b DESC; -- use index a_desc_b_desc Composite index \u00b6 A composite index is an index on multiple columns. MySQL allows you to create a composite index that consists up to 16 columns. A composite index is also known as a multiple-column index. The query optimizer uses the composite indexes for queries that test all columns in the index, or queries that test the first columns, the first two columns, and so on. To create a composite index at the time of table creation, you use the following statement: CREATE TABLE table_name ( c1 data_type PRIMARY KEY, c2 data_type, c3 data_type, c4 data_type, INDEX index_name (c2,c3,c4) ); In this syntax, the composite index consists of three columns c2, c3, c4. Or you can add a composite index to an existing table by using the CREATE INDEX statement: CREATE INDEX index_name ON table_name(c2,c3,c4); Notice that if you have a composite index on (c1, c2, c3), you will have indexed search capabilities on one the following column combinations: (c1) (c1,c2) (c1,c2,c3) SELECT * FROM table_name WHERE c1 = v1; SELECT * FROM table_name WHERE c1 = v1 AND c2 = v2; SELECT * FROM table_name WHERE c1 = v1 AND c2 = v2 AND c3 = v3; The query optimizer cannot use the index to perform lookups if the columns do not form a leftmost prefix of the index. For example the following queryies cannot use the composite for lookups: SELECT * FROM table_name WHERE c1 = v1 AND c3 = v3; Composite index example \u00b6 We will use the employees table which consist of following columns - employeeNumber, lastName, firstName, extension, email, officeCode, reportsTo, jobTitle. The following statement creates a composite index over the lastName and firstName columns. CREATE INDEX name ON employees(lastName, firstName); First, the name index can be used for lookups in the queries that specify a lastName value because the lastname column is the leftmost prefix of the index. Second, the name index can be used for queries that specify values for the combination of the lastName and firstName values. The name index, therefore, is used for lookups in the following queries: Find employees whose last name is Patterson Find employees whose last name is Patterson and first name is Steve Find employees whose last name is Patterson and first name is Steve or Mary . Clustered Index \u00b6 Typically, an index is a separate data structure such as B-Tree that stores the key values used for faster lookups. A clustered index, on the other hand is actually the table. It is an index that enforces the ordering on the rows of the table physically. Once a clustered index is created, all rows in the table will be stored according to the key columns used to create the clustered index. Because a clustered index store the rows in a stored order, each table have only one clustered index. Clustered indexes on InnoDB tables \u00b6 Each InnoDB table requires clustered index. The clustered index helps an InnoDB table optimize data manipulations such as SELECT , INSERT , UPDATE and DELETE . When you define a primary key for an InnoDB table, MySQL uses the primary key as the clustered index. If you do not have a primary key for the table, MySQL will search for the first UNIQUE index where all the key columns are NOT NULL and use this UNIQUE index as the clustered index. In case the InnoDB table has no primary key or suitable UNIQUE index, MySQL internally generates a hidden clustered index named GEN_CLIST_INDEX on a synthetic column which contains the row ID values. As the result, each InnoDB table always has one and only one clustered index. All indexes other than the clustered index are the non-clusteered indexes or secondary indexes.In InnoDB tables, each record in the secondary index contains the primary key columns for the row as well as the columns specified in the non-clustered index. MySQL uses this primary key value for the row lookups in the clustered index. Therefore, it is advantageous to have a short primary key otherwise, the secondary indexes will use more space. Typically, the auto-increment integer column is used for the primary key column. Index cardinality \u00b6 Index cardinality refers to the uniqueness of values stored in a specified column within an index. MySQL generates the index cardinality to generate an optimal query plan for a given query. It also uses the index cardinality to decide whether to use the index or not in the join operations. If the query optimizer chooses the index with a low cardinality, it may be more effective to scan rows without using the index. To view the index cardinality, you can use the SHOW INDEXES command. For example, the following statement returns the index information of the orders table in the sample database with the cardinality: mysql> SHOW INDEXES FROM orders; +--------+------------+----------------+--------------+----------------+-----------+-------------+----------+--------+------+------------+---------+---------------+---------+ | Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment | Visible | +--------+------------+----------------+--------------+----------------+-----------+-------------+----------+--------+------+------------+---------+---------------+---------+ | orders | 0 | PRIMARY | 1 | orderNumber | A | 326 | NULL | NULL | | BTREE | | | YES | | orders | 1 | customerNumber | 1 | customerNumber | A | 98 | NULL | NULL | | BTREE | | | YES | +--------+------------+----------------+--------------+----------------+-----------+-------------+----------+--------+------+------------+---------+---------------+---------+ 2 rows in set (0.01 sec) In the output, the PRIMARY KEY for the orderNumber column shows the table has 326 unique values, while the custmerNaumber column only has 98 distinct values. As mentioned earlier, index statistics are only approximate and may not represent the real size of the rows in the table. To generate more accurate statistical information, you use the ANALYZE TABLE command. USE INDEX Hint \u00b6 In MySQL, when you submit an SQL query, the query optimizer will try to make an optimal query execution plan. To determine the best possible plan, the query optimizer makes use of many parameters. One of the most important parameters for choosing which index to use is stored key distribution which is also known as cardinality. The cardinality, however, may be not accurate for example in case the table ghas been modified heavily with many inserts or deletes. To solve this issue, you should run the ANALYZE TABLE statement periodically to update the cardinality. In addition, MySQL provides an alternative way that allows you to recomment the indexes that the query optimizer should by using an index hind called USE INDEX . The following illustrates syntax of the MySQL USE INDEX hint: SELECT select_list FROM table_name USE INDEX(index_list) WHERE condition; In this syntax, the USE INDEX instructs the query optimizer to use one of the named indexes to find rows in the table. Notice that when you recomment the indexes to use, the query optimizer may either decide to use them or not depending on the query plan that it comes up with. Force Index \u00b6 The query optimizer is a component in the MySQL database server that makes the most optional execution plan for an SQL statement. The query optimizer uses the available statistics to come up with the plan that has the lowest cost among all candidate plans. For example, a query migh request for products whose prices are betweeen 10 and 80. If the statistics show that 80% of products have these price ranges, then it may decide that a full table scan is the most efficient. However if statistics show that very few products have these price ranges, then reading an index followed by a table access could be faster and more efficient that a full table scan. In case the query optimizer ignores the index, you can use the FORCE INDEX hint to instruct it to use the index instead. The following illustrates the FORCE INDEX hint syntax: SELECT * FROM table_name FORCE INDEX (index_list) WHERE condition; SELECT productName, buyPrice FROM products FORCE INDEX (idx_buyPrice) WHERE buyPrice BETWEEN 10 AND 80 ORDER BY buyPrice;","title":"MySQL indexes"},{"location":"Databases/MySQL/advanced/13_indexes/#mysql-indexes","text":"MySQL uses indexes to quickly find rows with specific column values. Without an index, MySQL must scan the whole table to locate the relevant rows. The larger table, the slower it searches.","title":"MySQL indexes"},{"location":"Databases/MySQL/advanced/13_indexes/#creating-indexes","text":"","title":"Creating indexes"},{"location":"Databases/MySQL/advanced/13_indexes/#the-phone-book-analogy","text":"Suppose you have a phone book that contains all the names and phone numbers of people in a city. Let's say you want to find Bob Cat's phone number. Knowing that the names are alphabetically ordered, you first look for the page where the last name is Cat, then you look for Bob and his phone number. Now, if the names in the phone book were not sorted alphabetically, you would need to go through all pages, reading every name in it ultil you find Bob Cat. This is called sequential searching. You go over all the entries until you find the person with the phone number that you are looking for. Relating the phone book to the database table, if you have a phonebook table and you have to find the phone number of Bob Cat, you would perform the following query: SELECT phone_number FROM phone_book WHERE first_name = 'Bob' AND last_name = 'Cat; It is pretty easy. Although thje query is fast, the database has to scan all the rows of the table until it finds the row. If the table has millions of rows, without an index, the data retrieval would take a lot of time to return the result.","title":"The phone book analogy"},{"location":"Databases/MySQL/advanced/13_indexes/#introduction-to-index","text":"An index is a data structure such as B-Tree that improves the speed of data retrieval on a table at the cost of additional writes and storage to maintain it. The query optimizer may use indexes to quickly locate data without having to scan every row in a table for a given query. When you create a table with a primary key or unique key, MySQL automatically creates a special index named PRIMARY . This index is called the clustered index. The PRIMARY index is special because the index itself is stored together with the data in the same table. The clustered index enforces the order of rows in the table. Other indexes other than the PRIMARY index are called secindary indexes or non-clustered indexes.","title":"Introduction to index"},{"location":"Databases/MySQL/advanced/13_indexes/#create-index-statement","text":"Typically, you create indexes for a table at the time of creation. For example, the following statement creates a new table with an index that consists of two columns c2 and c3. CREATE TABLE t( c1 INT PRIMARY KEY, c2 INT NOT NULL, c3 INT NOT NULL, c4 VARCHAR(10), INDEX (c2,c3) ); To add an index for a column or a set of columns you use CREATE INDEX statement as follows: CREATE INDEX index_name ON table_name (column_list) To create an index for a column or a list of columns, you specify the index name, the table to which the index belongs, and the column list. For example, to add a new index for the column c54, you use the following statement: CREATE INDEX idx_c4 ON t(c4); By default, MySQL creates the B-Tree index if you don't specify the index type. The following shows the permissible index type based on the storage engine of the table. Storage Engine Allowed Index Types InnoDB BTREE MyISAM BTREE MEMORY/HEAP HASH, BTREE","title":"CREATE INDEX statement"},{"location":"Databases/MySQL/advanced/13_indexes/#mysql-create-index-example","text":"The following statement finds employees whose job title is Sales Rep : SELECT employeeNumber, lastName, firstName FROM employees WHERE jobTitle = 'Sales Rep'; To see how MySQL internally performed this query, you add the EXPLAIN clause at the beginning of the SELECT statement as follows: Now, let's create an index for the jobTitle column by using the CREATE INDEX statement: CREATE INDEX jobTitle on employees(jobTitle); To show the indexes of a table, you use the SHOW IDNEXES statement, for example SHOW INDEXES FROM employees","title":"MySQL CREATE INDEX example"},{"location":"Databases/MySQL/advanced/13_indexes/#mysql-drop-index","text":"To remove an existing index from a table, you use DROP INDEX statement as follows: DROP INDEX index_name ON table_name [algorithm_option | lock_option]","title":"MySQL DROP INDEX"},{"location":"Databases/MySQL/advanced/13_indexes/#algorithm","text":"The algorithm_option allows you to specify a specific algorithm used for the index removal. The following shows the syntax of the algorithm_option clause: ALGORITHM [=] {DEFAULT|INPLACE|COPY} For the index removal, the following algorithms are supported: COPY : The table is copied to the new table row by row, the DROP INDEX is them performed on the copy of the original table. The concurrent data manipulation statements such as INSERT and UPDATE are not permitted. INPLACE the table is rebuild in place instead of copied to the new one. MySQL issues an exclussive metadata lock on the table during the preparation and execution phases of the index removal operation. This algorithm allows concurrent data manupulation statements. Note that the ALGORITHM clause is optional. MySQL uses INPLACE . In case the INPLACE is not supported, MySQL uses COPY .","title":"Algorithm"},{"location":"Databases/MySQL/advanced/13_indexes/#lock","text":"The lock_option controls the level of concurrent reads and writes on the table while the indewx is being removed. The following shows the syntax of the lock_option : LOCK [=] {DEFAULT|NONE\\SHARED|EXCLUSIVE} The following locking modes are supported: DEFAULT - this allows you to have the maximum level of concurrency for a given algorithm. First, it allows concurrent reads and writes if supported. If not, allow concurrent reads if supported. If not, eforce exclussive access. NONE if the NONE is supported, you can have concurrent reads and writes. Otherwise, MySQL issues an error. SHARED - if the SHARED is supported, you can have concurrent reads, but not writes. MySQL issues an error if the concurrent reads are not supported. EXCLUSIVE - this enforces exclusive access.","title":"Lock"},{"location":"Databases/MySQL/advanced/13_indexes/#drop-index-examples","text":"DROP INDEX name ON leads; DROP INDEX email ON leads ALGORITHM = INPLACE LOCK = DEFAULT; DROP INDEX `PRIMARY` ON table_name; DROP INDEX `PRIMARY` ON t;","title":"DROP INDEX examples"},{"location":"Databases/MySQL/advanced/13_indexes/#mysql-show-indexes","text":"To query the index information of a table, you use the SHOW INDEXES statement as follows: SHOW INDEXES FROM table_name To get the index of a table, you specify the table name after the FROM keyword. The statement will return the index information associated with the table in the current database. You can specify the database name if you are not connected to any database or you want to get the index information of a table in a different database: SHOW INDEXES FROM table_name IN database_name; SHOW INDEXES FROM database_name.table_name; The SHOW INDEXES will return following information table - name of the table non_unique - 1 if the index can contain duplicates, 0 if it can. key_name - the name of the index. The primary key index always has the name of PRIMARY . seq_in_index - The column sequence number in the index. The first column sequence number starts from 1. column_name - the column name collation - collation represents how the column is sorted in the index. A means ascending, B means descending, NULL means not sorted. cadinality - the cardinality returns an estimated number of unique values in the index. Note that the higher the cardinality, the greater the chance that the query optimizer uses the index for lookups. sub_part - The index prefix. It is null if the entire column is indexed. Otherwise, it shows the number of indexed charasteristics in case column is partially indexed. packed - indicates how the key is packed; NUL if it is not. null - YES - if the column may contain NULL values and blank if it does not. index_type prepresents the index method used such as BTREE , HASH , RTREE or FULLTEXT . comment - the information about the index not described in its own column. index_comment - shows the comment for the index specified when you create index with the COMMENT attribute. visible - whether the index is visible or invisible to the query optimizer or not; YES if is, NO if not. expression - if the index uses an expression rather than column or column prefix value, the expression indicates the expression for the key part and also the column_name column is NULL. To filter index information, you can use a WHERE clause as follows: SHOW INDEXES FROM table_name WHERE condition; You can use any information returned by the SHOW INDEXES statement to filter the index information. For example, the following statement returns only the invisible indexes of a table: SHOW INDEXES FROM table_name WHERE VISIBLE = 'NO';","title":"MySQL SHOW INDEXES"},{"location":"Databases/MySQL/advanced/13_indexes/#using-mysql-unique-index-to-prevent-duplicates","text":"To enforce the uniqueness value of one or more columns, you often use the PRIMARY KEY constraint. However, each table can have only one primary key. Hence, if you want to have a more than one column or a set of columns with unique values, you cannot use the primary key constraint. Luckily, MySQL provides another kind of index called UNIQUE index that allows you to enforce the uniqueness of values in one or more columns. Unlike the PRIMARY KEY index, you can have more that one UNIQUE index per table. To create a UNIQUE index, you use the CREATE UNIQUE INDEX statement as follows: CREATE UNIQUE INDEX index_name ON table_name(index_column_1,index_column_2,...); Another way to enforce the uniqueness of value in one or more columns is to use the UNIQUE constraing. When you create a UNIQUE constraint , MySQL creates an UNIQUE index behind the scenes. The following statement illustrates how to create a unique contraing when you create a table. CREATE TABLE table_name( ... UNIQUE KEY(index_column_,index_column_2,...) ); In this statement you can also use the UNIQUE INDEX instead of UNIQUE KEY because they are synonyms. If you want to add constraint to an existing table, you can use the ALTER TABLE statement as follows: ALTER TABLE table_name ADD CONSTRAINT constraint_name UNIQUE KEY(column_1,column_2,...);","title":"Using MySQL UNIQUE Index to prevent duplicates"},{"location":"Databases/MySQL/advanced/13_indexes/#unique-index-and-null","text":"Unlike other database systems, MySQL considers NULL values as distinct values. Therefore, you can have multiple NULL values in the UNIQUE index. This is how MySQL was designed. It is not a bug even though it was reported as a bug. Another important point is that the UNIQUE constraint does not apply to NULL values except for the BDB storage engine.","title":"UNIQUE Index and NULL"},{"location":"Databases/MySQL/advanced/13_indexes/#prefix-index","text":"When you create a secondary index for a column, MySQL stores the values of the columns in a separate data structure e.g., B-Tree and Hash. In case the columns are the string columns, the index will consume a lot of disk space and potentially slow down the INSERT operations. To address this issue, MySQL allows you to create an index for the leading part of the column values of the string columns using the following syntax: column_name(length) For example, the following statement creates the column prefix key part at the time of table creation: CREATE TABLE table_name( column_list, INDEX(column_name(length)) ) Or add an index to an existing table: CREATE INDEX index_name ON table_name(column_name(length)) In this syntax, the length is the number of characters for the non-binary string types such as CHAR , VARCHAR , and TEXT and the number of byters for binary string types e.g., BINARY , VARBINARY AND BLOB . MySQL allows you to optionally create column prefix key parts for CHAR , VARCHAR , BINARY and VARBINARY columns. If you create indexes for BLOB and TEXT columns, you must specify the column prefix key parts. Notice that the prefix support and lengths of prefixes it supported are storage engine dependent. For InnoDB tables with REDUNDAT or COMPACT row format, the maximum prefix length is 767 bytes. However, for the InnoDB tables with the DYNAMIC or COMPRESSED row format, the prefix lkength is 3072 bytes. MyISAM tables have the prefix length up to 1000 bytes.","title":"Prefix index"},{"location":"Databases/MySQL/advanced/13_indexes/#prefix-index-example","text":"The following query finds the products whose names start with the string 1970: SELECT productName, buyPrice, msrp FROM products WHERE productName LIKE '1970%'; Because there is not index for the productName column, the query optimizer has to scan all rows to return the result as shown in the output of the EXPLAIN statement below: EXPLAIN SELECT productName, buyPrice, msrp FROM products WHERE productName LIKE '1970%'; If you often find the products by the product name, then you should create an index for this column because if will be more efficient for searches. The size of the product name column is 70 characters. We can use the column prefix key parts. The next question is how do you choose the length of the prefix? For doing this you can investigate the existing data. The goal is to maximize the uniqueness of the values in the column when you use the prefix. Yo do this you can follow these steps: Find the number of rows in table: SELECT COUNT(*) FROM products; Evaluate different prefix length until you can achgieve the reasonable uniqueness or rows: SELECT COUNT(DISTINCT LEFT(productName, 20)) unique_rows FROM products; From the output, 20 is a good prefix length in this case because if we use the first 20 characters of the product name for the index, all product names are unique. Let's create an index with the prefix length 20 for the productName column: CREATE INDEX idx_productname ON products(producName(20)); And execute the query that finds products whose name starts with the string 1970 again: EXPLAIN SELECT productName, buyPrice, msrp FROM products WHERE productName LIKE '1970%'; Now the query optimizer uses the newly created index which is much faster and more efficient than before.","title":"Prefix index example"},{"location":"Databases/MySQL/advanced/13_indexes/#invisible-index","text":"The invisible indexes allow you to mark indexes as unavailable for the query optimizer. MySQL maintains the invisible indexes and keeps them up to date when the data in the columns associated with the indexes changes. By default, indexes are visible. To make them invisible, you have to explicitly declare its visiblility at the time of creating, or by using the ALTER TABLE command. MySQL provides us with the VISIBLE and INVISIBLE keywords to maintain index visibility. CREATE INDEX index_name ON table_name( c1, c2, ...) INVISIBLE; For example, the following statement creates an index on the extension column of the employees table in the sample database and marks it as an invisible index: CREATE INDEX extension ON employees(extension) INVISIBLE; To change the visibility of existing indexes, you use the following statement: ALTER TABLE table_name ALTER INDEX index_name [VISIBLE | INVISIBLE]; You can find the indexes and their visibility by querying the statistics table in the information_schema database: SELECT index_name, is_visible FROM information_schema.statistics WHERE table_schema = 'classicmodels' AND table_name = 'employees'; In addition, you can use the SHOW INDEXES command to display all indexes of table: SHOW INDEXES FROM employees; As mentioned earlier, the query optimizer does not use invisible index so why do you use the invisible index in the first place? Practically speaking, invisible indexes have a number of applications. For example, you can make an index invisible to see if has an impact to the performance and mark the index visible again if it does.","title":"Invisible index"},{"location":"Databases/MySQL/advanced/13_indexes/#invisible-index-and-primary-key","text":"The index of the primary key column cannot be invisible. If you try to do so, it MySQL will issue an error. In addition, an implicit primary key index also cannot be invisible. When you define a UNIQUE index on a NOT NULL column of a table that does not have a primary key, MySQL implicitly understands that this column is the primary key column and does not allow you to make the index invisible. Consider the following example.","title":"Invisible index and primary key"},{"location":"Databases/MySQL/advanced/13_indexes/#invisible-index-system-variables","text":"To control indexes used by the query optimizer, MySQL uses the use_invisible_indexes flag of the optimizer_switch system variable. By default, the use_invisible_indexes is off: SELECT @@optimizer_switch;","title":"Invisible index system variables"},{"location":"Databases/MySQL/advanced/13_indexes/#descending-index","text":"A descending index is an index that stores key values in a descending order. Before MySQL 8.0, you can specify the DESC in an index definition. However, MySQL ignored it. In the meantime, MySQL could scan the index in reverse order but it comes at a high cost. Starting from MySQL 8.0, the key values are stored in the descending order if you use the DESC keyword in the index definition. The query optimizer can take advantage of descending index when descending order is requested in the query. For example, we create t table with for indexes in different orders: DROP TABLE t; CREATE TABLE t ( a INT, b INT, INDEX a_asc_b_asc (a ASC , b ASC), INDEX a_asc_b_desc (a ASC , b DESC), INDEX a_desc_b_asc (a DESC , b ASC), INDEX a_desc_b_desc (a DESC , b DESC) ); Second, use the following the following stored procedure to insert rows into the t table: CREATE PROCEDURE insertSampleData( IN rowCount INT, IN low INT, IN high INT ) BEGIN DECLARE counter INT DEFAULT 0; REPEAT SET counter := counter + 1; -- insert data INSERT INTO t(a,b) VALUES( ROUND((RAND() * (high-low))+high), ROUND((RAND() * (high-low))+high) ); UNTIL counter >= rowCount END REPEAT; END$$ The stored procedure inserts a number or rows with the values between low and high into the a and b columns of the t table. CALL insertSampleData(10000,1,1000); EXPLAIN SELECT * FROM t ORDER BY a , b; -- use index a_asc_b_asc EXPLAIN SELECT * FROM t ORDER BY a , b DESC; -- use index a_asc_b_desc EXPLAIN SELECT * FROM t ORDER BY a DESC , b; -- use index a_desc_b_asc EXPLAIN SELECT * FROM t ORDER BY a DESC , b DESC; -- use index a_desc_b_desc","title":"Descending index"},{"location":"Databases/MySQL/advanced/13_indexes/#composite-index","text":"A composite index is an index on multiple columns. MySQL allows you to create a composite index that consists up to 16 columns. A composite index is also known as a multiple-column index. The query optimizer uses the composite indexes for queries that test all columns in the index, or queries that test the first columns, the first two columns, and so on. To create a composite index at the time of table creation, you use the following statement: CREATE TABLE table_name ( c1 data_type PRIMARY KEY, c2 data_type, c3 data_type, c4 data_type, INDEX index_name (c2,c3,c4) ); In this syntax, the composite index consists of three columns c2, c3, c4. Or you can add a composite index to an existing table by using the CREATE INDEX statement: CREATE INDEX index_name ON table_name(c2,c3,c4); Notice that if you have a composite index on (c1, c2, c3), you will have indexed search capabilities on one the following column combinations: (c1) (c1,c2) (c1,c2,c3) SELECT * FROM table_name WHERE c1 = v1; SELECT * FROM table_name WHERE c1 = v1 AND c2 = v2; SELECT * FROM table_name WHERE c1 = v1 AND c2 = v2 AND c3 = v3; The query optimizer cannot use the index to perform lookups if the columns do not form a leftmost prefix of the index. For example the following queryies cannot use the composite for lookups: SELECT * FROM table_name WHERE c1 = v1 AND c3 = v3;","title":"Composite index"},{"location":"Databases/MySQL/advanced/13_indexes/#composite-index-example","text":"We will use the employees table which consist of following columns - employeeNumber, lastName, firstName, extension, email, officeCode, reportsTo, jobTitle. The following statement creates a composite index over the lastName and firstName columns. CREATE INDEX name ON employees(lastName, firstName); First, the name index can be used for lookups in the queries that specify a lastName value because the lastname column is the leftmost prefix of the index. Second, the name index can be used for queries that specify values for the combination of the lastName and firstName values. The name index, therefore, is used for lookups in the following queries: Find employees whose last name is Patterson Find employees whose last name is Patterson and first name is Steve Find employees whose last name is Patterson and first name is Steve or Mary .","title":"Composite index example"},{"location":"Databases/MySQL/advanced/13_indexes/#clustered-index","text":"Typically, an index is a separate data structure such as B-Tree that stores the key values used for faster lookups. A clustered index, on the other hand is actually the table. It is an index that enforces the ordering on the rows of the table physically. Once a clustered index is created, all rows in the table will be stored according to the key columns used to create the clustered index. Because a clustered index store the rows in a stored order, each table have only one clustered index.","title":"Clustered Index"},{"location":"Databases/MySQL/advanced/13_indexes/#clustered-indexes-on-innodb-tables","text":"Each InnoDB table requires clustered index. The clustered index helps an InnoDB table optimize data manipulations such as SELECT , INSERT , UPDATE and DELETE . When you define a primary key for an InnoDB table, MySQL uses the primary key as the clustered index. If you do not have a primary key for the table, MySQL will search for the first UNIQUE index where all the key columns are NOT NULL and use this UNIQUE index as the clustered index. In case the InnoDB table has no primary key or suitable UNIQUE index, MySQL internally generates a hidden clustered index named GEN_CLIST_INDEX on a synthetic column which contains the row ID values. As the result, each InnoDB table always has one and only one clustered index. All indexes other than the clustered index are the non-clusteered indexes or secondary indexes.In InnoDB tables, each record in the secondary index contains the primary key columns for the row as well as the columns specified in the non-clustered index. MySQL uses this primary key value for the row lookups in the clustered index. Therefore, it is advantageous to have a short primary key otherwise, the secondary indexes will use more space. Typically, the auto-increment integer column is used for the primary key column.","title":"Clustered indexes on InnoDB tables"},{"location":"Databases/MySQL/advanced/13_indexes/#index-cardinality","text":"Index cardinality refers to the uniqueness of values stored in a specified column within an index. MySQL generates the index cardinality to generate an optimal query plan for a given query. It also uses the index cardinality to decide whether to use the index or not in the join operations. If the query optimizer chooses the index with a low cardinality, it may be more effective to scan rows without using the index. To view the index cardinality, you can use the SHOW INDEXES command. For example, the following statement returns the index information of the orders table in the sample database with the cardinality: mysql> SHOW INDEXES FROM orders; +--------+------------+----------------+--------------+----------------+-----------+-------------+----------+--------+------+------------+---------+---------------+---------+ | Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment | Visible | +--------+------------+----------------+--------------+----------------+-----------+-------------+----------+--------+------+------------+---------+---------------+---------+ | orders | 0 | PRIMARY | 1 | orderNumber | A | 326 | NULL | NULL | | BTREE | | | YES | | orders | 1 | customerNumber | 1 | customerNumber | A | 98 | NULL | NULL | | BTREE | | | YES | +--------+------------+----------------+--------------+----------------+-----------+-------------+----------+--------+------+------------+---------+---------------+---------+ 2 rows in set (0.01 sec) In the output, the PRIMARY KEY for the orderNumber column shows the table has 326 unique values, while the custmerNaumber column only has 98 distinct values. As mentioned earlier, index statistics are only approximate and may not represent the real size of the rows in the table. To generate more accurate statistical information, you use the ANALYZE TABLE command.","title":"Index cardinality"},{"location":"Databases/MySQL/advanced/13_indexes/#use-index-hint","text":"In MySQL, when you submit an SQL query, the query optimizer will try to make an optimal query execution plan. To determine the best possible plan, the query optimizer makes use of many parameters. One of the most important parameters for choosing which index to use is stored key distribution which is also known as cardinality. The cardinality, however, may be not accurate for example in case the table ghas been modified heavily with many inserts or deletes. To solve this issue, you should run the ANALYZE TABLE statement periodically to update the cardinality. In addition, MySQL provides an alternative way that allows you to recomment the indexes that the query optimizer should by using an index hind called USE INDEX . The following illustrates syntax of the MySQL USE INDEX hint: SELECT select_list FROM table_name USE INDEX(index_list) WHERE condition; In this syntax, the USE INDEX instructs the query optimizer to use one of the named indexes to find rows in the table. Notice that when you recomment the indexes to use, the query optimizer may either decide to use them or not depending on the query plan that it comes up with.","title":"USE INDEX Hint"},{"location":"Databases/MySQL/advanced/13_indexes/#force-index","text":"The query optimizer is a component in the MySQL database server that makes the most optional execution plan for an SQL statement. The query optimizer uses the available statistics to come up with the plan that has the lowest cost among all candidate plans. For example, a query migh request for products whose prices are betweeen 10 and 80. If the statistics show that 80% of products have these price ranges, then it may decide that a full table scan is the most efficient. However if statistics show that very few products have these price ranges, then reading an index followed by a table access could be faster and more efficient that a full table scan. In case the query optimizer ignores the index, you can use the FORCE INDEX hint to instruct it to use the index instead. The following illustrates the FORCE INDEX hint syntax: SELECT * FROM table_name FORCE INDEX (index_list) WHERE condition; SELECT productName, buyPrice FROM products FORCE INDEX (idx_buyPrice) WHERE buyPrice BETWEEN 10 AND 80 ORDER BY buyPrice;","title":"Force Index"},{"location":"Databases/MySQL/advanced/14_administration/","text":"MySQL administration \u00b6 Getting started with MySQL Access Control system \u00b6 MySQL implements a sophisticated access control and privilege system that allows you to create comprehensive access rules for handling client operations and effectively preventing unauthorized clients from accessing the database system. The MySQL access control has two stages when a client connects to the server: - Connection verification: aclient, which connects to the MySQL database server, needs to have a valid username and password. In addition, the host from which the client connects has to match the host in the MySQL grant table. - Request verification: once a connection is established successfully, for each statement issued by the client, MySQL checks whether the client has sufficient privileges to execute that particular statement. mySQL has the ability to check a privilege at the database, table, and field levels. There is a datbase named mysql created automatically by MySQL installer. The mysql database contains five main grant tables. You often manipulate these tables indirectly through the statements such as GRAND and REVOKE . - user - contains user account and global privilege columns. MySQL uses the user table to either accept or reject a connection from a host. A privilege granted in the user table is effective to all databases on the MySQL server. - db - contains database level privileges. MySQL uses the db table to determine which database a user can access and from which host. A privilege granted a the database level in the db table applies to the database and all objects belong to that database e.g. tables, triggers, views, stored procedures etc. - table_priv and columns_priv - contains table-level and column-level privileges. A privilege granted in the table_priv table applies to the table and it's columns while a privilege granted in columns_priv table applies only to a specific column of a table. procs_priv contains stored functions and stored procedures privileges. MySQL makes use of these tables to control the privileges of MySQL database server. Understanding tables is very important before you can implement you own flexible access control system. Create user accounts using CREATE USER statement \u00b6 In MySQL , you can specify not only who can connect to the database server but also from which host that the user connects. Therefore, a user account in MySQL consists of a username and a host name separated by the @ character. For example, if the admin user connects to the MySQL database server from localhost , the user account is admin@localhost . The admin user only can connect to the MySQL database server from the localhost , not from a remote host. This makes the MySQL database server even more secure. In addition, by combining the username and host, it is possible to setup multiple accounts with the same name but can connect from different hosts with different privileges. MySQL stored the user accounts in the user grants table of the mysql database. Creating user accounts using MySQL CREATE USER statement \u00b6 MySQL provides CREATE USER statement that allows you to create a new user account. The syntax of the CREATE USER statement is as follows: CREATE USER user_account IDENTIFIED BY password; The user_acount in the format 'username'@'hostname' is followed by the CREATE USER clause. The password is specified in the IDENTIFIED BY clause. The password must be in clear text. MySQL will encrypt the password before saving the user account into the user table. CREATE USER dbadmin@localhost IDENTIFIED BY 'secret'; To view the privileges of a user account, you use the SHOW GRANTS statements as follows: SHOW GRANTS FOR dbadmin@localhost; +---------------------------------------------+ | Grants for dbadmin@localhost | +---------------------------------------------+ | GRANT USAGE ON *.* TO `dbadmin`@`localhost` | +---------------------------------------------+ 1 row in set (0.00 sec) The . n the result shows that the dbadmin user account can only login to the database server and has no other privileges. To grant permission to the user, you use the GRANT statement. Note that the part before the dot represents the database and the dot represents the table. To allow a user to connect from any host, you use the percent (%) wildcard as shown in the example: CREATE USER superadmin@'%' IDENTIFIED BY 'secret'; The percent wildcard has the same effect as it is used in the LIKE operator, e.g., to allow mysqladmin user account to connect to the database server from any subdomain of the mysqltutorial.org host, you use the percent wildcard % as follows: CREATE USER mysqladmin@'%.mysqltutorial.org' IDENTIFIED by 'secret'; You can also use the underscode wilcard _ . If you omit the hostname part of the user account, MySQL will accept it and allow the user to connect from any host. Change user password \u00b6 Using UPDATE statement \u00b6 The first way to change the password is to use the UPDATE statement to update the user table of the mysql database. After executing the UPDATE statement, you also need to execute the FLUSH PRIVILEGES statement to reload privileges from the grant table in the mysql database. USE mysql; UPDATE user SET password = PASSWORD('dolphin') WHERE user = 'dbadmin' AND host = 'localhost'; FLUSH PRIVILEGES; Note that from MySQL 5.7.6, the user table uses the authentication_string column only to store the password. In addition, it removed the password column. Therefore if you use MySQL 5.7.6+, you must use the authentication_string column in the UPDATE statement instead: USE mysql; UPDATE user SET authentication_string = PASSWORD('dolphin') WHERE user = 'dbadmin' AND host = 'localhost'; FLUSH PRIVILEGES; Notice that the PASSWORD() function computes the hash value from plain text. Change user password using the SET PASSWORD \u00b6 SET PASSWORD FOR 'dbadmin'@'localhost' = PASSWORD('bigshark'); From version 5.7.6, MySQL deprecated this syintax and may remove it in the future releases. Instead, it uses the plaintet password as follows: SET PASSWORD FOR 'dbadmin'@'localhost' = bigshark; Change mysql user password using ALTER USER \u00b6 ALTER USER dbadmin@localhost IDENTIFIED BY 'lit In case you want to reset the password of the MySQL root account, you need to force the MySQL database server to stop and restart without using grant table validation. Use GRANT statement to grant privileges to a user \u00b6 After creating a new user account, the user doesnt have any privileges. To grant privileges to a user account, you use the GRANT statement: GRANT privilege,[privilege],.. ON privilege_level TO user [IDENTIFIED BY password] [REQUIRE tsl_option] [WITH [GRANT_OPTION | resource_option]]; First, specify one or more privileges after the GRANT keyword. If you grant the user multiple privileges, each privilege is separated by a comma. Next, specify the privilege_level that determines the level at which the privileges apply. MySQL supports global ( *.* ), database ( database.* ), table ( database.table ) and columns levels. If you use column privilege level, you must specify one or a list of comma-separated column after each privilege. Then, place the user that you want to grant privileges. If user already exists, the GRANT statement modifies its privilege. Otherwise, the GRANT statement creates a new user. The optional clause IDENTIFIED BY allows you set a new password for the user. After that, you specify whether the user has to connect to the database server over a secure connection such as SSL, X059, etc. Finally, the optional WITH GRANT option clause allows you to grant other users or remove from other users the privileges that you possess. In addition, you can use the WITH clause to allocate MySQL database server's resource e.g., to set how many connections or statements that the user can use per hour. This is very helpful in the shared environments such as MySQL shared hosting. Notice that in order to use the GRANT statement, you must have the GRANT OPTION privilege and the privileges that you are granting. If the read_only system variable is enabled, you need to have the SUPER privilege to execute the GRANT statement. CREATE USER super@localhost IDENTIFIED BY 'dolphin'; GRANT ALL ON *.* TO 'super'@'localhost' WITH GRANT OPTION; CREATE USER auditor@localhost IDENTIFIED BY 'whale'; GRANT ALL ON classicmodels.* TO auditor@localhost; CREATE USER rfc IDENTIFIED BY 'shark'; GRANT SELECT, UPDATE, DELETE ON classicmodels.* TO rfc; The following table illustrates all permissible privileges that you can use the GRANT and REVOKE statement: Privilege Meaning Level Global Database Table Column Procedure Proxy ALL [PRIVILEGES] Grant all privileges at specified access level except GRANT OPTION ALTER Allow user to use of ALTER TABLE statement X X X ALTER ROUTINE Allow user to alter or drop stored routine X X X CREATE Allow user to create database and table X X X CREATE ROUTINE Allow user to create stored routine X X CREATE TABLESPACE Allow user to create, alter or drop tablespaces and log file groups X CREATE TEMPORARY TABLES Allow user to create temporary table by using CREATE TEMPORARY TABLE X X CREATE USER Allow user to use the CREATE USER, DROP USER, RENAME USER, and REVOKE ALL PRIVILEGES statements. X CREATE VIEW Allow user to create or modify view. X X X DELETE Allow user to use DELETE X X X DROP Allow user to drop database, table and view X X X EVENT Enable use of events for the Event Scheduler. X X EXECUTE Allow user to execute stored routines X X X FILE Allow user to read any file in the database directory. X GRANT OPTION Allow user to have privileges to grant or revoke privileges from other accounts. X X X X X INDEX Allow user to create or remove indexes. X X X INSERT Allow user to use INSERT statement X X X X LOCK TABLES Allow user to use LOCK TABLES on tables for which you have the SELECT privilege X X PROCESS Allow user to see all processes with SHOW PROCESSLIST statement. X PROXY Enable user proxying. REFERENCES Allow user to create foreign key X X X X RELOAD Allow user to use FLUSH operations X REPLICATION CLIENT Allow user to query to see where master or slave servers are X REPLICATION SLAVE Allow the user to use replicate slaves to read binary log events from the master. X SELECT Allow user to use SELECT statement X X X X SHOW DATABASES Allow user to show all databases X SHOW VIEW Allow user to use SHOW CREATE VIEW statement X X X SHUTDOWN Allow user to use mysqladmin shutdown command X SUPER Allow user to use other administrative operations such as CHANGE MASTER TO, KILL, PURGE BINARY LOGS, SET GLOBAL, and mysqladmin command X TRIGGER Allow user to use TRIGGER operations. X X X UPDATE Allow user to use UPDATE statement X X X X USAGE Equivalent to \u201cno privileges\u201d Revoking privileges from users using MySQL REVOKE \u00b6 REVOKE privilege_type [(column_list)] [, priv_type [(column_list)]]... ON [object_type] privilege_level FROM user [, user].. Revoke all privileges from a user REVOKE ALL PRIVILEGES, GRANT OPTION FROM user [, user]\u2026 Revoke proxy user REVOKE PROXY ON user FROM user [, user]... View grants SHOW GRANTS FOR user; CREATE USER IF EXISTS rfc IDENTIFIED BY 'dolphin'; GRANT SELECT, UPDATE, DELETE ON classicmodels.* TO rfc; REVOKE UPDATE, DELETE ON classicmodels.* FROM rfc; MySQL Roles \u00b6 Typically, you have a number of users with the same set of privileges. Previously the only way to grant and revoke privileges to multiple users is to change privileges of each user individually, which is time-consuming. To make it easier, MySQL provided a new object called role that is named collection of privileges. If you want to grant the same set of privileges to multiple users you should do as follows: 1. Create a new role 2. Grant privileges to the role 3. Grant the role to the users In case you want to change the privileges of the users, you need to change the privileges of the granted role only. The changes will take effect to all users to which the role granted. CREATE ROLE crm_dev, crm_read, crm_write; GRANT ALL ON crm.* TO crm_dev; GRANT SELECT ON crm.* TO crm_read; GRANT INSERT, UPDATE, DELETE ON crm.* TO crm_write; -- developer user CREATE USER crm_dev1@localhost IDENTIFIED BY 'Secure$1782'; -- read access user CREATE USER crm_read1@localhost IDENTIFIED BY 'Secure$5432'; -- read/write users CREATE USER crm_write1@localhost IDENTIFIED BY 'Secure$9075'; CREATE USER crm_write2@localhost IDENTIFIED BY 'Secure$3452'; GRANT crm_dev TO crm_dev1@localhost; GRANT crm_read TO crm_read1@localhost; GRANT crm_read, crm_write TO crm_write1@localhost, crm_write2@localhost; Set default role SET DEFAULT ROLE ALL TO crm_read1@localhost; REVOKE INSERT, UPDATE, DELETE ON crm.* FROM crm_write; GRANT INSERT, UPDATE, DELETE ON crm.* FOR crm_write; DROP ROLE role_name, role_name, ...; DROP ROLE crm_read, crm_write; Copy privileges from one user to another GRANT crm_dev1@localhost TO crm_dev2@localhost; Setting active roles \u00b6 A user account can modify the current user's effective privileges within the current session by specifying which granted role is active. The following statement set the active role to NONE , meaning no active role. SET ROLE NONE; To set active roles to all granted role, you use: SET ROLE ALL; To set active roles to default roles that set by the SET DEFAULT ROLE statement, you use: SET ROLE DEFAULT; To set active named roles you use: SET ROLE granted_role_1, granted_role_2; ```` ## Remove user accounts using DROP USER statement ```sql DROP USER dbadmin@mysqltutorial.org; If the user has an active session, you should kill the session before using DROP USER . It can be done by finding the process id with SHOW PROCESSLIST and using the KILL command. Maintaining MySQL database tables \u00b6 MySQL provides several useful statements that allow you to maintain database tables effectively. Those statements enable you to analyze, optimize, check and repair database tables. Analyze table statement \u00b6 MySQL query optimizer is an important component of the MySQL server that creates an optimal query execution plan for a query. For a particular query, the query optimizer uses the stored key distribution and other factors to decide the order in which tables should be joined when you performing the join, and which index should be used for a specific table. However, the key distributions can be sometimes inaccurate e.g., after you have done a lot of data changes in the table including insert, delete or update. If the key distribution is not accurate, the query optimizer may pick a bad query execution plan that may cause a severe performance issue. To solve this problem you can run the ANALYZE TABLE statement for the table e.g., the following stateement analyzes the payments table: ANALYZE TABLE payments; If there is no change to the table since the ANALYZE TABLE statement ran, MySQL will not analyze the table again. If you run the statement again, it will say it is already up to date. Optimize table statement \u00b6 While working with the database, you do a log of changes surch as inser, update and delete data in the table that may cause the physical storage of the table fragmented. As a result the performance of database server is degraded. MySQL provides you wih a statement that allows you to optimize the table to avoid this defragmenting problem. The following illustrates how to optimize a table: OPTIMIZE TABLE table_name; It is recommended that you execute this statement for the tables that are updated frequently. Check table statement \u00b6 Something wrong can happen to the database server e.g., the server was shut down unexpectedly, error while writing data to the hard disk, etc. These situations could make the database operate incorrectly and in the worst case, it can be crashed. MySQL allows you to check the integrity of database tables by using the CHECK TABLE statement. The following illustrates the syntax of the CHECK TABLE statement: CHECK TABLE table_name; The CHECK TABLE statement checks both table and its indexes. This statement only detects problems in a database table but it does not repair them. To repair the table, you use REPAIR TABLE statement. Repair table statement \u00b6 The REPAIR TABLE statement allows you to repair some errors occurred in database tables. MySQL does not guarantee that the REPAIR TABLE statement can repair all errors that the tables may have. REPAIR TABLE table_name Backup databases using mysqldump tool \u00b6 mysqldump -u [username] \u2013p[password] [database_name] > [dump_file.sql] Backup structure only mysqldump -u [username] \u2013p[password] \u2013no-data [database_name] > [dump_file.sql] Backup multiple databases into a single file mysqldump -u [username] \u2013p[password] [dbname1,dbname2,\u2026] > [dump_file.sql] mysqldump -u [username] \u2013p[password] \u2013all-database > [dump_file.sql] Show databases: list all databases in MySQL \u00b6 mysql> SHOW DATABASES; +--------------------+ | Database | +--------------------+ | classicmodels | | information_schema | | mysql | | performance_schema | | sys | | test | +--------------------+ 6 rows in set (0.00 sec) SHOW DATABASES LIKE '%schema'; +--------------------+ | Database (%schema) | +--------------------+ | information_schema | | performance_schema | +--------------------+ 2 rows in set (0.00 sec) SELECT schema_name FROM information_schema.schemata WHERE schema_name LIKE '%schema' OR schema_name LIKE '%s'; +--------------------+ | SCHEMA_NAME | +--------------------+ | information_schema | | performance_schema | | sys | | classicmodels | +--------------------+ 4 rows in set (0.00 sec) Show tables: List tables in database \u00b6 +--------------------+ | SCHEMA_NAME | +--------------------+ | information_schema | | performance_schema | | sys | | classicmodels | +--------------------+ 4 rows in set (0.00 sec) Include table type (base table or view) > SHOW FULL TABLES +-------------------------+------------+ | Tables_in_classicmodels | Table_type | +-------------------------+------------+ | contacts | VIEW | | customers | BASE TABLE | | employees | BASE TABLE | | offices | BASE TABLE | | orderdetails | BASE TABLE | | orders | BASE TABLE | | payments | BASE TABLE | | productlines | BASE TABLE | | products | BASE TABLE | +-------------------------+------------+ 9 rows in set (0.00 sec) > SHOW TABLES LIKE 'p%'; +------------------------------+ | Tables_in_classicmodels (p%) | +------------------------------+ | payments | | productlines | | products | +------------------------------+ 3 rows in set (0.00 sec) > SHOW FULL TABLES WHERE table_type = 'VIEW'; +-------------------------+------------+ | Tables_in_classicmodels | Table_type | +-------------------------+------------+ | contacts | VIEW | +-------------------------+------------+ 1 row in set (0.00 sec) Show columns and describe \u00b6 mysql> DESCRIBE orders; +----------------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +----------------+-------------+------+-----+---------+-------+ | orderNumber | int(11) | NO | PRI | NULL | | | orderDate | date | NO | | NULL | | | requiredDate | date | NO | | NULL | | | shippedDate | date | YES | | NULL | | | status | varchar(15) | NO | | NULL | | | comments | text | YES | | NULL | | | customerNumber | int(11) | NO | MUL | NULL | | +----------------+-------------+------+-----+---------+-------+ 7 rows in set (0.01 sec) SHOW COLUMNS FROM table_name; SHOW COLUMNS FROM database_name.table_name; SHOW COLUMNS FROM table_name IN database_name; SHOW FULL COLUMNS FROM table_name; mysql> SHOW FULL COLUMNS FROM payments \\G; *************************** 1. row *************************** Field: customerNumber Type: int(11) Collation: NULL Null: NO Key: PRI Default: NULL Extra: Privileges: select,insert,update,references Comment: *************************** 2. row *************************** Field: checkNumber Type: varchar(50) Collation: latin1_swedish_ci Null: NO Key: PRI Default: NULL Extra: Privileges: select,insert,update,references Comment: *************************** 3. row *************************** Field: paymentDate Type: date Collation: NULL Null: NO Key: Default: NULL Extra: Privileges: select,insert,update,references Comment: *************************** 4. row *************************** Field: amount Type: decimal(10,2) Collation: NULL Null: NO Key: Default: NULL Extra: Privileges: select,insert,update,references Comment: 4 rows in set (0.01 sec) mysql> SHOW COLUMNS FROM payments LIKE 'c%'; +----------------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +----------------+-------------+------+-----+---------+-------+ | customerNumber | int(11) | NO | PRI | NULL | | | checkNumber | varchar(50) | NO | PRI | NULL | | +----------------+-------------+------+-----+---------+-------+ 2 rows in set (0.01 sec) List all users in server \u00b6 SELECT user from mysql.user Describe user DESC user; Show current user mysql> SELECT user(); +-----------------+ | user() | +-----------------+ | local@localhost | +-----------------+ 1 row in set (0.00 sec) mysql> SELECT current_user(); +----------------+ | current_user() | +----------------+ | local@localhost | +----------------+ 1 row in set (0.00 sec) Show current logged users SELECT user, host, db, command FROM information_schema.processlist; +-------+-----------------+---------------+---------+ | user | host | db | command | +-------+-----------------+---------------+---------+ | local | localhost:50591 | classicmodels | Sleep | | root | localhost:50557 | NULL | Query | +-------+-----------------+---------------+---------+ 2 rows in set (0.00 sec) Show processlist \u00b6 Sometimes you may get \"too many connections\" error returned by the server. To find out the reasons you can use SHOW PROCESSLIST command and use the KILL statement to kill the idle threads. SHOW [FULL] PROCESSLIST; mysql>SHOW PROCESSLIST; +----+-----------------+-----------------+---------------+---------+------+------------------------+------------------+ | Id | User | Host | db | Command | Time | State | Info | +----+-----------------+-----------------+---------------+---------+------+------------------------+------------------+ | 4 | event_scheduler | localhost | NULL | Daemon | 2246 | Waiting on empty queue | NULL | | 14 | root | localhost:50924 | NULL | Query | 0 | starting | SHOW PROCESSLIST | | 15 | car | localhost:50933 | classicmodels | Sleep | 2 | | NULL | +----+-----------------+-----------------+---------------+---------+------+------------------------+------------------+ 3 rows in set (0.00 sec) id - the client process id usder - the username associated with the thread host - the host to which the client is connected db - the default database if one selected otherwise NULL . command - the command type time - the number of seconds that the current thread has been in its current state state - the thread state which represents an action, event, or state that indicates what thread is executing. info the statement is executed, or NULL if it is not executing any statement. If you do not use the FULL keyword in the SHOW PROCESSLIST command, then only the first 100 characters of each statement are returned in the info column.","title":"MySQL administration"},{"location":"Databases/MySQL/advanced/14_administration/#mysql-administration","text":"","title":"MySQL administration"},{"location":"Databases/MySQL/advanced/14_administration/#getting-started-with-mysql-access-control-system","text":"MySQL implements a sophisticated access control and privilege system that allows you to create comprehensive access rules for handling client operations and effectively preventing unauthorized clients from accessing the database system. The MySQL access control has two stages when a client connects to the server: - Connection verification: aclient, which connects to the MySQL database server, needs to have a valid username and password. In addition, the host from which the client connects has to match the host in the MySQL grant table. - Request verification: once a connection is established successfully, for each statement issued by the client, MySQL checks whether the client has sufficient privileges to execute that particular statement. mySQL has the ability to check a privilege at the database, table, and field levels. There is a datbase named mysql created automatically by MySQL installer. The mysql database contains five main grant tables. You often manipulate these tables indirectly through the statements such as GRAND and REVOKE . - user - contains user account and global privilege columns. MySQL uses the user table to either accept or reject a connection from a host. A privilege granted in the user table is effective to all databases on the MySQL server. - db - contains database level privileges. MySQL uses the db table to determine which database a user can access and from which host. A privilege granted a the database level in the db table applies to the database and all objects belong to that database e.g. tables, triggers, views, stored procedures etc. - table_priv and columns_priv - contains table-level and column-level privileges. A privilege granted in the table_priv table applies to the table and it's columns while a privilege granted in columns_priv table applies only to a specific column of a table. procs_priv contains stored functions and stored procedures privileges. MySQL makes use of these tables to control the privileges of MySQL database server. Understanding tables is very important before you can implement you own flexible access control system.","title":"Getting started with MySQL Access Control system"},{"location":"Databases/MySQL/advanced/14_administration/#create-user-accounts-using-create-user-statement","text":"In MySQL , you can specify not only who can connect to the database server but also from which host that the user connects. Therefore, a user account in MySQL consists of a username and a host name separated by the @ character. For example, if the admin user connects to the MySQL database server from localhost , the user account is admin@localhost . The admin user only can connect to the MySQL database server from the localhost , not from a remote host. This makes the MySQL database server even more secure. In addition, by combining the username and host, it is possible to setup multiple accounts with the same name but can connect from different hosts with different privileges. MySQL stored the user accounts in the user grants table of the mysql database.","title":"Create user accounts using CREATE USER statement"},{"location":"Databases/MySQL/advanced/14_administration/#creating-user-accounts-using-mysql-create-user-statement","text":"MySQL provides CREATE USER statement that allows you to create a new user account. The syntax of the CREATE USER statement is as follows: CREATE USER user_account IDENTIFIED BY password; The user_acount in the format 'username'@'hostname' is followed by the CREATE USER clause. The password is specified in the IDENTIFIED BY clause. The password must be in clear text. MySQL will encrypt the password before saving the user account into the user table. CREATE USER dbadmin@localhost IDENTIFIED BY 'secret'; To view the privileges of a user account, you use the SHOW GRANTS statements as follows: SHOW GRANTS FOR dbadmin@localhost; +---------------------------------------------+ | Grants for dbadmin@localhost | +---------------------------------------------+ | GRANT USAGE ON *.* TO `dbadmin`@`localhost` | +---------------------------------------------+ 1 row in set (0.00 sec) The . n the result shows that the dbadmin user account can only login to the database server and has no other privileges. To grant permission to the user, you use the GRANT statement. Note that the part before the dot represents the database and the dot represents the table. To allow a user to connect from any host, you use the percent (%) wildcard as shown in the example: CREATE USER superadmin@'%' IDENTIFIED BY 'secret'; The percent wildcard has the same effect as it is used in the LIKE operator, e.g., to allow mysqladmin user account to connect to the database server from any subdomain of the mysqltutorial.org host, you use the percent wildcard % as follows: CREATE USER mysqladmin@'%.mysqltutorial.org' IDENTIFIED by 'secret'; You can also use the underscode wilcard _ . If you omit the hostname part of the user account, MySQL will accept it and allow the user to connect from any host.","title":"Creating user accounts using MySQL CREATE USER statement"},{"location":"Databases/MySQL/advanced/14_administration/#change-user-password","text":"","title":"Change user password"},{"location":"Databases/MySQL/advanced/14_administration/#using-update-statement","text":"The first way to change the password is to use the UPDATE statement to update the user table of the mysql database. After executing the UPDATE statement, you also need to execute the FLUSH PRIVILEGES statement to reload privileges from the grant table in the mysql database. USE mysql; UPDATE user SET password = PASSWORD('dolphin') WHERE user = 'dbadmin' AND host = 'localhost'; FLUSH PRIVILEGES; Note that from MySQL 5.7.6, the user table uses the authentication_string column only to store the password. In addition, it removed the password column. Therefore if you use MySQL 5.7.6+, you must use the authentication_string column in the UPDATE statement instead: USE mysql; UPDATE user SET authentication_string = PASSWORD('dolphin') WHERE user = 'dbadmin' AND host = 'localhost'; FLUSH PRIVILEGES; Notice that the PASSWORD() function computes the hash value from plain text.","title":"Using UPDATE statement"},{"location":"Databases/MySQL/advanced/14_administration/#change-user-password-using-the-set-password","text":"SET PASSWORD FOR 'dbadmin'@'localhost' = PASSWORD('bigshark'); From version 5.7.6, MySQL deprecated this syintax and may remove it in the future releases. Instead, it uses the plaintet password as follows: SET PASSWORD FOR 'dbadmin'@'localhost' = bigshark;","title":"Change user password using the SET PASSWORD"},{"location":"Databases/MySQL/advanced/14_administration/#change-mysql-user-password-using-alter-user","text":"ALTER USER dbadmin@localhost IDENTIFIED BY 'lit In case you want to reset the password of the MySQL root account, you need to force the MySQL database server to stop and restart without using grant table validation.","title":"Change mysql user password using ALTER USER"},{"location":"Databases/MySQL/advanced/14_administration/#use-grant-statement-to-grant-privileges-to-a-user","text":"After creating a new user account, the user doesnt have any privileges. To grant privileges to a user account, you use the GRANT statement: GRANT privilege,[privilege],.. ON privilege_level TO user [IDENTIFIED BY password] [REQUIRE tsl_option] [WITH [GRANT_OPTION | resource_option]]; First, specify one or more privileges after the GRANT keyword. If you grant the user multiple privileges, each privilege is separated by a comma. Next, specify the privilege_level that determines the level at which the privileges apply. MySQL supports global ( *.* ), database ( database.* ), table ( database.table ) and columns levels. If you use column privilege level, you must specify one or a list of comma-separated column after each privilege. Then, place the user that you want to grant privileges. If user already exists, the GRANT statement modifies its privilege. Otherwise, the GRANT statement creates a new user. The optional clause IDENTIFIED BY allows you set a new password for the user. After that, you specify whether the user has to connect to the database server over a secure connection such as SSL, X059, etc. Finally, the optional WITH GRANT option clause allows you to grant other users or remove from other users the privileges that you possess. In addition, you can use the WITH clause to allocate MySQL database server's resource e.g., to set how many connections or statements that the user can use per hour. This is very helpful in the shared environments such as MySQL shared hosting. Notice that in order to use the GRANT statement, you must have the GRANT OPTION privilege and the privileges that you are granting. If the read_only system variable is enabled, you need to have the SUPER privilege to execute the GRANT statement. CREATE USER super@localhost IDENTIFIED BY 'dolphin'; GRANT ALL ON *.* TO 'super'@'localhost' WITH GRANT OPTION; CREATE USER auditor@localhost IDENTIFIED BY 'whale'; GRANT ALL ON classicmodels.* TO auditor@localhost; CREATE USER rfc IDENTIFIED BY 'shark'; GRANT SELECT, UPDATE, DELETE ON classicmodels.* TO rfc; The following table illustrates all permissible privileges that you can use the GRANT and REVOKE statement: Privilege Meaning Level Global Database Table Column Procedure Proxy ALL [PRIVILEGES] Grant all privileges at specified access level except GRANT OPTION ALTER Allow user to use of ALTER TABLE statement X X X ALTER ROUTINE Allow user to alter or drop stored routine X X X CREATE Allow user to create database and table X X X CREATE ROUTINE Allow user to create stored routine X X CREATE TABLESPACE Allow user to create, alter or drop tablespaces and log file groups X CREATE TEMPORARY TABLES Allow user to create temporary table by using CREATE TEMPORARY TABLE X X CREATE USER Allow user to use the CREATE USER, DROP USER, RENAME USER, and REVOKE ALL PRIVILEGES statements. X CREATE VIEW Allow user to create or modify view. X X X DELETE Allow user to use DELETE X X X DROP Allow user to drop database, table and view X X X EVENT Enable use of events for the Event Scheduler. X X EXECUTE Allow user to execute stored routines X X X FILE Allow user to read any file in the database directory. X GRANT OPTION Allow user to have privileges to grant or revoke privileges from other accounts. X X X X X INDEX Allow user to create or remove indexes. X X X INSERT Allow user to use INSERT statement X X X X LOCK TABLES Allow user to use LOCK TABLES on tables for which you have the SELECT privilege X X PROCESS Allow user to see all processes with SHOW PROCESSLIST statement. X PROXY Enable user proxying. REFERENCES Allow user to create foreign key X X X X RELOAD Allow user to use FLUSH operations X REPLICATION CLIENT Allow user to query to see where master or slave servers are X REPLICATION SLAVE Allow the user to use replicate slaves to read binary log events from the master. X SELECT Allow user to use SELECT statement X X X X SHOW DATABASES Allow user to show all databases X SHOW VIEW Allow user to use SHOW CREATE VIEW statement X X X SHUTDOWN Allow user to use mysqladmin shutdown command X SUPER Allow user to use other administrative operations such as CHANGE MASTER TO, KILL, PURGE BINARY LOGS, SET GLOBAL, and mysqladmin command X TRIGGER Allow user to use TRIGGER operations. X X X UPDATE Allow user to use UPDATE statement X X X X USAGE Equivalent to \u201cno privileges\u201d","title":"Use GRANT statement to grant privileges to a user"},{"location":"Databases/MySQL/advanced/14_administration/#revoking-privileges-from-users-using-mysql-revoke","text":"REVOKE privilege_type [(column_list)] [, priv_type [(column_list)]]... ON [object_type] privilege_level FROM user [, user].. Revoke all privileges from a user REVOKE ALL PRIVILEGES, GRANT OPTION FROM user [, user]\u2026 Revoke proxy user REVOKE PROXY ON user FROM user [, user]... View grants SHOW GRANTS FOR user; CREATE USER IF EXISTS rfc IDENTIFIED BY 'dolphin'; GRANT SELECT, UPDATE, DELETE ON classicmodels.* TO rfc; REVOKE UPDATE, DELETE ON classicmodels.* FROM rfc;","title":"Revoking privileges from users using MySQL REVOKE"},{"location":"Databases/MySQL/advanced/14_administration/#mysql-roles","text":"Typically, you have a number of users with the same set of privileges. Previously the only way to grant and revoke privileges to multiple users is to change privileges of each user individually, which is time-consuming. To make it easier, MySQL provided a new object called role that is named collection of privileges. If you want to grant the same set of privileges to multiple users you should do as follows: 1. Create a new role 2. Grant privileges to the role 3. Grant the role to the users In case you want to change the privileges of the users, you need to change the privileges of the granted role only. The changes will take effect to all users to which the role granted. CREATE ROLE crm_dev, crm_read, crm_write; GRANT ALL ON crm.* TO crm_dev; GRANT SELECT ON crm.* TO crm_read; GRANT INSERT, UPDATE, DELETE ON crm.* TO crm_write; -- developer user CREATE USER crm_dev1@localhost IDENTIFIED BY 'Secure$1782'; -- read access user CREATE USER crm_read1@localhost IDENTIFIED BY 'Secure$5432'; -- read/write users CREATE USER crm_write1@localhost IDENTIFIED BY 'Secure$9075'; CREATE USER crm_write2@localhost IDENTIFIED BY 'Secure$3452'; GRANT crm_dev TO crm_dev1@localhost; GRANT crm_read TO crm_read1@localhost; GRANT crm_read, crm_write TO crm_write1@localhost, crm_write2@localhost; Set default role SET DEFAULT ROLE ALL TO crm_read1@localhost; REVOKE INSERT, UPDATE, DELETE ON crm.* FROM crm_write; GRANT INSERT, UPDATE, DELETE ON crm.* FOR crm_write; DROP ROLE role_name, role_name, ...; DROP ROLE crm_read, crm_write; Copy privileges from one user to another GRANT crm_dev1@localhost TO crm_dev2@localhost;","title":"MySQL Roles"},{"location":"Databases/MySQL/advanced/14_administration/#setting-active-roles","text":"A user account can modify the current user's effective privileges within the current session by specifying which granted role is active. The following statement set the active role to NONE , meaning no active role. SET ROLE NONE; To set active roles to all granted role, you use: SET ROLE ALL; To set active roles to default roles that set by the SET DEFAULT ROLE statement, you use: SET ROLE DEFAULT; To set active named roles you use: SET ROLE granted_role_1, granted_role_2; ```` ## Remove user accounts using DROP USER statement ```sql DROP USER dbadmin@mysqltutorial.org; If the user has an active session, you should kill the session before using DROP USER . It can be done by finding the process id with SHOW PROCESSLIST and using the KILL command.","title":"Setting active roles"},{"location":"Databases/MySQL/advanced/14_administration/#maintaining-mysql-database-tables","text":"MySQL provides several useful statements that allow you to maintain database tables effectively. Those statements enable you to analyze, optimize, check and repair database tables.","title":"Maintaining MySQL database tables"},{"location":"Databases/MySQL/advanced/14_administration/#analyze-table-statement","text":"MySQL query optimizer is an important component of the MySQL server that creates an optimal query execution plan for a query. For a particular query, the query optimizer uses the stored key distribution and other factors to decide the order in which tables should be joined when you performing the join, and which index should be used for a specific table. However, the key distributions can be sometimes inaccurate e.g., after you have done a lot of data changes in the table including insert, delete or update. If the key distribution is not accurate, the query optimizer may pick a bad query execution plan that may cause a severe performance issue. To solve this problem you can run the ANALYZE TABLE statement for the table e.g., the following stateement analyzes the payments table: ANALYZE TABLE payments; If there is no change to the table since the ANALYZE TABLE statement ran, MySQL will not analyze the table again. If you run the statement again, it will say it is already up to date.","title":"Analyze table statement"},{"location":"Databases/MySQL/advanced/14_administration/#optimize-table-statement","text":"While working with the database, you do a log of changes surch as inser, update and delete data in the table that may cause the physical storage of the table fragmented. As a result the performance of database server is degraded. MySQL provides you wih a statement that allows you to optimize the table to avoid this defragmenting problem. The following illustrates how to optimize a table: OPTIMIZE TABLE table_name; It is recommended that you execute this statement for the tables that are updated frequently.","title":"Optimize table statement"},{"location":"Databases/MySQL/advanced/14_administration/#check-table-statement","text":"Something wrong can happen to the database server e.g., the server was shut down unexpectedly, error while writing data to the hard disk, etc. These situations could make the database operate incorrectly and in the worst case, it can be crashed. MySQL allows you to check the integrity of database tables by using the CHECK TABLE statement. The following illustrates the syntax of the CHECK TABLE statement: CHECK TABLE table_name; The CHECK TABLE statement checks both table and its indexes. This statement only detects problems in a database table but it does not repair them. To repair the table, you use REPAIR TABLE statement.","title":"Check table statement"},{"location":"Databases/MySQL/advanced/14_administration/#repair-table-statement","text":"The REPAIR TABLE statement allows you to repair some errors occurred in database tables. MySQL does not guarantee that the REPAIR TABLE statement can repair all errors that the tables may have. REPAIR TABLE table_name","title":"Repair table statement"},{"location":"Databases/MySQL/advanced/14_administration/#backup-databases-using-mysqldump-tool","text":"mysqldump -u [username] \u2013p[password] [database_name] > [dump_file.sql] Backup structure only mysqldump -u [username] \u2013p[password] \u2013no-data [database_name] > [dump_file.sql] Backup multiple databases into a single file mysqldump -u [username] \u2013p[password] [dbname1,dbname2,\u2026] > [dump_file.sql] mysqldump -u [username] \u2013p[password] \u2013all-database > [dump_file.sql]","title":"Backup databases using mysqldump tool"},{"location":"Databases/MySQL/advanced/14_administration/#show-databases-list-all-databases-in-mysql","text":"mysql> SHOW DATABASES; +--------------------+ | Database | +--------------------+ | classicmodels | | information_schema | | mysql | | performance_schema | | sys | | test | +--------------------+ 6 rows in set (0.00 sec) SHOW DATABASES LIKE '%schema'; +--------------------+ | Database (%schema) | +--------------------+ | information_schema | | performance_schema | +--------------------+ 2 rows in set (0.00 sec) SELECT schema_name FROM information_schema.schemata WHERE schema_name LIKE '%schema' OR schema_name LIKE '%s'; +--------------------+ | SCHEMA_NAME | +--------------------+ | information_schema | | performance_schema | | sys | | classicmodels | +--------------------+ 4 rows in set (0.00 sec)","title":"Show databases: list all databases in MySQL"},{"location":"Databases/MySQL/advanced/14_administration/#show-tables-list-tables-in-database","text":"+--------------------+ | SCHEMA_NAME | +--------------------+ | information_schema | | performance_schema | | sys | | classicmodels | +--------------------+ 4 rows in set (0.00 sec) Include table type (base table or view) > SHOW FULL TABLES +-------------------------+------------+ | Tables_in_classicmodels | Table_type | +-------------------------+------------+ | contacts | VIEW | | customers | BASE TABLE | | employees | BASE TABLE | | offices | BASE TABLE | | orderdetails | BASE TABLE | | orders | BASE TABLE | | payments | BASE TABLE | | productlines | BASE TABLE | | products | BASE TABLE | +-------------------------+------------+ 9 rows in set (0.00 sec) > SHOW TABLES LIKE 'p%'; +------------------------------+ | Tables_in_classicmodels (p%) | +------------------------------+ | payments | | productlines | | products | +------------------------------+ 3 rows in set (0.00 sec) > SHOW FULL TABLES WHERE table_type = 'VIEW'; +-------------------------+------------+ | Tables_in_classicmodels | Table_type | +-------------------------+------------+ | contacts | VIEW | +-------------------------+------------+ 1 row in set (0.00 sec)","title":"Show tables: List tables in database"},{"location":"Databases/MySQL/advanced/14_administration/#show-columns-and-describe","text":"mysql> DESCRIBE orders; +----------------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +----------------+-------------+------+-----+---------+-------+ | orderNumber | int(11) | NO | PRI | NULL | | | orderDate | date | NO | | NULL | | | requiredDate | date | NO | | NULL | | | shippedDate | date | YES | | NULL | | | status | varchar(15) | NO | | NULL | | | comments | text | YES | | NULL | | | customerNumber | int(11) | NO | MUL | NULL | | +----------------+-------------+------+-----+---------+-------+ 7 rows in set (0.01 sec) SHOW COLUMNS FROM table_name; SHOW COLUMNS FROM database_name.table_name; SHOW COLUMNS FROM table_name IN database_name; SHOW FULL COLUMNS FROM table_name; mysql> SHOW FULL COLUMNS FROM payments \\G; *************************** 1. row *************************** Field: customerNumber Type: int(11) Collation: NULL Null: NO Key: PRI Default: NULL Extra: Privileges: select,insert,update,references Comment: *************************** 2. row *************************** Field: checkNumber Type: varchar(50) Collation: latin1_swedish_ci Null: NO Key: PRI Default: NULL Extra: Privileges: select,insert,update,references Comment: *************************** 3. row *************************** Field: paymentDate Type: date Collation: NULL Null: NO Key: Default: NULL Extra: Privileges: select,insert,update,references Comment: *************************** 4. row *************************** Field: amount Type: decimal(10,2) Collation: NULL Null: NO Key: Default: NULL Extra: Privileges: select,insert,update,references Comment: 4 rows in set (0.01 sec) mysql> SHOW COLUMNS FROM payments LIKE 'c%'; +----------------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +----------------+-------------+------+-----+---------+-------+ | customerNumber | int(11) | NO | PRI | NULL | | | checkNumber | varchar(50) | NO | PRI | NULL | | +----------------+-------------+------+-----+---------+-------+ 2 rows in set (0.01 sec)","title":"Show columns and describe"},{"location":"Databases/MySQL/advanced/14_administration/#list-all-users-in-server","text":"SELECT user from mysql.user Describe user DESC user; Show current user mysql> SELECT user(); +-----------------+ | user() | +-----------------+ | local@localhost | +-----------------+ 1 row in set (0.00 sec) mysql> SELECT current_user(); +----------------+ | current_user() | +----------------+ | local@localhost | +----------------+ 1 row in set (0.00 sec) Show current logged users SELECT user, host, db, command FROM information_schema.processlist; +-------+-----------------+---------------+---------+ | user | host | db | command | +-------+-----------------+---------------+---------+ | local | localhost:50591 | classicmodels | Sleep | | root | localhost:50557 | NULL | Query | +-------+-----------------+---------------+---------+ 2 rows in set (0.00 sec)","title":"List all users in server"},{"location":"Databases/MySQL/advanced/14_administration/#show-processlist","text":"Sometimes you may get \"too many connections\" error returned by the server. To find out the reasons you can use SHOW PROCESSLIST command and use the KILL statement to kill the idle threads. SHOW [FULL] PROCESSLIST; mysql>SHOW PROCESSLIST; +----+-----------------+-----------------+---------------+---------+------+------------------------+------------------+ | Id | User | Host | db | Command | Time | State | Info | +----+-----------------+-----------------+---------------+---------+------+------------------------+------------------+ | 4 | event_scheduler | localhost | NULL | Daemon | 2246 | Waiting on empty queue | NULL | | 14 | root | localhost:50924 | NULL | Query | 0 | starting | SHOW PROCESSLIST | | 15 | car | localhost:50933 | classicmodels | Sleep | 2 | | NULL | +----+-----------------+-----------------+---------------+---------+------+------------------------+------------------+ 3 rows in set (0.00 sec) id - the client process id usder - the username associated with the thread host - the host to which the client is connected db - the default database if one selected otherwise NULL . command - the command type time - the number of seconds that the current thread has been in its current state state - the thread state which represents an action, event, or state that indicates what thread is executing. info the statement is executed, or NULL if it is not executing any statement. If you do not use the FULL keyword in the SHOW PROCESSLIST command, then only the first 100 characters of each statement are returned in the info column.","title":"Show processlist"},{"location":"Databases/MySQL/advanced/15_full_text_search/","text":"MySQL Full-Text Search \u00b6 MySQL supports text searching by using the LIKE operator and regular expression. However, when the text column is large and the number of rows in a table is increased, using these methods has some limitations: Performance: MySQL has to scan the whole table to find the exact text based on a pattern in the LIKE statement or pattern in the regular expressions. Flexible search: with the LIKE operator and regular expression searches, it is difficult to have a flexible search query e.g., to find product whose description contains car but not classic. Revelance ranking: there is not way to specify which row in the result set is more relevant to the search terms. Because ofthese limitations, MySQL extended a very nice feature so-called full-text search. Technically, MySQL creates an index from the words of the enabled full-text search columns and performs searches on this index. MySQL uses a sophisticated algorithm to determine the rows matched against the search query. The following are important features of MySQL full-text search: Native SQL-like interface: you use the SQL-like statement to use the the full-text search. Fully dynamic index: MySQL automaticall updates the index of text column whenever the data of that column changes. Moderate index size: it doesn't take much meemory to store the index Last but not least, it is fast to search based on complex search queries. Notice that not all storage engines support the full-text search feature. In MySQL version 5.6 or later, only MyISAM and InnoDB storage engines support full-text search. Defining FULLTEXT Indexes for MySQL Full-Text Searching \u00b6 Before performing a full-text search in a column of a table, you must index its data. MySQL will recreate the full-text index whenever the data of the column changes. In MySQL, the full-text index is a kind of index that has a name FULLTEXT . MySQL supports indexing and re-indexing data automatically for a full-text search enabled column. MySQL version 5.6 or later allows you to defined afull-text index for a column whose data type is CHAR , VARCHAR or TEXT in MyISAM or InnoDB table type. Notice that MySQL supported full-text index in the InnoDB tables since 5.6 MySQL allows you to define the FULLTEXT index by using the CREATE TABLE statement when you create the table or ALTER TABLE or CREATE INDEX statement for the existing tables. CREATE TABLE table_name( column1 data_type, column2 data_type, column3 data_type, \u2026 PRIMARY_KEY(key_column), FULLTEXT (column1,column2,..) ); CREATE TABLE posts ( id int(4) NOT NULL AUTO_INCREMENT, title varchar(255) NOT NULL, post_content text, PRIMARY KEY (id), FULLTEXT KEY post_content (post_content) ); ALTER TABLE table_name ADD FULLTEXT(column_name1, column_name2,\u2026) ALTER TABLE products ADD FULLTEXT(productDescription,productLine) CREATE FULLTEXT INDEX index_name ON table_name(idx_column_name,...) CREATE FULLTEXT INDEX address ON offices(addressLine1,addressLine2) ALTER TABLE offices DROP INDEX address; Natural Language Full-Text searches \u00b6 In natural language full-text searches, MySQL looks for rows or documents that are relevant to the free-text natural human language query, for example, \"How to use MySQL natural language full-text searches\". Relevance is a positive floating-point number. When the relevance is zero, it means that there is no similarity. MySQL computes the relevance based on various factors including the number of words in the document, the number of unique words in the document, the total number of words in the collection, and the number of documents (rows) that contain a particular word. To perform natural language full-text ssearches, you use MATCH() and AGAINST() functions. The MATCH() function specifies the column where you want to search and the AGAINST() function determines the expression to be used. ALTER TABLE products ADD FULLTEXT(productline); SELECT productName, productline FROM products WHERE MATCH(productline) AGAINST('Classic'); To search for product whose product line contains Classic or Vintage term, you can perform the following query. SELECT productName, productline FROM products WHERE MATCH(productline) AGAINST('Classic,Vintage'); The AGAINST() function uses IN NATURAL LANGUAGE MODE search modifier by default therefore you can omit it in the query. There are other search modifieds e.g. IN BOOLEAN MODE for boolean text searches. SELECT productName, productline FROM products WHERE MATCH(productline) AGAINST('Classic,Vintage' IN NATURAL LANGUAGE MODE) By default, MySQL performs searches in the case-insensitive fashion. However, you can instruct MySQL to perform case-sensitive searches using binary collation for indexed columns. Sort the result set by relevance \u00b6 A very important feature of full-text search is how MySQL ranks the rows in the result set based on their relevance. When the MATCH() funciton is used in the WHERE clause, MySQL returns the rows that are more relevant first. The following example shows you how MySQL sorts the results set by relaveance. SELECT productName, productline FROM products WHERE MATCH(productName) AGAINST('1932,Ford') The products whose namaes contain both 1932 and Ford are returned first and the n the products whose names contains only the Ford keyword. There are some important points you should remember when using the full-text search: The minimum length of the search term defined in MySQL full-text search engine is 4. It means that if you search for the keyword whose length is less than 4 e.g. car, car etc. you will not get any results. Stop words are ignored. MySQL defines a list of stop words in the MySQL source code distibution storage/myisam/ft_static.c Boolean Full-text searches \u00b6 Besides the natural language full-text search, MySQL supports an additional form of full-text search that is called Boolean full-text search. In the Boolean mode, MySQL searches for words instead of the concept like in the natural language search. MySQL allows you to perform a full-text search based on very complex queries in the Boolean mode along with Boolean operators. This is why the full-text search in Boolean mode is suitable for experienced users. To perform a full-text search in the Boolean mode, you use the IN BOOLEAN MODE modifier in the AGAINST expression. The following example shows you how to search a product whose product name contains the Truck word. SELECT productName, productline FROM products WHERE MATCH(productName) AGAINST('Truck' IN BOOLEAN MODE ) To find the product whose product names contain the Truck word but not any rows that contain Pickup , you can use the exclude Boolean operator ( - ), which returns the result that excludes the Pickup keyword as the following query: SELECT productName, productline FROM products WHERE MATCH(productName) AGAINST('Truck -Pickup' IN BOOLEAN MODE ) MySQL Boolean full-text search operators \u00b6 The following table illustrates the full-text seach Boolean operators and their meanings: Operator Description + Include, the word must be present. \u2013 Exclude, the word must not be present. > Include, and increase ranking value. < Include, and decrease the ranking value. () Group words into subexpressions (allowing them to be included, excluded, ranked, and so forth as a group). ~ Negate a word\u2019s ranking value. * Wildcard at the end of the word. \u201c\u201d Defines a phrase (as opposed to a list of individual words, the entire phrase is matched for inclusion or exclusion). The following examples illustrate how to use boolean full-text operators in search query mysql tutorial - to search for rows that contain at least one of the two words +mysql +tutorial - to search for rows that contain both words +mysql tutorial - to search for rows that contain the word mysql , but put the higher rank for the rank for the rows that contain tutorial. +mysql -tutorial - To search for rows that contain the word \"mysql\" but not \"tutorial\" +mysql ~tutorial - to search rows that contain word \"mysql\" and rank the row lower if it contains \"tutorial\". +mysql +(>tutorial <training>) - To search for rows that contain the words \"mysql\" and \"tutorial\", or \"mysql\" and \"training\" in whatever order, but put the rows that contain \"mysql tutorial\" higher than \"mysql training\". my* - To find rows that contain words starting with \"my\" such as mysql ... MySQL boolean full-text search main features \u00b6 MySQL does not automatically sort rows in the order of decreasing relevance in Boolean full-text search. To perform Boolean queries, InnoDBtables require all columns of the MATCH expression has a FULLTEXT index. Notice that MyISAM tables do not require this, although the search is quite slow. MySQL does not support multiple Boolean operators on a search query on InnoDB tables e.g., ++mysql . MySQL will return an error if you do so. However, MyISAM behaves differently. It ignores other operators and uses the operator that is closest to the search word, for example, +-mysql will become -mysql . InnoDB full-text search does not support trailing plus (+) or minus (-) sign.It only supports leading plus or munis sign. MySQL will report an error if you search word is mysql+ or mysql- . In addition, the following leading plus or minus with wildcard are invalid: +* , +- The 50% threshold is not applied. By the way, 50% threshold means if a word appears in more than 50% of the rows, MySQL will ignore it in the search result. MySQL Query expansion \u00b6 In some cases, users want to search for information based on the knowledge that they have. Users use their experience to define keywords to search for information, and typically those keywords are too short. To help users to find information based on the too-short keywords, MySQL full-text search engine introduces a concept called query expansion. The query expansion is used to widen the search result of the full-text searches based on automatic relevance feedback (or blin query expansion). Technically, MySQL full-text search engine performs the following steps when the query expansion is used: MySQL full-text search engine looks for all rows that match the search query It checks all rows in the search result and finds the relevant words It performs a search again based on the relevant words instead of the original keywords provided by the users From the application perspective, you can use the query expansion when the search results are too few. You perform the searches again but with query expansion to offer users more information that is related and relevant to what they are looking for. To use the query expansion, you use the WITH QUERY EXPANSION search modifier in the AGAINST() function the following ilustrates the syntax of the query using the WITH QUERY EXPANSION search modifier. SELECT column1, column2 FROM table1 WHERE MATCH(column1,column2) AGAINST('keyword',WITH QUERY EXPANSION); SELECT productName FROM products WHERE MATCH(productName) AGAINST('1992'); +-----------------------------------+ | productName | +-----------------------------------+ | 1992 Ferrari 360 Spider red | | 1992 Porsche Cayenne Turbo Silver | +-----------------------------------+ 2 rows in set (0.00 sec) SELECT productName FROM products WHERE MATCH(productName) AGAINST('1992' WITH QUERY EXPANSION); +-------------------------------------+ | productName | +-------------------------------------+ | 1992 Porsche Cayenne Turbo Silver | | 1992 Ferrari 360 Spider red | | 2001 Ferrari Enzo | | 1932 Alfa Romeo 8C2300 Spider Sport | | 1948 Porsche 356-A Roadster | | 1948 Porsche Type 356 Roadster | | 1956 Porsche 356A Coupe | +-------------------------------------+ 7 rows in set (0.00 sec) Notice that blind query expansion tends to increase noise significantly by returning non-relevant results. It is highly recommended that you use query expansion only when the searched keyword is hort. ngram Full-Text parser \u00b6 The built-in MySQL full-text parser determines the beginning and ending of words using white space. When it comes to ideographic languages such as Chinese, Japanese or Koren etc, this is a limitation because these languages do not use word delimiters. To address this issue, MySQL provided the ngram full-text parser. Since version 5.7.6, MySQL included ngram full-text parser as a built-in server plugin, meaning that MySQL loads this plugin automatically when the MySQL database server starts. MySQL supports ngram full-text parser for both InnoDB and MyISAM storage engines. By definition, angram is a contigous sequence of characyters from a sequence of text. The main function of ngram full-text parser is tokenizing a sequence of text into a contigous sequence of n characters. The following illustrates how the ngram full-text parser tokenizes a sequence of text for different value of n: n = 1: 'm','y','s','q','l' n = 2: 'my', 'ys', 'sq','ql' n = 3: 'mys', 'ysq', 'sql' n = 4: 'mysq', 'ysql' n = 5: 'mysql' Creating FULLTEXT indexes with ngram parser \u00b6 To create a FULL TEXT index that uses ngram full-text parser, you add the WITH PARSER ngram in the CREATE TABLE, ALTER TABLE, or CREATE INDEX statement. For example the following statements creates new posts table and adds the title and body columns to the FULLTEXT index that use ngram full-text parser. CREATE TABLE posts ( id INT PRIMARY KEY AUTO_INCREMENT, title VARCHAR(255), body TEXT, FULLTEXT ( title , body ) WITH PARSER NGRAM ) ENGINE=INNODB CHARACTER SET UTF8MB4; CREATE TABLE posts ( id INT PRIMARY KEY AUTO_INCREMENT, title VARCHAR(255), body TEXT, FULLTEXT ( title , body ) WITH PARSER NGRAM ) ENGINE=INNODB CHARACTER SET UTF8MB4; SET GLOBAL innodb_ft_aux_table=\"test/posts\"; SELECT * FROM information_schema.innodb_ft_index_cache ORDER BY doc_id , position; Setting ngram token size \u00b6 As you can see the previous example, the token size (n) in the ngram by default is 2. To change the token size, you use the ngram_token_size configuration option, which has a value between 1 and 10. Note that a smaller token size makes smaller full-text index and allows you to search faster. Because ngram_token_size is a read-only variable, therefore you only can set its value using two options mysqld --ngram_token_size=1 [mysqld] ngram_token_size=1 MySQL converts a phrase searc into ngram phrase searches. For examplem \"abc\" is converted into \"ab bc\", which returns documents that contain \"ab bc\" and \"abc\". processing search result with ngram \u00b6 In NATURAL LANGUAGE MODE searches, the search term is converted to a union of ngram values. Suppose the token size is 2 or bigram, the search term mysql is converted to my ys sq ql . In BOOLEAN MODE searches, the search term is converted to a ngram phrase search. ngram wildcard search \u00b6 The ngram FULLTEXT index contains only ngrams, therefore it does not know the beginning of terms. When you perform wildcard searches, it may return unexpected result. The following rultes are applied to wildcard search using ngram FULLTEXT search indexes: If the prefix term in the wildcard is shorter than ngram token size, the query returns all documents that contain ngram tokens starting with the prefix term. SELECT id, title, body FROM posts WHERE MATCH (title , body) AGAINST ('my*' ); In case the prefix term in the wildcard is longer than ngram token size, MySQL will convert the prefix term into ngram phrases and ignore the wildcard operator. SELECT id, title, body FROM posts WHERE MATCH (title , body) AGAINST ('mysqld*' ); In this example, the term \"mysqld\" is converted into ngram phrases \"my\" \"ys\" \"sq\" \"ql\" \"ld\" . Therefore all documents that contain one of these phrases are returned. Handling stopword \u00b6 The ngram parser excludes tokens that contain the stopword in the stopword list. For example, suppose the ngram_token_size is 2 and document contains \"abd\". The ngram parser will tokenize the document to \"ab\" \"bc\". If \"b\" is a stopword, ngram will exclude both \"ab\" and \"bc\" because they contain \"b\". Note that you must define your own stopword list if the language is other than English. In addition the stopwords with lengthhs that are greater than ngram_token_size are ignored.","title":"MySQL Full-Text Search"},{"location":"Databases/MySQL/advanced/15_full_text_search/#mysql-full-text-search","text":"MySQL supports text searching by using the LIKE operator and regular expression. However, when the text column is large and the number of rows in a table is increased, using these methods has some limitations: Performance: MySQL has to scan the whole table to find the exact text based on a pattern in the LIKE statement or pattern in the regular expressions. Flexible search: with the LIKE operator and regular expression searches, it is difficult to have a flexible search query e.g., to find product whose description contains car but not classic. Revelance ranking: there is not way to specify which row in the result set is more relevant to the search terms. Because ofthese limitations, MySQL extended a very nice feature so-called full-text search. Technically, MySQL creates an index from the words of the enabled full-text search columns and performs searches on this index. MySQL uses a sophisticated algorithm to determine the rows matched against the search query. The following are important features of MySQL full-text search: Native SQL-like interface: you use the SQL-like statement to use the the full-text search. Fully dynamic index: MySQL automaticall updates the index of text column whenever the data of that column changes. Moderate index size: it doesn't take much meemory to store the index Last but not least, it is fast to search based on complex search queries. Notice that not all storage engines support the full-text search feature. In MySQL version 5.6 or later, only MyISAM and InnoDB storage engines support full-text search.","title":"MySQL Full-Text Search"},{"location":"Databases/MySQL/advanced/15_full_text_search/#defining-fulltext-indexes-for-mysql-full-text-searching","text":"Before performing a full-text search in a column of a table, you must index its data. MySQL will recreate the full-text index whenever the data of the column changes. In MySQL, the full-text index is a kind of index that has a name FULLTEXT . MySQL supports indexing and re-indexing data automatically for a full-text search enabled column. MySQL version 5.6 or later allows you to defined afull-text index for a column whose data type is CHAR , VARCHAR or TEXT in MyISAM or InnoDB table type. Notice that MySQL supported full-text index in the InnoDB tables since 5.6 MySQL allows you to define the FULLTEXT index by using the CREATE TABLE statement when you create the table or ALTER TABLE or CREATE INDEX statement for the existing tables. CREATE TABLE table_name( column1 data_type, column2 data_type, column3 data_type, \u2026 PRIMARY_KEY(key_column), FULLTEXT (column1,column2,..) ); CREATE TABLE posts ( id int(4) NOT NULL AUTO_INCREMENT, title varchar(255) NOT NULL, post_content text, PRIMARY KEY (id), FULLTEXT KEY post_content (post_content) ); ALTER TABLE table_name ADD FULLTEXT(column_name1, column_name2,\u2026) ALTER TABLE products ADD FULLTEXT(productDescription,productLine) CREATE FULLTEXT INDEX index_name ON table_name(idx_column_name,...) CREATE FULLTEXT INDEX address ON offices(addressLine1,addressLine2) ALTER TABLE offices DROP INDEX address;","title":"Defining FULLTEXT Indexes for MySQL Full-Text Searching"},{"location":"Databases/MySQL/advanced/15_full_text_search/#natural-language-full-text-searches","text":"In natural language full-text searches, MySQL looks for rows or documents that are relevant to the free-text natural human language query, for example, \"How to use MySQL natural language full-text searches\". Relevance is a positive floating-point number. When the relevance is zero, it means that there is no similarity. MySQL computes the relevance based on various factors including the number of words in the document, the number of unique words in the document, the total number of words in the collection, and the number of documents (rows) that contain a particular word. To perform natural language full-text ssearches, you use MATCH() and AGAINST() functions. The MATCH() function specifies the column where you want to search and the AGAINST() function determines the expression to be used. ALTER TABLE products ADD FULLTEXT(productline); SELECT productName, productline FROM products WHERE MATCH(productline) AGAINST('Classic'); To search for product whose product line contains Classic or Vintage term, you can perform the following query. SELECT productName, productline FROM products WHERE MATCH(productline) AGAINST('Classic,Vintage'); The AGAINST() function uses IN NATURAL LANGUAGE MODE search modifier by default therefore you can omit it in the query. There are other search modifieds e.g. IN BOOLEAN MODE for boolean text searches. SELECT productName, productline FROM products WHERE MATCH(productline) AGAINST('Classic,Vintage' IN NATURAL LANGUAGE MODE) By default, MySQL performs searches in the case-insensitive fashion. However, you can instruct MySQL to perform case-sensitive searches using binary collation for indexed columns.","title":"Natural Language Full-Text searches"},{"location":"Databases/MySQL/advanced/15_full_text_search/#sort-the-result-set-by-relevance","text":"A very important feature of full-text search is how MySQL ranks the rows in the result set based on their relevance. When the MATCH() funciton is used in the WHERE clause, MySQL returns the rows that are more relevant first. The following example shows you how MySQL sorts the results set by relaveance. SELECT productName, productline FROM products WHERE MATCH(productName) AGAINST('1932,Ford') The products whose namaes contain both 1932 and Ford are returned first and the n the products whose names contains only the Ford keyword. There are some important points you should remember when using the full-text search: The minimum length of the search term defined in MySQL full-text search engine is 4. It means that if you search for the keyword whose length is less than 4 e.g. car, car etc. you will not get any results. Stop words are ignored. MySQL defines a list of stop words in the MySQL source code distibution storage/myisam/ft_static.c","title":"Sort the result set by relevance"},{"location":"Databases/MySQL/advanced/15_full_text_search/#boolean-full-text-searches","text":"Besides the natural language full-text search, MySQL supports an additional form of full-text search that is called Boolean full-text search. In the Boolean mode, MySQL searches for words instead of the concept like in the natural language search. MySQL allows you to perform a full-text search based on very complex queries in the Boolean mode along with Boolean operators. This is why the full-text search in Boolean mode is suitable for experienced users. To perform a full-text search in the Boolean mode, you use the IN BOOLEAN MODE modifier in the AGAINST expression. The following example shows you how to search a product whose product name contains the Truck word. SELECT productName, productline FROM products WHERE MATCH(productName) AGAINST('Truck' IN BOOLEAN MODE ) To find the product whose product names contain the Truck word but not any rows that contain Pickup , you can use the exclude Boolean operator ( - ), which returns the result that excludes the Pickup keyword as the following query: SELECT productName, productline FROM products WHERE MATCH(productName) AGAINST('Truck -Pickup' IN BOOLEAN MODE )","title":"Boolean Full-text searches"},{"location":"Databases/MySQL/advanced/15_full_text_search/#mysql-boolean-full-text-search-operators","text":"The following table illustrates the full-text seach Boolean operators and their meanings: Operator Description + Include, the word must be present. \u2013 Exclude, the word must not be present. > Include, and increase ranking value. < Include, and decrease the ranking value. () Group words into subexpressions (allowing them to be included, excluded, ranked, and so forth as a group). ~ Negate a word\u2019s ranking value. * Wildcard at the end of the word. \u201c\u201d Defines a phrase (as opposed to a list of individual words, the entire phrase is matched for inclusion or exclusion). The following examples illustrate how to use boolean full-text operators in search query mysql tutorial - to search for rows that contain at least one of the two words +mysql +tutorial - to search for rows that contain both words +mysql tutorial - to search for rows that contain the word mysql , but put the higher rank for the rank for the rows that contain tutorial. +mysql -tutorial - To search for rows that contain the word \"mysql\" but not \"tutorial\" +mysql ~tutorial - to search rows that contain word \"mysql\" and rank the row lower if it contains \"tutorial\". +mysql +(>tutorial <training>) - To search for rows that contain the words \"mysql\" and \"tutorial\", or \"mysql\" and \"training\" in whatever order, but put the rows that contain \"mysql tutorial\" higher than \"mysql training\". my* - To find rows that contain words starting with \"my\" such as mysql ...","title":"MySQL Boolean full-text search operators"},{"location":"Databases/MySQL/advanced/15_full_text_search/#mysql-boolean-full-text-search-main-features","text":"MySQL does not automatically sort rows in the order of decreasing relevance in Boolean full-text search. To perform Boolean queries, InnoDBtables require all columns of the MATCH expression has a FULLTEXT index. Notice that MyISAM tables do not require this, although the search is quite slow. MySQL does not support multiple Boolean operators on a search query on InnoDB tables e.g., ++mysql . MySQL will return an error if you do so. However, MyISAM behaves differently. It ignores other operators and uses the operator that is closest to the search word, for example, +-mysql will become -mysql . InnoDB full-text search does not support trailing plus (+) or minus (-) sign.It only supports leading plus or munis sign. MySQL will report an error if you search word is mysql+ or mysql- . In addition, the following leading plus or minus with wildcard are invalid: +* , +- The 50% threshold is not applied. By the way, 50% threshold means if a word appears in more than 50% of the rows, MySQL will ignore it in the search result.","title":"MySQL boolean full-text search main features"},{"location":"Databases/MySQL/advanced/15_full_text_search/#mysql-query-expansion","text":"In some cases, users want to search for information based on the knowledge that they have. Users use their experience to define keywords to search for information, and typically those keywords are too short. To help users to find information based on the too-short keywords, MySQL full-text search engine introduces a concept called query expansion. The query expansion is used to widen the search result of the full-text searches based on automatic relevance feedback (or blin query expansion). Technically, MySQL full-text search engine performs the following steps when the query expansion is used: MySQL full-text search engine looks for all rows that match the search query It checks all rows in the search result and finds the relevant words It performs a search again based on the relevant words instead of the original keywords provided by the users From the application perspective, you can use the query expansion when the search results are too few. You perform the searches again but with query expansion to offer users more information that is related and relevant to what they are looking for. To use the query expansion, you use the WITH QUERY EXPANSION search modifier in the AGAINST() function the following ilustrates the syntax of the query using the WITH QUERY EXPANSION search modifier. SELECT column1, column2 FROM table1 WHERE MATCH(column1,column2) AGAINST('keyword',WITH QUERY EXPANSION); SELECT productName FROM products WHERE MATCH(productName) AGAINST('1992'); +-----------------------------------+ | productName | +-----------------------------------+ | 1992 Ferrari 360 Spider red | | 1992 Porsche Cayenne Turbo Silver | +-----------------------------------+ 2 rows in set (0.00 sec) SELECT productName FROM products WHERE MATCH(productName) AGAINST('1992' WITH QUERY EXPANSION); +-------------------------------------+ | productName | +-------------------------------------+ | 1992 Porsche Cayenne Turbo Silver | | 1992 Ferrari 360 Spider red | | 2001 Ferrari Enzo | | 1932 Alfa Romeo 8C2300 Spider Sport | | 1948 Porsche 356-A Roadster | | 1948 Porsche Type 356 Roadster | | 1956 Porsche 356A Coupe | +-------------------------------------+ 7 rows in set (0.00 sec) Notice that blind query expansion tends to increase noise significantly by returning non-relevant results. It is highly recommended that you use query expansion only when the searched keyword is hort.","title":"MySQL Query expansion"},{"location":"Databases/MySQL/advanced/15_full_text_search/#ngram-full-text-parser","text":"The built-in MySQL full-text parser determines the beginning and ending of words using white space. When it comes to ideographic languages such as Chinese, Japanese or Koren etc, this is a limitation because these languages do not use word delimiters. To address this issue, MySQL provided the ngram full-text parser. Since version 5.7.6, MySQL included ngram full-text parser as a built-in server plugin, meaning that MySQL loads this plugin automatically when the MySQL database server starts. MySQL supports ngram full-text parser for both InnoDB and MyISAM storage engines. By definition, angram is a contigous sequence of characyters from a sequence of text. The main function of ngram full-text parser is tokenizing a sequence of text into a contigous sequence of n characters. The following illustrates how the ngram full-text parser tokenizes a sequence of text for different value of n: n = 1: 'm','y','s','q','l' n = 2: 'my', 'ys', 'sq','ql' n = 3: 'mys', 'ysq', 'sql' n = 4: 'mysq', 'ysql' n = 5: 'mysql'","title":"ngram Full-Text parser"},{"location":"Databases/MySQL/advanced/15_full_text_search/#creating-fulltext-indexes-with-ngram-parser","text":"To create a FULL TEXT index that uses ngram full-text parser, you add the WITH PARSER ngram in the CREATE TABLE, ALTER TABLE, or CREATE INDEX statement. For example the following statements creates new posts table and adds the title and body columns to the FULLTEXT index that use ngram full-text parser. CREATE TABLE posts ( id INT PRIMARY KEY AUTO_INCREMENT, title VARCHAR(255), body TEXT, FULLTEXT ( title , body ) WITH PARSER NGRAM ) ENGINE=INNODB CHARACTER SET UTF8MB4; CREATE TABLE posts ( id INT PRIMARY KEY AUTO_INCREMENT, title VARCHAR(255), body TEXT, FULLTEXT ( title , body ) WITH PARSER NGRAM ) ENGINE=INNODB CHARACTER SET UTF8MB4; SET GLOBAL innodb_ft_aux_table=\"test/posts\"; SELECT * FROM information_schema.innodb_ft_index_cache ORDER BY doc_id , position;","title":"Creating FULLTEXT indexes with ngram parser"},{"location":"Databases/MySQL/advanced/15_full_text_search/#setting-ngram-token-size","text":"As you can see the previous example, the token size (n) in the ngram by default is 2. To change the token size, you use the ngram_token_size configuration option, which has a value between 1 and 10. Note that a smaller token size makes smaller full-text index and allows you to search faster. Because ngram_token_size is a read-only variable, therefore you only can set its value using two options mysqld --ngram_token_size=1 [mysqld] ngram_token_size=1 MySQL converts a phrase searc into ngram phrase searches. For examplem \"abc\" is converted into \"ab bc\", which returns documents that contain \"ab bc\" and \"abc\".","title":"Setting ngram token size"},{"location":"Databases/MySQL/advanced/15_full_text_search/#processing-search-result-with-ngram","text":"In NATURAL LANGUAGE MODE searches, the search term is converted to a union of ngram values. Suppose the token size is 2 or bigram, the search term mysql is converted to my ys sq ql . In BOOLEAN MODE searches, the search term is converted to a ngram phrase search.","title":"processing search result with ngram"},{"location":"Databases/MySQL/advanced/15_full_text_search/#ngram-wildcard-search","text":"The ngram FULLTEXT index contains only ngrams, therefore it does not know the beginning of terms. When you perform wildcard searches, it may return unexpected result. The following rultes are applied to wildcard search using ngram FULLTEXT search indexes: If the prefix term in the wildcard is shorter than ngram token size, the query returns all documents that contain ngram tokens starting with the prefix term. SELECT id, title, body FROM posts WHERE MATCH (title , body) AGAINST ('my*' ); In case the prefix term in the wildcard is longer than ngram token size, MySQL will convert the prefix term into ngram phrases and ignore the wildcard operator. SELECT id, title, body FROM posts WHERE MATCH (title , body) AGAINST ('mysqld*' ); In this example, the term \"mysqld\" is converted into ngram phrases \"my\" \"ys\" \"sq\" \"ql\" \"ld\" . Therefore all documents that contain one of these phrases are returned.","title":"ngram wildcard search"},{"location":"Databases/MySQL/advanced/15_full_text_search/#handling-stopword","text":"The ngram parser excludes tokens that contain the stopword in the stopword list. For example, suppose the ngram_token_size is 2 and document contains \"abd\". The ngram parser will tokenize the document to \"ab\" \"bc\". If \"b\" is a stopword, ngram will exclude both \"ab\" and \"bc\" because they contain \"b\". Note that you must define your own stopword list if the language is other than English. In addition the stopwords with lengthhs that are greater than ngram_token_size are ignored.","title":"Handling stopword"},{"location":"Databases/MySQL/basics/","text":"The materials are taken from http://www.mysqltutorial.org/","title":"Index"},{"location":"Databases/MySQL/basics/01_query_data/","text":"Query data \u00b6 SELECT \u00b6 The SELECT statement allows to get data from tables or views. It controls which columns and rows you want to see. SELECT column_1, column_2, ... FROM table_1 [INNER | LEFT |RIGHT] JOIN table_2 ON conditions WHERE conditions GROUP BY column_1 HAVING group_conditions ORDER BY column_1 LIMIT offset, length; SELECT DISTINCT \u00b6 When querying, it is possible to get duplicate rows. In order to remove duplicates, you can use DISTINCT . SELECT DISTINCT columns FROM table_name WHERE where_conditions; If a column has NULL value in the column, it will keep only the first occurence. When using multiple columns, it will look for unique combinations between the columns. DISTINCT vs GROUP BY \u00b6 If you use GROUP BY without any aggregation functions, it behaves like DISTINCT. SELECT state FROM customers GROUP BY state; is equal to SELECT DISTINCT state FROM customers; DISTINCT and aggregate function \u00b6 You can also use DISTINCT in aggregate functions like this: SELECT COUNT(DISTINCT state) FROM customers WHERE country = 'USA';","title":"Query data"},{"location":"Databases/MySQL/basics/01_query_data/#query-data","text":"","title":"Query data"},{"location":"Databases/MySQL/basics/01_query_data/#select","text":"The SELECT statement allows to get data from tables or views. It controls which columns and rows you want to see. SELECT column_1, column_2, ... FROM table_1 [INNER | LEFT |RIGHT] JOIN table_2 ON conditions WHERE conditions GROUP BY column_1 HAVING group_conditions ORDER BY column_1 LIMIT offset, length;","title":"SELECT"},{"location":"Databases/MySQL/basics/01_query_data/#select-distinct","text":"When querying, it is possible to get duplicate rows. In order to remove duplicates, you can use DISTINCT . SELECT DISTINCT columns FROM table_name WHERE where_conditions; If a column has NULL value in the column, it will keep only the first occurence. When using multiple columns, it will look for unique combinations between the columns.","title":"SELECT DISTINCT"},{"location":"Databases/MySQL/basics/01_query_data/#distinct-vs-group-by","text":"If you use GROUP BY without any aggregation functions, it behaves like DISTINCT. SELECT state FROM customers GROUP BY state; is equal to SELECT DISTINCT state FROM customers;","title":"DISTINCT vs GROUP BY"},{"location":"Databases/MySQL/basics/01_query_data/#distinct-and-aggregate-function","text":"You can also use DISTINCT in aggregate functions like this: SELECT COUNT(DISTINCT state) FROM customers WHERE country = 'USA';","title":"DISTINCT and aggregate function"},{"location":"Databases/MySQL/basics/02_filter_data/","text":"Filter data \u00b6 MySQL WHERE \u00b6 Where clause allows to specify the search condition. SELECT select_list FROM table_name WHERE search_condition; search combination of one or more predicates that are combined with AND , OR , NOT operators. In SQL predicate is an expression that evaluates to true, false or unknown. Any row that causes the search condition to be true, will be included in the result set. Where statement can be used not only in select, but in update and delete statements as well. SELECT * FROM table_name WHERE (a > 0 AND b < 5 AND c IS NULL) OR d > 100 Some of the most popular logical operators in where statements are as follows: AND - all expressions must be true SELECT customername, country, state FROM customers WHERE country = 'USA' AND state = 'CA'; OR - one of expressions must be true SELECT customername, country, state FROM customers WHERE country = 'USA' OR country = 'Canada'; IN - determine if the column value is in a list of values SELECT customername, country, state FROM customers WHERE country IN ('USA' , 'Canada') Basicaly a shorter OR statement. Can be also used with subqueries. SELECT orderNumber, customerNumber, status, shippedDate FROM orders WHERE orderNumber IN (SELECT orderNumber FROM orderDetails GROUP BY orderNumber HAVING SUM(quantityOrdered * priceEach) > 60000); BETWEEN - value must be in a given range SELECT customername, country, state FROM customers WHERE country = 'USA' AND state = 'CA'; LIKE - must follow a specific pattern. Provides two operators % and _ . Where % is a string of zero or more characters. _ matches any single character. SELECT employeeNumber, lastName, firstName FROM employees WHERE lastname LIKE '%on%'; SELECT employeeNumber, lastName, firstName FROM employees WHERE firstname LIKE 'T_m'; In order to match the _ symbol, you need to escape it: SELECT productCode, productName FROM products WHERE productCode LIKE '%\\_20%'; IS NULL - the value is null SELECT customerName, country, salesrepemployeenumber FROM customers WHERE salesRepEmployeeNumber = 1370 OR salesRepEmployeeNumber IS NULL;","title":"Filter data"},{"location":"Databases/MySQL/basics/02_filter_data/#filter-data","text":"","title":"Filter data"},{"location":"Databases/MySQL/basics/02_filter_data/#mysql-where","text":"Where clause allows to specify the search condition. SELECT select_list FROM table_name WHERE search_condition; search combination of one or more predicates that are combined with AND , OR , NOT operators. In SQL predicate is an expression that evaluates to true, false or unknown. Any row that causes the search condition to be true, will be included in the result set. Where statement can be used not only in select, but in update and delete statements as well. SELECT * FROM table_name WHERE (a > 0 AND b < 5 AND c IS NULL) OR d > 100 Some of the most popular logical operators in where statements are as follows: AND - all expressions must be true SELECT customername, country, state FROM customers WHERE country = 'USA' AND state = 'CA'; OR - one of expressions must be true SELECT customername, country, state FROM customers WHERE country = 'USA' OR country = 'Canada'; IN - determine if the column value is in a list of values SELECT customername, country, state FROM customers WHERE country IN ('USA' , 'Canada') Basicaly a shorter OR statement. Can be also used with subqueries. SELECT orderNumber, customerNumber, status, shippedDate FROM orders WHERE orderNumber IN (SELECT orderNumber FROM orderDetails GROUP BY orderNumber HAVING SUM(quantityOrdered * priceEach) > 60000); BETWEEN - value must be in a given range SELECT customername, country, state FROM customers WHERE country = 'USA' AND state = 'CA'; LIKE - must follow a specific pattern. Provides two operators % and _ . Where % is a string of zero or more characters. _ matches any single character. SELECT employeeNumber, lastName, firstName FROM employees WHERE lastname LIKE '%on%'; SELECT employeeNumber, lastName, firstName FROM employees WHERE firstname LIKE 'T_m'; In order to match the _ symbol, you need to escape it: SELECT productCode, productName FROM products WHERE productCode LIKE '%\\_20%'; IS NULL - the value is null SELECT customerName, country, salesrepemployeenumber FROM customers WHERE salesRepEmployeeNumber = 1370 OR salesRepEmployeeNumber IS NULL;","title":"MySQL WHERE"},{"location":"Databases/MySQL/basics/03_sorting_data/","text":"Sorting data \u00b6 When using SELECT statement, it is possible to use ORDER BY to order the results: single column or multiple column in ascending or descending order SELECT column1, column2,... FROM tbl ORDER BY column1 [ASC|DESC], column2 [ASC|DESC],... Custom sort order \u00b6 The ORDER BY clause enables you to define own custom sort order for the values in a column using FIELD function. SELECT orderNumber, status FROM orders ORDER BY FIELD(status, 'In Process', 'On Hold', 'Cancelled', 'Resolved', 'Disputed', 'Shipped');","title":"Sorting data"},{"location":"Databases/MySQL/basics/03_sorting_data/#sorting-data","text":"When using SELECT statement, it is possible to use ORDER BY to order the results: single column or multiple column in ascending or descending order SELECT column1, column2,... FROM tbl ORDER BY column1 [ASC|DESC], column2 [ASC|DESC],...","title":"Sorting data"},{"location":"Databases/MySQL/basics/03_sorting_data/#custom-sort-order","text":"The ORDER BY clause enables you to define own custom sort order for the values in a column using FIELD function. SELECT orderNumber, status FROM orders ORDER BY FIELD(status, 'In Process', 'On Hold', 'Cancelled', 'Resolved', 'Disputed', 'Shipped');","title":"Custom sort order"},{"location":"Databases/MySQL/basics/04_joining_tables/","text":"Jointing tables \u00b6 Aliases \u00b6 Mysql supports 2 kinds of aliases - column aliases and table aliases. SELECT [column_1 | expression] AS descriptive_name FROM table_name; The AS is optional. SELECT CONCAT_WS(', ', lastName, firstname) `Full name` FROM employees ORDER BY `Full name`; SELECT customerName, COUNT(o.orderNumber) total FROM customers c INNER JOIN orders o ON c.customerNumber = o.customerNumber GROUP BY customerName ORDER BY total DESC; Join \u00b6 The relational database consists of multiple related tables linking together using common columns called as foreign key columns. A MySQL JOIN is a method of linking data between one (self-join) or more tables based on values of the common column between tables. MySQL supports the following types of joins: Cross join Inner join Left join Right join Cross join \u00b6 The CROSS JOIN makes a cartesian product of rows from multiple tables. When joining tables t1 and t2 , it will include combinations of rows from the t1 table with the t2 table. SELECT t1.id, t2.id FROM t1 CROSS JOIN t2; Inner join \u00b6 To form an INNER JOIN , there is a join-predicate needed that esentialy is a condition on which the columns get combined. The INNER JOIN requires rows in the two joined tables to have matching column values. To join two tables, the INNER JOIN compares each row in the first table with each row in the second table to faind pairs of rows that satisfy the join-predicate. Whenever the join-predicate is satisfied by matching non-NULL values, column values for each matched pair of rows of the two tables are included in the result set. SELECT t1.id, t2.id FROM t1 INNER JOIN t2 ON t1.pattern = t2.pattern; This means that rows in t1 and t2 must have the same values in the pattern column to be included in the result. Left join \u00b6 Left join also requires a join predicate. Unlike the inner join, the left join returns all rows in the left table including rows that satisfy join-predicate and rows that do not. For the rows that do not match the join predicate, NULL values appear in the column of the right table dataset. SELECT t1.id, t2.id FROM t1 LEFT JOIN t2 ON t1.pattern = t2.pattern ORDER BY t1.id; Right join \u00b6 The Righ join is similar to left join except that it' s reversed - every row of the right side table will appear and the left one will contain NULLs if the join-predicate is not satisfied.","title":"Jointing tables"},{"location":"Databases/MySQL/basics/04_joining_tables/#jointing-tables","text":"","title":"Jointing tables"},{"location":"Databases/MySQL/basics/04_joining_tables/#aliases","text":"Mysql supports 2 kinds of aliases - column aliases and table aliases. SELECT [column_1 | expression] AS descriptive_name FROM table_name; The AS is optional. SELECT CONCAT_WS(', ', lastName, firstname) `Full name` FROM employees ORDER BY `Full name`; SELECT customerName, COUNT(o.orderNumber) total FROM customers c INNER JOIN orders o ON c.customerNumber = o.customerNumber GROUP BY customerName ORDER BY total DESC;","title":"Aliases"},{"location":"Databases/MySQL/basics/04_joining_tables/#join","text":"The relational database consists of multiple related tables linking together using common columns called as foreign key columns. A MySQL JOIN is a method of linking data between one (self-join) or more tables based on values of the common column between tables. MySQL supports the following types of joins: Cross join Inner join Left join Right join","title":"Join"},{"location":"Databases/MySQL/basics/04_joining_tables/#cross-join","text":"The CROSS JOIN makes a cartesian product of rows from multiple tables. When joining tables t1 and t2 , it will include combinations of rows from the t1 table with the t2 table. SELECT t1.id, t2.id FROM t1 CROSS JOIN t2;","title":"Cross join"},{"location":"Databases/MySQL/basics/04_joining_tables/#inner-join","text":"To form an INNER JOIN , there is a join-predicate needed that esentialy is a condition on which the columns get combined. The INNER JOIN requires rows in the two joined tables to have matching column values. To join two tables, the INNER JOIN compares each row in the first table with each row in the second table to faind pairs of rows that satisfy the join-predicate. Whenever the join-predicate is satisfied by matching non-NULL values, column values for each matched pair of rows of the two tables are included in the result set. SELECT t1.id, t2.id FROM t1 INNER JOIN t2 ON t1.pattern = t2.pattern; This means that rows in t1 and t2 must have the same values in the pattern column to be included in the result.","title":"Inner join"},{"location":"Databases/MySQL/basics/04_joining_tables/#left-join","text":"Left join also requires a join predicate. Unlike the inner join, the left join returns all rows in the left table including rows that satisfy join-predicate and rows that do not. For the rows that do not match the join predicate, NULL values appear in the column of the right table dataset. SELECT t1.id, t2.id FROM t1 LEFT JOIN t2 ON t1.pattern = t2.pattern ORDER BY t1.id;","title":"Left join"},{"location":"Databases/MySQL/basics/04_joining_tables/#right-join","text":"The Righ join is similar to left join except that it' s reversed - every row of the right side table will appear and the left one will contain NULLs if the join-predicate is not satisfied.","title":"Right join"},{"location":"Databases/MySQL/basics/05_grouping_data/","text":"Grouping data \u00b6 GROUP BY \u00b6 the GROUP BY clause groups a set of rows into a set of summary rows by values of columns or expressions. It returns one row for each group. SELECT c1, c2,..., cn, aggregate_function(ci) FROM table WHERE where_conditions GROUP BY c1 , c2,...,cn; The GROUP BY clause must appear after FROM and WHERE clauses. SELECT status, COUNT(*) FROM orders GROUP BY status; It also allows to sort the groups: SELECT status, COUNT(*) FROM orders GROUP BY status DESC; HAVING \u00b6 The HAVING clause is often used with the GROUP BY to filter groups based on a specific condition. If it' s ommited, HAVING behaves like WHERE . SELECT ordernumber, SUM(quantityOrdered) AS itemsCount, SUM(priceeach*quantityOrdered) AS total FROM orderdetails GROUP BY ordernumber HAVING total > 1000; ROLLUP \u00b6 A grouping set is a set of columns to which you want to group. The ROLLUP clause is an extension of the GROUP BY clause with the following syntax: SELECT select_list FROM table_name GROUP BY c1, c2, c3 WITH ROLLUP; The ROLLUP generates multiple grouping sets based on the columns or expression specified in the GROUP BY clause. GROUPING function \u00b6 To check whether NULL in the results set represents the subtotals or grand totals, you can use GROUPING() function. The GROUPING function will return 1 when NULL occurs in a supper-aggregate row, otherwise it will return 0. SELECT IF(GROUPING(orderYear), 'All Years', orderYear) orderYear, IF(GROUPING(productLine), 'All Product Lines', productLine) productLine, SUM(orderValue) totalOrderValue FROM sales GROUP BY orderYear , productline WITH ROLLUP;","title":"Grouping data"},{"location":"Databases/MySQL/basics/05_grouping_data/#grouping-data","text":"","title":"Grouping data"},{"location":"Databases/MySQL/basics/05_grouping_data/#group-by","text":"the GROUP BY clause groups a set of rows into a set of summary rows by values of columns or expressions. It returns one row for each group. SELECT c1, c2,..., cn, aggregate_function(ci) FROM table WHERE where_conditions GROUP BY c1 , c2,...,cn; The GROUP BY clause must appear after FROM and WHERE clauses. SELECT status, COUNT(*) FROM orders GROUP BY status; It also allows to sort the groups: SELECT status, COUNT(*) FROM orders GROUP BY status DESC;","title":"GROUP BY"},{"location":"Databases/MySQL/basics/05_grouping_data/#having","text":"The HAVING clause is often used with the GROUP BY to filter groups based on a specific condition. If it' s ommited, HAVING behaves like WHERE . SELECT ordernumber, SUM(quantityOrdered) AS itemsCount, SUM(priceeach*quantityOrdered) AS total FROM orderdetails GROUP BY ordernumber HAVING total > 1000;","title":"HAVING"},{"location":"Databases/MySQL/basics/05_grouping_data/#rollup","text":"A grouping set is a set of columns to which you want to group. The ROLLUP clause is an extension of the GROUP BY clause with the following syntax: SELECT select_list FROM table_name GROUP BY c1, c2, c3 WITH ROLLUP; The ROLLUP generates multiple grouping sets based on the columns or expression specified in the GROUP BY clause.","title":"ROLLUP"},{"location":"Databases/MySQL/basics/05_grouping_data/#grouping-function","text":"To check whether NULL in the results set represents the subtotals or grand totals, you can use GROUPING() function. The GROUPING function will return 1 when NULL occurs in a supper-aggregate row, otherwise it will return 0. SELECT IF(GROUPING(orderYear), 'All Years', orderYear) orderYear, IF(GROUPING(productLine), 'All Product Lines', productLine) productLine, SUM(orderValue) totalOrderValue FROM sales GROUP BY orderYear , productline WITH ROLLUP;","title":"GROUPING function"},{"location":"Databases/MySQL/basics/06_sub_query/","text":"MySQL Subquery \u00b6 A MySQL subquery is a query that is nested within another query. It can be nested into another subquery as well. SELECT lastName, firstName FROM employees WHERE officeCode IN (SELECT officeCode FROM offices WHERE country = 'USA'); The subqueries can located in WHERE clauses, IN , EXISTS , operators, FROM clauses.","title":"MySQL Subquery"},{"location":"Databases/MySQL/basics/06_sub_query/#mysql-subquery","text":"A MySQL subquery is a query that is nested within another query. It can be nested into another subquery as well. SELECT lastName, firstName FROM employees WHERE officeCode IN (SELECT officeCode FROM offices WHERE country = 'USA'); The subqueries can located in WHERE clauses, IN , EXISTS , operators, FROM clauses.","title":"MySQL Subquery"},{"location":"Databases/MySQL/basics/07_cte/","text":"Common table expression or CTE \u00b6 A common table expression is not stored as an object and last only during the execution of a query. Different from a derived table, a CTE can be self-referencing or can be referenced multiple times in the same query. In addition, a CTE provides better readability and performance in comparison of a derived table. The structure of a CTE includes the name, an optional column list, and a query that defines it. After the CTE definition, you can use it lika view in SELECT , INSERT , UPDATE , DELETE , or CREATE VIEW statement. WITH cte_name (column_list) AS ( query ) SELECT * FROM cte_name; The number of columns in the query must be the same as the number of columns in the column_list. WITH customers_in_usa AS ( SELECT customerName, state FROM customers WHERE country = 'USA' ) SELECT customerName FROM customers_in_usa WHERE state = 'CA' ORDER BY customerName; ITH salesrep AS ( SELECT employeeNumber, CONCAT(firstName, ' ', lastName) AS salesrepName FROM employees WHERE jobTitle = 'Sales Rep' ), customer_salesrep AS ( SELECT customerName, salesrepName FROM customers INNER JOIN salesrep ON employeeNumber = salesrepEmployeeNumber ) SELECT * FROM customer_salesrep ORDER BY customerName; Recursive CTE \u00b6 WITH RECURSIVE cte_name AS ( initial_query -- anchor member UNION ALL recursive_query -- recursive member that references to the CTE name ) SELECT * FROM cte_name; WITH RECURSIVE employee_paths AS ( SELECT employeeNumber, reportsTo managerNumber, officeCode, 1 lvl FROM employees WHERE reportsTo IS NULL UNION ALL SELECT e.employeeNumber, e.reportsTo, e.officeCode, lvl+1 FROM employees e INNER JOIN employee_paths ep ON ep.employeeNumber = e.reportsTo ) SELECT employeeNumber, managerNumber, lvl, city FROM employee_paths ep INNER JOIN offices o USING (officeCode) ORDER BY lvl, city;","title":"Common table expression or CTE"},{"location":"Databases/MySQL/basics/07_cte/#common-table-expression-or-cte","text":"A common table expression is not stored as an object and last only during the execution of a query. Different from a derived table, a CTE can be self-referencing or can be referenced multiple times in the same query. In addition, a CTE provides better readability and performance in comparison of a derived table. The structure of a CTE includes the name, an optional column list, and a query that defines it. After the CTE definition, you can use it lika view in SELECT , INSERT , UPDATE , DELETE , or CREATE VIEW statement. WITH cte_name (column_list) AS ( query ) SELECT * FROM cte_name; The number of columns in the query must be the same as the number of columns in the column_list. WITH customers_in_usa AS ( SELECT customerName, state FROM customers WHERE country = 'USA' ) SELECT customerName FROM customers_in_usa WHERE state = 'CA' ORDER BY customerName; ITH salesrep AS ( SELECT employeeNumber, CONCAT(firstName, ' ', lastName) AS salesrepName FROM employees WHERE jobTitle = 'Sales Rep' ), customer_salesrep AS ( SELECT customerName, salesrepName FROM customers INNER JOIN salesrep ON employeeNumber = salesrepEmployeeNumber ) SELECT * FROM customer_salesrep ORDER BY customerName;","title":"Common table expression or CTE"},{"location":"Databases/MySQL/basics/07_cte/#recursive-cte","text":"WITH RECURSIVE cte_name AS ( initial_query -- anchor member UNION ALL recursive_query -- recursive member that references to the CTE name ) SELECT * FROM cte_name; WITH RECURSIVE employee_paths AS ( SELECT employeeNumber, reportsTo managerNumber, officeCode, 1 lvl FROM employees WHERE reportsTo IS NULL UNION ALL SELECT e.employeeNumber, e.reportsTo, e.officeCode, lvl+1 FROM employees e INNER JOIN employee_paths ep ON ep.employeeNumber = e.reportsTo ) SELECT employeeNumber, managerNumber, lvl, city FROM employee_paths ep INNER JOIN offices o USING (officeCode) ORDER BY lvl, city;","title":"Recursive CTE"},{"location":"Databases/MySQL/basics/09_set_operators/","text":"Using set operators \u00b6 MySQL UNION operator \u00b6 MySQL UNION operator allows you to combine two or more result sets of queries into a single result set. SELECT column_list UNION [DISTINCT | ALL] SELECT column_list UNION [DISTINCT | ALL] SELECT column_list ... To combine result set of two or more queries using the UNION operator, there are the basic rules that you must follow: First, the number and the orders of columns that appear in all SELECT statements must be the same; Second, the data types of columns must be the same or convertible. By default, the UNION operator removes duplicate rows even if you don't specify the DISTINCT operator explicity. If you use UNION ALL explicitly, the duplicate rows, if available, remain in the results. Because UNION ALL does not need to handle duplicates, it performs faster than UNION DISTINCT . INTERSECT \u00b6 The INTERSECT operator is a set operator that returns only distinct rows of two queries or more. (SELECT column_list FROM table_1) INTERSECT (SELECT column_list FROM table_2); Same as the UNION, it has to have the same order and number of columns. The data types must be compatible. MINUS \u00b6 MINUS compares results of two queries and returns distinct rows from the first query that aren' t output by the second query. SELECT column_list_1 FROM table_1 MINUS SELECT columns_list_2 FROM table_2; Same as the UNION, it has to have the same order and number of columns. The data types must be compatible.","title":"Using set operators"},{"location":"Databases/MySQL/basics/09_set_operators/#using-set-operators","text":"","title":"Using set operators"},{"location":"Databases/MySQL/basics/09_set_operators/#mysql-union-operator","text":"MySQL UNION operator allows you to combine two or more result sets of queries into a single result set. SELECT column_list UNION [DISTINCT | ALL] SELECT column_list UNION [DISTINCT | ALL] SELECT column_list ... To combine result set of two or more queries using the UNION operator, there are the basic rules that you must follow: First, the number and the orders of columns that appear in all SELECT statements must be the same; Second, the data types of columns must be the same or convertible. By default, the UNION operator removes duplicate rows even if you don't specify the DISTINCT operator explicity. If you use UNION ALL explicitly, the duplicate rows, if available, remain in the results. Because UNION ALL does not need to handle duplicates, it performs faster than UNION DISTINCT .","title":"MySQL UNION operator"},{"location":"Databases/MySQL/basics/09_set_operators/#intersect","text":"The INTERSECT operator is a set operator that returns only distinct rows of two queries or more. (SELECT column_list FROM table_1) INTERSECT (SELECT column_list FROM table_2); Same as the UNION, it has to have the same order and number of columns. The data types must be compatible.","title":"INTERSECT"},{"location":"Databases/MySQL/basics/09_set_operators/#minus","text":"MINUS compares results of two queries and returns distinct rows from the first query that aren' t output by the second query. SELECT column_list_1 FROM table_1 MINUS SELECT columns_list_2 FROM table_2; Same as the UNION, it has to have the same order and number of columns. The data types must be compatible.","title":"MINUS"},{"location":"Databases/MySQL/basics/10_modifying_data/","text":"Modifying data \u00b6 INSERT \u00b6 The INSERT statement allows you to insert one or more rows into a table. The following illustrates the syntax: INSERT INTO table(c1,c2,...) VALUES (v1,v2,...); First, you specify the table nme and list of comma-seperated columns inside parentheses. Then, you put a comma-separated lists of values of the corresponding columns inside. INSERT INTO table(c1,c2,...) VALUES (v11,v12,...), (v21,v22,...), ... (vnn,vn2,...); INSERT INTO SELECT \u00b6 Besides using row values in the VALUES clause, you can use the result of a SELECT statement as the data source for the INSERT statement. INSERT INTO table_name(column_list) SELECT select_list FROM another_table; INSERT IGNORE \u00b6 When you use INSERT statement to add multiple rows to a table and if an error occurs during the processing, MySQL termintates the statement and returns an error. As the result, no rows are inserted into the table. However, if you use INSERT IGNORE statement, the rows with invalid date that cause the error are ignored and the rows with valid data are inserted into the table. INSERT IGNORE INTO table(column_list) VALUES( value_list), ( value_list), ... UPDATE \u00b6 We use UPDATE statement to update existing data in a table. We can use the UPDATE statement to change column values of a single row, group of rows or all rows in a table. UPDATE [LOW_PRIORITY] [IGNORE] table_name SET column_name1 = expr1, column_name2 = expr2, ... WHERE condition; Note when not providing WHERE statemtnt, it will update all the rows in the table. MySQL supports two modifiers to the UPDATE statement. The LOW_PRIORITY modifier instructs the UPDATE statement to delay the update until there is no connection reading data from the table. The LOW_PRIORITY takes effect for the storage engines that use table-level locking only. The IGNORE modifier enables the UPDATE statement to continue updating rows even if errors occured. UPDATE employees SET email = 'mary.patterson@classicmodelcars.com' WHERE employeeNumber = 1056; UPDATE customers SET salesRepEmployeeNumber = (SELECT employeeNumber FROM employees WHERE jobtitle = 'Sales Rep' LIMIT 1) WHERE salesRepEmployeeNumber IS NULL; UPDATE JOIN \u00b6 We often use join clauses to query rows in a table that have or may not have corresponding rows in another table. In MySQL we can use the JOIN clauses in the UPDATE statement to perform cross-table update. UPDATE T1, T2, [INNER JOIN | LEFT JOIN] T1 ON T1.C1 = T2. C1 SET T1.C2 = T2.C2, T2.C3 = expr WHERE condition UPDATE T1,T2 INNER JOIN T2 ON T1.C1 = T2.C1 SET T1.C2 = T2.C2, T2.C3 = expr WHERE condition DELETE \u00b6 To delete data from a table you can use the DELETE statement. DELETE FROM table_name WHERE condition; You must specify the table you want to delete data from. Use a condition to select which records you want to delete. The WHERE clause is optional, but with out it - it will delete every single record in the table. DELETE FROM employees WHERE officeCode = 4; DELETE FROM customers WHERE country = 'France' ORDER BY creditLimit LIMIT 5; ON DELETE CASCADE \u00b6 The ON DELETE CASCADE is used to delete referential data from child tables automatically when you delete the data from parent table. CREATE TABLE buildings ( building_no INT PRIMARY KEY AUTO_INCREMENT, building_name VARCHAR(255) NOT NULL, address VARCHAR(255) NOT NULL ); CREATE TABLE rooms ( room_no INT PRIMARY KEY AUTO_INCREMENT, room_name VARCHAR(255) NOT NULL, building_no INT NOT NULL, FOREIGN KEY (building_no) REFERENCES buildings (building_no) ON DELETE CASCADE ); Whenever the building gets deleted that room is referenced to. The room will be deleted too. Finding tables affected by MySQL ON DELETE CASCADE action \u00b6 USE information_schema; SELECT table_name FROM referential_constraints WHERE constraint_schema = 'classicmodels' AND referenced_table_name = 'buildings' AND delete_rule = 'CASCADE' DELETE JOIN \u00b6 MySQL also allows you to use JOIN clause in the DELETE statement to delete rows from a table and the matching rows in another table. DELETE T1, T2 FROM T1 INNER JOIN T2 ON T1.key = T2.key WHERE condition; DELETE t1,t2 FROM t1 INNER JOIN t2 ON t2.ref = t1.id WHERE t1.id = 1; DELETE T1 FROM T1 LEFT JOIN T2 ON T1.key = T2.key WHERE T2.key IS NULL; REPLACE \u00b6 The >REPLACE statement is a MySQL extension to the standard SQL. The MySQL REPLACE statement works as follows: If the new row already does not exist, the REPLACEMENT statement inserts a new row. If the new row already exist, the REPLACE statement deletes the old row and then inserts a new row. To determine whether the new row already exists in the table, MySQL uses PRIMARY KEY or UNIQUE KEY index. If the table does not have one of these indexes, the REPLACE statement is equivalent to insert statement. REPLACE INTO cities(name) VALUES('Houston'); REPLACE INTO cities SET id = 4, name = 'Phoenix', population = 1768980; REPLACE INTO cities(name,population) SELECT name,population FROM cities WHERE id = 1; Prepared statement \u00b6 Prior MySQL version 4.1, the query is sent to the MySQL server in the textual format. In turn, MySQL returns the data to the client using textual protocol. MySQL has to fully parse the query and transform the result set into a string before returning it to the client. The textual protocol has serious performance implication. To resolve this problem, MySQL added a new feature called prepared statements. SELECT * FROM products WHERE productCode = ?; Usage \u00b6 In order to use MySQL prepared statement, you need to use other three MySQL statements as follows: PREPARE - prepares statement for execution EXECUTE - Executes a prepared statement preparing by a PREPARE statement. DEALLOCATE PREPARE - Releases prepared statement. PREPARE stmt1 FROM 'SELECT productCode, productName FROM products WHERE productCode = ?'; SET @pc = 'S10_1678'; EXECUTE stmt1 USING @pc; DEALLOCATE PREPARE stmt1;","title":"Modifying data"},{"location":"Databases/MySQL/basics/10_modifying_data/#modifying-data","text":"","title":"Modifying data"},{"location":"Databases/MySQL/basics/10_modifying_data/#insert","text":"The INSERT statement allows you to insert one or more rows into a table. The following illustrates the syntax: INSERT INTO table(c1,c2,...) VALUES (v1,v2,...); First, you specify the table nme and list of comma-seperated columns inside parentheses. Then, you put a comma-separated lists of values of the corresponding columns inside. INSERT INTO table(c1,c2,...) VALUES (v11,v12,...), (v21,v22,...), ... (vnn,vn2,...);","title":"INSERT"},{"location":"Databases/MySQL/basics/10_modifying_data/#insert-into-select","text":"Besides using row values in the VALUES clause, you can use the result of a SELECT statement as the data source for the INSERT statement. INSERT INTO table_name(column_list) SELECT select_list FROM another_table;","title":"INSERT INTO SELECT"},{"location":"Databases/MySQL/basics/10_modifying_data/#insert-ignore","text":"When you use INSERT statement to add multiple rows to a table and if an error occurs during the processing, MySQL termintates the statement and returns an error. As the result, no rows are inserted into the table. However, if you use INSERT IGNORE statement, the rows with invalid date that cause the error are ignored and the rows with valid data are inserted into the table. INSERT IGNORE INTO table(column_list) VALUES( value_list), ( value_list), ...","title":"INSERT IGNORE"},{"location":"Databases/MySQL/basics/10_modifying_data/#update","text":"We use UPDATE statement to update existing data in a table. We can use the UPDATE statement to change column values of a single row, group of rows or all rows in a table. UPDATE [LOW_PRIORITY] [IGNORE] table_name SET column_name1 = expr1, column_name2 = expr2, ... WHERE condition; Note when not providing WHERE statemtnt, it will update all the rows in the table. MySQL supports two modifiers to the UPDATE statement. The LOW_PRIORITY modifier instructs the UPDATE statement to delay the update until there is no connection reading data from the table. The LOW_PRIORITY takes effect for the storage engines that use table-level locking only. The IGNORE modifier enables the UPDATE statement to continue updating rows even if errors occured. UPDATE employees SET email = 'mary.patterson@classicmodelcars.com' WHERE employeeNumber = 1056; UPDATE customers SET salesRepEmployeeNumber = (SELECT employeeNumber FROM employees WHERE jobtitle = 'Sales Rep' LIMIT 1) WHERE salesRepEmployeeNumber IS NULL;","title":"UPDATE"},{"location":"Databases/MySQL/basics/10_modifying_data/#update-join","text":"We often use join clauses to query rows in a table that have or may not have corresponding rows in another table. In MySQL we can use the JOIN clauses in the UPDATE statement to perform cross-table update. UPDATE T1, T2, [INNER JOIN | LEFT JOIN] T1 ON T1.C1 = T2. C1 SET T1.C2 = T2.C2, T2.C3 = expr WHERE condition UPDATE T1,T2 INNER JOIN T2 ON T1.C1 = T2.C1 SET T1.C2 = T2.C2, T2.C3 = expr WHERE condition","title":"UPDATE JOIN"},{"location":"Databases/MySQL/basics/10_modifying_data/#delete","text":"To delete data from a table you can use the DELETE statement. DELETE FROM table_name WHERE condition; You must specify the table you want to delete data from. Use a condition to select which records you want to delete. The WHERE clause is optional, but with out it - it will delete every single record in the table. DELETE FROM employees WHERE officeCode = 4; DELETE FROM customers WHERE country = 'France' ORDER BY creditLimit LIMIT 5;","title":"DELETE"},{"location":"Databases/MySQL/basics/10_modifying_data/#on-delete-cascade","text":"The ON DELETE CASCADE is used to delete referential data from child tables automatically when you delete the data from parent table. CREATE TABLE buildings ( building_no INT PRIMARY KEY AUTO_INCREMENT, building_name VARCHAR(255) NOT NULL, address VARCHAR(255) NOT NULL ); CREATE TABLE rooms ( room_no INT PRIMARY KEY AUTO_INCREMENT, room_name VARCHAR(255) NOT NULL, building_no INT NOT NULL, FOREIGN KEY (building_no) REFERENCES buildings (building_no) ON DELETE CASCADE ); Whenever the building gets deleted that room is referenced to. The room will be deleted too.","title":"ON DELETE CASCADE"},{"location":"Databases/MySQL/basics/10_modifying_data/#finding-tables-affected-by-mysql-on-delete-cascade-action","text":"USE information_schema; SELECT table_name FROM referential_constraints WHERE constraint_schema = 'classicmodels' AND referenced_table_name = 'buildings' AND delete_rule = 'CASCADE'","title":"Finding tables affected by MySQL ON DELETE CASCADE action"},{"location":"Databases/MySQL/basics/10_modifying_data/#delete-join","text":"MySQL also allows you to use JOIN clause in the DELETE statement to delete rows from a table and the matching rows in another table. DELETE T1, T2 FROM T1 INNER JOIN T2 ON T1.key = T2.key WHERE condition; DELETE t1,t2 FROM t1 INNER JOIN t2 ON t2.ref = t1.id WHERE t1.id = 1; DELETE T1 FROM T1 LEFT JOIN T2 ON T1.key = T2.key WHERE T2.key IS NULL;","title":"DELETE JOIN"},{"location":"Databases/MySQL/basics/10_modifying_data/#replace","text":"The >REPLACE statement is a MySQL extension to the standard SQL. The MySQL REPLACE statement works as follows: If the new row already does not exist, the REPLACEMENT statement inserts a new row. If the new row already exist, the REPLACE statement deletes the old row and then inserts a new row. To determine whether the new row already exists in the table, MySQL uses PRIMARY KEY or UNIQUE KEY index. If the table does not have one of these indexes, the REPLACE statement is equivalent to insert statement. REPLACE INTO cities(name) VALUES('Houston'); REPLACE INTO cities SET id = 4, name = 'Phoenix', population = 1768980; REPLACE INTO cities(name,population) SELECT name,population FROM cities WHERE id = 1;","title":"REPLACE"},{"location":"Databases/MySQL/basics/10_modifying_data/#prepared-statement","text":"Prior MySQL version 4.1, the query is sent to the MySQL server in the textual format. In turn, MySQL returns the data to the client using textual protocol. MySQL has to fully parse the query and transform the result set into a string before returning it to the client. The textual protocol has serious performance implication. To resolve this problem, MySQL added a new feature called prepared statements. SELECT * FROM products WHERE productCode = ?;","title":"Prepared statement"},{"location":"Databases/MySQL/basics/10_modifying_data/#usage","text":"In order to use MySQL prepared statement, you need to use other three MySQL statements as follows: PREPARE - prepares statement for execution EXECUTE - Executes a prepared statement preparing by a PREPARE statement. DEALLOCATE PREPARE - Releases prepared statement. PREPARE stmt1 FROM 'SELECT productCode, productName FROM products WHERE productCode = ?'; SET @pc = 'S10_1678'; EXECUTE stmt1 USING @pc; DEALLOCATE PREPARE stmt1;","title":"Usage"},{"location":"Databases/MySQL/basics/11_transactions_and_table_locking/","text":"Transactions \u00b6 MySQL transaction allows you to execute a set of MySQL operations to ensure that the database never contains the result of parial operations. In a set of operations, if one of them fails, the rollback occurs to restore the database to it's original state. If no error occurs, the entire set of statements is commited to the database. Transaction statements \u00b6 To start a transaction you use START TRANSACTION statement. The BEGIN or BEGIN WORK are aliases to it. To commit the current transaction and make it' s changes permanent, you use COMMIT statement. To roll back the current tranaction and cancel it's changes, you use ROLLBACK statement. To disable or enable the auto-commit mode for the current transaction, you use SET autocommit SET autocommit = 0; Transaction example: -- 1. start a new transaction START TRANSACTION; -- 2. Get the latest order number SELECT @orderNumber:=MAX(orderNUmber)+1 FROM orders; -- 3. insert a new order for customer 145 INSERT INTO orders(orderNumber, orderDate, requiredDate, shippedDate, status, customerNumber) VALUES(@orderNumber, '2005-05-31', '2005-06-10', '2005-06-11', 'In Process', 145); -- 4. Insert order line items INSERT INTO orderdetails(orderNumber, productCode, quantityOrdered, priceEach, orderLineNumber) VALUES(@orderNumber,'S18_1749', 30, '136', 1), (@orderNumber,'S18_2248', 50, '55.09', 2); -- 5. commit changes COMMIT; Table locking \u00b6 A lock is a flag associated with a table. MySQL allows a client session to explicitly acquire a table lock for preventing other sessions from accessing the same table during a specific period. A client session can acquire or release table locks only for itself. LOCK TABLES table_name [READ | WRITE] To lock a table you specify its name after the LOCK TABLES keywords. In addition, you specify the type of lock, either READ or WRITE . To release a lock for a table: UNLOCK TABLES; Read locks \u00b6 A READ lock for a table can be acquired by multiple sessions at the same time. In addition, other sessions can read data from the table without acquiring the lock. The session that holds the READ lockcan only read data from the table, but cannot write. In addition, other sessions cannot write data to the table until READ lock is released. The write operations from another session will be put into the waiting states until the READ lock is released. If the session is terminated, either normally or abnormally, MySQL will release all the locks implicitly. Write locks \u00b6 The only session that holds the lock of a table can read and write data from the table. Other sessions cannot read data from and write data to the table until the WRITE lock is released.","title":"Transactions"},{"location":"Databases/MySQL/basics/11_transactions_and_table_locking/#transactions","text":"MySQL transaction allows you to execute a set of MySQL operations to ensure that the database never contains the result of parial operations. In a set of operations, if one of them fails, the rollback occurs to restore the database to it's original state. If no error occurs, the entire set of statements is commited to the database.","title":"Transactions"},{"location":"Databases/MySQL/basics/11_transactions_and_table_locking/#transaction-statements","text":"To start a transaction you use START TRANSACTION statement. The BEGIN or BEGIN WORK are aliases to it. To commit the current transaction and make it' s changes permanent, you use COMMIT statement. To roll back the current tranaction and cancel it's changes, you use ROLLBACK statement. To disable or enable the auto-commit mode for the current transaction, you use SET autocommit SET autocommit = 0; Transaction example: -- 1. start a new transaction START TRANSACTION; -- 2. Get the latest order number SELECT @orderNumber:=MAX(orderNUmber)+1 FROM orders; -- 3. insert a new order for customer 145 INSERT INTO orders(orderNumber, orderDate, requiredDate, shippedDate, status, customerNumber) VALUES(@orderNumber, '2005-05-31', '2005-06-10', '2005-06-11', 'In Process', 145); -- 4. Insert order line items INSERT INTO orderdetails(orderNumber, productCode, quantityOrdered, priceEach, orderLineNumber) VALUES(@orderNumber,'S18_1749', 30, '136', 1), (@orderNumber,'S18_2248', 50, '55.09', 2); -- 5. commit changes COMMIT;","title":"Transaction statements"},{"location":"Databases/MySQL/basics/11_transactions_and_table_locking/#table-locking","text":"A lock is a flag associated with a table. MySQL allows a client session to explicitly acquire a table lock for preventing other sessions from accessing the same table during a specific period. A client session can acquire or release table locks only for itself. LOCK TABLES table_name [READ | WRITE] To lock a table you specify its name after the LOCK TABLES keywords. In addition, you specify the type of lock, either READ or WRITE . To release a lock for a table: UNLOCK TABLES;","title":"Table locking"},{"location":"Databases/MySQL/basics/11_transactions_and_table_locking/#read-locks","text":"A READ lock for a table can be acquired by multiple sessions at the same time. In addition, other sessions can read data from the table without acquiring the lock. The session that holds the READ lockcan only read data from the table, but cannot write. In addition, other sessions cannot write data to the table until READ lock is released. The write operations from another session will be put into the waiting states until the READ lock is released. If the session is terminated, either normally or abnormally, MySQL will release all the locks implicitly.","title":"Read locks"},{"location":"Databases/MySQL/basics/11_transactions_and_table_locking/#write-locks","text":"The only session that holds the lock of a table can read and write data from the table. Other sessions cannot read data from and write data to the table until the WRITE lock is released.","title":"Write locks"},{"location":"Databases/MySQL/basics/12_managing_databases_and_tables/","text":"Managing databases and tables \u00b6 Managing a database \u00b6 Use / select database USE database_name Get the name of currently selected database SELECT database() Create database CREATE DATABASE [IF NOT EXISTS] database_name [CHARACTER SET character_set] [COLLATE collation_name]; Displaying databases SHOW DATABASES Removing database DROP DATABASE [IF EXISTS] database_name; Tables \u00b6 Create table CREATE TABLE IF NOT EXISTS tasks ( task_id INT AUTO_INCREMENT, title VARCHAR(255) NOT NULL, start_date DATE, due_date DATE, status TINYINT NOT NULL, priority TINYINT NOT NULL, description TEXT, PRIMARY KEY (task_id) ) ENGINE=INNODB; Alter table ALTER TABLE tasks CHANGE COLUMN task_id task_id INT(11) NOT NULL AUTO_INCREMENT; ALTER TABLE tasks ADD COLUMN complete DECIMAL(2,1) NULL AFTER description; ALTER TABLE tasks DROP COLUMN description; ALTER TABLE tasks RENAME TO work_items; Drop table DROP [TEMPORARY] TABLE [IF EXISTS] table_name [, table_name] ... [RESTRICT | CASCADE] Temporary table CREATE TEMPORARY TABLE top10customers SELECT p.customerNumber, c.customerName, ROUND(SUM(p.amount),2) sales FROM payments p INNER JOIN customers c ON c.customerNumber = p.customerNumber GROUP BY p.customerNumber ORDER BY sales DESC LIMIT 10; DROP TEMPORARY TABLE top10customers;","title":"Managing databases and tables"},{"location":"Databases/MySQL/basics/12_managing_databases_and_tables/#managing-databases-and-tables","text":"","title":"Managing databases and tables"},{"location":"Databases/MySQL/basics/12_managing_databases_and_tables/#managing-a-database","text":"Use / select database USE database_name Get the name of currently selected database SELECT database() Create database CREATE DATABASE [IF NOT EXISTS] database_name [CHARACTER SET character_set] [COLLATE collation_name]; Displaying databases SHOW DATABASES Removing database DROP DATABASE [IF EXISTS] database_name;","title":"Managing a database"},{"location":"Databases/MySQL/basics/12_managing_databases_and_tables/#tables","text":"Create table CREATE TABLE IF NOT EXISTS tasks ( task_id INT AUTO_INCREMENT, title VARCHAR(255) NOT NULL, start_date DATE, due_date DATE, status TINYINT NOT NULL, priority TINYINT NOT NULL, description TEXT, PRIMARY KEY (task_id) ) ENGINE=INNODB; Alter table ALTER TABLE tasks CHANGE COLUMN task_id task_id INT(11) NOT NULL AUTO_INCREMENT; ALTER TABLE tasks ADD COLUMN complete DECIMAL(2,1) NULL AFTER description; ALTER TABLE tasks DROP COLUMN description; ALTER TABLE tasks RENAME TO work_items; Drop table DROP [TEMPORARY] TABLE [IF EXISTS] table_name [, table_name] ... [RESTRICT | CASCADE] Temporary table CREATE TEMPORARY TABLE top10customers SELECT p.customerNumber, c.customerName, ROUND(SUM(p.amount),2) sales FROM payments p INNER JOIN customers c ON c.customerNumber = p.customerNumber GROUP BY p.customerNumber ORDER BY sales DESC LIMIT 10; DROP TEMPORARY TABLE top10customers;","title":"Tables"},{"location":"Databases/MySQL/basics/13_data_types/","text":"Data types \u00b6 Numeric types \u00b6 Type Description TINYINT A very small integer SMALLINT A small integer MEDIUMINT A medium-sized integer INT A standard integer BIGINT A large integer DECIMAL A fixed-point number FLOAT A single-precision floating point number DOUBLE A double-precision floating point number BIT A bit field MySQL does not have the built-in BOOLEAN or BOOL data type. To represent boolean values, the MySQL uses the smallest integer type which is TINYINT(1) . String types \u00b6 Can hold anything from plain text to binary data such as images or files. Strings can be compared and searched in pattern matching by using LIKE operator, regular expression and full text search. Type Description CHAR A fixed-length nonbinary (character) string VARCHAR A variable-length non-binary string BINARY A fixed-length binary string VARBINARY A variable-length binary string TINYBLOB A very small BLOB (binary large object) BLOB A small BLOB MEDIUMBLOB A medium-sized BLOB LONGBLOB A large BLOB TINYTEXT A very small non-binary string TEXT A small non-binary string MEDIUMTEXT A medium-sized non-binary string LONGTEXT A large non-binary string ENUM An enumeration; each column value may be assigned one enumeration member SET A set; each column value may be assigned zero or more SET members Date and time types \u00b6 Type Description DATE A date value in CCYY-MM-DD format TIME A time value in hh:mm:ss format DATETIME A date and time value inCCYY-MM-DD hh:mm:ssformat TIMESTAMP A timestamp value in CCYY-MM-DD hh:mm:ss format YEAR A year value in CCYY or YY format Spatial data types \u00b6 Type Description GEOMETRY A spatial value of any type POINT A point (a pair of X-Y coordinates) LINESTRING A curve (one or more POINT values) POLYGON A polygon GEOMETRYCOLLECTION A collection of GEOMETRYvalues MULTILINESTRING A collection of LINESTRINGvalues MULTIPOINT A collection of POINTvalues MULTIPOLYGON A collection of POLYGONvalues","title":"Data types"},{"location":"Databases/MySQL/basics/13_data_types/#data-types","text":"","title":"Data types"},{"location":"Databases/MySQL/basics/13_data_types/#numeric-types","text":"Type Description TINYINT A very small integer SMALLINT A small integer MEDIUMINT A medium-sized integer INT A standard integer BIGINT A large integer DECIMAL A fixed-point number FLOAT A single-precision floating point number DOUBLE A double-precision floating point number BIT A bit field MySQL does not have the built-in BOOLEAN or BOOL data type. To represent boolean values, the MySQL uses the smallest integer type which is TINYINT(1) .","title":"Numeric types"},{"location":"Databases/MySQL/basics/13_data_types/#string-types","text":"Can hold anything from plain text to binary data such as images or files. Strings can be compared and searched in pattern matching by using LIKE operator, regular expression and full text search. Type Description CHAR A fixed-length nonbinary (character) string VARCHAR A variable-length non-binary string BINARY A fixed-length binary string VARBINARY A variable-length binary string TINYBLOB A very small BLOB (binary large object) BLOB A small BLOB MEDIUMBLOB A medium-sized BLOB LONGBLOB A large BLOB TINYTEXT A very small non-binary string TEXT A small non-binary string MEDIUMTEXT A medium-sized non-binary string LONGTEXT A large non-binary string ENUM An enumeration; each column value may be assigned one enumeration member SET A set; each column value may be assigned zero or more SET members","title":"String types"},{"location":"Databases/MySQL/basics/13_data_types/#date-and-time-types","text":"Type Description DATE A date value in CCYY-MM-DD format TIME A time value in hh:mm:ss format DATETIME A date and time value inCCYY-MM-DD hh:mm:ssformat TIMESTAMP A timestamp value in CCYY-MM-DD hh:mm:ss format YEAR A year value in CCYY or YY format","title":"Date and time types"},{"location":"Databases/MySQL/basics/13_data_types/#spatial-data-types","text":"Type Description GEOMETRY A spatial value of any type POINT A point (a pair of X-Y coordinates) LINESTRING A curve (one or more POINT values) POLYGON A polygon GEOMETRYCOLLECTION A collection of GEOMETRYvalues MULTILINESTRING A collection of LINESTRINGvalues MULTIPOINT A collection of POINTvalues MULTIPOLYGON A collection of POLYGONvalues","title":"Spatial data types"},{"location":"Databases/MySQL/basics/14_constraints/","text":"Constraints \u00b6 NOT NULL Constraint \u00b6 The NOT NULL constraint is a column constraint that forces the values of a column to non-NULL values only. CREATE TABLE tasks ( id INT AUTO_INCREMENT PRIMARY KEY, title VARCHAR(255) NOT NULL, start_date DATE NOT NULL, end_date DATE ); ALTER TABLE tasks CHANGE end_date end_date DATE NOT NULL; Verify change DESCRIBE tasks; Primary key \u00b6 A primary key is a column or a set of columns that uniquely identifies each row in the table. You must follow the rules below when you define a primary key for a table: Primary key must contain unique values. If the primary key consists of multiple columns, the combination of values in these columns must be unique. A primary key column cannot contain NULL values. It means that you have to declare primary key column with the NOT NULL attribute. I f you dont, MySQL will force the primary key column as NOT NULL implicitly. A table has only one primary key. Because MySQL works faster with integers, the data type of the primary key columns should be integer. However, you should make sure that the value ranges of the integer type for the primary key is sufficient for storing all possible rows that table may have. A primary key often has AUTO_INCREMENT attribute that generates a unique sequence for the key automatically. CREATE TABLE users( user_id INT AUTO_INCREMENT PRIMARY KEY, username VARCHAR(40), password VARCHAR(255), email VARCHAR(255) ); CREATE TABLE roles( role_id INT AUTO_INCREMENT, role_name VARCHAR(50), PRIMARY KEY(role_id) ); ALTER TABLE table_name ADD PRIMARY KEY(primary_key_column); Foreign key \u00b6 A foreign key is a field in a table that matches another field of another table. A freign key places constraints on data in the related tables. ALTER TABLE products ADD FOREIGN KEY fk_vendor(vdr_id) REFERENCES vendors(vdr_id) ON DELETE NO ACTION ON UPDATE CASCADE; ALTER TABLE table_name DROP FOREIGN KEY constraint_name; CREATE TABLE products ( prd_id int(11) NOT NULL AUTO_INCREMENT, prd_name varchar(355) NOT NULL, prd_price decimal(10,0) DEFAULT NULL, cat_id int(11) NOT NULL, vdr_id int(11) NOT NULL, PRIMARY KEY (prd_id), KEY fk_cat (cat_id), KEY fk_vendor(vdr_id), CONSTRAINT products_ibfk_2 FOREIGN KEY (vdr_id) REFERENCES vendors (vdr_id) ON DELETE NO ACTION ON UPDATE CASCADE, CONSTRAINT products_ibfk_1 FOREIGN KEY (cat_id) REFERENCES categories (cat_id) ON UPDATE CASCADE ) ENGINE=InnoDB; Disable foreign key checks SET foreign_key_checks = 0; Unique constraint \u00b6 The UNIQUE constraint is either column constraint or table constraint that defines a rule that constraints values in a column or a group of columns to be unique. CREATE TABLE table_1( ... column_name_1 data_type, column_name_2 data type, ... UNIQUE(column_name_1,column_name_2) ); CREATE TABLE table_1( ... column_name_1 data_type, column_name_2 data type, ... CONSTRAINT constraint_name UNIQUE(column_name_1,column_name_2) );","title":"Constraints"},{"location":"Databases/MySQL/basics/14_constraints/#constraints","text":"","title":"Constraints"},{"location":"Databases/MySQL/basics/14_constraints/#not-null-constraint","text":"The NOT NULL constraint is a column constraint that forces the values of a column to non-NULL values only. CREATE TABLE tasks ( id INT AUTO_INCREMENT PRIMARY KEY, title VARCHAR(255) NOT NULL, start_date DATE NOT NULL, end_date DATE ); ALTER TABLE tasks CHANGE end_date end_date DATE NOT NULL; Verify change DESCRIBE tasks;","title":"NOT NULL Constraint"},{"location":"Databases/MySQL/basics/14_constraints/#primary-key","text":"A primary key is a column or a set of columns that uniquely identifies each row in the table. You must follow the rules below when you define a primary key for a table: Primary key must contain unique values. If the primary key consists of multiple columns, the combination of values in these columns must be unique. A primary key column cannot contain NULL values. It means that you have to declare primary key column with the NOT NULL attribute. I f you dont, MySQL will force the primary key column as NOT NULL implicitly. A table has only one primary key. Because MySQL works faster with integers, the data type of the primary key columns should be integer. However, you should make sure that the value ranges of the integer type for the primary key is sufficient for storing all possible rows that table may have. A primary key often has AUTO_INCREMENT attribute that generates a unique sequence for the key automatically. CREATE TABLE users( user_id INT AUTO_INCREMENT PRIMARY KEY, username VARCHAR(40), password VARCHAR(255), email VARCHAR(255) ); CREATE TABLE roles( role_id INT AUTO_INCREMENT, role_name VARCHAR(50), PRIMARY KEY(role_id) ); ALTER TABLE table_name ADD PRIMARY KEY(primary_key_column);","title":"Primary key"},{"location":"Databases/MySQL/basics/14_constraints/#foreign-key","text":"A foreign key is a field in a table that matches another field of another table. A freign key places constraints on data in the related tables. ALTER TABLE products ADD FOREIGN KEY fk_vendor(vdr_id) REFERENCES vendors(vdr_id) ON DELETE NO ACTION ON UPDATE CASCADE; ALTER TABLE table_name DROP FOREIGN KEY constraint_name; CREATE TABLE products ( prd_id int(11) NOT NULL AUTO_INCREMENT, prd_name varchar(355) NOT NULL, prd_price decimal(10,0) DEFAULT NULL, cat_id int(11) NOT NULL, vdr_id int(11) NOT NULL, PRIMARY KEY (prd_id), KEY fk_cat (cat_id), KEY fk_vendor(vdr_id), CONSTRAINT products_ibfk_2 FOREIGN KEY (vdr_id) REFERENCES vendors (vdr_id) ON DELETE NO ACTION ON UPDATE CASCADE, CONSTRAINT products_ibfk_1 FOREIGN KEY (cat_id) REFERENCES categories (cat_id) ON UPDATE CASCADE ) ENGINE=InnoDB; Disable foreign key checks SET foreign_key_checks = 0;","title":"Foreign key"},{"location":"Databases/MySQL/basics/14_constraints/#unique-constraint","text":"The UNIQUE constraint is either column constraint or table constraint that defines a rule that constraints values in a column or a group of columns to be unique. CREATE TABLE table_1( ... column_name_1 data_type, column_name_2 data type, ... UNIQUE(column_name_1,column_name_2) ); CREATE TABLE table_1( ... column_name_1 data_type, column_name_2 data type, ... CONSTRAINT constraint_name UNIQUE(column_name_1,column_name_2) );","title":"Unique constraint"},{"location":"Databases/MySQL/basics/15_character_sets_and_collation/","text":"Character set and collation \u00b6 Character set \u00b6 SHOW CHARACTER SET SET @str = CONVERT('MySQL Character Set' USING ucs2); SELECT LENGTH(@str), CHAR_LENGTH(@str); CONVERT(expression USING character_set_name) CAST(string AS character_type CHARACTER SET character_set_name) SELECT CAST(_latin1'MySQL character set' AS CHAR CHARACTER SET utf8); SET NAMES 'utf8'; Collation \u00b6 SHOW COLLATION LIKE 'character_set_name%'; SHOW COLLATION LIKE 'latin1%'; ALTER TABLE t1 CHARACTER SET latin1 COLLATE latin1_german1_ci; ALTER TABLE t2 MODIFY c1 VARCHAR(25) CHARACTER SET latin1;","title":"Character set and collation"},{"location":"Databases/MySQL/basics/15_character_sets_and_collation/#character-set-and-collation","text":"","title":"Character set and collation"},{"location":"Databases/MySQL/basics/15_character_sets_and_collation/#character-set","text":"SHOW CHARACTER SET SET @str = CONVERT('MySQL Character Set' USING ucs2); SELECT LENGTH(@str), CHAR_LENGTH(@str); CONVERT(expression USING character_set_name) CAST(string AS character_type CHARACTER SET character_set_name) SELECT CAST(_latin1'MySQL character set' AS CHAR CHARACTER SET utf8); SET NAMES 'utf8';","title":"Character set"},{"location":"Databases/MySQL/basics/15_character_sets_and_collation/#collation","text":"SHOW COLLATION LIKE 'character_set_name%'; SHOW COLLATION LIKE 'latin1%'; ALTER TABLE t1 CHARACTER SET latin1 COLLATE latin1_german1_ci; ALTER TABLE t2 MODIFY c1 VARCHAR(25) CHARACTER SET latin1;","title":"Collation"},{"location":"Databases/MySQL/basics/16_import_export_csv_into_table/","text":"Import CSV file into MySQL table \u00b6 LOAD DATA INFILE 'c:/tmp/discounts.csv' INTO TABLE discounts FIELDS TERMINATED BY ',' ENCLOSED BY '\"' LINES TERMINATED BY '\\n' IGNORE 1 ROWS; LOAD DATA INFILE 'c:/tmp/discounts_2.csv' INTO TABLE discounts FIELDS TERMINATED BY ',' ENCLOSED BY '\"' LINES TERMINATED BY '\\n' IGNORE 1 ROWS (title,@expired_date,amount) SET expired_date = STR_TO_DATE(@expired_date, '%m/%d/%Y'); Export to CSV file \u00b6 SELECT orderNumber, status, orderDate, requiredDate, comments FROM orders WHERE status = 'Cancelled' INTO OUTFILE 'C:/tmp/cancelled_orders.csv' FIELDS ENCLOSED BY '\"' TERMINATED BY ';' ESCAPED BY '\"' LINES TERMINATED BY '\\r\\n'; SET @TS = DATE_FORMAT(NOW(),'_%Y_%m_%d_%H_%i_%s'); SET @FOLDER = 'c:/tmp/'; SET @PREFIX = 'orders'; SET @EXT = '.csv'; SET @CMD = CONCAT(\"SELECT * FROM orders INTO OUTFILE '\",@FOLDER,@PREFIX,@TS,@EXT, \"' FIELDS ENCLOSED BY '\\\"' TERMINATED BY ';' ESCAPED BY '\\\"'\", \" LINES TERMINATED BY '\\r\\n';\"); PREPARE statement FROM @CMD; EXECUTE statement; (SELECT 'Order Number','Order Date','Status') UNION (SELECT orderNumber,orderDate, status FROM orders INTO OUTFILE 'C:/tmp/orders.csv' FIELDS ENCLOSED BY '\"' TERMINATED BY ';' ESCAPED BY '\"' LINES TERMINATED BY '\\r\\n'); SELECT orderNumber, orderDate, IFNULL(shippedDate, 'N/A') FROM orders INTO OUTFILE 'C:/tmp/orders2.csv' FIELDS ENCLOSED BY '\"' TERMINATED BY ';' ESCAPED BY '\"' LINES TERMINATED BY '\\r\\n';","title":"Import CSV file into MySQL table"},{"location":"Databases/MySQL/basics/16_import_export_csv_into_table/#import-csv-file-into-mysql-table","text":"LOAD DATA INFILE 'c:/tmp/discounts.csv' INTO TABLE discounts FIELDS TERMINATED BY ',' ENCLOSED BY '\"' LINES TERMINATED BY '\\n' IGNORE 1 ROWS; LOAD DATA INFILE 'c:/tmp/discounts_2.csv' INTO TABLE discounts FIELDS TERMINATED BY ',' ENCLOSED BY '\"' LINES TERMINATED BY '\\n' IGNORE 1 ROWS (title,@expired_date,amount) SET expired_date = STR_TO_DATE(@expired_date, '%m/%d/%Y');","title":"Import CSV file into MySQL table"},{"location":"Databases/MySQL/basics/16_import_export_csv_into_table/#export-to-csv-file","text":"SELECT orderNumber, status, orderDate, requiredDate, comments FROM orders WHERE status = 'Cancelled' INTO OUTFILE 'C:/tmp/cancelled_orders.csv' FIELDS ENCLOSED BY '\"' TERMINATED BY ';' ESCAPED BY '\"' LINES TERMINATED BY '\\r\\n'; SET @TS = DATE_FORMAT(NOW(),'_%Y_%m_%d_%H_%i_%s'); SET @FOLDER = 'c:/tmp/'; SET @PREFIX = 'orders'; SET @EXT = '.csv'; SET @CMD = CONCAT(\"SELECT * FROM orders INTO OUTFILE '\",@FOLDER,@PREFIX,@TS,@EXT, \"' FIELDS ENCLOSED BY '\\\"' TERMINATED BY ';' ESCAPED BY '\\\"'\", \" LINES TERMINATED BY '\\r\\n';\"); PREPARE statement FROM @CMD; EXECUTE statement; (SELECT 'Order Number','Order Date','Status') UNION (SELECT orderNumber,orderDate, status FROM orders INTO OUTFILE 'C:/tmp/orders.csv' FIELDS ENCLOSED BY '\"' TERMINATED BY ';' ESCAPED BY '\"' LINES TERMINATED BY '\\r\\n'); SELECT orderNumber, orderDate, IFNULL(shippedDate, 'N/A') FROM orders INTO OUTFILE 'C:/tmp/orders2.csv' FIELDS ENCLOSED BY '\"' TERMINATED BY ';' ESCAPED BY '\"' LINES TERMINATED BY '\\r\\n';","title":"Export to CSV file"},{"location":"Databases/PostgreSQL/","text":"Sources: \u00b6 http://www.postgresqltutorial.com/","title":"Sources:"},{"location":"Databases/PostgreSQL/#sources","text":"http://www.postgresqltutorial.com/","title":"Sources:"},{"location":"Databases/PostgreSQL/SELECT%20Queries%20in%20PostgreSQL/","text":"Querying data - SELECT \u00b6 One of the most common tasks, when you work with PostgreSQL, is to query data from tables using the SELECT statement. The SELECT is one of the most complex statements in PostgreSQL. It has many clauses that you can combine to form a powerful query. Because of it's complexity, we divide the the PostgreSQL SELECT statement tutorial into many shorter tutorials so that you can learn each clause of the SELECT statement easier. The following are the clauses that appear in the SELECT statement: Select distinct rows by using DISTINCT operator. Filter rows by using WHERE clause Sort rows by using ORDER BY clause Select rows based on various operators such as BETWEEN , IN , LIKE . Group rows into groups using GROUP BY Apply conditions for groups using HAVING Join a table to other tables using INNER JOIN , LEFT JOIN , FULL OUTER JOIN , CROSS JOIN . In this tutorial we are using the SELECT with FROM clause. PostgreSQL SELECT statement syntax \u00b6 Let's start with a basic form of the SELECT statement that retrieves data from a single table. SELECT column1, column2, ... FROM table_name;","title":"Querying data - SELECT"},{"location":"Databases/PostgreSQL/SELECT%20Queries%20in%20PostgreSQL/#querying-data-select","text":"One of the most common tasks, when you work with PostgreSQL, is to query data from tables using the SELECT statement. The SELECT is one of the most complex statements in PostgreSQL. It has many clauses that you can combine to form a powerful query. Because of it's complexity, we divide the the PostgreSQL SELECT statement tutorial into many shorter tutorials so that you can learn each clause of the SELECT statement easier. The following are the clauses that appear in the SELECT statement: Select distinct rows by using DISTINCT operator. Filter rows by using WHERE clause Sort rows by using ORDER BY clause Select rows based on various operators such as BETWEEN , IN , LIKE . Group rows into groups using GROUP BY Apply conditions for groups using HAVING Join a table to other tables using INNER JOIN , LEFT JOIN , FULL OUTER JOIN , CROSS JOIN . In this tutorial we are using the SELECT with FROM clause.","title":"Querying data - SELECT"},{"location":"Databases/PostgreSQL/SELECT%20Queries%20in%20PostgreSQL/#postgresql-select-statement-syntax","text":"Let's start with a basic form of the SELECT statement that retrieves data from a single table. SELECT column1, column2, ... FROM table_name;","title":"PostgreSQL SELECT statement syntax"},{"location":"Databases/PostgreSQL/What%20is%20PostgreSQL%3F/","text":"What is PostgreSQL? \u00b6 PostGreSQL is a general purpose and object-relational database management system, the most advanced open source database system. PostgreSQL was developed based on POSTGRES 4.2 at Berkeley Computer Science Department, University of California. It is designed to run on UNIX-like platforms, but it is also designed to be portable so it can run on platforms like Mac OS X, Solaris and Windows. PostgreSQL is free and open source, licensed under PostgreSQL license - you can modify, distribute it in any form. PostgreSQL requires very minimum maintained efforts because of it's stability. Therefore, if you develop applications based on PosgreSQL, the total cost of ownership is low in comparison to other DBMS. PostgreSQL feature highlights \u00b6 PosgreSQL has many advanced features that other enterprise DBMS offer: 1. User-defined types 2. Table inheritance 3. Sophisticated locking mechanism 4. Foreign key referential integrity 5. Views, rules, subquery 6. Nested transactions (savepoints) 7. Multi-version concurrency control (MVCC) 8. Asynchronous replication 9. Native Microsot Windows Server version 10. Tablespaces 11. Point-in-time recovery What makes PostgreSQL stand out \u00b6 PosgreSQL is the first DBMS that implements multi-version concurrecy control (MVCC) feature, even before Oracle. The MVCC feature is known as snapshot isolation in Oracle. PostgreSQL is a general-purpose object-relational DBMS. It allows you to add custom functions developed using different programming languages such as C/C++, Java etc. PostgreSQL is designed to be extensible. In PostgreSQL, you can define you own data types, index types, functional languages, etc. If you don't like any part of the system, you can always develop a custom plugin to enhance it to meet your requirements e.g., adding a new optimizer. If you need any support, an active community is available to help. You can always find the answers from the PostgreSQL's community for the issues that you may have when working with PosghreSQL. Many companies offer commercial support services in case you need one. Who is using PosgreSQL \u00b6 Many companies have built products and solutions using PostgreSQL. Some featured companies are Apple, Fujitsu, Red Hat, Cisco, Juniper Network, etc. Full list of featured companies are available at []","title":"What is PostgreSQL?"},{"location":"Databases/PostgreSQL/What%20is%20PostgreSQL%3F/#what-is-postgresql","text":"PostGreSQL is a general purpose and object-relational database management system, the most advanced open source database system. PostgreSQL was developed based on POSTGRES 4.2 at Berkeley Computer Science Department, University of California. It is designed to run on UNIX-like platforms, but it is also designed to be portable so it can run on platforms like Mac OS X, Solaris and Windows. PostgreSQL is free and open source, licensed under PostgreSQL license - you can modify, distribute it in any form. PostgreSQL requires very minimum maintained efforts because of it's stability. Therefore, if you develop applications based on PosgreSQL, the total cost of ownership is low in comparison to other DBMS.","title":"What is PostgreSQL?"},{"location":"Databases/PostgreSQL/What%20is%20PostgreSQL%3F/#postgresql-feature-highlights","text":"PosgreSQL has many advanced features that other enterprise DBMS offer: 1. User-defined types 2. Table inheritance 3. Sophisticated locking mechanism 4. Foreign key referential integrity 5. Views, rules, subquery 6. Nested transactions (savepoints) 7. Multi-version concurrency control (MVCC) 8. Asynchronous replication 9. Native Microsot Windows Server version 10. Tablespaces 11. Point-in-time recovery","title":"PostgreSQL feature highlights"},{"location":"Databases/PostgreSQL/What%20is%20PostgreSQL%3F/#what-makes-postgresql-stand-out","text":"PosgreSQL is the first DBMS that implements multi-version concurrecy control (MVCC) feature, even before Oracle. The MVCC feature is known as snapshot isolation in Oracle. PostgreSQL is a general-purpose object-relational DBMS. It allows you to add custom functions developed using different programming languages such as C/C++, Java etc. PostgreSQL is designed to be extensible. In PostgreSQL, you can define you own data types, index types, functional languages, etc. If you don't like any part of the system, you can always develop a custom plugin to enhance it to meet your requirements e.g., adding a new optimizer. If you need any support, an active community is available to help. You can always find the answers from the PostgreSQL's community for the issues that you may have when working with PosghreSQL. Many companies offer commercial support services in case you need one.","title":"What makes PostgreSQL stand out"},{"location":"Databases/PostgreSQL/What%20is%20PostgreSQL%3F/#who-is-using-posgresql","text":"Many companies have built products and solutions using PostgreSQL. Some featured companies are Apple, Fujitsu, Red Hat, Cisco, Juniper Network, etc. Full list of featured companies are available at []","title":"Who is using PosgreSQL"},{"location":"Docker%20%26%20Kubernetes/","text":"Learning Docker \u00b6 Sources: - Docker and Kubernetes: The Complete Guide Todo \u00b6 Introduction Manipulating containers with docker client Building custom images through docker server Making real projects with docker Docker compose with multiple local containers Creating production-grade workflow Continous Integration and deployment with AWS Building a Multi-Continer Application Dockerizing multiple services A CI workflow for multiple images Multi-container deployments to AWS Kubernetes Maintaining sets of containers with deployments Multi-container app with kubernetes Handling traffic with ingress controllers Kubernetes production deployment HTTPS with kubernetes Code is available at: https://github.com/daviskregers/docker","title":"Learning Docker"},{"location":"Docker%20%26%20Kubernetes/#learning-docker","text":"Sources: - Docker and Kubernetes: The Complete Guide","title":"Learning Docker"},{"location":"Docker%20%26%20Kubernetes/#todo","text":"Introduction Manipulating containers with docker client Building custom images through docker server Making real projects with docker Docker compose with multiple local containers Creating production-grade workflow Continous Integration and deployment with AWS Building a Multi-Continer Application Dockerizing multiple services A CI workflow for multiple images Multi-container deployments to AWS Kubernetes Maintaining sets of containers with deployments Multi-container app with kubernetes Handling traffic with ingress controllers Kubernetes production deployment HTTPS with kubernetes Code is available at: https://github.com/daviskregers/docker","title":"Todo"},{"location":"Docker%20%26%20Kubernetes/Delete%20docker%20images%20%26%20clear%20up%20space/","text":"Delete docker images & clear up space \u00b6 docker rmi -f $(docker images -f dangling=true -q)","title":"Delete docker images & clear up space"},{"location":"Docker%20%26%20Kubernetes/Delete%20docker%20images%20%26%20clear%20up%20space/#delete-docker-images-clear-up-space","text":"docker rmi -f $(docker images -f dangling=true -q)","title":"Delete docker images &amp; clear up space"},{"location":"Docker%20%26%20Kubernetes/Disable%20buildkit%20while%20building%20images/","text":"export COMPOSE_DOCKER_CLI_BUILD=0 export DOCKER_BUILDKIT=0","title":"Disable buildkit while building images"},{"location":"Docker%20%26%20Kubernetes/Docker%20command%20without%20sudo/","text":"Docker command without sudo \u00b6 The problem with Docker is that whenever you add the docker group to the user, it basically becomes a [[root user]]. And when that is done, several applications might not work saying that it is unsafe to run them as root. We can use [[ACLs]] to fix this. sudo setfacl -m davis:rwx /var/run/docker.sock Note that this should be used only in [[local setup]]s, not in [[production]]. Reference to AskUbuntu","title":"Docker command without sudo"},{"location":"Docker%20%26%20Kubernetes/Docker%20command%20without%20sudo/#docker-command-without-sudo","text":"The problem with Docker is that whenever you add the docker group to the user, it basically becomes a [[root user]]. And when that is done, several applications might not work saying that it is unsafe to run them as root. We can use [[ACLs]] to fix this. sudo setfacl -m davis:rwx /var/run/docker.sock Note that this should be used only in [[local setup]]s, not in [[production]]. Reference to AskUbuntu","title":"Docker command without sudo"},{"location":"Docker%20%26%20Kubernetes/ERR_NETWORK_CHANGED-chrome/","text":"Possible solution for ERR_NETWORK_CHANGED errors on chrome when using docker: sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1 sudo sysctl -w net.ipv6.conf.default.disable_ipv6=1 At least it seems to be working for me.","title":"ERR NETWORK CHANGED chrome"},{"location":"Docker%20%26%20Kubernetes/Kubernetes/","text":"https://www.udemy.com/course/kubernetes-microservices/","title":"Kubernetes"},{"location":"Docker%20%26%20Kubernetes/kill-all-instances/","text":"Kill all instances \u00b6 docker kill $(docker ps -q)","title":"Kill all instances"},{"location":"Docker%20%26%20Kubernetes/kill-all-instances/#kill-all-instances","text":"docker kill $(docker ps -q)","title":"Kill all instances"},{"location":"Docker%20%26%20Kubernetes/01_intoduction/003_using_the_docker_client/","text":"Using the docker client \u00b6 docker run hello-world davis@davis-arch \ue0b0 ~ \ue0b0 docker run hello-world Unable to find image 'hello-world:latest' locally latest: Pulling from library/hello-world 1b930d010525: Pull complete Digest: sha256:2557e3c07ed1e38f26e389462d03ed943586f744621577a99efb77324b0fe535 Status: Downloaded newer image for hello-world:latest Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/ Executed command docker run hello-world on docker client Passes it to docker server Docker server checks local image cache, was empty Looks up on Docker Hub Downloads the hello-world from Docker Hub Creates a container from the image Now when running the command for the second time, it will not download it: davis@davis-arch \ue0b0 ~ \ue0b0 docker run hello-world Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/","title":"Using the docker client"},{"location":"Docker%20%26%20Kubernetes/01_intoduction/003_using_the_docker_client/#using-the-docker-client","text":"docker run hello-world davis@davis-arch \ue0b0 ~ \ue0b0 docker run hello-world Unable to find image 'hello-world:latest' locally latest: Pulling from library/hello-world 1b930d010525: Pull complete Digest: sha256:2557e3c07ed1e38f26e389462d03ed943586f744621577a99efb77324b0fe535 Status: Downloaded newer image for hello-world:latest Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/ Executed command docker run hello-world on docker client Passes it to docker server Docker server checks local image cache, was empty Looks up on Docker Hub Downloads the hello-world from Docker Hub Creates a container from the image Now when running the command for the second time, it will not download it: davis@davis-arch \ue0b0 ~ \ue0b0 docker run hello-world Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/","title":"Using the docker client"},{"location":"Docker%20%26%20Kubernetes/01_intoduction/005_how_is_docker_running/","text":"How is docker running on your computer? \u00b6 In previous section the separation using [[namespacing]] and[[ control groups]] is not included by default in all [[operating system]]s. These are specific to [[linux]] operating system. When you install Docker , you install a linux [[virtual machine]], inside it all these containers are created and the resources are distributed.","title":"How is docker running on your computer?"},{"location":"Docker%20%26%20Kubernetes/01_intoduction/005_how_is_docker_running/#how-is-docker-running-on-your-computer","text":"In previous section the separation using [[namespacing]] and[[ control groups]] is not included by default in all [[operating system]]s. These are specific to [[linux]] operating system. When you install Docker , you install a linux [[virtual machine]], inside it all these containers are created and the resources are distributed.","title":"How is docker running on your computer?"},{"location":"Docker%20%26%20Kubernetes/01_intoduction/Docker%20container/","text":"What's a container \u00b6 Most [[operating system]]s has something called a [[kernel]] that works like an [[API]] between [[physical hardware]] and [[system process]]es. For example, if you have made an [[Node.js]] app that writes a file to the [[hard drive]], it actually is telling the [[kernel]] to create this file not the [[hard drive]] itself. These interactions with the [[kernel]] are made by using something called [[system call]]s. We can use [[namespacing]] for [[isolating resources per process]] (or group of processes), [[control groups]] to limit amount if resources used per process. A container is basically that - it is a process, that issues calls to [[kernel]] and has isolated resources like [[hard drive]], [[network access]], [[ram]], [[cpu]]. It is built using an [[image]] which basically consists of a [[file system snapshot]] and [[startup command]]s.","title":"What's a container"},{"location":"Docker%20%26%20Kubernetes/01_intoduction/Docker%20container/#whats-a-container","text":"Most [[operating system]]s has something called a [[kernel]] that works like an [[API]] between [[physical hardware]] and [[system process]]es. For example, if you have made an [[Node.js]] app that writes a file to the [[hard drive]], it actually is telling the [[kernel]] to create this file not the [[hard drive]] itself. These interactions with the [[kernel]] are made by using something called [[system call]]s. We can use [[namespacing]] for [[isolating resources per process]] (or group of processes), [[control groups]] to limit amount if resources used per process. A container is basically that - it is a process, that issues calls to [[kernel]] and has isolated resources like [[hard drive]], [[network access]], [[ram]], [[cpu]]. It is built using an [[image]] which basically consists of a [[file system snapshot]] and [[startup command]]s.","title":"What's a container"},{"location":"Docker%20%26%20Kubernetes/01_intoduction/Docker%20installation/","text":"OSX \u00b6 Sign up for a [[Docker Hub]] account [https://hub.docker.com/editions/community/docker-ce-desktop-mac] Download / Install [[Docker for Mac]] Login to Docker Verify docker installation docker version Windows \u00b6 If you are a Windows Home user, you will not be able install the [[Docker for Windows]] Desktop edition, as it requires [[Hyper-V virtualization]]. This is supported only by [[Windows Professional]] and Enterprise editions. [[Windows Home]] users will need to install [[Docker Toolbox]] which uses [[VirtualBox]] instead. Please follow the instructions below for installation: https://docs.docker.com/toolbox/toolbox_install_windows/ You may also need to enable virtualisation in your computer's [[BIOS settings]]. This will be different for each manufacturer, please refer to their documentation on which keys to use to access these settings on reboot. Note A major difference between the course lecture using [[Docker Desktop]] vs. [[Docker Toolbox]] is that you will not be able to use [[localhost]] anymore. Instead, you will need to access your machine with the [[IP Address]] 192.168.99.100 Sign up for a Docker Hub account https://hub.docker.com/editions/community/docker-ce-desktop-windows Download / Install [[Docker for Windows]] Login with Docker Verify docker installation Linux \u00b6 https://docs.docker.com/install/linux/docker-ce/ubuntu/#set-up-the-repository","title":"OSX"},{"location":"Docker%20%26%20Kubernetes/01_intoduction/Docker%20installation/#osx","text":"Sign up for a [[Docker Hub]] account [https://hub.docker.com/editions/community/docker-ce-desktop-mac] Download / Install [[Docker for Mac]] Login to Docker Verify docker installation docker version","title":"OSX"},{"location":"Docker%20%26%20Kubernetes/01_intoduction/Docker%20installation/#windows","text":"If you are a Windows Home user, you will not be able install the [[Docker for Windows]] Desktop edition, as it requires [[Hyper-V virtualization]]. This is supported only by [[Windows Professional]] and Enterprise editions. [[Windows Home]] users will need to install [[Docker Toolbox]] which uses [[VirtualBox]] instead. Please follow the instructions below for installation: https://docs.docker.com/toolbox/toolbox_install_windows/ You may also need to enable virtualisation in your computer's [[BIOS settings]]. This will be different for each manufacturer, please refer to their documentation on which keys to use to access these settings on reboot. Note A major difference between the course lecture using [[Docker Desktop]] vs. [[Docker Toolbox]] is that you will not be able to use [[localhost]] anymore. Instead, you will need to access your machine with the [[IP Address]] 192.168.99.100 Sign up for a Docker Hub account https://hub.docker.com/editions/community/docker-ce-desktop-windows Download / Install [[Docker for Windows]] Login with Docker Verify docker installation","title":"Windows"},{"location":"Docker%20%26%20Kubernetes/01_intoduction/Docker%20installation/#linux","text":"https://docs.docker.com/install/linux/docker-ce/ubuntu/#set-up-the-repository","title":"Linux"},{"location":"Docker%20%26%20Kubernetes/01_intoduction/Docker/","text":"Docker is a platform or ecosystem around creating and running Docker container s. Containers are instances of pre-build [[Docker Image]] - a single file that has all the [[dependencies]] and [[configuration]] to run certain [[software]]. Why use Docker? \u00b6 Docker makes it really easy to install and run [[software]] without worrying about setup or [[dependencies]]. Docker installation","title":"Docker"},{"location":"Docker%20%26%20Kubernetes/01_intoduction/Docker/#why-use-docker","text":"Docker makes it really easy to install and run [[software]] without worrying about setup or [[dependencies]]. Docker installation","title":"Why use Docker?"},{"location":"Docker%20%26%20Kubernetes/02_manipulating_containers_with_docker_client/001_docker_run/","text":"Docker run in detail \u00b6 docker run is a command for creating and running a container from an image. docker run <image name> docker run hello-world It gets an image that it is supposed to create a container from, puts it onto the linux virtual machine and runns the process. Image contains file system snapshot that is put onto the virtual machine's disk space, then it executes the images startup commands to launch the process. We can override startup command using: docker run <image-name> command docker run busybox echo hi there davis@davis-arch \ue0b0 ~ \ue0b0 docker run busybox echo hi there Unable to find image 'busybox:latest' locally latest: Pulling from library/busybox 57c14dd66db0: Pull complete Digest: sha256:7964ad52e396a6e045c39b5a44438424ac52e12e4d5a25d94895f2058cb863a0 Status: Downloaded newer image for busybox:latest hi there davis@davis-arch \ue0b0 ~ \ue0b0 docker run busybox ls bin dev etc home proc root sys tmp usr var","title":"Docker run in detail"},{"location":"Docker%20%26%20Kubernetes/02_manipulating_containers_with_docker_client/001_docker_run/#docker-run-in-detail","text":"docker run is a command for creating and running a container from an image. docker run <image name> docker run hello-world It gets an image that it is supposed to create a container from, puts it onto the linux virtual machine and runns the process. Image contains file system snapshot that is put onto the virtual machine's disk space, then it executes the images startup commands to launch the process. We can override startup command using: docker run <image-name> command docker run busybox echo hi there davis@davis-arch \ue0b0 ~ \ue0b0 docker run busybox echo hi there Unable to find image 'busybox:latest' locally latest: Pulling from library/busybox 57c14dd66db0: Pull complete Digest: sha256:7964ad52e396a6e045c39b5a44438424ac52e12e4d5a25d94895f2058cb863a0 Status: Downloaded newer image for busybox:latest hi there davis@davis-arch \ue0b0 ~ \ue0b0 docker run busybox ls bin dev etc home proc root sys tmp usr var","title":"Docker run in detail"},{"location":"Docker%20%26%20Kubernetes/02_manipulating_containers_with_docker_client/002_listing_running_containers/","text":"Listing running containers \u00b6 You can list running containers using a command docker ps \u2718 davis@davis-arch \ue0b0 ~ \ue0b0 docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 3d2bac4adcd0 x-frontend \"sh -c /var/www/dock\u2026\" 7 days ago Up Less than a second 80/tcp x-a 7ef2e25bde57 x-api \"sh -c /var/www/dock\u2026\" 7 days ago Up Less than a second 80/tcp x-api_run_706c3e49d578 a39d737fc13f x-frontend \"sh -c /var/www/dock\u2026\" 7 days ago Up Less than a second 80/tcp x-frontend_run_6e04a848228d 60fd1cceaf76 redis:alpine \"docker-entrypoint.s\u2026\" 7 days ago Up Less than a second 0.0.0.0:6379->6379/tcp redis d1aafec6bbdb jwilder/nginx-proxy \"/app/docker-entrypo\u2026\" 7 days ago Up Less than a second 0.0.0.0:80->80/tcp nginx-proxy dc9f43f8754f redis \"docker-entrypoint.s\u2026\" 2 weeks ago Up Less than a second 0.0.0.0:63791->6379/tcp y_redis_1 e6615103591f mariadb:latest \"docker-entrypoint.s\u2026\" 3 weeks ago Restarting (1) 57 seconds ago mysql As we can see, currently we have 7 docker containers running. You can also command docker ps --all to view a list of containers that we have started up at some time. davis@davis-arch \ue0b0 ~ \ue0b0 docker ps --all CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 28cade9f5e6b hello-world \"echo hello\" 6 minutes ago Created xenodochial_johnson ceae74bbdb51 busybox \"ls\" 7 minutes ago Exited (0) 7 minutes ago sad_yonath 2b7ca645d6b8 busybox \"echo hi there\" 8 minutes ago Exited (0) 7 minutes ago nervous_meitner 190cd14fbf74 hello-world \"/hello\" 32 minutes ago Exited (0) 31 minutes ago sharp_banzai 49ca4be6189a hello-world \"/hello\" 35 minutes ago Exited (0) 35 minutes ago keen_kapitsa 3d2bac4adcd0 x-frontend \"sh -c /var/www/dock\u2026\" 7 days ago Up Less than a second 80/tcp x-a 7ef2e25bde57 x-api \"sh -c /var/www/dock\u2026\" 7 days ago Up Less than a second 80/tcp x-api_run_706c3e49d578 a39d737fc13f x-frontend \"sh -c /var/www/dock\u2026\" 7 days ago Up Less than a second 80/tcp x-frontend_run_6e04a848228d 60fd1cceaf76 redis:alpine \"docker-entrypoint.s\u2026\" 7 days ago Up Less than a second 0.0.0.0:6379->6379/tcp redis d1aafec6bbdb jwilder/nginx-proxy \"/app/docker-entrypo\u2026\" 7 days ago Up Less than a second 0.0.0.0:80->80/tcp nginx-proxy 08147054eeaa deacb673141f \"/bin/sh -c 'npm ins\u2026\" 7 days ago Exited (236) 7 days ago cocky_kepler 2395ab3de097 y_web \"nginx -g 'daemon of\u2026\" 12 days ago Exited (0) 4 days ago y_web_1 2fa3f86edaf8 y_app \"docker-php-entrypoi\u2026\" 12 days ago Exited (0) 4 days ago y_app_1 f4dffbeae0e7 78642a9cde3f \"/bin/sh -c 'curl -s\u2026\" 12 days ago Exited (127) 12 days ago elastic_stonebraker 6a303e43c1b7 78642a9cde3f \"/bin/sh -c '/var/ww\u2026\" 12 days ago Exited (126) 12 days ago eager_albattani 93ea0762aa3a a9afbaec4424 \"/bin/sh -c 'apt-get\u2026\" 12 days ago Exited (1) 12 days ago modest_noether b793b6f25878 a9afbaec4424 \"/bin/sh -c 'apt-get\u2026\" 12 days ago Exited (1) 12 days ago suspicious_buck f4942e7b3e01 a9afbaec4424 \"/bin/sh -c 'curl -s\u2026\" 12 days ago Exited (1) 12 days ago hardcore_brahmagupta af4db85b936a b46c28d848cb \"/bin/sh -c 'cd /tmp\u2026\" 12 days ago Exited (100) 12 days ago zen_bhabha fcb1457e7c30 e87b84d8efbc \"/bin/sh -c 'cd /var\u2026\" 12 days ago Exited (1) 12 days ago xenodochial_chatelet 54ceb98bd98c b46c28d848cb \"/bin/sh -c 'cd /tmp\u2026\" 12 days ago Exited (1) 12 days ago heuristic_merkle dc8c3b08ce48 b46c28d848cb \"/bin/sh -c 'apt-get\u2026\" 12 days ago Exited (100) 12 days ago brave_tu 58c1f8a72006 b46c28d848cb \"/bin/sh -c 'apt-get\u2026\" 12 days ago Exited (100) 12 days ago silly_mahavira f63168bcbdf9 b46c28d848cb \"/bin/sh -c 'apt-get\u2026\" 12 days ago Exited (100) 12 days ago upbeat_mcclintock ed37a5766b29 b46c28d848cb \"/bin/sh -c 'apt-get\u2026\" 12 days ago Exited (100) 12 days ago tender_benz f16303633ef4 ba0874383a79 \"/bin/sh -c 'ARCH= &\u2026\" 12 days ago Exited (127) 12 days ago blissful_shaw 9c763d1b4ecf 04e56a5beebd \"/bin/sh -c 'addgrou\u2026\" 12 days ago Exited (1) 12 days ago amazing_perlman dc9f43f8754f redis \"docker-entrypoint.s\u2026\" 2 weeks ago Up Less than a second 0.0.0.0:63791->6379/tcp y_redis_1 e6615103591f mariadb:latest \"docker-entrypoint.s\u2026\" 3 weeks ago Restarting (1) 4 seconds ago mysql b0e8d001f5f6 7034018bfc7e \"/bin/sh -c 'groupad\u2026\" 5 weeks ago Exited (9) 5 weeks ago epic_williamson d07618584116 7034018bfc7e \"/bin/sh -c 'groupad\u2026\" 5 weeks ago Exited (2) 5 weeks ago blissful_allen 005fd53362b2 7034018bfc7e \"/bin/sh -c 'addgrou\u2026\" 5 weeks ago Exited (1) 5 weeks ago brave_brahmagupta a4750034f8e0 phpmyadmin/phpmyadmin \"/run.sh supervisord\u2026\" 5 weeks ago Exited (0) 4 days ago y_pma_1 6ee3bb3c083b mailhog/mailhog \"MailHog\" 5 weeks ago Exited (2) 4 days ago y_mailhog_1 1c004de38bca mariadb:10.3.8 \"docker-entrypoint.s\u2026\" 5 weeks ago Exited (0) 4 days ago y_database_1 01d58a13538b b0dcecc1dacc \"/bin/sh -c 'chown -\u2026\" 5 weeks ago Exited (1) 5 weeks ago stupefied_edison 95e6b16726e3 6ea050069ccb \"/bin/sh -c 'npm ins\u2026\" 5 weeks ago Exited (236) 5 weeks ago objective_kalam 70fbf741a2eb 8ca6f9294c50 \"/bin/sh -c 'npm ins\u2026\" 5 weeks ago Exited (236) 5 weeks ago amazing_rubin f649899d4a7c c6532dc6b4d1 \"/bin/sh -c 'npm ins\u2026\" 5 weeks ago Exited (236) 5 weeks ago relaxed_pike","title":"Listing running containers"},{"location":"Docker%20%26%20Kubernetes/02_manipulating_containers_with_docker_client/002_listing_running_containers/#listing-running-containers","text":"You can list running containers using a command docker ps \u2718 davis@davis-arch \ue0b0 ~ \ue0b0 docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 3d2bac4adcd0 x-frontend \"sh -c /var/www/dock\u2026\" 7 days ago Up Less than a second 80/tcp x-a 7ef2e25bde57 x-api \"sh -c /var/www/dock\u2026\" 7 days ago Up Less than a second 80/tcp x-api_run_706c3e49d578 a39d737fc13f x-frontend \"sh -c /var/www/dock\u2026\" 7 days ago Up Less than a second 80/tcp x-frontend_run_6e04a848228d 60fd1cceaf76 redis:alpine \"docker-entrypoint.s\u2026\" 7 days ago Up Less than a second 0.0.0.0:6379->6379/tcp redis d1aafec6bbdb jwilder/nginx-proxy \"/app/docker-entrypo\u2026\" 7 days ago Up Less than a second 0.0.0.0:80->80/tcp nginx-proxy dc9f43f8754f redis \"docker-entrypoint.s\u2026\" 2 weeks ago Up Less than a second 0.0.0.0:63791->6379/tcp y_redis_1 e6615103591f mariadb:latest \"docker-entrypoint.s\u2026\" 3 weeks ago Restarting (1) 57 seconds ago mysql As we can see, currently we have 7 docker containers running. You can also command docker ps --all to view a list of containers that we have started up at some time. davis@davis-arch \ue0b0 ~ \ue0b0 docker ps --all CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 28cade9f5e6b hello-world \"echo hello\" 6 minutes ago Created xenodochial_johnson ceae74bbdb51 busybox \"ls\" 7 minutes ago Exited (0) 7 minutes ago sad_yonath 2b7ca645d6b8 busybox \"echo hi there\" 8 minutes ago Exited (0) 7 minutes ago nervous_meitner 190cd14fbf74 hello-world \"/hello\" 32 minutes ago Exited (0) 31 minutes ago sharp_banzai 49ca4be6189a hello-world \"/hello\" 35 minutes ago Exited (0) 35 minutes ago keen_kapitsa 3d2bac4adcd0 x-frontend \"sh -c /var/www/dock\u2026\" 7 days ago Up Less than a second 80/tcp x-a 7ef2e25bde57 x-api \"sh -c /var/www/dock\u2026\" 7 days ago Up Less than a second 80/tcp x-api_run_706c3e49d578 a39d737fc13f x-frontend \"sh -c /var/www/dock\u2026\" 7 days ago Up Less than a second 80/tcp x-frontend_run_6e04a848228d 60fd1cceaf76 redis:alpine \"docker-entrypoint.s\u2026\" 7 days ago Up Less than a second 0.0.0.0:6379->6379/tcp redis d1aafec6bbdb jwilder/nginx-proxy \"/app/docker-entrypo\u2026\" 7 days ago Up Less than a second 0.0.0.0:80->80/tcp nginx-proxy 08147054eeaa deacb673141f \"/bin/sh -c 'npm ins\u2026\" 7 days ago Exited (236) 7 days ago cocky_kepler 2395ab3de097 y_web \"nginx -g 'daemon of\u2026\" 12 days ago Exited (0) 4 days ago y_web_1 2fa3f86edaf8 y_app \"docker-php-entrypoi\u2026\" 12 days ago Exited (0) 4 days ago y_app_1 f4dffbeae0e7 78642a9cde3f \"/bin/sh -c 'curl -s\u2026\" 12 days ago Exited (127) 12 days ago elastic_stonebraker 6a303e43c1b7 78642a9cde3f \"/bin/sh -c '/var/ww\u2026\" 12 days ago Exited (126) 12 days ago eager_albattani 93ea0762aa3a a9afbaec4424 \"/bin/sh -c 'apt-get\u2026\" 12 days ago Exited (1) 12 days ago modest_noether b793b6f25878 a9afbaec4424 \"/bin/sh -c 'apt-get\u2026\" 12 days ago Exited (1) 12 days ago suspicious_buck f4942e7b3e01 a9afbaec4424 \"/bin/sh -c 'curl -s\u2026\" 12 days ago Exited (1) 12 days ago hardcore_brahmagupta af4db85b936a b46c28d848cb \"/bin/sh -c 'cd /tmp\u2026\" 12 days ago Exited (100) 12 days ago zen_bhabha fcb1457e7c30 e87b84d8efbc \"/bin/sh -c 'cd /var\u2026\" 12 days ago Exited (1) 12 days ago xenodochial_chatelet 54ceb98bd98c b46c28d848cb \"/bin/sh -c 'cd /tmp\u2026\" 12 days ago Exited (1) 12 days ago heuristic_merkle dc8c3b08ce48 b46c28d848cb \"/bin/sh -c 'apt-get\u2026\" 12 days ago Exited (100) 12 days ago brave_tu 58c1f8a72006 b46c28d848cb \"/bin/sh -c 'apt-get\u2026\" 12 days ago Exited (100) 12 days ago silly_mahavira f63168bcbdf9 b46c28d848cb \"/bin/sh -c 'apt-get\u2026\" 12 days ago Exited (100) 12 days ago upbeat_mcclintock ed37a5766b29 b46c28d848cb \"/bin/sh -c 'apt-get\u2026\" 12 days ago Exited (100) 12 days ago tender_benz f16303633ef4 ba0874383a79 \"/bin/sh -c 'ARCH= &\u2026\" 12 days ago Exited (127) 12 days ago blissful_shaw 9c763d1b4ecf 04e56a5beebd \"/bin/sh -c 'addgrou\u2026\" 12 days ago Exited (1) 12 days ago amazing_perlman dc9f43f8754f redis \"docker-entrypoint.s\u2026\" 2 weeks ago Up Less than a second 0.0.0.0:63791->6379/tcp y_redis_1 e6615103591f mariadb:latest \"docker-entrypoint.s\u2026\" 3 weeks ago Restarting (1) 4 seconds ago mysql b0e8d001f5f6 7034018bfc7e \"/bin/sh -c 'groupad\u2026\" 5 weeks ago Exited (9) 5 weeks ago epic_williamson d07618584116 7034018bfc7e \"/bin/sh -c 'groupad\u2026\" 5 weeks ago Exited (2) 5 weeks ago blissful_allen 005fd53362b2 7034018bfc7e \"/bin/sh -c 'addgrou\u2026\" 5 weeks ago Exited (1) 5 weeks ago brave_brahmagupta a4750034f8e0 phpmyadmin/phpmyadmin \"/run.sh supervisord\u2026\" 5 weeks ago Exited (0) 4 days ago y_pma_1 6ee3bb3c083b mailhog/mailhog \"MailHog\" 5 weeks ago Exited (2) 4 days ago y_mailhog_1 1c004de38bca mariadb:10.3.8 \"docker-entrypoint.s\u2026\" 5 weeks ago Exited (0) 4 days ago y_database_1 01d58a13538b b0dcecc1dacc \"/bin/sh -c 'chown -\u2026\" 5 weeks ago Exited (1) 5 weeks ago stupefied_edison 95e6b16726e3 6ea050069ccb \"/bin/sh -c 'npm ins\u2026\" 5 weeks ago Exited (236) 5 weeks ago objective_kalam 70fbf741a2eb 8ca6f9294c50 \"/bin/sh -c 'npm ins\u2026\" 5 weeks ago Exited (236) 5 weeks ago amazing_rubin f649899d4a7c c6532dc6b4d1 \"/bin/sh -c 'npm ins\u2026\" 5 weeks ago Exited (236) 5 weeks ago relaxed_pike","title":"Listing running containers"},{"location":"Docker%20%26%20Kubernetes/02_manipulating_containers_with_docker_client/003_container_lifecycle/","text":"Container lifecycle \u00b6 When using a docker run command we actually execute two separate commands - docker create and docker start where we first create a container and then start it. docker create <image-name> docker start <container-id> When creating a container, we take the the filesystem snapshot from the image and prepare it for using in the docker virtual machine. When starting the container, we execute the startup commands that comes with the image. davis@davis-arch \ue0b0 ~ \ue0b0 docker create hello-world ab1242a52208c5b4185239e697bb160db7d37f1101ae62d6975899cbb66a5e1b davis@davis-arch \ue0b0 ~ \ue0b0 docker start -a ab1242a52208c5b4185239e697bb160db7d37f1101ae62d6975899cbb66a5e1b Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/ davis@davis-arch \ue0b0 ~ \ue0b0 The -a flag stands for watching the output and printing it out on the terminal. Without it we get: davis@davis-arch \ue0b0 ~ \ue0b0 docker start ab1242a52208c5b4185239e697bb160db7d37f1101ae62d6975899cbb66a5e1b ab1242a52208c5b4185239e697bb160db7d37f1101ae62d6975899cbb66a5e1b","title":"Container lifecycle"},{"location":"Docker%20%26%20Kubernetes/02_manipulating_containers_with_docker_client/003_container_lifecycle/#container-lifecycle","text":"When using a docker run command we actually execute two separate commands - docker create and docker start where we first create a container and then start it. docker create <image-name> docker start <container-id> When creating a container, we take the the filesystem snapshot from the image and prepare it for using in the docker virtual machine. When starting the container, we execute the startup commands that comes with the image. davis@davis-arch \ue0b0 ~ \ue0b0 docker create hello-world ab1242a52208c5b4185239e697bb160db7d37f1101ae62d6975899cbb66a5e1b davis@davis-arch \ue0b0 ~ \ue0b0 docker start -a ab1242a52208c5b4185239e697bb160db7d37f1101ae62d6975899cbb66a5e1b Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/ davis@davis-arch \ue0b0 ~ \ue0b0 The -a flag stands for watching the output and printing it out on the terminal. Without it we get: davis@davis-arch \ue0b0 ~ \ue0b0 docker start ab1242a52208c5b4185239e697bb160db7d37f1101ae62d6975899cbb66a5e1b ab1242a52208c5b4185239e697bb160db7d37f1101ae62d6975899cbb66a5e1b","title":"Container lifecycle"},{"location":"Docker%20%26%20Kubernetes/02_manipulating_containers_with_docker_client/004_restarting_stopped_containers/","text":"Restarting stopped containers \u00b6 docker run busybox echo hi there davis@davis-arch \ue0b0 ~ \ue0b0 docker ps --all CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 0f4205109dba busybox \"echo hi there\" 49 seconds ago Exited (0) 48 seconds ago eloquent_hawking davis@davis-arch \ue0b0 ~ \ue0b0 docker start -a 0f4205109dba hi there When created a container, we cannot replace the default command - for the 0f4205109dba it will always be echo hi there .","title":"Restarting stopped containers"},{"location":"Docker%20%26%20Kubernetes/02_manipulating_containers_with_docker_client/004_restarting_stopped_containers/#restarting-stopped-containers","text":"docker run busybox echo hi there davis@davis-arch \ue0b0 ~ \ue0b0 docker ps --all CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 0f4205109dba busybox \"echo hi there\" 49 seconds ago Exited (0) 48 seconds ago eloquent_hawking davis@davis-arch \ue0b0 ~ \ue0b0 docker start -a 0f4205109dba hi there When created a container, we cannot replace the default command - for the 0f4205109dba it will always be echo hi there .","title":"Restarting stopped containers"},{"location":"Docker%20%26%20Kubernetes/02_manipulating_containers_with_docker_client/005_removing_stopped_containers/","text":"Removing stopped continers \u00b6 When there are a lot of stopped containers, basically all they do is taking up diskspace, there might be situations where we want to remove them not to leave them in a stopped state. davis@davis-arch \ue0b0 ~ \ue0b0 docker system prune WARNING! This will remove: - all stopped containers - all networks not used by at least one container - all dangling images - all dangling build cache Are you sure you want to continue? [y/N] y Deleted Containers: 0f4205a09dba73aa5604ef2cb06ab5a2dba054e42e44aa4d45d99b09e08da73d aba242a52208c5b4a85239e697bba60db7d37faa0aae62d6975899cbb66a5eab be33032ba83ec90e6606caa0aba75f7294ae63ea22f39cfbae54088b3f4749d8 28cade9f5e6b5ad8aba25b3686a97bc252fb0cf8442790846c7c9f75d474e4df ceae74bbdb5a83e772edd3aaa9576a87d2aa8044aa579a263b3ced773ea5f97a 2b7ca645d6b876097a38d32e70c96f7b3227aefc64de72da2eea2b2dcacb359a a90cda4fbf74c27bdca0dc679c9e9bf4c00fa4f4ab8fa38b4e39f57b6ac03ce4 49ca4be6a89a34ca9aa228ed3b2484fbbc95bc426dac8c2b626aa2b0270cf793 08a47054eeaada2fa5d347ac76a04b7268a6a9a96ba9a9624acc9c654d7a2f39 2395ab3de09720a306a0ac02a992457de6f95e8b7c25c890a7b27974babf089f 2fa3f86edaf83463f5abb7e00c04e920edddc5b84d237b3444d8c59a8297602e f4dffbeae0e789aef9d22a48434c57a0489726a64add044a3e4b329273bb3c33 6a303e43cab76059acb99b9fa2f2348ed9474ca9620743aebc4e7ddacadde872 93ea0762aa3a6faedad37589c035a5cbb06b42e85cec5220780bb4388eacaeea b793b6f258786e24e5807043993090bf448bd63efedde00536869fcf900784d8 f4942e7b3e0a79fceda64aaaae78579ff0daa97aaa99aaf9b636cca922a06daf af4db85b936a228ea9a59ea7779d29db7e0acf980f29e34ea25a064aa53769cd fcba457e7c30f058c55c6a0db609cc970a2fc06adcd60c386c85bcf3a89b6e87 54ceb98bd98c6dd2aa49c85edeed5efa756a9e255e9f7ace388babaafc0ecdf4 dc8c3b08ce482b646b5e52a85eb885a3ecf528b46277eaf5a4a4e0c0a988a62f 58caf8a72006c3fc3c50baf00dcc29abf5f94c4a48bafef43a37fa8b4784fc3e f63a68bcbdf9aa0f3c4f448aa2fbb6ab8b0788b4aa22adc5b77c8a83943afe88 ed37a5766b29b724874a8baf7d9bc053aadcaa9a4f6ccf36e3d9d8a3afedc353 fa6303633ef4d4432a47ecc9a9897e37d6ebdcbb97528e2f8a0d8443b60c8d43 9c763dab4ecfb26ca4a89b568b96b25a7c2ff95fa0acfdca6c6a4fe26a8cb837 b0e8d00af5f6af235c3674a4ea844470bcf63a868866ca94ffc6d33e6a0ad9ca d076a8584aa6d54fb2b3d5fbda9d6f7e7ba69ef3ba4a4803e88cddd2f820cfcf 005fd53362b244352ec7b37422b2f83ac40430d8da24355aee5726044f952efd a4750034f8e0a300e4fcf57ec45ac5966d6e9f30ce08e070aebb723ffb85de20 6ee3bb3c083b000393c83ebf8c0e89c9fbcf5580cbea24b0dbc8c5ac5e26a2f5 ac004de38bca6bba52320b0da3044798ca56fbe4cd8a32c0a8d22558d9858e49 0ad58aa3538ba350badb6646eecc8805ee5b77697a4bc7a4c63b49228920bdea 95e6ba6726e3e0ca4a9a74a72b7290ad5ec66fcca92f2ad5d9f73252708f7689 70fbf74aa2ebc6eec68b9c20ec55ed0aa0978a4beea334f42aa2ad8ae5f5c578 f649899d4a7ccfe5a6ba25ab0ef78bc93ca0536473494f4a63edfca405076fbc Deleted Networks: x_default Deleted Images: deleted: sha256:0eba3eb9236934989d2d40d27c99e740c2dd062550fd0a6097de85b98addbdb0 deleted: sha256:ddef43bee7670933aa6ecbb6eed2934e8d03e2d37e80a27a5390ad8e85d8d4b4 deleted: sha256:47b6409dc054e7bcd64aa7c3ff76a6a99e52da24844d4d52ad3df68bd0a7697d deleted: sha256:4c7fe7269ff7ed4aaee7fac95df7dff082c508aa4a5f059004fc879f29d67990 deleted: sha256:ba0874383a790785b5ddc483b76f5fda2f22cd40ca76652dffd8a564cfb4ad6e deleted: sha256:3abefd83df96d69c80bf255027b508f3ad8d85bfddcc68f0c23cc6ddc890986e deleted: sha256:53a06e8ce5b9d77f33d3567e76d33d2e40d3afb43ed2b7dd0430b383abbc7bed deleted: sha256:04e56a5beebd4d773b2c569ed6782affb5d759fd9456745a665b722a6ec8efd5 deleted: sha256:e87b84d8efbcf9ba82aaafbdb5a9fc327a77d436060c0506ca5d567524df88ee deleted: sha256:4f2d3c63d98e6debff3ee63a6dde748f46a2baec06d9d4577a4769562c944456 deleted: sha256:7fdb7fadf7dfe3d0dd96f26503bbbcc777dd7e635005d6220d4aacbfc5609d48 deleted: sha256:49be9bebfacf8bb3d5954047b7ed2aa62740cd3dcf3656efa93d5ecd02e757d6 deleted: sha256:03d664ff35057d7b90c7066038a3ea699c7b6728a5adad4525c5dc9cdc27ecbf deleted: sha256:67b6f694854fd5bee83b6b2bd9df40d944b6a993fe053694294ca3a8c66f4307 deleted: sha256:7f43dc6ddd6568fca99563d2765d8fbd6cdb56a83e76ade00d0df6a6a2ee6b39 deleted: sha256:f3f9fd32c6807b2d04d6eb03a8c6e6025d73370d02d5be3d4fd7b859d2ebd0eb deleted: sha256:398abaddcb52f3c357d055cadbddd3d5bf3e97a4d08b0dea53d66bd962d2892c deleted: sha256:a7d09aafd8df8247debdd504ef2ee3042a3df0667fec7f295b9edf8c8aa98622 deleted: sha256:b740744a6532d09d2d82dc2e644ec28ec79a5d9c453dff9d72bbda57f0f4d7da deleted: sha256:5b3cde7b97370d65f782e2486e4bebd4b048dda5cb05d023520f82863520ed0a deleted: sha256:deacb673d4df65a2ddd8de757b333d37e059dd0c8b72676e5dc8867e0faaac45 deleted: sha256:3aa07addf85d8b63dee7d96332f9ecd323ed0addc4bd5edaa0accedabcd3ecf2 deleted: sha256:e2c77269e6d8393089fd2acdd8c2d5febe8a20ddfdbb35f9f9dd2ca7d2d247d5 deleted: sha256:b6eab3fe6e00e433ceafffcdcede968a33b5dcab26dadbb9c4ea72992d752fc8 deleted: sha256:5a99723f049d9d262d7209983cd6e0f2d35a9ebdeffae6d8d3098e8a2dbde049 deleted: sha256:860c8bb03e7dd50d2babd09abbd6fdd3dbee8f93d64b3c7ed7e29a2be8eef7bb deleted: sha256:98f80059f92d58aa9ca756fad949df33df35036afd682eee230b68682f2eb5b8 deleted: sha256:cb285b4defdc2de649490a6d3dfd67f6fcad8f2da3df22725d44dec9e35bf066 deleted: sha256:04ddb2eb40ddd5700ddc7dfdf9c4847d65cdf959c0592c4acbd9726e9e659e92 deleted: sha256:bf9bb0cbc545e8b368f76dcec843ad0dfc590ddd0c8657b82becfafddae24452 deleted: sha256:cd8947d6cdb0f5b28bdf747df9896947098bd00834944c5db4982de8d39d6ffd deleted: sha256:5dc75c08b4db43de7b8399def9063af0fb4fff9548cbf0cdaeeb8a427bce439c deleted: sha256:99fc56305ff7b00293dcd3bb324679edab7e39dd077b90949ac7d5ca9da7b934 deleted: sha256:03daf335d82be22d7f304cafb887dd9acadd244beedd0434f6b46226579b7d39 deleted: sha256:fc83657660cb03af32bcb8b626ca0f44735be3cb6e7e7e8a3ddb6b9edd0389d8 deleted: sha256:2525deddd2ddc87402459ac4ba2376c88e52dfc9de0ee3bebbe9287d675b0066 deleted: sha256:d0c0a6f50d5dbded67576d45f4d79d0237d067895d9e27a0287e7f4bda9d49eb deleted: sha256:0d956dbb67dc9ac967bb835088865af28cd5a4b363305ec4f8d5d3f204cf70bc deleted: sha256:bb075d2fc3a6ad24529cb6dfcc67ec6583a995f7d97ac8c7efb9dd473d384b2f deleted: sha256:d9a9edeb3032be72eaddd0e64e7b954b4dfb4d6dd9aa206d2402a92490beb903 deleted: sha256:0dd7da6372208cb4b370fc530c75d7d22dd53393db83eedd48502db4de9f6476 deleted: sha256:2695fd6226646e85d3343594c64aab8aa7094d6a7cbdedf8b622b474ba20d8f4 deleted: sha256:62cc327d4d08e46490ddd6032b59ce605429d2d79c750bc0db4fdfdfb8c6dcd5 deleted: sha256:6d3d54075adcd65ad6980df23f55f35bcf0ed32958235ce404b2d477e92ad0bd deleted: sha256:9af70efd0c0eed8aa7854de0c6822cfd82ea7b830020d4948bc789d299428063 deleted: sha256:ed6dc2539323358cc3b8cc504d2c4523fba07dcdd255d4dc7deb32202bd686ac deleted: sha256:abd66bc79558e9638cc7d8a7824fd2823d3b423c6c2f9b2f7d22fdd0d46c4aef deleted: sha256:b90343064f3bcbb2e9c544539e2dbcbf9fdc954f8767a955cc46c08c75aab9dd deleted: sha256:e24ef2dce7ae9eaf0392baa06575685656cd5f833a5ccab060f64d493da2e034 deleted: sha256:8d8b729a262f048a75fc49b282892faada09c85d94a3da86d7a66724989a2c8a deleted: sha256:a6f7fa296775e0a9e7773f7d7a4b9a566b689227dfa27a4aff3638acd446e235 deleted: sha256:0dbbd9405f6ba039bac0964ad678609c98eb5f35a2ee73c5a5fcdffaab7889a8 Total reclaimed space: 568.2MB davis@davis-arch \ue0b0 ~ \ue0b0 Now we can check the docker ps --all and we'll see only our running containers: davis@davis-arch \ue0b0 ~ \ue0b0 docker ps --all CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 3d2bac4adcd0 x-frontend \"sh -c /var/www/dock\u2026\" 7 days ago Up Less than a second 80/tcp x-frontend_run_71ec1574b4b9 7ef2e25bde57 x-api \"sh -c /var/www/dock\u2026\" 7 days ago Up Less than a second 80/tcp x-api_run_706c3e49d578 a39d737fc13f x-frontend \"sh -c /var/www/dock\u2026\" 7 days ago Up Less than a second 80/tcp x-frontend_run_6e04a848228d 60fd1cceaf76 redis:alpine \"docker-entrypoint.s\u2026\" 7 days ago Up Less than a second 0.0.0.0:6379->6379/tcp redis d1aafec6bbdb jwilder/nginx-proxy \"/app/docker-entrypo\u2026\" 7 days ago Up Less than a second 0.0.0.0:80->80/tcp nginx-proxy dc9f43f8754f redis \"docker-entrypoint.s\u2026\" 2 weeks ago Up Less than a second 0.0.0.0:63791->6379/tcp x_redis_1 e6615103591f mariadb:latest \"docker-entrypoint.s\u2026\" 3 weeks ago Restarting (1) 21 seconds ago mysql","title":"Removing stopped continers"},{"location":"Docker%20%26%20Kubernetes/02_manipulating_containers_with_docker_client/005_removing_stopped_containers/#removing-stopped-continers","text":"When there are a lot of stopped containers, basically all they do is taking up diskspace, there might be situations where we want to remove them not to leave them in a stopped state. davis@davis-arch \ue0b0 ~ \ue0b0 docker system prune WARNING! This will remove: - all stopped containers - all networks not used by at least one container - all dangling images - all dangling build cache Are you sure you want to continue? [y/N] y Deleted Containers: 0f4205a09dba73aa5604ef2cb06ab5a2dba054e42e44aa4d45d99b09e08da73d aba242a52208c5b4a85239e697bba60db7d37faa0aae62d6975899cbb66a5eab be33032ba83ec90e6606caa0aba75f7294ae63ea22f39cfbae54088b3f4749d8 28cade9f5e6b5ad8aba25b3686a97bc252fb0cf8442790846c7c9f75d474e4df ceae74bbdb5a83e772edd3aaa9576a87d2aa8044aa579a263b3ced773ea5f97a 2b7ca645d6b876097a38d32e70c96f7b3227aefc64de72da2eea2b2dcacb359a a90cda4fbf74c27bdca0dc679c9e9bf4c00fa4f4ab8fa38b4e39f57b6ac03ce4 49ca4be6a89a34ca9aa228ed3b2484fbbc95bc426dac8c2b626aa2b0270cf793 08a47054eeaada2fa5d347ac76a04b7268a6a9a96ba9a9624acc9c654d7a2f39 2395ab3de09720a306a0ac02a992457de6f95e8b7c25c890a7b27974babf089f 2fa3f86edaf83463f5abb7e00c04e920edddc5b84d237b3444d8c59a8297602e f4dffbeae0e789aef9d22a48434c57a0489726a64add044a3e4b329273bb3c33 6a303e43cab76059acb99b9fa2f2348ed9474ca9620743aebc4e7ddacadde872 93ea0762aa3a6faedad37589c035a5cbb06b42e85cec5220780bb4388eacaeea b793b6f258786e24e5807043993090bf448bd63efedde00536869fcf900784d8 f4942e7b3e0a79fceda64aaaae78579ff0daa97aaa99aaf9b636cca922a06daf af4db85b936a228ea9a59ea7779d29db7e0acf980f29e34ea25a064aa53769cd fcba457e7c30f058c55c6a0db609cc970a2fc06adcd60c386c85bcf3a89b6e87 54ceb98bd98c6dd2aa49c85edeed5efa756a9e255e9f7ace388babaafc0ecdf4 dc8c3b08ce482b646b5e52a85eb885a3ecf528b46277eaf5a4a4e0c0a988a62f 58caf8a72006c3fc3c50baf00dcc29abf5f94c4a48bafef43a37fa8b4784fc3e f63a68bcbdf9aa0f3c4f448aa2fbb6ab8b0788b4aa22adc5b77c8a83943afe88 ed37a5766b29b724874a8baf7d9bc053aadcaa9a4f6ccf36e3d9d8a3afedc353 fa6303633ef4d4432a47ecc9a9897e37d6ebdcbb97528e2f8a0d8443b60c8d43 9c763dab4ecfb26ca4a89b568b96b25a7c2ff95fa0acfdca6c6a4fe26a8cb837 b0e8d00af5f6af235c3674a4ea844470bcf63a868866ca94ffc6d33e6a0ad9ca d076a8584aa6d54fb2b3d5fbda9d6f7e7ba69ef3ba4a4803e88cddd2f820cfcf 005fd53362b244352ec7b37422b2f83ac40430d8da24355aee5726044f952efd a4750034f8e0a300e4fcf57ec45ac5966d6e9f30ce08e070aebb723ffb85de20 6ee3bb3c083b000393c83ebf8c0e89c9fbcf5580cbea24b0dbc8c5ac5e26a2f5 ac004de38bca6bba52320b0da3044798ca56fbe4cd8a32c0a8d22558d9858e49 0ad58aa3538ba350badb6646eecc8805ee5b77697a4bc7a4c63b49228920bdea 95e6ba6726e3e0ca4a9a74a72b7290ad5ec66fcca92f2ad5d9f73252708f7689 70fbf74aa2ebc6eec68b9c20ec55ed0aa0978a4beea334f42aa2ad8ae5f5c578 f649899d4a7ccfe5a6ba25ab0ef78bc93ca0536473494f4a63edfca405076fbc Deleted Networks: x_default Deleted Images: deleted: sha256:0eba3eb9236934989d2d40d27c99e740c2dd062550fd0a6097de85b98addbdb0 deleted: sha256:ddef43bee7670933aa6ecbb6eed2934e8d03e2d37e80a27a5390ad8e85d8d4b4 deleted: sha256:47b6409dc054e7bcd64aa7c3ff76a6a99e52da24844d4d52ad3df68bd0a7697d deleted: sha256:4c7fe7269ff7ed4aaee7fac95df7dff082c508aa4a5f059004fc879f29d67990 deleted: sha256:ba0874383a790785b5ddc483b76f5fda2f22cd40ca76652dffd8a564cfb4ad6e deleted: sha256:3abefd83df96d69c80bf255027b508f3ad8d85bfddcc68f0c23cc6ddc890986e deleted: sha256:53a06e8ce5b9d77f33d3567e76d33d2e40d3afb43ed2b7dd0430b383abbc7bed deleted: sha256:04e56a5beebd4d773b2c569ed6782affb5d759fd9456745a665b722a6ec8efd5 deleted: sha256:e87b84d8efbcf9ba82aaafbdb5a9fc327a77d436060c0506ca5d567524df88ee deleted: sha256:4f2d3c63d98e6debff3ee63a6dde748f46a2baec06d9d4577a4769562c944456 deleted: sha256:7fdb7fadf7dfe3d0dd96f26503bbbcc777dd7e635005d6220d4aacbfc5609d48 deleted: sha256:49be9bebfacf8bb3d5954047b7ed2aa62740cd3dcf3656efa93d5ecd02e757d6 deleted: sha256:03d664ff35057d7b90c7066038a3ea699c7b6728a5adad4525c5dc9cdc27ecbf deleted: sha256:67b6f694854fd5bee83b6b2bd9df40d944b6a993fe053694294ca3a8c66f4307 deleted: sha256:7f43dc6ddd6568fca99563d2765d8fbd6cdb56a83e76ade00d0df6a6a2ee6b39 deleted: sha256:f3f9fd32c6807b2d04d6eb03a8c6e6025d73370d02d5be3d4fd7b859d2ebd0eb deleted: sha256:398abaddcb52f3c357d055cadbddd3d5bf3e97a4d08b0dea53d66bd962d2892c deleted: sha256:a7d09aafd8df8247debdd504ef2ee3042a3df0667fec7f295b9edf8c8aa98622 deleted: sha256:b740744a6532d09d2d82dc2e644ec28ec79a5d9c453dff9d72bbda57f0f4d7da deleted: sha256:5b3cde7b97370d65f782e2486e4bebd4b048dda5cb05d023520f82863520ed0a deleted: sha256:deacb673d4df65a2ddd8de757b333d37e059dd0c8b72676e5dc8867e0faaac45 deleted: sha256:3aa07addf85d8b63dee7d96332f9ecd323ed0addc4bd5edaa0accedabcd3ecf2 deleted: sha256:e2c77269e6d8393089fd2acdd8c2d5febe8a20ddfdbb35f9f9dd2ca7d2d247d5 deleted: sha256:b6eab3fe6e00e433ceafffcdcede968a33b5dcab26dadbb9c4ea72992d752fc8 deleted: sha256:5a99723f049d9d262d7209983cd6e0f2d35a9ebdeffae6d8d3098e8a2dbde049 deleted: sha256:860c8bb03e7dd50d2babd09abbd6fdd3dbee8f93d64b3c7ed7e29a2be8eef7bb deleted: sha256:98f80059f92d58aa9ca756fad949df33df35036afd682eee230b68682f2eb5b8 deleted: sha256:cb285b4defdc2de649490a6d3dfd67f6fcad8f2da3df22725d44dec9e35bf066 deleted: sha256:04ddb2eb40ddd5700ddc7dfdf9c4847d65cdf959c0592c4acbd9726e9e659e92 deleted: sha256:bf9bb0cbc545e8b368f76dcec843ad0dfc590ddd0c8657b82becfafddae24452 deleted: sha256:cd8947d6cdb0f5b28bdf747df9896947098bd00834944c5db4982de8d39d6ffd deleted: sha256:5dc75c08b4db43de7b8399def9063af0fb4fff9548cbf0cdaeeb8a427bce439c deleted: sha256:99fc56305ff7b00293dcd3bb324679edab7e39dd077b90949ac7d5ca9da7b934 deleted: sha256:03daf335d82be22d7f304cafb887dd9acadd244beedd0434f6b46226579b7d39 deleted: sha256:fc83657660cb03af32bcb8b626ca0f44735be3cb6e7e7e8a3ddb6b9edd0389d8 deleted: sha256:2525deddd2ddc87402459ac4ba2376c88e52dfc9de0ee3bebbe9287d675b0066 deleted: sha256:d0c0a6f50d5dbded67576d45f4d79d0237d067895d9e27a0287e7f4bda9d49eb deleted: sha256:0d956dbb67dc9ac967bb835088865af28cd5a4b363305ec4f8d5d3f204cf70bc deleted: sha256:bb075d2fc3a6ad24529cb6dfcc67ec6583a995f7d97ac8c7efb9dd473d384b2f deleted: sha256:d9a9edeb3032be72eaddd0e64e7b954b4dfb4d6dd9aa206d2402a92490beb903 deleted: sha256:0dd7da6372208cb4b370fc530c75d7d22dd53393db83eedd48502db4de9f6476 deleted: sha256:2695fd6226646e85d3343594c64aab8aa7094d6a7cbdedf8b622b474ba20d8f4 deleted: sha256:62cc327d4d08e46490ddd6032b59ce605429d2d79c750bc0db4fdfdfb8c6dcd5 deleted: sha256:6d3d54075adcd65ad6980df23f55f35bcf0ed32958235ce404b2d477e92ad0bd deleted: sha256:9af70efd0c0eed8aa7854de0c6822cfd82ea7b830020d4948bc789d299428063 deleted: sha256:ed6dc2539323358cc3b8cc504d2c4523fba07dcdd255d4dc7deb32202bd686ac deleted: sha256:abd66bc79558e9638cc7d8a7824fd2823d3b423c6c2f9b2f7d22fdd0d46c4aef deleted: sha256:b90343064f3bcbb2e9c544539e2dbcbf9fdc954f8767a955cc46c08c75aab9dd deleted: sha256:e24ef2dce7ae9eaf0392baa06575685656cd5f833a5ccab060f64d493da2e034 deleted: sha256:8d8b729a262f048a75fc49b282892faada09c85d94a3da86d7a66724989a2c8a deleted: sha256:a6f7fa296775e0a9e7773f7d7a4b9a566b689227dfa27a4aff3638acd446e235 deleted: sha256:0dbbd9405f6ba039bac0964ad678609c98eb5f35a2ee73c5a5fcdffaab7889a8 Total reclaimed space: 568.2MB davis@davis-arch \ue0b0 ~ \ue0b0 Now we can check the docker ps --all and we'll see only our running containers: davis@davis-arch \ue0b0 ~ \ue0b0 docker ps --all CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 3d2bac4adcd0 x-frontend \"sh -c /var/www/dock\u2026\" 7 days ago Up Less than a second 80/tcp x-frontend_run_71ec1574b4b9 7ef2e25bde57 x-api \"sh -c /var/www/dock\u2026\" 7 days ago Up Less than a second 80/tcp x-api_run_706c3e49d578 a39d737fc13f x-frontend \"sh -c /var/www/dock\u2026\" 7 days ago Up Less than a second 80/tcp x-frontend_run_6e04a848228d 60fd1cceaf76 redis:alpine \"docker-entrypoint.s\u2026\" 7 days ago Up Less than a second 0.0.0.0:6379->6379/tcp redis d1aafec6bbdb jwilder/nginx-proxy \"/app/docker-entrypo\u2026\" 7 days ago Up Less than a second 0.0.0.0:80->80/tcp nginx-proxy dc9f43f8754f redis \"docker-entrypoint.s\u2026\" 2 weeks ago Up Less than a second 0.0.0.0:63791->6379/tcp x_redis_1 e6615103591f mariadb:latest \"docker-entrypoint.s\u2026\" 3 weeks ago Restarting (1) 21 seconds ago mysql","title":"Removing stopped continers"},{"location":"Docker%20%26%20Kubernetes/02_manipulating_containers_with_docker_client/006_retrieving_log_outputs/","text":"Retrieving log outputs \u00b6 When using the docker start without the -a flag and we want to see the output of the logs, we can use docker logs <container id> davis@davis-arch \ue0b0 ~ \ue0b0 docker create busybox echo hi there c057588ee5fb84bbfb25c99935f6f2414e6ad2f42f69169cc4f86b36957ca1dc davis@davis-arch \ue0b0 ~ \ue0b0 docker start c057588ee5fb84bbfb25c99935f6f2414e6ad2f42f69169cc4f86b36957ca1dc c057588ee5fb84bbfb25c99935f6f2414e6ad2f42f69169cc4f86b36957ca1dc davis@davis-arch \ue0b0 ~ \ue0b0 docker logs c057588ee5fb84bbfb25c99935f6f2414e6ad2f42f69169cc4f86b36957ca1dc hi there","title":"Retrieving log outputs"},{"location":"Docker%20%26%20Kubernetes/02_manipulating_containers_with_docker_client/006_retrieving_log_outputs/#retrieving-log-outputs","text":"When using the docker start without the -a flag and we want to see the output of the logs, we can use docker logs <container id> davis@davis-arch \ue0b0 ~ \ue0b0 docker create busybox echo hi there c057588ee5fb84bbfb25c99935f6f2414e6ad2f42f69169cc4f86b36957ca1dc davis@davis-arch \ue0b0 ~ \ue0b0 docker start c057588ee5fb84bbfb25c99935f6f2414e6ad2f42f69169cc4f86b36957ca1dc c057588ee5fb84bbfb25c99935f6f2414e6ad2f42f69169cc4f86b36957ca1dc davis@davis-arch \ue0b0 ~ \ue0b0 docker logs c057588ee5fb84bbfb25c99935f6f2414e6ad2f42f69169cc4f86b36957ca1dc hi there","title":"Retrieving log outputs"},{"location":"Docker%20%26%20Kubernetes/02_manipulating_containers_with_docker_client/007_stopping_containers/","text":"Stopping containers \u00b6 You can use either docker stop or docker kill command to stop a container. When using docker stop you will issue a SIGTERM message to the process. It gives the process a little bit of time to shut down and do cleanup. The docker kill issues SIGKILL message, which kills the process immediately. If the docker stop does not stop within 10 seconds - it issues the kill command. davis@davis-arch \ue0b0 ~ \ue0b0 docker create busybox ping google.com 2fa3cb41f84d8576b634451c288b1ae4462d98314aa832275633139b94efe284 davis@davis-arch \ue0b0 ~ \ue0b0 docker start 2fa3cb41f84d8576b634451c288b1ae4462d98314aa832275633139b94efe284 2fa3cb41f84d8576b634451c288b1ae4462d98314aa832275633139b94efe284 davis@davis-arch \ue0b0 ~ \ue0b0 docker logs 2fa3cb41f84d8576b634451c288b1ae4462d98314aa832275633139b94efe284 PING google.com (64.233.162.101): 56 data bytes 64 bytes from 64.233.162.101: seq=0 ttl=46 time=23.328 ms 64 bytes from 64.233.162.101: seq=1 ttl=46 time=23.068 ms 64 bytes from 64.233.162.101: seq=2 ttl=46 time=23.231 ms 64 bytes from 64.233.162.101: seq=3 ttl=46 time=23.133 ms 64 bytes from 64.233.162.101: seq=4 ttl=46 time=23.241 ms 64 bytes from 64.233.162.101: seq=5 ttl=46 time=23.158 ms 64 bytes from 64.233.162.101: seq=6 ttl=46 time=23.231 ms 64 bytes from 64.233.162.101: seq=7 ttl=46 time=23.132 ms 64 bytes from 64.233.162.101: seq=8 ttl=46 time=23.010 ms 64 bytes from 64.233.162.101: seq=9 ttl=46 time=23.221 ms 64 bytes from 64.233.162.101: seq=10 ttl=46 time=23.220 ms 64 bytes from 64.233.162.101: seq=11 ttl=46 time=23.112 ms 64 bytes from 64.233.162.101: seq=12 ttl=46 time=24.198 ms davis@davis-arch \ue0b0 ~ \ue0b0 docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2fa3cb41f84d busybox \"ping google.com\" 28 seconds ago Up 20 seconds xenodochial_wiles davis@davis-arch \ue0b0 ~ \ue0b0 docker stop 2fa3cb41f84d 2fa3cb41f84d davis@davis-arch \ue0b0 ~ \ue0b0 docker start 2fa3cb41f84d 2fa3cb41f84d davis@davis-arch \ue0b0 ~ \ue0b0 docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2fa3cb41f84d busybox \"ping google.com\" 5 minutes ago Up 7 seconds xenodochial_wiles davis@davis-arch \ue0b0 ~ \ue0b0 docker kill 2fa3cb41f84d 2fa3cb41f84d","title":"Stopping containers"},{"location":"Docker%20%26%20Kubernetes/02_manipulating_containers_with_docker_client/007_stopping_containers/#stopping-containers","text":"You can use either docker stop or docker kill command to stop a container. When using docker stop you will issue a SIGTERM message to the process. It gives the process a little bit of time to shut down and do cleanup. The docker kill issues SIGKILL message, which kills the process immediately. If the docker stop does not stop within 10 seconds - it issues the kill command. davis@davis-arch \ue0b0 ~ \ue0b0 docker create busybox ping google.com 2fa3cb41f84d8576b634451c288b1ae4462d98314aa832275633139b94efe284 davis@davis-arch \ue0b0 ~ \ue0b0 docker start 2fa3cb41f84d8576b634451c288b1ae4462d98314aa832275633139b94efe284 2fa3cb41f84d8576b634451c288b1ae4462d98314aa832275633139b94efe284 davis@davis-arch \ue0b0 ~ \ue0b0 docker logs 2fa3cb41f84d8576b634451c288b1ae4462d98314aa832275633139b94efe284 PING google.com (64.233.162.101): 56 data bytes 64 bytes from 64.233.162.101: seq=0 ttl=46 time=23.328 ms 64 bytes from 64.233.162.101: seq=1 ttl=46 time=23.068 ms 64 bytes from 64.233.162.101: seq=2 ttl=46 time=23.231 ms 64 bytes from 64.233.162.101: seq=3 ttl=46 time=23.133 ms 64 bytes from 64.233.162.101: seq=4 ttl=46 time=23.241 ms 64 bytes from 64.233.162.101: seq=5 ttl=46 time=23.158 ms 64 bytes from 64.233.162.101: seq=6 ttl=46 time=23.231 ms 64 bytes from 64.233.162.101: seq=7 ttl=46 time=23.132 ms 64 bytes from 64.233.162.101: seq=8 ttl=46 time=23.010 ms 64 bytes from 64.233.162.101: seq=9 ttl=46 time=23.221 ms 64 bytes from 64.233.162.101: seq=10 ttl=46 time=23.220 ms 64 bytes from 64.233.162.101: seq=11 ttl=46 time=23.112 ms 64 bytes from 64.233.162.101: seq=12 ttl=46 time=24.198 ms davis@davis-arch \ue0b0 ~ \ue0b0 docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2fa3cb41f84d busybox \"ping google.com\" 28 seconds ago Up 20 seconds xenodochial_wiles davis@davis-arch \ue0b0 ~ \ue0b0 docker stop 2fa3cb41f84d 2fa3cb41f84d davis@davis-arch \ue0b0 ~ \ue0b0 docker start 2fa3cb41f84d 2fa3cb41f84d davis@davis-arch \ue0b0 ~ \ue0b0 docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2fa3cb41f84d busybox \"ping google.com\" 5 minutes ago Up 7 seconds xenodochial_wiles davis@davis-arch \ue0b0 ~ \ue0b0 docker kill 2fa3cb41f84d 2fa3cb41f84d","title":"Stopping containers"},{"location":"Docker%20%26%20Kubernetes/02_manipulating_containers_with_docker_client/008_execute_commands_in_running_containers/","text":"Execute commands in running containers \u00b6 We can execute an additional command in a container using the docker exec -it <container id> <command> The -it flags allows us to provide input to the container, it connects the STDIN , STDOUT and STDERR channels between your terminal and the running process. The -i flag connects the channels, the -t basically formats the output so it is prettier. davis@davis-arch \ue0b0 ~ \ue0b0 docker create redis 5e993caa8ff4debe7e0bf6879546c76a8e21b372b560b8f348158c0b1c98ca86 davis@davis-arch \ue0b0 ~ \ue0b0 docker start 5e993caa8ff4debe7e0bf6879546c76a8e21b372b560b8f348158c0b1c98ca86 5e993caa8ff4debe7e0bf6879546c76a8e21b372b560b8f348158c0b1c98ca86 davis@davis-arch \ue0b0 ~ \ue0b0 docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 5e993caa8ff4 redis \"docker-entrypoint.s\u2026\" 8 seconds ago Up 1 second 6379/tcp relaxed_williamson davis@davis-arch \ue0b0 ~ \ue0b0 docker exec -it 5e993caa8ff4debe7e0bf6879546c76a8e21b372b560b8f348158c0b1c98ca86 redis-cli 127.0.0.1:6379> set myvalue 5 OK 127.0.0.1:6379> get myvalue \"5\" When using the exec without the -it it will just execute the redis-cli command and we will not be able to interact with it. You can also use the exec command to get full terminal access of the container for debugging: davis@davis-arch \ue0b0 ~ \ue0b0 docker exec -it 5e993caa8ff4debe7e0bf6879546c76a8e21b372b560b8f348158c0b1c98ca86 bash root@5e993caa8ff4:/data# ls / bin boot data dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var root@5e993caa8ff4:/data# hostname 5e993caa8ff4 You can also create the image at the start to use the terminal: davis@davis-arch \ue0b0 ~ \ue0b0 docker run -it busybox sh / # ls / bin dev etc home proc root sys tmp usr var / # hostname 946745b94a5e","title":"Execute commands in running containers"},{"location":"Docker%20%26%20Kubernetes/02_manipulating_containers_with_docker_client/008_execute_commands_in_running_containers/#execute-commands-in-running-containers","text":"We can execute an additional command in a container using the docker exec -it <container id> <command> The -it flags allows us to provide input to the container, it connects the STDIN , STDOUT and STDERR channels between your terminal and the running process. The -i flag connects the channels, the -t basically formats the output so it is prettier. davis@davis-arch \ue0b0 ~ \ue0b0 docker create redis 5e993caa8ff4debe7e0bf6879546c76a8e21b372b560b8f348158c0b1c98ca86 davis@davis-arch \ue0b0 ~ \ue0b0 docker start 5e993caa8ff4debe7e0bf6879546c76a8e21b372b560b8f348158c0b1c98ca86 5e993caa8ff4debe7e0bf6879546c76a8e21b372b560b8f348158c0b1c98ca86 davis@davis-arch \ue0b0 ~ \ue0b0 docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 5e993caa8ff4 redis \"docker-entrypoint.s\u2026\" 8 seconds ago Up 1 second 6379/tcp relaxed_williamson davis@davis-arch \ue0b0 ~ \ue0b0 docker exec -it 5e993caa8ff4debe7e0bf6879546c76a8e21b372b560b8f348158c0b1c98ca86 redis-cli 127.0.0.1:6379> set myvalue 5 OK 127.0.0.1:6379> get myvalue \"5\" When using the exec without the -it it will just execute the redis-cli command and we will not be able to interact with it. You can also use the exec command to get full terminal access of the container for debugging: davis@davis-arch \ue0b0 ~ \ue0b0 docker exec -it 5e993caa8ff4debe7e0bf6879546c76a8e21b372b560b8f348158c0b1c98ca86 bash root@5e993caa8ff4:/data# ls / bin boot data dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var root@5e993caa8ff4:/data# hostname 5e993caa8ff4 You can also create the image at the start to use the terminal: davis@davis-arch \ue0b0 ~ \ue0b0 docker run -it busybox sh / # ls / bin dev etc home proc root sys tmp usr var / # hostname 946745b94a5e","title":"Execute commands in running containers"},{"location":"Docker%20%26%20Kubernetes/02_manipulating_containers_with_docker_client/009_contianer_isolation/","text":"Container isolation \u00b6 By default all containers have completely separate file systems, they do not share files between them.","title":"Container isolation"},{"location":"Docker%20%26%20Kubernetes/02_manipulating_containers_with_docker_client/009_contianer_isolation/#container-isolation","text":"By default all containers have completely separate file systems, they do not share files between them.","title":"Container isolation"},{"location":"Docker%20%26%20Kubernetes/03_building_custom_images_with_docker_server/001-creating-docker-images/","text":"Creating docker images \u00b6 Previously we have made use of docker images that are already built by someone. We can create our own docker images by creating a Dockerfile that defines the configuration of the container, then we build it using by docker client, the docker server builds it into a usable image. The Dockerfile creation flow looks something like this: 1. Specify a base image 2. Run some commands to install additional programs 3. Specify a command to run on container startup","title":"Creating docker images"},{"location":"Docker%20%26%20Kubernetes/03_building_custom_images_with_docker_server/001-creating-docker-images/#creating-docker-images","text":"Previously we have made use of docker images that are already built by someone. We can create our own docker images by creating a Dockerfile that defines the configuration of the container, then we build it using by docker client, the docker server builds it into a usable image. The Dockerfile creation flow looks something like this: 1. Specify a base image 2. Run some commands to install additional programs 3. Specify a command to run on container startup","title":"Creating docker images"},{"location":"Docker%20%26%20Kubernetes/03_building_custom_images_with_docker_server/002-building-a-dockerfile/","text":"Building a dockerfile \u00b6 To build an image that runs a redis-server, we can create a new directory for our project davis@davis-arch \ue0b0 ~/projects/docker \ue0b0 \ue0a0 master \ue0b0 mkdir 01_redis-image davis@davis-arch \ue0b0 ~/projects/docker \ue0b0 \ue0a0 master \ue0b0 cd 01_redis-image davis@davis-arch \ue0b0 ~/projects/docker \ue0b0 \ue0a0 master \ue0b0 code . And create a new Dockerfile with following content: # Use an existing docker image as a base FROM alpine # Download and install all dependencies RUN apk add --update redis # Tell the image what to do when it starts as a container CMD [\"redis-server\"] Now we can build the image: davis@davis-arch \ue0b0 ~/projects/docker \ue0b0 \ue0a0 master \ue0b0 cd 01_redis-image davis@davis-arch \ue0b0 ~/projects/docker/01_redis-image \ue0b0 \ue0a0 master \ue0b0 docker build . Sending build context to Docker daemon 2.048kB Step 1/3 : FROM alpine latest: Pulling from library/alpine 6c40cc604d8e: Pull complete Digest: sha256:b3dbf31b77fd99d9c08f780ce6f5282aba076d70a513a8be859d8d3a4d0c92b8 Status: Downloaded newer image for alpine:latest ---> caf27325b298 Step 2/3 : RUN apk add --update redis ---> Running in db036af84433 fetch http://dl-cdn.alpinelinux.org/alpine/v3.9/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.9/community/x86_64/APKINDEX.tar.gz (1/1) Installing redis (4.0.12-r0) Executing redis-4.0.12-r0.pre-install Executing redis-4.0.12-r0.post-install Executing busybox-1.29.3-r10.trigger OK: 7 MiB in 15 packages Removing intermediate container db036af84433 ---> 6af5b67c4479 Step 3/3 : CMD [\"redis-server\"] ---> Running in 30207662f6e4 Removing intermediate container 30207662f6e4 ---> f84d58cd025b Successfully built f84d58cd025b Now we can run it: davis@davis-arch \ue0b0 ~/projects/docker/01_redis-image \ue0b0 \ue0a0 master \ue0b0 docker run f84d58cd025b 1:C 01 Feb 08:59:01.892 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 1:C 01 Feb 08:59:01.892 # Redis version=4.0.12, bits=64, commit=1be97168, modified=0, pid=1, just started 1:C 01 Feb 08:59:01.892 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf 1:M 01 Feb 08:59:01.894 * Running mode=standalone, port=6379. 1:M 01 Feb 08:59:01.894 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. 1:M 01 Feb 08:59:01.894 # Server initialized 1:M 01 Feb 08:59:01.894 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect. 1:M 01 Feb 08:59:01.894 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled. 1:M 01 Feb 08:59:01.894 * Ready to accept connections","title":"Building a dockerfile"},{"location":"Docker%20%26%20Kubernetes/03_building_custom_images_with_docker_server/002-building-a-dockerfile/#building-a-dockerfile","text":"To build an image that runs a redis-server, we can create a new directory for our project davis@davis-arch \ue0b0 ~/projects/docker \ue0b0 \ue0a0 master \ue0b0 mkdir 01_redis-image davis@davis-arch \ue0b0 ~/projects/docker \ue0b0 \ue0a0 master \ue0b0 cd 01_redis-image davis@davis-arch \ue0b0 ~/projects/docker \ue0b0 \ue0a0 master \ue0b0 code . And create a new Dockerfile with following content: # Use an existing docker image as a base FROM alpine # Download and install all dependencies RUN apk add --update redis # Tell the image what to do when it starts as a container CMD [\"redis-server\"] Now we can build the image: davis@davis-arch \ue0b0 ~/projects/docker \ue0b0 \ue0a0 master \ue0b0 cd 01_redis-image davis@davis-arch \ue0b0 ~/projects/docker/01_redis-image \ue0b0 \ue0a0 master \ue0b0 docker build . Sending build context to Docker daemon 2.048kB Step 1/3 : FROM alpine latest: Pulling from library/alpine 6c40cc604d8e: Pull complete Digest: sha256:b3dbf31b77fd99d9c08f780ce6f5282aba076d70a513a8be859d8d3a4d0c92b8 Status: Downloaded newer image for alpine:latest ---> caf27325b298 Step 2/3 : RUN apk add --update redis ---> Running in db036af84433 fetch http://dl-cdn.alpinelinux.org/alpine/v3.9/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.9/community/x86_64/APKINDEX.tar.gz (1/1) Installing redis (4.0.12-r0) Executing redis-4.0.12-r0.pre-install Executing redis-4.0.12-r0.post-install Executing busybox-1.29.3-r10.trigger OK: 7 MiB in 15 packages Removing intermediate container db036af84433 ---> 6af5b67c4479 Step 3/3 : CMD [\"redis-server\"] ---> Running in 30207662f6e4 Removing intermediate container 30207662f6e4 ---> f84d58cd025b Successfully built f84d58cd025b Now we can run it: davis@davis-arch \ue0b0 ~/projects/docker/01_redis-image \ue0b0 \ue0a0 master \ue0b0 docker run f84d58cd025b 1:C 01 Feb 08:59:01.892 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 1:C 01 Feb 08:59:01.892 # Redis version=4.0.12, bits=64, commit=1be97168, modified=0, pid=1, just started 1:C 01 Feb 08:59:01.892 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf 1:M 01 Feb 08:59:01.894 * Running mode=standalone, port=6379. 1:M 01 Feb 08:59:01.894 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. 1:M 01 Feb 08:59:01.894 # Server initialized 1:M 01 Feb 08:59:01.894 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect. 1:M 01 Feb 08:59:01.894 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled. 1:M 01 Feb 08:59:01.894 * Ready to accept connections","title":"Building a dockerfile"},{"location":"Docker%20%26%20Kubernetes/03_building_custom_images_with_docker_server/003-dockerfile-teardown/","text":"Dockerfile teardown \u00b6 Every line on the dockerfile instructs docker server what to do. FROM instructions the server to use a base image like alpine . RUN instructions the server to execute a command in the base image. CMD instructions the server to the command that is used when the container is started up. More commands are available at https://docs.docker.com/engine/reference/builder/#from","title":"Dockerfile teardown"},{"location":"Docker%20%26%20Kubernetes/03_building_custom_images_with_docker_server/003-dockerfile-teardown/#dockerfile-teardown","text":"Every line on the dockerfile instructs docker server what to do. FROM instructions the server to use a base image like alpine . RUN instructions the server to execute a command in the base image. CMD instructions the server to the command that is used when the container is started up. More commands are available at https://docs.docker.com/engine/reference/builder/#from","title":"Dockerfile teardown"},{"location":"Docker%20%26%20Kubernetes/03_building_custom_images_with_docker_server/004-what-is-a-base-image/","text":"What is a base image? \u00b6 When writing a dockerfile, we'll always need an initial step of having an operating system where to execute all the additional steps of setting everything up. When we specify using a base image, we do an initial step of setting up an operating system, all the programs we need in order to start working on our project. In previous section we made use of alpine base image because it is a small linux operating system that includes a package manager program, that makes it very convenient to install additional software like redis. You can use other base images like: scratch , busybox , ubuntu , centos","title":"What is a base image?"},{"location":"Docker%20%26%20Kubernetes/03_building_custom_images_with_docker_server/004-what-is-a-base-image/#what-is-a-base-image","text":"When writing a dockerfile, we'll always need an initial step of having an operating system where to execute all the additional steps of setting everything up. When we specify using a base image, we do an initial step of setting up an operating system, all the programs we need in order to start working on our project. In previous section we made use of alpine base image because it is a small linux operating system that includes a package manager program, that makes it very convenient to install additional software like redis. You can use other base images like: scratch , busybox , ubuntu , centos","title":"What is a base image?"},{"location":"Docker%20%26%20Kubernetes/03_building_custom_images_with_docker_server/005-build-process-in-detail/","text":"Build process in detail \u00b6 Previously, when we ran the command davis@davis-arch \ue0b0 ~/projects/docker/01_redis-image \ue0b0 \ue0a0 master \ue0b0 docker build . Sending build context to Docker daemon 2.048kB Step 1/3 : FROM alpine latest: Pulling from library/alpine 6c40cc604d8e: Pull complete Digest: sha256:b3dbf31b77fd99d9c08f780ce6f5282aba076d70a513a8be859d8d3a4d0c92b8 Status: Downloaded newer image for alpine:latest ---> caf27325b298 Step 2/3 : RUN apk add --update redis ---> Running in db036af84433 fetch http://dl-cdn.alpinelinux.org/alpine/v3.9/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.9/community/x86_64/APKINDEX.tar.gz (1/1) Installing redis (4.0.12-r0) Executing redis-4.0.12-r0.pre-install Executing redis-4.0.12-r0.post-install Executing busybox-1.29.3-r10.trigger OK: 7 MiB in 15 packages Removing intermediate container db036af84433 ---> 6af5b67c4479 Step 3/3 : CMD [\"redis-server\"] ---> Running in 30207662f6e4 Removing intermediate container 30207662f6e4 ---> f84d58cd025b Successfully built f84d58cd025b It gave our dockerfile to our cli. The . represents the build context that represents the files and folders that we want encapsulate in our container. We had 3 steps in building an image: 1. Docker server checked our image cache and looked if we have ever downloaded an image called alpine , downloads it. 2. Docker server creates a container db036af84433 of alpine where it runs the apk add --update redis command, takes the file system snapshot and removes the container. 3. Docker server creates a container 30207662f6e4 of previously created snapshot, sets up the primary command. Takes a snapshop and removes the container. When all steps are done, we get the resulting snapshot, primary command and save it as an image as f84d58cd025b .","title":"Build process in detail"},{"location":"Docker%20%26%20Kubernetes/03_building_custom_images_with_docker_server/005-build-process-in-detail/#build-process-in-detail","text":"Previously, when we ran the command davis@davis-arch \ue0b0 ~/projects/docker/01_redis-image \ue0b0 \ue0a0 master \ue0b0 docker build . Sending build context to Docker daemon 2.048kB Step 1/3 : FROM alpine latest: Pulling from library/alpine 6c40cc604d8e: Pull complete Digest: sha256:b3dbf31b77fd99d9c08f780ce6f5282aba076d70a513a8be859d8d3a4d0c92b8 Status: Downloaded newer image for alpine:latest ---> caf27325b298 Step 2/3 : RUN apk add --update redis ---> Running in db036af84433 fetch http://dl-cdn.alpinelinux.org/alpine/v3.9/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.9/community/x86_64/APKINDEX.tar.gz (1/1) Installing redis (4.0.12-r0) Executing redis-4.0.12-r0.pre-install Executing redis-4.0.12-r0.post-install Executing busybox-1.29.3-r10.trigger OK: 7 MiB in 15 packages Removing intermediate container db036af84433 ---> 6af5b67c4479 Step 3/3 : CMD [\"redis-server\"] ---> Running in 30207662f6e4 Removing intermediate container 30207662f6e4 ---> f84d58cd025b Successfully built f84d58cd025b It gave our dockerfile to our cli. The . represents the build context that represents the files and folders that we want encapsulate in our container. We had 3 steps in building an image: 1. Docker server checked our image cache and looked if we have ever downloaded an image called alpine , downloads it. 2. Docker server creates a container db036af84433 of alpine where it runs the apk add --update redis command, takes the file system snapshot and removes the container. 3. Docker server creates a container 30207662f6e4 of previously created snapshot, sets up the primary command. Takes a snapshop and removes the container. When all steps are done, we get the resulting snapshot, primary command and save it as an image as f84d58cd025b .","title":"Build process in detail"},{"location":"Docker%20%26%20Kubernetes/03_building_custom_images_with_docker_server/006-rebuilds-with-cache/","text":"Rebuilds with cache \u00b6 When building we have a filesystem snapshot of every step in the build process. If we modify the previously build Dockerfile to install gcc : # Use an existing docker image as a base FROM alpine # Download and install all dependencies RUN apk add --update redis RUN apk add --update gcc # Tell the image what to do when it starts as a container CMD [\"redis-server\"] And run the davis@davis-arch \ue0b0 ~/projects/docker/01_redis-image \ue0b0 \ue0b0 docker build . Sending build context to Docker daemon 2.048kB Step 1/4 : FROM alpine ---> caf27325b298 Step 2/4 : RUN apk add --update redis ---> Using cache ---> 6af5b67c4479 Step 3/4 : RUN apk add --update gcc ---> Running in 2c413c45fe6d fetch http://dl-cdn.alpinelinux.org/alpine/v3.9/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.9/community/x86_64/APKINDEX.tar.gz (1/10) Installing binutils (2.31.1-r2) (2/10) Installing gmp (6.1.2-r1) (3/10) Installing isl (0.18-r0) (4/10) Installing libgomp (8.2.0-r2) (5/10) Installing libatomic (8.2.0-r2) (6/10) Installing libgcc (8.2.0-r2) (7/10) Installing mpfr3 (3.1.5-r1) (8/10) Installing mpc1 (1.0.3-r1) (9/10) Installing libstdc++ (8.2.0-r2) (10/10) Installing gcc (8.2.0-r2) Executing busybox-1.29.3-r10.trigger OK: 93 MiB in 25 packages Removing intermediate container 2c413c45fe6d ---> 28c2cb2354aa Step 4/4 : CMD [\"redis-server\"] ---> Running in 4fdc91f5f841 Removing intermediate container 4fdc91f5f841 ---> 7874f97a4551 Successfully built 7874f97a4551 You can see that the 1st and the 2nd step is not executed - docker realised that nothing has changed in these steps and it can use previously created file system snapshots. When it sees that there is a new instruction to be executed, from that point on it executes the actual building steps.","title":"Rebuilds with cache"},{"location":"Docker%20%26%20Kubernetes/03_building_custom_images_with_docker_server/006-rebuilds-with-cache/#rebuilds-with-cache","text":"When building we have a filesystem snapshot of every step in the build process. If we modify the previously build Dockerfile to install gcc : # Use an existing docker image as a base FROM alpine # Download and install all dependencies RUN apk add --update redis RUN apk add --update gcc # Tell the image what to do when it starts as a container CMD [\"redis-server\"] And run the davis@davis-arch \ue0b0 ~/projects/docker/01_redis-image \ue0b0 \ue0b0 docker build . Sending build context to Docker daemon 2.048kB Step 1/4 : FROM alpine ---> caf27325b298 Step 2/4 : RUN apk add --update redis ---> Using cache ---> 6af5b67c4479 Step 3/4 : RUN apk add --update gcc ---> Running in 2c413c45fe6d fetch http://dl-cdn.alpinelinux.org/alpine/v3.9/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.9/community/x86_64/APKINDEX.tar.gz (1/10) Installing binutils (2.31.1-r2) (2/10) Installing gmp (6.1.2-r1) (3/10) Installing isl (0.18-r0) (4/10) Installing libgomp (8.2.0-r2) (5/10) Installing libatomic (8.2.0-r2) (6/10) Installing libgcc (8.2.0-r2) (7/10) Installing mpfr3 (3.1.5-r1) (8/10) Installing mpc1 (1.0.3-r1) (9/10) Installing libstdc++ (8.2.0-r2) (10/10) Installing gcc (8.2.0-r2) Executing busybox-1.29.3-r10.trigger OK: 93 MiB in 25 packages Removing intermediate container 2c413c45fe6d ---> 28c2cb2354aa Step 4/4 : CMD [\"redis-server\"] ---> Running in 4fdc91f5f841 Removing intermediate container 4fdc91f5f841 ---> 7874f97a4551 Successfully built 7874f97a4551 You can see that the 1st and the 2nd step is not executed - docker realised that nothing has changed in these steps and it can use previously created file system snapshots. When it sees that there is a new instruction to be executed, from that point on it executes the actual building steps.","title":"Rebuilds with cache"},{"location":"Docker%20%26%20Kubernetes/03_building_custom_images_with_docker_server/007-tagging-an-image/","text":"Tagging an image \u00b6 When we previously built the image: davis@davis-arch \ue0b0 ~/projects/docker/01_redis-image \ue0b0 \ue0b0 docker build . Sending build context to Docker daemon 2.048kB Step 1/4 : FROM alpine ---> caf27325b298 Step 2/4 : RUN apk add --update redis ---> Using cache ---> 6af5b67c4479 Step 3/4 : RUN apk add --update gcc ---> Running in 2c413c45fe6d fetch http://dl-cdn.alpinelinux.org/alpine/v3.9/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.9/community/x86_64/APKINDEX.tar.gz (1/10) Installing binutils (2.31.1-r2) (2/10) Installing gmp (6.1.2-r1) (3/10) Installing isl (0.18-r0) (4/10) Installing libgomp (8.2.0-r2) (5/10) Installing libatomic (8.2.0-r2) (6/10) Installing libgcc (8.2.0-r2) (7/10) Installing mpfr3 (3.1.5-r1) (8/10) Installing mpc1 (1.0.3-r1) (9/10) Installing libstdc++ (8.2.0-r2) (10/10) Installing gcc (8.2.0-r2) Executing busybox-1.29.3-r10.trigger OK: 93 MiB in 25 packages Removing intermediate container 2c413c45fe6d ---> 28c2cb2354aa Step 4/4 : CMD [\"redis-server\"] ---> Running in 4fdc91f5f841 Removing intermediate container 4fdc91f5f841 ---> 7874f97a4551 Successfully built 7874f97a4551 We get an image with an ID 7874f97a4551 , we can use tagging to give the image a more memorable name: davis@davis-arch \ue0b0 ~/projects/docker/01_redis-image \ue0b0 \ue0a0 master \ue0b0 docker build -t daviskregers/redis:latest . Sending build context to Docker daemon 2.048kB Step 1/4 : FROM alpine ---> caf27325b298 Step 2/4 : RUN apk add --update redis ---> Using cache ---> 6af5b67c4479 Step 3/4 : RUN apk add --update gcc ---> Using cache ---> 28c2cb2354aa Step 4/4 : CMD [\"redis-server\"] ---> Using cache ---> 7874f97a4551 Successfully built 7874f97a4551 Successfully tagged daviskregers/redis:latest The convention for tagging - your_docker_id/project_name:version . Now we can start a container by using: davis@davis-arch \ue0b0 ~/projects/docker/01_redis-image \ue0b0 \ue0a0 master \ue0b0 docker run daviskregers/redis 1:C 01 Feb 09:48:03.415 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 1:C 01 Feb 09:48:03.415 # Redis version=4.0.12, bits=64, commit=1be97168, modified=0, pid=1, just started 1:C 01 Feb 09:48:03.415 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf 1:M 01 Feb 09:48:03.417 * Running mode=standalone, port=6379. 1:M 01 Feb 09:48:03.417 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. 1:M 01 Feb 09:48:03.417 # Server initialized 1:M 01 Feb 09:48:03.417 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect. 1:M 01 Feb 09:48:03.417 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled. 1:M 01 Feb 09:48:03.417 * Ready to accept connections","title":"Tagging an image"},{"location":"Docker%20%26%20Kubernetes/03_building_custom_images_with_docker_server/007-tagging-an-image/#tagging-an-image","text":"When we previously built the image: davis@davis-arch \ue0b0 ~/projects/docker/01_redis-image \ue0b0 \ue0b0 docker build . Sending build context to Docker daemon 2.048kB Step 1/4 : FROM alpine ---> caf27325b298 Step 2/4 : RUN apk add --update redis ---> Using cache ---> 6af5b67c4479 Step 3/4 : RUN apk add --update gcc ---> Running in 2c413c45fe6d fetch http://dl-cdn.alpinelinux.org/alpine/v3.9/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.9/community/x86_64/APKINDEX.tar.gz (1/10) Installing binutils (2.31.1-r2) (2/10) Installing gmp (6.1.2-r1) (3/10) Installing isl (0.18-r0) (4/10) Installing libgomp (8.2.0-r2) (5/10) Installing libatomic (8.2.0-r2) (6/10) Installing libgcc (8.2.0-r2) (7/10) Installing mpfr3 (3.1.5-r1) (8/10) Installing mpc1 (1.0.3-r1) (9/10) Installing libstdc++ (8.2.0-r2) (10/10) Installing gcc (8.2.0-r2) Executing busybox-1.29.3-r10.trigger OK: 93 MiB in 25 packages Removing intermediate container 2c413c45fe6d ---> 28c2cb2354aa Step 4/4 : CMD [\"redis-server\"] ---> Running in 4fdc91f5f841 Removing intermediate container 4fdc91f5f841 ---> 7874f97a4551 Successfully built 7874f97a4551 We get an image with an ID 7874f97a4551 , we can use tagging to give the image a more memorable name: davis@davis-arch \ue0b0 ~/projects/docker/01_redis-image \ue0b0 \ue0a0 master \ue0b0 docker build -t daviskregers/redis:latest . Sending build context to Docker daemon 2.048kB Step 1/4 : FROM alpine ---> caf27325b298 Step 2/4 : RUN apk add --update redis ---> Using cache ---> 6af5b67c4479 Step 3/4 : RUN apk add --update gcc ---> Using cache ---> 28c2cb2354aa Step 4/4 : CMD [\"redis-server\"] ---> Using cache ---> 7874f97a4551 Successfully built 7874f97a4551 Successfully tagged daviskregers/redis:latest The convention for tagging - your_docker_id/project_name:version . Now we can start a container by using: davis@davis-arch \ue0b0 ~/projects/docker/01_redis-image \ue0b0 \ue0a0 master \ue0b0 docker run daviskregers/redis 1:C 01 Feb 09:48:03.415 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 1:C 01 Feb 09:48:03.415 # Redis version=4.0.12, bits=64, commit=1be97168, modified=0, pid=1, just started 1:C 01 Feb 09:48:03.415 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf 1:M 01 Feb 09:48:03.417 * Running mode=standalone, port=6379. 1:M 01 Feb 09:48:03.417 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. 1:M 01 Feb 09:48:03.417 # Server initialized 1:M 01 Feb 09:48:03.417 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect. 1:M 01 Feb 09:48:03.417 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled. 1:M 01 Feb 09:48:03.417 * Ready to accept connections","title":"Tagging an image"},{"location":"Docker%20%26%20Kubernetes/03_building_custom_images_with_docker_server/008-manual-image-generation-with-docker-commit/","text":"Manual Image Generation with Docker commit \u00b6 When looking at the build process, we can see that we can create a container, execute commands in it and generate an image from that container. We can start up an empty alpine image: / # apk add --update redis fetch http://dl-cdn.alpinelinux.org/alpine/v3.9/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.9/community/x86_64/APKINDEX.tar.gz (1/1) Installing redis (4.0.12-r0) Executing redis-4.0.12-r0.pre-install Executing redis-4.0.12-r0.post-install Executing busybox-1.29.3-r10.trigger OK: 7 MiB in 15 packages / # Now, while that is running, in a second terminal we can execute: davis@davis-arch \ue0b0 ~ \ue0b0 docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES cd1e9a4372d7 alpine \"sh\" 48 seconds ago Up 46 seconds keen_swirles davis@davis-arch \ue0b0 ~ \ue0b0 docker commit -c 'CMD [\"redis-server\"]' cd1e9a4372d7 sha256:e1380e3df76f340d8f848942547a0d7f9e11f8843bef80d8de9df84dd186084d Now we can run it: davis@davis-arch \ue0b0 ~ \ue0b0 docker run e1380e3df76f340d8f84894 1:C 01 Feb 09:54:36.233 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 1:C 01 Feb 09:54:36.233 # Redis version=4.0.12, bits=64, commit=1be97168, modified=0, pid=1, just started 1:C 01 Feb 09:54:36.233 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf 1:M 01 Feb 09:54:36.234 * Running mode=standalone, port=6379. 1:M 01 Feb 09:54:36.234 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. 1:M 01 Feb 09:54:36.234 # Server initialized 1:M 01 Feb 09:54:36.234 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect. 1:M 01 Feb 09:54:36.235 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled. 1:M 01 Feb 09:54:36.235 * Ready to accept connections","title":"Manual Image Generation with Docker commit"},{"location":"Docker%20%26%20Kubernetes/03_building_custom_images_with_docker_server/008-manual-image-generation-with-docker-commit/#manual-image-generation-with-docker-commit","text":"When looking at the build process, we can see that we can create a container, execute commands in it and generate an image from that container. We can start up an empty alpine image: / # apk add --update redis fetch http://dl-cdn.alpinelinux.org/alpine/v3.9/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.9/community/x86_64/APKINDEX.tar.gz (1/1) Installing redis (4.0.12-r0) Executing redis-4.0.12-r0.pre-install Executing redis-4.0.12-r0.post-install Executing busybox-1.29.3-r10.trigger OK: 7 MiB in 15 packages / # Now, while that is running, in a second terminal we can execute: davis@davis-arch \ue0b0 ~ \ue0b0 docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES cd1e9a4372d7 alpine \"sh\" 48 seconds ago Up 46 seconds keen_swirles davis@davis-arch \ue0b0 ~ \ue0b0 docker commit -c 'CMD [\"redis-server\"]' cd1e9a4372d7 sha256:e1380e3df76f340d8f848942547a0d7f9e11f8843bef80d8de9df84dd186084d Now we can run it: davis@davis-arch \ue0b0 ~ \ue0b0 docker run e1380e3df76f340d8f84894 1:C 01 Feb 09:54:36.233 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 1:C 01 Feb 09:54:36.233 # Redis version=4.0.12, bits=64, commit=1be97168, modified=0, pid=1, just started 1:C 01 Feb 09:54:36.233 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf 1:M 01 Feb 09:54:36.234 * Running mode=standalone, port=6379. 1:M 01 Feb 09:54:36.234 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. 1:M 01 Feb 09:54:36.234 # Server initialized 1:M 01 Feb 09:54:36.234 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect. 1:M 01 Feb 09:54:36.235 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled. 1:M 01 Feb 09:54:36.235 * Ready to accept connections","title":"Manual Image Generation with Docker commit"},{"location":"Docker%20%26%20Kubernetes/04_making_real_projects_with_docker/001_project_outline/","text":"Project outline \u00b6 The goal of the project is to make a tiny node.js web application that can be accessed via browser from a local machine. The project will be divided into several steps: 1. Create Node JS web app 2. Create a Dockerfile 3. Build image from dockerfile 4. Run image as container 5. Connect to web app from browser In the process of making the project, there will be a few mistakes made so we can learn on how to fix them.","title":"Project outline"},{"location":"Docker%20%26%20Kubernetes/04_making_real_projects_with_docker/001_project_outline/#project-outline","text":"The goal of the project is to make a tiny node.js web application that can be accessed via browser from a local machine. The project will be divided into several steps: 1. Create Node JS web app 2. Create a Dockerfile 3. Build image from dockerfile 4. Run image as container 5. Connect to web app from browser In the process of making the project, there will be a few mistakes made so we can learn on how to fix them.","title":"Project outline"},{"location":"Docker%20%26%20Kubernetes/04_making_real_projects_with_docker/002-node-server-setup/","text":"Node server setup \u00b6 In this section we set up the node.js server: davis@davis-arch \ue0b0 ~/projects/docker \ue0b0 \ue0a0 master \ue0b0 mkdir 02_nodejs_web_server davis@davis-arch \ue0b0 ~/projects/docker \ue0b0 \ue0a0 master \ue0b0 cd 02_nodejs_web_server Make a package.json davis@davis-arch \ue0b0 ~/projects/docker/02_nodejs_web_server \ue0b0 \ue0a0 master \ue0b0 touch package.json With following content: { \"dependencies\": { \"express\": \"~4.16.4\" }, \"scripts\": { \"start\": \"node index.js\" } } Make index.js: davis@davis-arch \ue0b0 ~/projects/docker/02_nodejs_web_server \ue0b0 \ue0a0 master \ue0b0 touch index.js With content: const express = require('express'); const app = express(); app.get('/', (req, res) => { res.send('Hi there!'); }); app.listen(8080, () => { console.log('Listening on port 8080'); } );","title":"Node server setup"},{"location":"Docker%20%26%20Kubernetes/04_making_real_projects_with_docker/002-node-server-setup/#node-server-setup","text":"In this section we set up the node.js server: davis@davis-arch \ue0b0 ~/projects/docker \ue0b0 \ue0a0 master \ue0b0 mkdir 02_nodejs_web_server davis@davis-arch \ue0b0 ~/projects/docker \ue0b0 \ue0a0 master \ue0b0 cd 02_nodejs_web_server Make a package.json davis@davis-arch \ue0b0 ~/projects/docker/02_nodejs_web_server \ue0b0 \ue0a0 master \ue0b0 touch package.json With following content: { \"dependencies\": { \"express\": \"~4.16.4\" }, \"scripts\": { \"start\": \"node index.js\" } } Make index.js: davis@davis-arch \ue0b0 ~/projects/docker/02_nodejs_web_server \ue0b0 \ue0a0 master \ue0b0 touch index.js With content: const express = require('express'); const app = express(); app.get('/', (req, res) => { res.send('Hi there!'); }); app.listen(8080, () => { console.log('Listening on port 8080'); } );","title":"Node server setup"},{"location":"Docker%20%26%20Kubernetes/04_making_real_projects_with_docker/003-creating-dockerfile/","text":"Creating a dockerfile & few planned errors \u00b6 When using the node server, we need to run two commands npm install to install the dependencies and npm start to start the application. So, when creating a dockerfile, we'll need to: 1. Specify a base image 2. Run some commands to install dependencies 3. Specify a command to run on starup davis@davis-arch \ue0b0 ~/projects/docker/02_nodejs_web_server \ue0b0 \ue0a0 master \ue0b0 touch Dockerfile # Specify a base image FROM alpine # Install dependencies RUN npm install # Default command CMD [\"npm\", \"start\"] When trying to build the image: davis@davis-arch \ue0b0 ~/projects/docker/02_nodejs_web_server \ue0b0 \ue0a0 master \ue0b0 docker build . Sending build context to Docker daemon 4.096kB Step 1/3 : FROM alpine ---> caf27325b298 Step 2/3 : RUN npm install ---> Running in 262e1fdf7e56 /bin/sh: npm: not found The command '/bin/sh -c npm install' returned a non-zero code: 127 This throws an error that we do not have the npm program. We can fix this in 2 ways - find a different base image that has npm installed or write some extra steps to install it. We'll use a different image, we can find those at https://hub.docker.com . # Specify a base image FROM node:alpine # Install dependencies RUN npm install # Default command CMD [\"npm\", \"start\"] Now we can build it: \u2718 davis@davis-arch \ue0b0 ~/projects/docker/02_nodejs_web_server \ue0b0 \ue0a0 master \ue0b0 docker build . Sending build context to Docker daemon 4.096kB Step 1/3 : FROM node:alpine alpine: Pulling from library/node 169185f82c45: Pull complete 80e1b3e484f0: Pull complete 93e33f1be740: Pull complete Digest: sha256:2558617051e2c402a1bc6df5a73838e3e2832efff356ff38b9ffaa1ce562f9cb Status: Downloaded newer image for node:alpine ---> ebbf98230a82 Step 2/3 : RUN npm install ---> Running in 95f311d239ab npm WARN saveError ENOENT: no such file or directory, open '/package.json' npm notice created a lockfile as package-lock.json. You should commit this file. npm WARN enoent ENOENT: no such file or directory, open '/package.json' npm WARN !invalid#2 No description npm WARN !invalid#2 No repository field. npm WARN !invalid#2 No README data npm WARN !invalid#2 No license field. up to date in 0.614s found 0 vulnerabilities Removing intermediate container 95f311d239ab ---> 0ade00c69ba0 Step 3/3 : CMD [\"npm\", \"start\"] ---> Running in 09e408a91110 Removing intermediate container 09e408a91110 ---> 7beb9507123a Successfully built 7beb9507123a Now we can see that we are missing the package.json file, this can be fixed by using the COPY <local path> <remote path> instruction: Note that the local path is relative to local build context. # Specify a base image FROM node:alpine # Install dependencies COPY ./ ./ RUN npm install # Default command CMD [\"npm\", \"start\"] Now we can successfully build it: davis@davis-arch \ue0b0 ~/projects/docker/02_nodejs_web_server \ue0b0 \ue0a0 master \ue0b0 docker build . Sending build context to Docker daemon 4.096kB Step 1/4 : FROM node:alpine ---> ebbf98230a82 Step 2/4 : COPY ./ ./ ---> 0babb3e5ff58 Step 3/4 : RUN npm install ---> Running in 6969bdd45b86 npm notice created a lockfile as package-lock.json. You should commit this file. npm WARN !invalid#2 No description npm WARN !invalid#2 No repository field. npm WARN !invalid#2 No license field. added 48 packages from 36 contributors and audited 121 packages in 5.019s found 0 vulnerabilities Removing intermediate container 6969bdd45b86 ---> 0421b41270aa Step 4/4 : CMD [\"npm\", \"start\"] ---> Running in dec101331c5c Removing intermediate container dec101331c5c ---> 7c639816f391 Successfully built 7c639816f391 Tag it: davis@davis-arch \ue0b0 ~/projects/docker/02_nodejs_web_server \ue0b0 \ue0a0 master \u25cf \ue0b0 docker build -t daviskregers/simpleweb . Sending build context to Docker daemon 4.096kB Step 1/4 : FROM node:alpine ---> ebbf98230a82 Step 2/4 : COPY ./ ./ ---> Using cache ---> 0babb3e5ff58 Step 3/4 : RUN npm install ---> Using cache ---> 0421b41270aa Step 4/4 : CMD [\"npm\", \"start\"] ---> Using cache ---> 7c639816f391 Successfully built 7c639816f391 Successfully tagged daviskregers/simpleweb:latest And run it: davis@davis-arch \ue0b0 ~/projects/docker/02_nodejs_web_server \ue0b0 \ue0a0 master \u25cf \ue0b0 docker run daviskregers/simpleweb > @ start / > node index.js Listening on port 8080 Now the app is running in the container, but we havent specified that we allow network access to it, so we cannot open the port 8080 and view it in our browser. We can fix it specifying port mapping. docker run -p <outer port>:<port inside container> <image> \u2718 davis@davis-arch \ue0b0 ~/projects/docker/02_nodejs_web_server \ue0b0 \ue0a0 master \ue0b0 docker run -p 8080:8080 daviskregers/simpleweb > @ start / > node index.js Listening on port 8080 Now we can access it:","title":"Creating a dockerfile & few planned errors"},{"location":"Docker%20%26%20Kubernetes/04_making_real_projects_with_docker/003-creating-dockerfile/#creating-a-dockerfile-few-planned-errors","text":"When using the node server, we need to run two commands npm install to install the dependencies and npm start to start the application. So, when creating a dockerfile, we'll need to: 1. Specify a base image 2. Run some commands to install dependencies 3. Specify a command to run on starup davis@davis-arch \ue0b0 ~/projects/docker/02_nodejs_web_server \ue0b0 \ue0a0 master \ue0b0 touch Dockerfile # Specify a base image FROM alpine # Install dependencies RUN npm install # Default command CMD [\"npm\", \"start\"] When trying to build the image: davis@davis-arch \ue0b0 ~/projects/docker/02_nodejs_web_server \ue0b0 \ue0a0 master \ue0b0 docker build . Sending build context to Docker daemon 4.096kB Step 1/3 : FROM alpine ---> caf27325b298 Step 2/3 : RUN npm install ---> Running in 262e1fdf7e56 /bin/sh: npm: not found The command '/bin/sh -c npm install' returned a non-zero code: 127 This throws an error that we do not have the npm program. We can fix this in 2 ways - find a different base image that has npm installed or write some extra steps to install it. We'll use a different image, we can find those at https://hub.docker.com . # Specify a base image FROM node:alpine # Install dependencies RUN npm install # Default command CMD [\"npm\", \"start\"] Now we can build it: \u2718 davis@davis-arch \ue0b0 ~/projects/docker/02_nodejs_web_server \ue0b0 \ue0a0 master \ue0b0 docker build . Sending build context to Docker daemon 4.096kB Step 1/3 : FROM node:alpine alpine: Pulling from library/node 169185f82c45: Pull complete 80e1b3e484f0: Pull complete 93e33f1be740: Pull complete Digest: sha256:2558617051e2c402a1bc6df5a73838e3e2832efff356ff38b9ffaa1ce562f9cb Status: Downloaded newer image for node:alpine ---> ebbf98230a82 Step 2/3 : RUN npm install ---> Running in 95f311d239ab npm WARN saveError ENOENT: no such file or directory, open '/package.json' npm notice created a lockfile as package-lock.json. You should commit this file. npm WARN enoent ENOENT: no such file or directory, open '/package.json' npm WARN !invalid#2 No description npm WARN !invalid#2 No repository field. npm WARN !invalid#2 No README data npm WARN !invalid#2 No license field. up to date in 0.614s found 0 vulnerabilities Removing intermediate container 95f311d239ab ---> 0ade00c69ba0 Step 3/3 : CMD [\"npm\", \"start\"] ---> Running in 09e408a91110 Removing intermediate container 09e408a91110 ---> 7beb9507123a Successfully built 7beb9507123a Now we can see that we are missing the package.json file, this can be fixed by using the COPY <local path> <remote path> instruction: Note that the local path is relative to local build context. # Specify a base image FROM node:alpine # Install dependencies COPY ./ ./ RUN npm install # Default command CMD [\"npm\", \"start\"] Now we can successfully build it: davis@davis-arch \ue0b0 ~/projects/docker/02_nodejs_web_server \ue0b0 \ue0a0 master \ue0b0 docker build . Sending build context to Docker daemon 4.096kB Step 1/4 : FROM node:alpine ---> ebbf98230a82 Step 2/4 : COPY ./ ./ ---> 0babb3e5ff58 Step 3/4 : RUN npm install ---> Running in 6969bdd45b86 npm notice created a lockfile as package-lock.json. You should commit this file. npm WARN !invalid#2 No description npm WARN !invalid#2 No repository field. npm WARN !invalid#2 No license field. added 48 packages from 36 contributors and audited 121 packages in 5.019s found 0 vulnerabilities Removing intermediate container 6969bdd45b86 ---> 0421b41270aa Step 4/4 : CMD [\"npm\", \"start\"] ---> Running in dec101331c5c Removing intermediate container dec101331c5c ---> 7c639816f391 Successfully built 7c639816f391 Tag it: davis@davis-arch \ue0b0 ~/projects/docker/02_nodejs_web_server \ue0b0 \ue0a0 master \u25cf \ue0b0 docker build -t daviskregers/simpleweb . Sending build context to Docker daemon 4.096kB Step 1/4 : FROM node:alpine ---> ebbf98230a82 Step 2/4 : COPY ./ ./ ---> Using cache ---> 0babb3e5ff58 Step 3/4 : RUN npm install ---> Using cache ---> 0421b41270aa Step 4/4 : CMD [\"npm\", \"start\"] ---> Using cache ---> 7c639816f391 Successfully built 7c639816f391 Successfully tagged daviskregers/simpleweb:latest And run it: davis@davis-arch \ue0b0 ~/projects/docker/02_nodejs_web_server \ue0b0 \ue0a0 master \u25cf \ue0b0 docker run daviskregers/simpleweb > @ start / > node index.js Listening on port 8080 Now the app is running in the container, but we havent specified that we allow network access to it, so we cannot open the port 8080 and view it in our browser. We can fix it specifying port mapping. docker run -p <outer port>:<port inside container> <image> \u2718 davis@davis-arch \ue0b0 ~/projects/docker/02_nodejs_web_server \ue0b0 \ue0a0 master \ue0b0 docker run -p 8080:8080 daviskregers/simpleweb > @ start / > node index.js Listening on port 8080 Now we can access it:","title":"Creating a dockerfile &amp; few planned errors"},{"location":"Docker%20%26%20Kubernetes/04_making_real_projects_with_docker/004-specifying-working-directory/","text":"Specifying working directory \u00b6 When running: davis@davis-arch \ue0b0 ~/projects/docker/02_nodejs_web_server \ue0b0 \ue0a0 master \ue0b0 docker run daviskregers/simpleweb sh davis@davis-arch \ue0b0 ~/projects/docker/02_nodejs_web_server \ue0b0 \ue0a0 master \ue0b0 docker run -it daviskregers/simpleweb sh / # ls Dockerfile lib package.json sys bin media proc tmp dev mnt root usr etc node_modules run var home opt sbin index.js package-lock.json srv You can notice that we copied all the files to the root directory of the system, which is not a good practice. We can use working directory instruction to fix this: # Specify a base image FROM node:alpine WORKDIR /usr/app # Install dependencies COPY ./ ./ RUN npm install # Default command CMD [\"npm\", \"start\"] All the instructions after the WORKDIR instruction, will be executed relative to the location provided. davis@davis-arch \ue0b0 ~/projects/docker/02_nodejs_web_server \ue0b0 \ue0a0 master \u25cf \ue0b0 docker build -t daviskregers/simpleweb . Sending build context to Docker daemon 4.096kB Step 1/5 : FROM node:alpine ---> ebbf98230a82 Step 2/5 : WORKDIR /usr/app ---> Running in 78a45d7af5f5 Removing intermediate container 78a45d7af5f5 ---> 81d0968ef0fb Step 3/5 : COPY ./ ./ ---> 50acb7b5ba76 Step 4/5 : RUN npm install ---> Running in 07c874380805 npm notice created a lockfile as package-lock.json. You should commit this file. npm WARN app No description npm WARN app No repository field. npm WARN app No license field. added 48 packages from 36 contributors and audited 121 packages in 2.2s found 0 vulnerabilities Removing intermediate container 07c874380805 ---> 69eec757ea8c Step 5/5 : CMD [\"npm\", \"start\"] ---> Running in 882e18a3c487 Removing intermediate container 882e18a3c487 ---> 11a6a82ed208 Successfully built 11a6a82ed208 Successfully tagged daviskregers/simpleweb:latest davis@davis-arch \ue0b0 ~/projects/docker/02_nodejs_web_server \ue0b0 \ue0a0 master \u25cf \ue0b0 docker run -it daviskregers/simpleweb sh /usr/app # ls Dockerfile node_modules package.json index.js package-lock.json /usr/app # pwd /usr/app","title":"Specifying working directory"},{"location":"Docker%20%26%20Kubernetes/04_making_real_projects_with_docker/004-specifying-working-directory/#specifying-working-directory","text":"When running: davis@davis-arch \ue0b0 ~/projects/docker/02_nodejs_web_server \ue0b0 \ue0a0 master \ue0b0 docker run daviskregers/simpleweb sh davis@davis-arch \ue0b0 ~/projects/docker/02_nodejs_web_server \ue0b0 \ue0a0 master \ue0b0 docker run -it daviskregers/simpleweb sh / # ls Dockerfile lib package.json sys bin media proc tmp dev mnt root usr etc node_modules run var home opt sbin index.js package-lock.json srv You can notice that we copied all the files to the root directory of the system, which is not a good practice. We can use working directory instruction to fix this: # Specify a base image FROM node:alpine WORKDIR /usr/app # Install dependencies COPY ./ ./ RUN npm install # Default command CMD [\"npm\", \"start\"] All the instructions after the WORKDIR instruction, will be executed relative to the location provided. davis@davis-arch \ue0b0 ~/projects/docker/02_nodejs_web_server \ue0b0 \ue0a0 master \u25cf \ue0b0 docker build -t daviskregers/simpleweb . Sending build context to Docker daemon 4.096kB Step 1/5 : FROM node:alpine ---> ebbf98230a82 Step 2/5 : WORKDIR /usr/app ---> Running in 78a45d7af5f5 Removing intermediate container 78a45d7af5f5 ---> 81d0968ef0fb Step 3/5 : COPY ./ ./ ---> 50acb7b5ba76 Step 4/5 : RUN npm install ---> Running in 07c874380805 npm notice created a lockfile as package-lock.json. You should commit this file. npm WARN app No description npm WARN app No repository field. npm WARN app No license field. added 48 packages from 36 contributors and audited 121 packages in 2.2s found 0 vulnerabilities Removing intermediate container 07c874380805 ---> 69eec757ea8c Step 5/5 : CMD [\"npm\", \"start\"] ---> Running in 882e18a3c487 Removing intermediate container 882e18a3c487 ---> 11a6a82ed208 Successfully built 11a6a82ed208 Successfully tagged daviskregers/simpleweb:latest davis@davis-arch \ue0b0 ~/projects/docker/02_nodejs_web_server \ue0b0 \ue0a0 master \u25cf \ue0b0 docker run -it daviskregers/simpleweb sh /usr/app # ls Dockerfile node_modules package.json index.js package-lock.json /usr/app # pwd /usr/app","title":"Specifying working directory"},{"location":"Docker%20%26%20Kubernetes/04_making_real_projects_with_docker/005-unnecessary-rebuilds/","text":"Unnecessary rebuilds \u00b6 When starting the server: davis@davis-arch \ue0b0 ~/projects/docker/02_nodejs_web_server \ue0b0 \ue0a0 master \u25cf \ue0b0 docker run -p 8080:8080 daviskregers/simpleweb > @ start /usr/app > node index.js Listening on port 8080 And changing the index.js file: const express = require('express'); const app = express(); app.get('/', (req, res) => { res.send('Hello there!'); }); app.listen(8080, () => { console.log('Listening on port 8080'); } ); When we refresh the page, we will still see the old text: The change is not automatically reflected in the container, in order to see it, we will have to rebuild the image. Also, when rebuilding the application we can see: davis@davis-arch \ue0b0 ~/projects/docker/02_nodejs_web_server \ue0b0 \ue0a0 master \u25cf \ue0b0 docker build -t daviskregers/simpleweb . Sending build context to Docker daemon 4.096kB Step 1/5 : FROM node:alpine ---> ebbf98230a82 Step 2/5 : WORKDIR /usr/app ---> Using cache ---> 81d0968ef0fb Step 3/5 : COPY ./ ./ ---> 8ad383e24c1f Step 4/5 : RUN npm install ---> Running in 73dee9dfef3a npm notice created a lockfile as package-lock.json. You should commit this file. npm WARN app No description npm WARN app No repository field. npm WARN app No license field. added 48 packages from 36 contributors and audited 121 packages in 2.103s found 0 vulnerabilities Removing intermediate container 73dee9dfef3a ---> ecb7066fcd67 Step 5/5 : CMD [\"npm\", \"start\"] ---> Running in 7f798d274301 Removing intermediate container 7f798d274301 ---> 94ea4320239d Successfully built 94ea4320239d Successfully tagged daviskregers/simpleweb:latest Since one file changed in the copy instruction, all the cache after it was invalidated and we needed to install all the dependencies once again. We can fix this by splitting the copy instruction into two different steps: # Specify a base image FROM node:alpine WORKDIR /usr/app # Install dependencies COPY ./package.json ./ RUN npm install COPY ./ ./ # Default command CMD [\"npm\", \"start\"] Now, on the rebuild, the dependencies cache will not be invalidated unless the package.json has been changed. Now, when rebuilding and changing the index.js file once again: const express = require('express'); const app = express(); app.get('/', (req, res) => { res.send('Howdy!'); }); app.listen(8080, () => { console.log('Listening on port 8080'); } ); davis@davis-arch \ue0b0 ~/projects/docker/02_nodejs_web_server \ue0b0 \ue0a0 master \u25cf \ue0b0 docker build -t daviskregers/simpleweb . Sending build context to Docker daemon 4.096kB Step 1/6 : FROM node:alpine ---> ebbf98230a82 Step 2/6 : WORKDIR /usr/app ---> Using cache ---> 81d0968ef0fb Step 3/6 : COPY ./package.json ./ ---> Using cache ---> 5891da144997 Step 4/6 : RUN npm install ---> Using cache ---> 45270c2edb04 Step 5/6 : COPY ./ ./ ---> bebf5f56dc1c Step 6/6 : CMD [\"npm\", \"start\"] ---> Running in 372a0769702a Removing intermediate container 372a0769702a ---> ce5638e74c96 Successfully built ce5638e74c96 Successfully tagged daviskregers/simpleweb:latest We can see that the npm install used the cached version instead of installing everything all over again.","title":"Unnecessary rebuilds"},{"location":"Docker%20%26%20Kubernetes/04_making_real_projects_with_docker/005-unnecessary-rebuilds/#unnecessary-rebuilds","text":"When starting the server: davis@davis-arch \ue0b0 ~/projects/docker/02_nodejs_web_server \ue0b0 \ue0a0 master \u25cf \ue0b0 docker run -p 8080:8080 daviskregers/simpleweb > @ start /usr/app > node index.js Listening on port 8080 And changing the index.js file: const express = require('express'); const app = express(); app.get('/', (req, res) => { res.send('Hello there!'); }); app.listen(8080, () => { console.log('Listening on port 8080'); } ); When we refresh the page, we will still see the old text: The change is not automatically reflected in the container, in order to see it, we will have to rebuild the image. Also, when rebuilding the application we can see: davis@davis-arch \ue0b0 ~/projects/docker/02_nodejs_web_server \ue0b0 \ue0a0 master \u25cf \ue0b0 docker build -t daviskregers/simpleweb . Sending build context to Docker daemon 4.096kB Step 1/5 : FROM node:alpine ---> ebbf98230a82 Step 2/5 : WORKDIR /usr/app ---> Using cache ---> 81d0968ef0fb Step 3/5 : COPY ./ ./ ---> 8ad383e24c1f Step 4/5 : RUN npm install ---> Running in 73dee9dfef3a npm notice created a lockfile as package-lock.json. You should commit this file. npm WARN app No description npm WARN app No repository field. npm WARN app No license field. added 48 packages from 36 contributors and audited 121 packages in 2.103s found 0 vulnerabilities Removing intermediate container 73dee9dfef3a ---> ecb7066fcd67 Step 5/5 : CMD [\"npm\", \"start\"] ---> Running in 7f798d274301 Removing intermediate container 7f798d274301 ---> 94ea4320239d Successfully built 94ea4320239d Successfully tagged daviskregers/simpleweb:latest Since one file changed in the copy instruction, all the cache after it was invalidated and we needed to install all the dependencies once again. We can fix this by splitting the copy instruction into two different steps: # Specify a base image FROM node:alpine WORKDIR /usr/app # Install dependencies COPY ./package.json ./ RUN npm install COPY ./ ./ # Default command CMD [\"npm\", \"start\"] Now, on the rebuild, the dependencies cache will not be invalidated unless the package.json has been changed. Now, when rebuilding and changing the index.js file once again: const express = require('express'); const app = express(); app.get('/', (req, res) => { res.send('Howdy!'); }); app.listen(8080, () => { console.log('Listening on port 8080'); } ); davis@davis-arch \ue0b0 ~/projects/docker/02_nodejs_web_server \ue0b0 \ue0a0 master \u25cf \ue0b0 docker build -t daviskregers/simpleweb . Sending build context to Docker daemon 4.096kB Step 1/6 : FROM node:alpine ---> ebbf98230a82 Step 2/6 : WORKDIR /usr/app ---> Using cache ---> 81d0968ef0fb Step 3/6 : COPY ./package.json ./ ---> Using cache ---> 5891da144997 Step 4/6 : RUN npm install ---> Using cache ---> 45270c2edb04 Step 5/6 : COPY ./ ./ ---> bebf5f56dc1c Step 6/6 : CMD [\"npm\", \"start\"] ---> Running in 372a0769702a Removing intermediate container 372a0769702a ---> ce5638e74c96 Successfully built ce5638e74c96 Successfully tagged daviskregers/simpleweb:latest We can see that the npm install used the cached version instead of installing everything all over again.","title":"Unnecessary rebuilds"},{"location":"Docker%20%26%20Kubernetes/05_docker_compose_with_multiple_local_containers/01_overview/","text":"Overview \u00b6 In this section we'll build a docker image, that serves an application for counting number of times a webpage has been visited. The application will consist of 2 servers - node.js application and redis. We will have separate node app servers that connects to the redis server so later on it could be scaled up properly.","title":"Overview"},{"location":"Docker%20%26%20Kubernetes/05_docker_compose_with_multiple_local_containers/01_overview/#overview","text":"In this section we'll build a docker image, that serves an application for counting number of times a webpage has been visited. The application will consist of 2 servers - node.js application and redis. We will have separate node app servers that connects to the redis server so later on it could be scaled up properly.","title":"Overview"},{"location":"Docker%20%26%20Kubernetes/05_docker_compose_with_multiple_local_containers/02_setting_up_node/","text":"Setting up Node app \u00b6 davis@davis-arch \ue0b0 ~/projects/docker \ue0b0 \ue0a0 master \ue0b0 mkdir 03_visits davis@davis-arch \ue0b0 ~/projects/docker \ue0b0 \ue0a0 master \ue0b0 cd 03_visits davis@davis-arch \ue0b0 ~/projects/docker/03_visits \ue0b0 \ue0a0 master \ue0b0 touch package.json davis@davis-arch \ue0b0 ~/projects/docker/03_visits \ue0b0 \ue0a0 master \ue0b0 touch index.js package.json: { \"dependencies\":{ \"express\": \"~4.16.4\", \"redis\": \"~2.8.0\" }, \"scripts\": { \"start\": \"node index.js\" } } index.js: const express = require('express'); const redis = require('redis'); const app = express(); const client = redis.createClient(); client.set('visits', 0); app.get('/', (req, res) => { client.get('visits', (err, visits) => { res.send('Visits: ' + visits); client.set('visits', parseInt(visits) + 1); }); }); app.listen(8080, () => { console.log('Listening on port 8080'); });","title":"Setting up Node app"},{"location":"Docker%20%26%20Kubernetes/05_docker_compose_with_multiple_local_containers/02_setting_up_node/#setting-up-node-app","text":"davis@davis-arch \ue0b0 ~/projects/docker \ue0b0 \ue0a0 master \ue0b0 mkdir 03_visits davis@davis-arch \ue0b0 ~/projects/docker \ue0b0 \ue0a0 master \ue0b0 cd 03_visits davis@davis-arch \ue0b0 ~/projects/docker/03_visits \ue0b0 \ue0a0 master \ue0b0 touch package.json davis@davis-arch \ue0b0 ~/projects/docker/03_visits \ue0b0 \ue0a0 master \ue0b0 touch index.js package.json: { \"dependencies\":{ \"express\": \"~4.16.4\", \"redis\": \"~2.8.0\" }, \"scripts\": { \"start\": \"node index.js\" } } index.js: const express = require('express'); const redis = require('redis'); const app = express(); const client = redis.createClient(); client.set('visits', 0); app.get('/', (req, res) => { client.get('visits', (err, visits) => { res.send('Visits: ' + visits); client.set('visits', parseInt(visits) + 1); }); }); app.listen(8080, () => { console.log('Listening on port 8080'); });","title":"Setting up Node app"},{"location":"Docker%20%26%20Kubernetes/05_docker_compose_with_multiple_local_containers/03_setting_up_dockerfile/","text":"Setting up dockerfile \u00b6 davis@davis-arch \ue0b0 ~/projects/docker/03_visits \ue0b0 \ue0a0 master \ue0b0 touch Dockerfile FROM node:alpine WORKDIR /app COPY package.json . RUN npm install COPY . . CMD [\"npm\", \"start\"] davis@davis-arch \ue0b0 ~/projects/docker/03_visits \ue0b0 \ue0a0 master \ue0b0 docker build . Sending build context to Docker daemon 4.096kB Step 1/6 : FROM node:alpine ---> ebbf98230a82 Step 2/6 : WORKDIR /app ---> Running in d4519a0af637 Removing intermediate container d4519a0af637 ---> 93b2648262c0 Step 3/6 : COPY package.json . ---> 320ca3885ebb Step 4/6 : RUN npm install ---> Running in 891c0beb8c1c npm notice created a lockfile as package-lock.json. You should commit this file. npm WARN app No description npm WARN app No repository field. npm WARN app No license field. added 52 packages from 40 contributors and audited 125 packages in 2.091s found 0 vulnerabilities Removing intermediate container 891c0beb8c1c ---> 0841db5e3d2d Step 5/6 : COPY . . ---> 340dc4bf7dd5 Step 6/6 : CMD ['npm', 'start'] ---> Running in 3f37edffadb0 Removing intermediate container 3f37edffadb0 ---> 535452064704 Successfully built 535452064704 davis@davis-arch \ue0b0 ~/projects/docker/03_visits \ue0b0 \ue0a0 master \ue0b0 docker build -t daviskregers/visits:latest . Sending build context to Docker daemon 4.096kB Step 1/6 : FROM node:alpine ---> ebbf98230a82 Step 2/6 : WORKDIR /app ---> Using cache ---> 93b2648262c0 Step 3/6 : COPY package.json . ---> Using cache ---> 320ca3885ebb Step 4/6 : RUN npm install ---> Using cache ---> 0841db5e3d2d Step 5/6 : COPY . . ---> Using cache ---> 340dc4bf7dd5 Step 6/6 : CMD ['npm', 'start'] ---> Using cache ---> 535452064704 Successfully built 535452064704 Successfully tagged daviskregers/visits:latest","title":"Setting up dockerfile"},{"location":"Docker%20%26%20Kubernetes/05_docker_compose_with_multiple_local_containers/03_setting_up_dockerfile/#setting-up-dockerfile","text":"davis@davis-arch \ue0b0 ~/projects/docker/03_visits \ue0b0 \ue0a0 master \ue0b0 touch Dockerfile FROM node:alpine WORKDIR /app COPY package.json . RUN npm install COPY . . CMD [\"npm\", \"start\"] davis@davis-arch \ue0b0 ~/projects/docker/03_visits \ue0b0 \ue0a0 master \ue0b0 docker build . Sending build context to Docker daemon 4.096kB Step 1/6 : FROM node:alpine ---> ebbf98230a82 Step 2/6 : WORKDIR /app ---> Running in d4519a0af637 Removing intermediate container d4519a0af637 ---> 93b2648262c0 Step 3/6 : COPY package.json . ---> 320ca3885ebb Step 4/6 : RUN npm install ---> Running in 891c0beb8c1c npm notice created a lockfile as package-lock.json. You should commit this file. npm WARN app No description npm WARN app No repository field. npm WARN app No license field. added 52 packages from 40 contributors and audited 125 packages in 2.091s found 0 vulnerabilities Removing intermediate container 891c0beb8c1c ---> 0841db5e3d2d Step 5/6 : COPY . . ---> 340dc4bf7dd5 Step 6/6 : CMD ['npm', 'start'] ---> Running in 3f37edffadb0 Removing intermediate container 3f37edffadb0 ---> 535452064704 Successfully built 535452064704 davis@davis-arch \ue0b0 ~/projects/docker/03_visits \ue0b0 \ue0a0 master \ue0b0 docker build -t daviskregers/visits:latest . Sending build context to Docker daemon 4.096kB Step 1/6 : FROM node:alpine ---> ebbf98230a82 Step 2/6 : WORKDIR /app ---> Using cache ---> 93b2648262c0 Step 3/6 : COPY package.json . ---> Using cache ---> 320ca3885ebb Step 4/6 : RUN npm install ---> Using cache ---> 0841db5e3d2d Step 5/6 : COPY . . ---> Using cache ---> 340dc4bf7dd5 Step 6/6 : CMD ['npm', 'start'] ---> Using cache ---> 535452064704 Successfully built 535452064704 Successfully tagged daviskregers/visits:latest","title":"Setting up dockerfile"},{"location":"Docker%20%26%20Kubernetes/05_docker_compose_with_multiple_local_containers/04_docker_compose/","text":"Introducting docker compose \u00b6 We have built a node.js server that relies on redis in order to work. If we start it up now, it will fail because it cannot connect to redis. Even if we run a docker run redis , it will not work because they are separate services and cannot talk to each other. We need to setup networking between them. This can be achieved either by using docker CLI's networking features or using docker compose. Docker compose is a separate tool that: 1. Used to start up multiple docker containers at the same time 2. Automates some of the long-winded arguments we were passing to docker run .","title":"Introducting docker compose"},{"location":"Docker%20%26%20Kubernetes/05_docker_compose_with_multiple_local_containers/04_docker_compose/#introducting-docker-compose","text":"We have built a node.js server that relies on redis in order to work. If we start it up now, it will fail because it cannot connect to redis. Even if we run a docker run redis , it will not work because they are separate services and cannot talk to each other. We need to setup networking between them. This can be achieved either by using docker CLI's networking features or using docker compose. Docker compose is a separate tool that: 1. Used to start up multiple docker containers at the same time 2. Automates some of the long-winded arguments we were passing to docker run .","title":"Introducting docker compose"},{"location":"Docker%20%26%20Kubernetes/05_docker_compose_with_multiple_local_containers/05_docker_compose_files/","text":"Docker compose files \u00b6 We can create a docker-compose.yml file that contains all the options we'd normally pass to the docker-cli. With this project, we'll specify that we want docker to create 2 containers - redis and node server. The networking between them will be automatically set up. davis@davis-arch \ue0b0 ~/projects/docker/03_visits \ue0b0 \ue0a0 master \ue0b0 touch docker-compose.yml version: '3' services: redis-server: image: redis node-app: build: . ports: - 8080:8080 Change the index.js to connect to redis const express = require('express'); const redis = require('redis'); const app = express(); const client = redis.createClient({ host: 'redis-server', port: 6379 }); client.set('visits', 0); app.get('/', (req, res) => { client.get('visits', (err, visits) => { res.send('Visits: ' + visits); client.set('visits', parseInt(visits) + 1); }); }); app.listen(8080, () => { console.log('Listening on port 8080'); });","title":"Docker compose files"},{"location":"Docker%20%26%20Kubernetes/05_docker_compose_with_multiple_local_containers/05_docker_compose_files/#docker-compose-files","text":"We can create a docker-compose.yml file that contains all the options we'd normally pass to the docker-cli. With this project, we'll specify that we want docker to create 2 containers - redis and node server. The networking between them will be automatically set up. davis@davis-arch \ue0b0 ~/projects/docker/03_visits \ue0b0 \ue0a0 master \ue0b0 touch docker-compose.yml version: '3' services: redis-server: image: redis node-app: build: . ports: - 8080:8080 Change the index.js to connect to redis const express = require('express'); const redis = require('redis'); const app = express(); const client = redis.createClient({ host: 'redis-server', port: 6379 }); client.set('visits', 0); app.get('/', (req, res) => { client.get('visits', (err, visits) => { res.send('Visits: ' + visits); client.set('visits', parseInt(visits) + 1); }); }); app.listen(8080, () => { console.log('Listening on port 8080'); });","title":"Docker compose files"},{"location":"Docker%20%26%20Kubernetes/05_docker_compose_with_multiple_local_containers/06_docker_compose_commands/","text":"Docker compose commands \u00b6 We can use docker-compose up to run something like docker run <image> for each service in the docker-compose.yml . If we want to build or rebuild images, we can use docker-compose up --build . You can use -d to start them in background / detached mode. davis@davis-arch \ue0b0 ~/projects/docker/03_visits \ue0b0 \ue0a0 master \u25cf \ue0b0 docker-compose up --build Building node-app Step 1/6 : FROM node:alpine ---> ebbf98230a82 Step 2/6 : WORKDIR /app ---> Using cache ---> 93b2648262c0 Step 3/6 : COPY package.json . ---> Using cache ---> 475937e9ad91 Step 4/6 : RUN npm install ---> Using cache ---> 7bec86db7832 Step 5/6 : COPY . . ---> b05a3e5a00ac Step 6/6 : CMD [\"npm\", \"start\"] ---> Running in 4bd60eba1461 Removing intermediate container 4bd60eba1461 ---> 1d9a755bfa0f Successfully built 1d9a755bfa0f Successfully tagged 03_visits_node-app:latest Starting 03_visits_redis-server_1 ... done Recreating 0dce2607593a_03_visits_node-app_1 ... done Attaching to 03_visits_redis-server_1, 03_visits_node-app_1 redis-server_1 | 1:C 02 Feb 2019 09:44:39.477 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo redis-server_1 | 1:C 02 Feb 2019 09:44:39.477 # Redis version=5.0.3, bits=64, commit=00000000, modified=0, pid=1, just started redis-server_1 | 1:C 02 Feb 2019 09:44:39.477 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf redis-server_1 | 1:M 02 Feb 2019 09:44:39.479 * Running mode=standalone, port=6379. redis-server_1 | 1:M 02 Feb 2019 09:44:39.479 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. redis-server_1 | 1:M 02 Feb 2019 09:44:39.479 # Server initialized redis-server_1 | 1:M 02 Feb 2019 09:44:39.479 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect. redis-server_1 | 1:M 02 Feb 2019 09:44:39.479 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled. redis-server_1 | 1:M 02 Feb 2019 09:44:39.479 * DB loaded from disk: 0.000 seconds redis-server_1 | 1:M 02 Feb 2019 09:44:39.479 * Ready to accept connections node-app_1 | node-app_1 | > @ start /app node-app_1 | > node index.js node-app_1 | node-app_1 | Listening on port 8080 If we visit the https://localhost:8080 now: Stopping docker-compose containers \u00b6 To stop the docker-compose containers: docker-compose down davis@davis-arch \ue0b0 ~/projects/docker/03_visits \ue0b0 \ue0a0 master \ue0b0 docker-compose up -d Starting 03_visits_redis-server_1 ... done Starting 03_visits_node-app_1 ... done davis@davis-arch \ue0b0 ~/projects/docker/03_visits \ue0b0 \ue0a0 master \ue0b0 docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES e8438de60eef 03_visits_node-app \"npm start\" 5 minutes ago Up 3 seconds 0.0.0.0:8080->8080/tcp 03_visits_node-app_1 429d035c26f2 redis \"docker-entrypoint.s\u2026\" 7 minutes ago Up 3 seconds 6379/tcp 03_visits_redis-server_1 \u2718 davis@davis-arch \ue0b0 ~/projects/docker/03_visits \ue0b0 \ue0a0 master \ue0b0 docker-compose down Stopping 03_visits_node-app_1 ... done Stopping 03_visits_redis-server_1 ... done Removing 03_visits_node-app_1 ... done Removing 03_visits_redis-server_1 ... done Removing network 03_visits_default Container maintenance \u00b6 We can do extra logic on what to do if our servers crash using container maintenance with docker-compose. To better illustrate it, we'll break the code in index.js : const express = require('express'); const redis = require('redis'); const process = require('process'); const app = express(); const client = redis.createClient({ host: 'redis-server', port: 6379 }); client.set('visits', 0); app.get('/', (req, res) => { process.exit(0); client.get('visits', (err, visits) => { res.send('Visits: ' + visits); client.set('visits', parseInt(visits) + 1); }); }); app.listen(8080, () => { console.log('Listening on port 8080'); }); When we start up the containers and visit the app via the browsers, it exits with code 0: \u2718 davis@davis-arch \ue0b0 ~/projects/docker/03_visits \ue0b0 \ue0a0 master \ue0b0 docker-compose down Stopping 03_visits_node-app_1 ... done Stopping 03_visits_redis-server_1 ... done Removing 03_visits_node-app_1 ... done Removing 03_visits_redis-server_1 ... done Removing network 03_visits_default davis@davis-arch \ue0b0 ~/projects/docker/03_visits \ue0b0 \ue0a0 master \ue0b0 clear davis@davis-arch \ue0b0 ~/projects/docker/03_visits \ue0b0 \ue0a0 master \u25cf \ue0b0 docker-compose up --build Creating network \"03_visits_default\" with the default driver Building node-app Step 1/6 : FROM node:alpine ---> ebbf98230a82 Step 2/6 : WORKDIR /app ---> Using cache ---> 93b2648262c0 Step 3/6 : COPY package.json . ---> Using cache ---> 475937e9ad91 Step 4/6 : RUN npm install ---> Using cache ---> 7bec86db7832 Step 5/6 : COPY . . ---> 36613a4e6acb Step 6/6 : CMD [\"npm\", \"start\"] ---> Running in 519ebb227041 Removing intermediate container 519ebb227041 ---> 49f18048dbfd Successfully built 49f18048dbfd Successfully tagged 03_visits_node-app:latest Creating 03_visits_node-app_1 ... done Creating 03_visits_redis-server_1 ... done Attaching to 03_visits_node-app_1, 03_visits_redis-server_1 redis-server_1 | 1:C 02 Feb 2019 09:53:23.669 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo redis-server_1 | 1:C 02 Feb 2019 09:53:23.669 # Redis version=5.0.3, bits=64, commit=00000000, modified=0, pid=1, just started redis-server_1 | 1:C 02 Feb 2019 09:53:23.669 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf redis-server_1 | 1:M 02 Feb 2019 09:53:23.670 * Running mode=standalone, port=6379. redis-server_1 | 1:M 02 Feb 2019 09:53:23.670 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. redis-server_1 | 1:M 02 Feb 2019 09:53:23.670 # Server initialized redis-server_1 | 1:M 02 Feb 2019 09:53:23.670 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect. redis-server_1 | 1:M 02 Feb 2019 09:53:23.670 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled. redis-server_1 | 1:M 02 Feb 2019 09:53:23.670 * Ready to accept connections node-app_1 | node-app_1 | > @ start /app node-app_1 | > node index.js node-app_1 | node-app_1 | Listening on port 8080 03_visits_node-app_1 exited with code 0 Note that code 0 stands for status when program exited and everything is OK. If the status code is anything other than 0, it is interpreted as an error code. We can use something called Restart policies to combat this, we can specify no , always , on-failure , unless-stopped policies. By default is uses the no restart policy. version: '3' services: redis-server: image: redis node-app: restart: always build: . ports: - 8080:8080 If we start the containers and visit them now: davis@davis-arch \ue0b0 ~/projects/docker/03_visits \ue0b0 \ue0a0 master \u25cf \ue0b0 docker-compose up --build Building node-app Step 1/6 : FROM node:alpine ---> ebbf98230a82 Step 2/6 : WORKDIR /app ---> Using cache ---> 93b2648262c0 Step 3/6 : COPY package.json . ---> Using cache ---> 475937e9ad91 Step 4/6 : RUN npm install ---> Using cache ---> 7bec86db7832 Step 5/6 : COPY . . ---> e7125c7627d6 Step 6/6 : CMD [\"npm\", \"start\"] ---> Running in bfae00963192 Removing intermediate container bfae00963192 ---> a7513eef9e83 Successfully built a7513eef9e83 Successfully tagged 03_visits_node-app:latest Starting 03_visits_redis-server_1 ... done Recreating 03_visits_node-app_1 ... done Attaching to 03_visits_redis-server_1, 03_visits_node-app_1 redis-server_1 | 1:C 02 Feb 2019 10:00:08.535 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo redis-server_1 | 1:C 02 Feb 2019 10:00:08.535 # Redis version=5.0.3, bits=64, commit=00000000, modified=0, pid=1, just started redis-server_1 | 1:C 02 Feb 2019 10:00:08.535 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf redis-server_1 | 1:M 02 Feb 2019 10:00:08.536 * Running mode=standalone, port=6379. redis-server_1 | 1:M 02 Feb 2019 10:00:08.536 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. redis-server_1 | 1:M 02 Feb 2019 10:00:08.536 # Server initialized redis-server_1 | 1:M 02 Feb 2019 10:00:08.536 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect. redis-server_1 | 1:M 02 Feb 2019 10:00:08.536 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled. redis-server_1 | 1:M 02 Feb 2019 10:00:08.536 * DB loaded from disk: 0.000 seconds redis-server_1 | 1:M 02 Feb 2019 10:00:08.536 * Ready to accept connections node-app_1 | node-app_1 | > @ start /app node-app_1 | > node index.js node-app_1 | node-app_1 | Listening on port 8080 node-app_1 | node-app_1 | > @ start /app node-app_1 | > node index.js node-app_1 | node-app_1 | Listening on port 8080 03_visits_node-app_1 exited with code 0 node-app_1 | node-app_1 | > @ start /app node-app_1 | > node index.js node-app_1 | node-app_1 | Listening on port 8080 03_visits_node-app_1 exited with code 0 node-app_1 | node-app_1 | > @ start /app node-app_1 | > node index.js node-app_1 | node-app_1 | Listening on port 8080 Container status with docker compose \u00b6 We can get a similar status like docker ps with the docker-compose as well. davis@davis-arch \ue0b0 ~/projects/docker/03_visits \ue0b0 \ue0a0 master \ue0b0 docker-compose up -d Starting 03_visits_redis-server_1 ... done Starting 03_visits_node-app_1 ... done davis@davis-arch \ue0b0 ~/projects/docker/03_visits \ue0b0 \ue0a0 master \ue0b0 docker-compose ps Name Command State Ports ------------------------------------------------------------------------------------------ 03_visits_node-app_1 npm start Up 0.0.0.0:8080->8080/tcp 03_visits_redis-server_1 docker-entrypoint.sh redis ... Up 6379/tcp davis@davis-arch \ue0b0 ~/projects/docker/03_visits \ue0b0 \ue0a0 master \ue0b0 docker-compose down Stopping 03_visits_node-app_1 ... done Stopping 03_visits_redis-server_1 ... done Removing 03_visits_node-app_1 ... done Removing 03_visits_redis-server_1 ... done Removing network 03_visits_default davis@davis-arch \ue0b0 ~/projects/docker/03_visits \ue0b0 \ue0a0 master \ue0b0 docker-compose ps Name Command State Ports ------------------------------ Note that this command is to be run from the same directory as the docker-compose.yml file.","title":"Docker compose commands"},{"location":"Docker%20%26%20Kubernetes/05_docker_compose_with_multiple_local_containers/06_docker_compose_commands/#docker-compose-commands","text":"We can use docker-compose up to run something like docker run <image> for each service in the docker-compose.yml . If we want to build or rebuild images, we can use docker-compose up --build . You can use -d to start them in background / detached mode. davis@davis-arch \ue0b0 ~/projects/docker/03_visits \ue0b0 \ue0a0 master \u25cf \ue0b0 docker-compose up --build Building node-app Step 1/6 : FROM node:alpine ---> ebbf98230a82 Step 2/6 : WORKDIR /app ---> Using cache ---> 93b2648262c0 Step 3/6 : COPY package.json . ---> Using cache ---> 475937e9ad91 Step 4/6 : RUN npm install ---> Using cache ---> 7bec86db7832 Step 5/6 : COPY . . ---> b05a3e5a00ac Step 6/6 : CMD [\"npm\", \"start\"] ---> Running in 4bd60eba1461 Removing intermediate container 4bd60eba1461 ---> 1d9a755bfa0f Successfully built 1d9a755bfa0f Successfully tagged 03_visits_node-app:latest Starting 03_visits_redis-server_1 ... done Recreating 0dce2607593a_03_visits_node-app_1 ... done Attaching to 03_visits_redis-server_1, 03_visits_node-app_1 redis-server_1 | 1:C 02 Feb 2019 09:44:39.477 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo redis-server_1 | 1:C 02 Feb 2019 09:44:39.477 # Redis version=5.0.3, bits=64, commit=00000000, modified=0, pid=1, just started redis-server_1 | 1:C 02 Feb 2019 09:44:39.477 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf redis-server_1 | 1:M 02 Feb 2019 09:44:39.479 * Running mode=standalone, port=6379. redis-server_1 | 1:M 02 Feb 2019 09:44:39.479 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. redis-server_1 | 1:M 02 Feb 2019 09:44:39.479 # Server initialized redis-server_1 | 1:M 02 Feb 2019 09:44:39.479 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect. redis-server_1 | 1:M 02 Feb 2019 09:44:39.479 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled. redis-server_1 | 1:M 02 Feb 2019 09:44:39.479 * DB loaded from disk: 0.000 seconds redis-server_1 | 1:M 02 Feb 2019 09:44:39.479 * Ready to accept connections node-app_1 | node-app_1 | > @ start /app node-app_1 | > node index.js node-app_1 | node-app_1 | Listening on port 8080 If we visit the https://localhost:8080 now:","title":"Docker compose commands"},{"location":"Docker%20%26%20Kubernetes/05_docker_compose_with_multiple_local_containers/06_docker_compose_commands/#stopping-docker-compose-containers","text":"To stop the docker-compose containers: docker-compose down davis@davis-arch \ue0b0 ~/projects/docker/03_visits \ue0b0 \ue0a0 master \ue0b0 docker-compose up -d Starting 03_visits_redis-server_1 ... done Starting 03_visits_node-app_1 ... done davis@davis-arch \ue0b0 ~/projects/docker/03_visits \ue0b0 \ue0a0 master \ue0b0 docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES e8438de60eef 03_visits_node-app \"npm start\" 5 minutes ago Up 3 seconds 0.0.0.0:8080->8080/tcp 03_visits_node-app_1 429d035c26f2 redis \"docker-entrypoint.s\u2026\" 7 minutes ago Up 3 seconds 6379/tcp 03_visits_redis-server_1 \u2718 davis@davis-arch \ue0b0 ~/projects/docker/03_visits \ue0b0 \ue0a0 master \ue0b0 docker-compose down Stopping 03_visits_node-app_1 ... done Stopping 03_visits_redis-server_1 ... done Removing 03_visits_node-app_1 ... done Removing 03_visits_redis-server_1 ... done Removing network 03_visits_default","title":"Stopping docker-compose containers"},{"location":"Docker%20%26%20Kubernetes/05_docker_compose_with_multiple_local_containers/06_docker_compose_commands/#container-maintenance","text":"We can do extra logic on what to do if our servers crash using container maintenance with docker-compose. To better illustrate it, we'll break the code in index.js : const express = require('express'); const redis = require('redis'); const process = require('process'); const app = express(); const client = redis.createClient({ host: 'redis-server', port: 6379 }); client.set('visits', 0); app.get('/', (req, res) => { process.exit(0); client.get('visits', (err, visits) => { res.send('Visits: ' + visits); client.set('visits', parseInt(visits) + 1); }); }); app.listen(8080, () => { console.log('Listening on port 8080'); }); When we start up the containers and visit the app via the browsers, it exits with code 0: \u2718 davis@davis-arch \ue0b0 ~/projects/docker/03_visits \ue0b0 \ue0a0 master \ue0b0 docker-compose down Stopping 03_visits_node-app_1 ... done Stopping 03_visits_redis-server_1 ... done Removing 03_visits_node-app_1 ... done Removing 03_visits_redis-server_1 ... done Removing network 03_visits_default davis@davis-arch \ue0b0 ~/projects/docker/03_visits \ue0b0 \ue0a0 master \ue0b0 clear davis@davis-arch \ue0b0 ~/projects/docker/03_visits \ue0b0 \ue0a0 master \u25cf \ue0b0 docker-compose up --build Creating network \"03_visits_default\" with the default driver Building node-app Step 1/6 : FROM node:alpine ---> ebbf98230a82 Step 2/6 : WORKDIR /app ---> Using cache ---> 93b2648262c0 Step 3/6 : COPY package.json . ---> Using cache ---> 475937e9ad91 Step 4/6 : RUN npm install ---> Using cache ---> 7bec86db7832 Step 5/6 : COPY . . ---> 36613a4e6acb Step 6/6 : CMD [\"npm\", \"start\"] ---> Running in 519ebb227041 Removing intermediate container 519ebb227041 ---> 49f18048dbfd Successfully built 49f18048dbfd Successfully tagged 03_visits_node-app:latest Creating 03_visits_node-app_1 ... done Creating 03_visits_redis-server_1 ... done Attaching to 03_visits_node-app_1, 03_visits_redis-server_1 redis-server_1 | 1:C 02 Feb 2019 09:53:23.669 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo redis-server_1 | 1:C 02 Feb 2019 09:53:23.669 # Redis version=5.0.3, bits=64, commit=00000000, modified=0, pid=1, just started redis-server_1 | 1:C 02 Feb 2019 09:53:23.669 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf redis-server_1 | 1:M 02 Feb 2019 09:53:23.670 * Running mode=standalone, port=6379. redis-server_1 | 1:M 02 Feb 2019 09:53:23.670 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. redis-server_1 | 1:M 02 Feb 2019 09:53:23.670 # Server initialized redis-server_1 | 1:M 02 Feb 2019 09:53:23.670 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect. redis-server_1 | 1:M 02 Feb 2019 09:53:23.670 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled. redis-server_1 | 1:M 02 Feb 2019 09:53:23.670 * Ready to accept connections node-app_1 | node-app_1 | > @ start /app node-app_1 | > node index.js node-app_1 | node-app_1 | Listening on port 8080 03_visits_node-app_1 exited with code 0 Note that code 0 stands for status when program exited and everything is OK. If the status code is anything other than 0, it is interpreted as an error code. We can use something called Restart policies to combat this, we can specify no , always , on-failure , unless-stopped policies. By default is uses the no restart policy. version: '3' services: redis-server: image: redis node-app: restart: always build: . ports: - 8080:8080 If we start the containers and visit them now: davis@davis-arch \ue0b0 ~/projects/docker/03_visits \ue0b0 \ue0a0 master \u25cf \ue0b0 docker-compose up --build Building node-app Step 1/6 : FROM node:alpine ---> ebbf98230a82 Step 2/6 : WORKDIR /app ---> Using cache ---> 93b2648262c0 Step 3/6 : COPY package.json . ---> Using cache ---> 475937e9ad91 Step 4/6 : RUN npm install ---> Using cache ---> 7bec86db7832 Step 5/6 : COPY . . ---> e7125c7627d6 Step 6/6 : CMD [\"npm\", \"start\"] ---> Running in bfae00963192 Removing intermediate container bfae00963192 ---> a7513eef9e83 Successfully built a7513eef9e83 Successfully tagged 03_visits_node-app:latest Starting 03_visits_redis-server_1 ... done Recreating 03_visits_node-app_1 ... done Attaching to 03_visits_redis-server_1, 03_visits_node-app_1 redis-server_1 | 1:C 02 Feb 2019 10:00:08.535 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo redis-server_1 | 1:C 02 Feb 2019 10:00:08.535 # Redis version=5.0.3, bits=64, commit=00000000, modified=0, pid=1, just started redis-server_1 | 1:C 02 Feb 2019 10:00:08.535 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf redis-server_1 | 1:M 02 Feb 2019 10:00:08.536 * Running mode=standalone, port=6379. redis-server_1 | 1:M 02 Feb 2019 10:00:08.536 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. redis-server_1 | 1:M 02 Feb 2019 10:00:08.536 # Server initialized redis-server_1 | 1:M 02 Feb 2019 10:00:08.536 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect. redis-server_1 | 1:M 02 Feb 2019 10:00:08.536 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled. redis-server_1 | 1:M 02 Feb 2019 10:00:08.536 * DB loaded from disk: 0.000 seconds redis-server_1 | 1:M 02 Feb 2019 10:00:08.536 * Ready to accept connections node-app_1 | node-app_1 | > @ start /app node-app_1 | > node index.js node-app_1 | node-app_1 | Listening on port 8080 node-app_1 | node-app_1 | > @ start /app node-app_1 | > node index.js node-app_1 | node-app_1 | Listening on port 8080 03_visits_node-app_1 exited with code 0 node-app_1 | node-app_1 | > @ start /app node-app_1 | > node index.js node-app_1 | node-app_1 | Listening on port 8080 03_visits_node-app_1 exited with code 0 node-app_1 | node-app_1 | > @ start /app node-app_1 | > node index.js node-app_1 | node-app_1 | Listening on port 8080","title":"Container maintenance"},{"location":"Docker%20%26%20Kubernetes/05_docker_compose_with_multiple_local_containers/06_docker_compose_commands/#container-status-with-docker-compose","text":"We can get a similar status like docker ps with the docker-compose as well. davis@davis-arch \ue0b0 ~/projects/docker/03_visits \ue0b0 \ue0a0 master \ue0b0 docker-compose up -d Starting 03_visits_redis-server_1 ... done Starting 03_visits_node-app_1 ... done davis@davis-arch \ue0b0 ~/projects/docker/03_visits \ue0b0 \ue0a0 master \ue0b0 docker-compose ps Name Command State Ports ------------------------------------------------------------------------------------------ 03_visits_node-app_1 npm start Up 0.0.0.0:8080->8080/tcp 03_visits_redis-server_1 docker-entrypoint.sh redis ... Up 6379/tcp davis@davis-arch \ue0b0 ~/projects/docker/03_visits \ue0b0 \ue0a0 master \ue0b0 docker-compose down Stopping 03_visits_node-app_1 ... done Stopping 03_visits_redis-server_1 ... done Removing 03_visits_node-app_1 ... done Removing 03_visits_redis-server_1 ... done Removing network 03_visits_default davis@davis-arch \ue0b0 ~/projects/docker/03_visits \ue0b0 \ue0a0 master \ue0b0 docker-compose ps Name Command State Ports ------------------------------ Note that this command is to be run from the same directory as the docker-compose.yml file.","title":"Container status with docker compose"},{"location":"Docker%20%26%20Kubernetes/06_creating_a_production_grade_workflow/01_development_workflow/","text":"Development workflow \u00b6 In this section we'll start looking at the production-grade workflow which consists of following steps: 1. Development 2. Testing 3. Deployment In order to setup this development workflow, will revolve around creating a github repository from where it will be deployed to a hosting service. Github workflow will use feature branch workflow. Changes that is to be merged in master will be issued by a pull request, which automatically will also invoke: 1. TravisCI tests After the merge we'll invoke: 1. TravisCI tests 2. Deploy to AWS Elastic Beanstalk","title":"Development workflow"},{"location":"Docker%20%26%20Kubernetes/06_creating_a_production_grade_workflow/01_development_workflow/#development-workflow","text":"In this section we'll start looking at the production-grade workflow which consists of following steps: 1. Development 2. Testing 3. Deployment In order to setup this development workflow, will revolve around creating a github repository from where it will be deployed to a hosting service. Github workflow will use feature branch workflow. Changes that is to be merged in master will be issued by a pull request, which automatically will also invoke: 1. TravisCI tests After the merge we'll invoke: 1. TravisCI tests 2. Deploy to AWS Elastic Beanstalk","title":"Development workflow"},{"location":"Docker%20%26%20Kubernetes/06_creating_a_production_grade_workflow/02_project_generation/","text":"We'll use React for this project \u00b6 davis@davis-arch \ue0b0 ~ \ue0b0 sudo pacman -S nodejs davis@davis-arch \ue0b0 ~ \ue0b0 sudo pacman -S npm davis@davis-arch \ue0b0 ~ \ue0b0 node -v v11.9.0 davis@davis-arch \ue0b0 ~ \ue0b0 npm -v 6.7.0 \u2718 davis@davis-arch \ue0b0 ~ \ue0b0 sudo npm i -g create-react-app /usr/bin/create-react-app -> /usr/lib/node_modules/create-react-app/index.js + create-react-app@2.1.3 added 63 packages from 20 contributors in 8.894s \u2718 davis@davis-arch \ue0b0 ~/projects/docker \ue0b0 \ue0a0 master \ue0b0 create-react-app 04_react_app Creating a new React app in /home/davis/projects/docker/04_react_app. Installing packages. This might take a couple of minutes. Installing react, react-dom, and react-scripts... + react-dom@16.7.0 + react@16.7.0 + react-scripts@2.1.3 added 1949 packages from 671 contributors and audited 35817 packages in 78.645s found 0 vulnerabilities Success! Created 04_react_app at /home/davis/projects/docker/04_react_app Inside that directory, you can run several commands: npm start Starts the development server. npm run build Bundles the app into static files for production. npm test Starts the test runner. npm run eject Removes this tool and copies build dependencies, configuration files and scripts into the app directory. If you do this, you can\u2019t go back! We suggest that you begin by typing: cd 04_react_app npm start Happy hacking!","title":"We'll use React for this project"},{"location":"Docker%20%26%20Kubernetes/06_creating_a_production_grade_workflow/02_project_generation/#well-use-react-for-this-project","text":"davis@davis-arch \ue0b0 ~ \ue0b0 sudo pacman -S nodejs davis@davis-arch \ue0b0 ~ \ue0b0 sudo pacman -S npm davis@davis-arch \ue0b0 ~ \ue0b0 node -v v11.9.0 davis@davis-arch \ue0b0 ~ \ue0b0 npm -v 6.7.0 \u2718 davis@davis-arch \ue0b0 ~ \ue0b0 sudo npm i -g create-react-app /usr/bin/create-react-app -> /usr/lib/node_modules/create-react-app/index.js + create-react-app@2.1.3 added 63 packages from 20 contributors in 8.894s \u2718 davis@davis-arch \ue0b0 ~/projects/docker \ue0b0 \ue0a0 master \ue0b0 create-react-app 04_react_app Creating a new React app in /home/davis/projects/docker/04_react_app. Installing packages. This might take a couple of minutes. Installing react, react-dom, and react-scripts... + react-dom@16.7.0 + react@16.7.0 + react-scripts@2.1.3 added 1949 packages from 671 contributors and audited 35817 packages in 78.645s found 0 vulnerabilities Success! Created 04_react_app at /home/davis/projects/docker/04_react_app Inside that directory, you can run several commands: npm start Starts the development server. npm run build Bundles the app into static files for production. npm test Starts the test runner. npm run eject Removes this tool and copies build dependencies, configuration files and scripts into the app directory. If you do this, you can\u2019t go back! We suggest that you begin by typing: cd 04_react_app npm start Happy hacking!","title":"We'll use React for this project"},{"location":"Docker%20%26%20Kubernetes/06_creating_a_production_grade_workflow/03_creating_dev_dockerfile/","text":"Creating Dev Dockerfile \u00b6 Most likely it will make more sense to create 2 separate dockerfiles - one for development and one for production. davis@davis-arch \ue0b0 ~/projects/docker \ue0b0 \ue0a0 master \ue0b0 touch Dockerfile.dev FROM node:alpine WORKDIR /app COPY package.json . RUN npm install COPY . . CMD [\"npm\", \"run\", \"start\"] Note, that since the filename is changed, we'll need to use -f flag to specify it: davis@davis-arch \ue0b0 ~/projects/docker/04_react_app \ue0b0 \ue0a0 master \ue0b0 docker build . unable to prepare context: unable to evaluate symlinks in Dockerfile path: lstat /home/davis/projects/docker/04_react_app/Dockerfile: no such file or directory \u2718 davis@davis-arch \ue0b0 ~/projects/docker/04_react_app \ue0b0 \ue0a0 master \ue0b0 docker build -f Dockerfile.dev . Sending build context to Docker daemon 244MB Step 1/6 : FROM node:alpine ---> ebbf98230a82 Step 2/6 : WORKDIR /app ---> Using cache ---> 93b2648262c0 Step 3/6 : COPY package.json . ---> a33dd4926364 Step 4/6 : RUN npm install ---> Running in 0f7cda2e8098 npm WARN deprecated circular-json@0.3.3: CircularJSON is in maintenance only, flatted is its successor. npm WARN deprecated kleur@2.0.2: Please upgrade to kleur@3 or migrate to 'ansi-colors' if you prefer the old syntax. Visit <https://github.com/lukeed/kleur/releases/tag/v3.0.0\\> for migration path(s). npm notice created a lockfile as package-lock.json. You should commit this file. npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules/fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"}) added 1949 packages from 671 contributors and audited 35817 packages in 43.316s found 0 vulnerabilities Removing intermediate container 0f7cda2e8098 ---> 2f4e0c88fbee Step 5/6 : COPY . . ---> 64ba876f1a5c Step 6/6 : CMD [\"npm\", \"run\", \"start\"] ---> Running in a50100fb5243 Removing intermediate container a50100fb5243 ---> 4a42cf02f626 Successfully built 4a42cf02f626","title":"Creating Dev Dockerfile"},{"location":"Docker%20%26%20Kubernetes/06_creating_a_production_grade_workflow/03_creating_dev_dockerfile/#creating-dev-dockerfile","text":"Most likely it will make more sense to create 2 separate dockerfiles - one for development and one for production. davis@davis-arch \ue0b0 ~/projects/docker \ue0b0 \ue0a0 master \ue0b0 touch Dockerfile.dev FROM node:alpine WORKDIR /app COPY package.json . RUN npm install COPY . . CMD [\"npm\", \"run\", \"start\"] Note, that since the filename is changed, we'll need to use -f flag to specify it: davis@davis-arch \ue0b0 ~/projects/docker/04_react_app \ue0b0 \ue0a0 master \ue0b0 docker build . unable to prepare context: unable to evaluate symlinks in Dockerfile path: lstat /home/davis/projects/docker/04_react_app/Dockerfile: no such file or directory \u2718 davis@davis-arch \ue0b0 ~/projects/docker/04_react_app \ue0b0 \ue0a0 master \ue0b0 docker build -f Dockerfile.dev . Sending build context to Docker daemon 244MB Step 1/6 : FROM node:alpine ---> ebbf98230a82 Step 2/6 : WORKDIR /app ---> Using cache ---> 93b2648262c0 Step 3/6 : COPY package.json . ---> a33dd4926364 Step 4/6 : RUN npm install ---> Running in 0f7cda2e8098 npm WARN deprecated circular-json@0.3.3: CircularJSON is in maintenance only, flatted is its successor. npm WARN deprecated kleur@2.0.2: Please upgrade to kleur@3 or migrate to 'ansi-colors' if you prefer the old syntax. Visit <https://github.com/lukeed/kleur/releases/tag/v3.0.0\\> for migration path(s). npm notice created a lockfile as package-lock.json. You should commit this file. npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules/fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"}) added 1949 packages from 671 contributors and audited 35817 packages in 43.316s found 0 vulnerabilities Removing intermediate container 0f7cda2e8098 ---> 2f4e0c88fbee Step 5/6 : COPY . . ---> 64ba876f1a5c Step 6/6 : CMD [\"npm\", \"run\", \"start\"] ---> Running in a50100fb5243 Removing intermediate container a50100fb5243 ---> 4a42cf02f626 Successfully built 4a42cf02f626","title":"Creating Dev Dockerfile"},{"location":"Docker%20%26%20Kubernetes/06_creating_a_production_grade_workflow/04_duplicating_dependencies/","text":"Duplicating dependencies \u00b6 If you notice in the last section when we ran the build process, there was a line: \u2718 davis@davis-arch \ue0b0 ~/projects/docker/04_react_app \ue0b0 \ue0a0 master \ue0b0 docker build -f Dockerfile.dev . Sending build context to Docker daemon 244MB This is thrown because we have a duplicate dependencies folder - we have a copy on our local machine and then the docker is installing them. The easiest way to solve this problem is simply by deleting the node_modules directory on our local machine. davis@davis-arch \ue0b0 ~/projects/docker/04_react_app \ue0b0 \ue0a0 master \ue0b0 rm -r node_modules davis@davis-arch \ue0b0 ~/projects/docker/04_react_app \ue0b0 \ue0a0 master \ue0b0 docker build -f Dockerfile.dev . Sending build context to Docker daemon 715.3kB Step 1/6 : FROM node:alpine ---> ebbf98230a82 Step 2/6 : WORKDIR /app ---> Using cache ---> 93b2648262c0 Step 3/6 : COPY package.json . ---> Using cache ---> a33dd4926364 Step 4/6 : RUN npm install ---> Using cache ---> 2f4e0c88fbee Step 5/6 : COPY . . ---> 297ad62ace07 Step 6/6 : CMD [\"npm\", \"run\", \"start\"] ---> Running in 92f50d91be7e Removing intermediate container 92f50d91be7e ---> 3b1edaee87fb Successfully built 3b1edaee87fb Now the build process is much faster.","title":"Duplicating dependencies"},{"location":"Docker%20%26%20Kubernetes/06_creating_a_production_grade_workflow/04_duplicating_dependencies/#duplicating-dependencies","text":"If you notice in the last section when we ran the build process, there was a line: \u2718 davis@davis-arch \ue0b0 ~/projects/docker/04_react_app \ue0b0 \ue0a0 master \ue0b0 docker build -f Dockerfile.dev . Sending build context to Docker daemon 244MB This is thrown because we have a duplicate dependencies folder - we have a copy on our local machine and then the docker is installing them. The easiest way to solve this problem is simply by deleting the node_modules directory on our local machine. davis@davis-arch \ue0b0 ~/projects/docker/04_react_app \ue0b0 \ue0a0 master \ue0b0 rm -r node_modules davis@davis-arch \ue0b0 ~/projects/docker/04_react_app \ue0b0 \ue0a0 master \ue0b0 docker build -f Dockerfile.dev . Sending build context to Docker daemon 715.3kB Step 1/6 : FROM node:alpine ---> ebbf98230a82 Step 2/6 : WORKDIR /app ---> Using cache ---> 93b2648262c0 Step 3/6 : COPY package.json . ---> Using cache ---> a33dd4926364 Step 4/6 : RUN npm install ---> Using cache ---> 2f4e0c88fbee Step 5/6 : COPY . . ---> 297ad62ace07 Step 6/6 : CMD [\"npm\", \"run\", \"start\"] ---> Running in 92f50d91be7e Removing intermediate container 92f50d91be7e ---> 3b1edaee87fb Successfully built 3b1edaee87fb Now the build process is much faster.","title":"Duplicating dependencies"},{"location":"Docker%20%26%20Kubernetes/06_creating_a_production_grade_workflow/05_starting_the_container/","text":"Starting the container \u00b6 We can use docker run to start the development server. davis@davis-arch \ue0b0 ~/projects/docker/04_react_app \ue0b0 \ue0a0 master \ue0b0 docker run -p 3000:3000 3b1edaee87fb > frontend@0.1.0 start /app > react-scripts start Starting the development server... Compiled successfully! You can now view frontend in the browser. Local: http://localhost:3000/ On Your Network: http://172.17.0.2:3000/ Note that the development build is not optimized. To create a production build, use npm run build. And we'll see the react app: But, if we change the source code, it will not change in the browser - they are not reflected in the container until we rebuild the image.","title":"Starting the container"},{"location":"Docker%20%26%20Kubernetes/06_creating_a_production_grade_workflow/05_starting_the_container/#starting-the-container","text":"We can use docker run to start the development server. davis@davis-arch \ue0b0 ~/projects/docker/04_react_app \ue0b0 \ue0a0 master \ue0b0 docker run -p 3000:3000 3b1edaee87fb > frontend@0.1.0 start /app > react-scripts start Starting the development server... Compiled successfully! You can now view frontend in the browser. Local: http://localhost:3000/ On Your Network: http://172.17.0.2:3000/ Note that the development build is not optimized. To create a production build, use npm run build. And we'll see the react app: But, if we change the source code, it will not change in the browser - they are not reflected in the container until we rebuild the image.","title":"Starting the container"},{"location":"Docker%20%26%20Kubernetes/06_creating_a_production_grade_workflow/06_docker_volumes/","text":"Docker volumes \u00b6 In previous section we had a running react development server, but if we changed any source, it will not reflect the changes unless the docker image is rebuilt. We can fix this problem by using docker volumes, which basically references a folder on the local machine. davis@davis-arch \ue0b0 ~/projects/docker/04_react_app \ue0b0 \ue0a0 master \ue0b0 docker build -f Dockerfile.dev . Sending build context to Docker daemon 715.8kB Step 1/6 : FROM node:alpine ---> ebbf98230a82 Step 2/6 : WORKDIR /app ---> Using cache ---> 93b2648262c0 Step 3/6 : COPY package.json . ---> Using cache ---> a33dd4926364 Step 4/6 : RUN npm install ---> Using cache ---> 2f4e0c88fbee Step 5/6 : COPY . . ---> Using cache ---> 592d9fd6172b Step 6/6 : CMD [\"npm\", \"run\", \"start\"] ---> Running in 509374b381c9 Removing intermediate container 509374b381c9 ---> 7aa5ed21de27 Successfully built 7aa5ed21de27 davis@davis-arch \ue0b0 ~/projects/docker/04_react_app \ue0b0 \ue0a0 master \ue0b0 docker run -p 3000:3000 -v /app/node_modules -v $(pwd):/app 542f2b7bf48a > frontend@0.1.0 start /app > react-scripts start Starting the development server... Compiled successfully! You can now view frontend in the browser. Local: http://localhost:3000/ On Your Network: http://172.17.0.2:3000/ Note that the development build is not optimized. To create a production build, use npm run build. Now, when making a change, you'll notice that the changes are recompiled and reflected automatically. Compiling... Compiled successfully!","title":"Docker volumes"},{"location":"Docker%20%26%20Kubernetes/06_creating_a_production_grade_workflow/06_docker_volumes/#docker-volumes","text":"In previous section we had a running react development server, but if we changed any source, it will not reflect the changes unless the docker image is rebuilt. We can fix this problem by using docker volumes, which basically references a folder on the local machine. davis@davis-arch \ue0b0 ~/projects/docker/04_react_app \ue0b0 \ue0a0 master \ue0b0 docker build -f Dockerfile.dev . Sending build context to Docker daemon 715.8kB Step 1/6 : FROM node:alpine ---> ebbf98230a82 Step 2/6 : WORKDIR /app ---> Using cache ---> 93b2648262c0 Step 3/6 : COPY package.json . ---> Using cache ---> a33dd4926364 Step 4/6 : RUN npm install ---> Using cache ---> 2f4e0c88fbee Step 5/6 : COPY . . ---> Using cache ---> 592d9fd6172b Step 6/6 : CMD [\"npm\", \"run\", \"start\"] ---> Running in 509374b381c9 Removing intermediate container 509374b381c9 ---> 7aa5ed21de27 Successfully built 7aa5ed21de27 davis@davis-arch \ue0b0 ~/projects/docker/04_react_app \ue0b0 \ue0a0 master \ue0b0 docker run -p 3000:3000 -v /app/node_modules -v $(pwd):/app 542f2b7bf48a > frontend@0.1.0 start /app > react-scripts start Starting the development server... Compiled successfully! You can now view frontend in the browser. Local: http://localhost:3000/ On Your Network: http://172.17.0.2:3000/ Note that the development build is not optimized. To create a production build, use npm run build. Now, when making a change, you'll notice that the changes are recompiled and reflected automatically. Compiling... Compiled successfully!","title":"Docker volumes"},{"location":"Docker%20%26%20Kubernetes/06_creating_a_production_grade_workflow/07_shorthand_with_docker_compose/","text":"Shorthand with docker compose \u00b6 The previous approach is working well, but the command can get ridiculously long and that can be a pain. We can make a docker-compose.yml to simplify the command we have to run. davis@davis-arch \ue0b0 ~/projects/docker/04_react_app \ue0b0 \ue0a0 master \ue0b0 touch docker-compose.yml version: '3' services: web: build: context: . dockerfile: Dockerfile.dev ports: - \"3000:3000\" volumes: - /app/node_modules - .:/app Now we can use: davis@davis-arch \ue0b0 ~/projects/docker/04_react_app \ue0b0 \ue0a0 master \ue0b0 docker-compose up Creating network \"04_react_app_default\" with the default driver Building web Step 1/6 : FROM node:alpine ---> ebbf98230a82 Step 2/6 : WORKDIR /app ---> Using cache ---> 93b2648262c0 Step 3/6 : COPY package.json . ---> c1b398fa1193 Step 4/6 : RUN npm install ---> Running in b75a8612d065 npm WARN deprecated kleur@2.0.2: Please upgrade to kleur@3 or migrate to 'ansi-colors' if you prefer the old syntax. Visit <https://github.com/lukeed/kleur/releases/tag/v3.0.0\\> for migration path(s). npm WARN deprecated circular-json@0.3.3: CircularJSON is in maintenance only, flatted is its successor. npm notice created a lockfile as package-lock.json. You should commit this file. npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules/fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"}) added 1949 packages from 671 contributors and audited 35817 packages in 39.824s found 0 vulnerabilities Removing intermediate container b75a8612d065 ---> f1a451e57b5f Step 5/6 : COPY . . ---> 706a952c1afc Step 6/6 : CMD [\"npm\", \"run\", \"start\"] ---> Running in 4284cd9f9286 Removing intermediate container 4284cd9f9286 ---> 092bf95af1b7 Successfully built 092bf95af1b7 Successfully tagged 04_react_app_web:latest WARNING: Image for service web was built because it did not already exist. To rebuild this image you must use `docker-compose build` or `docker-compose up --build`. Creating 04_react_app_web_1 ... done Attaching to 04_react_app_web_1 web_1 | web_1 | > frontend@0.1.0 start /app web_1 | > react-scripts start web_1 | web_1 | Starting the development server... web_1 | web_1 | Compiled successfully! web_1 | web_1 | You can now view frontend in the browser. web_1 | web_1 | Local: http://localhost:3000/ web_1 | On Your Network: http://172.21.0.2:3000/ web_1 | web_1 | Note that the development build is not optimized. web_1 | To create a production build, use npm run build. web_1 | And the app will work in the browser like before.","title":"Shorthand with docker compose"},{"location":"Docker%20%26%20Kubernetes/06_creating_a_production_grade_workflow/07_shorthand_with_docker_compose/#shorthand-with-docker-compose","text":"The previous approach is working well, but the command can get ridiculously long and that can be a pain. We can make a docker-compose.yml to simplify the command we have to run. davis@davis-arch \ue0b0 ~/projects/docker/04_react_app \ue0b0 \ue0a0 master \ue0b0 touch docker-compose.yml version: '3' services: web: build: context: . dockerfile: Dockerfile.dev ports: - \"3000:3000\" volumes: - /app/node_modules - .:/app Now we can use: davis@davis-arch \ue0b0 ~/projects/docker/04_react_app \ue0b0 \ue0a0 master \ue0b0 docker-compose up Creating network \"04_react_app_default\" with the default driver Building web Step 1/6 : FROM node:alpine ---> ebbf98230a82 Step 2/6 : WORKDIR /app ---> Using cache ---> 93b2648262c0 Step 3/6 : COPY package.json . ---> c1b398fa1193 Step 4/6 : RUN npm install ---> Running in b75a8612d065 npm WARN deprecated kleur@2.0.2: Please upgrade to kleur@3 or migrate to 'ansi-colors' if you prefer the old syntax. Visit <https://github.com/lukeed/kleur/releases/tag/v3.0.0\\> for migration path(s). npm WARN deprecated circular-json@0.3.3: CircularJSON is in maintenance only, flatted is its successor. npm notice created a lockfile as package-lock.json. You should commit this file. npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules/fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"}) added 1949 packages from 671 contributors and audited 35817 packages in 39.824s found 0 vulnerabilities Removing intermediate container b75a8612d065 ---> f1a451e57b5f Step 5/6 : COPY . . ---> 706a952c1afc Step 6/6 : CMD [\"npm\", \"run\", \"start\"] ---> Running in 4284cd9f9286 Removing intermediate container 4284cd9f9286 ---> 092bf95af1b7 Successfully built 092bf95af1b7 Successfully tagged 04_react_app_web:latest WARNING: Image for service web was built because it did not already exist. To rebuild this image you must use `docker-compose build` or `docker-compose up --build`. Creating 04_react_app_web_1 ... done Attaching to 04_react_app_web_1 web_1 | web_1 | > frontend@0.1.0 start /app web_1 | > react-scripts start web_1 | web_1 | Starting the development server... web_1 | web_1 | Compiled successfully! web_1 | web_1 | You can now view frontend in the browser. web_1 | web_1 | Local: http://localhost:3000/ web_1 | On Your Network: http://172.21.0.2:3000/ web_1 | web_1 | Note that the development build is not optimized. web_1 | To create a production build, use npm run build. web_1 | And the app will work in the browser like before.","title":"Shorthand with docker compose"},{"location":"Docker%20%26%20Kubernetes/06_creating_a_production_grade_workflow/08_executing_tests/","text":"Executing tests \u00b6 On react executing tests are extremely straigh forward, basically all we have to do is to use the same Dockerfile and just change the last command to npm run test . davis@davis-arch \ue0b0 ~/projects/docker/04_react_app \ue0b0 \ue0a0 master \ue0b0 docker build -f Dockerfile.dev . Sending build context to Docker daemon 716.8kB Step 1/6 : FROM node:alpine ---> ebbf98230a82 Step 2/6 : WORKDIR /app ---> Using cache ---> 93b2648262c0 Step 3/6 : COPY package.json . ---> Using cache ---> a33dd4926364 Step 4/6 : RUN npm install ---> Using cache ---> 2f4e0c88fbee Step 5/6 : COPY . . ---> 387cee59068b Step 6/6 : CMD [\"npm\", \"run\", \"start\"] ---> Running in 8a7360864aba Removing intermediate container 8a7360864aba ---> 940d69d2b0af Successfully built 940d69d2b0af davis@davis-arch \ue0b0 ~/projects/docker/04_react_app \ue0b0 \ue0a0 master \ue0b0 docker run 940d69d2b0af npm run test > frontend@0.1.0 test /app > react-scripts test PASS src/App.test.js \u2713 renders without crashing (27ms) Test Suites: 1 passed, 1 total Tests: 1 passed, 1 total Snapshots: 0 total Time: 1.185s Ran all test suites. When running the command with -it flags, we get a fullscreen app with PASS src/App.test.js \u2713 renders without crashing (23ms) Test Suites: 1 passed, 1 total Tests: 1 passed, 1 total Snapshots: 0 total Time: 1.197s Ran all test suites. Watch Usage \u203a Press f to run only failed tests. \u203a Press o to only run tests related to changed files. \u203a Press q to quit watch mode. \u203a Press t to filter by a test name regex pattern. \u203a Press p to filter by a filename regex pattern. \u203a Press Enter to trigger a test run. But, if we make any changes, they will not reflect on the changes because we have no volumes mounted. We'll modify the docker-compose.yml file for tests. version: '3' services: web: build: context: . dockerfile: Dockerfile.dev ports: - \"3000:3000\" volumes: - /app/node_modules - .:/app tests: build: context: . dockerfile: Dockerfile.dev volumes: - /app/node_modules - .:/app command: [\"npm\", \"run\", \"test\"] Now we can run davis@davis-arch \ue0b0 ~/projects/docker/04_react_app \ue0b0 \ue0a0 master \ue0b0 docker-compose up --build Building web Step 1/6 : FROM node:alpine ---> ebbf98230a82 Step 2/6 : WORKDIR /app ---> Using cache ---> 93b2648262c0 Step 3/6 : COPY package.json . ---> Using cache ---> c1b398fa1193 Step 4/6 : RUN npm install ---> Using cache ---> f1a451e57b5f Step 5/6 : COPY . . ---> 8055397f2914 Step 6/6 : CMD [\"npm\", \"run\", \"start\"] ---> Running in 3929c0160587 Removing intermediate container 3929c0160587 ---> 8f8dc4a4edce Successfully built 8f8dc4a4edce Successfully tagged 04_react_app_web:latest Building tests Step 1/6 : FROM node:alpine ---> ebbf98230a82 Step 2/6 : WORKDIR /app ---> Using cache ---> 93b2648262c0 Step 3/6 : COPY package.json . ---> Using cache ---> c1b398fa1193 Step 4/6 : RUN npm install ---> Using cache ---> f1a451e57b5f Step 5/6 : COPY . . ---> Using cache ---> 8055397f2914 Step 6/6 : CMD [\"npm\", \"run\", \"start\"] ---> Using cache ---> 8f8dc4a4edce Successfully built 8f8dc4a4edce Successfully tagged 04_react_app_tests:latest Recreating 04_react_app_web_1 ... done Creating 04_react_app_tests_1 ... done Attaching to 04_react_app_web_1, 04_react_app_tests_1 web_1 | web_1 | > frontend@0.1.0 start /app web_1 | > react-scripts start web_1 | web_1 | Starting the development server... web_1 | tests_1 | tests_1 | > frontend@0.1.0 test /app tests_1 | > react-scripts test tests_1 | web_1 | Compiled successfully! web_1 | web_1 | You can now view frontend in the browser. web_1 | web_1 | Local: http://localhost:3000/ web_1 | On Your Network: http://172.21.0.2:3000/ web_1 | web_1 | Note that the development build is not optimized. web_1 | To create a production build, use npm run build. web_1 | tests_1 | PASS src/App.test.js tests_1 | \u2713 renders without crashing (24ms) tests_1 | tests_1 | Test Suites: 1 passed, 1 total tests_1 | Tests: 1 passed, 1 total tests_1 | Snapshots: 0 total tests_1 | Time: 1.197s tests_1 | Ran all test suites. tests_1 | If we change the tests: tests_1 | PASS src/App.test.js tests_1 | \u2713 renders without crashing (4ms) tests_1 | \u2713 is true (1ms) tests_1 | tests_1 | console.log src/App.test.js:12 tests_1 | true tests_1 | tests_1 | Test Suites: 1 passed, 1 total tests_1 | Tests: 2 passed, 2 total tests_1 | Snapshots: 0 total tests_1 | Time: 0.243s, estimated 1s tests_1 | Ran all test suites. tests_1 | But still, the workflow is not ideal, we cannot interact with the tests, but to do that you will need to do docker exec -it tests_1 sh and run the tests by hand.","title":"Executing tests"},{"location":"Docker%20%26%20Kubernetes/06_creating_a_production_grade_workflow/08_executing_tests/#executing-tests","text":"On react executing tests are extremely straigh forward, basically all we have to do is to use the same Dockerfile and just change the last command to npm run test . davis@davis-arch \ue0b0 ~/projects/docker/04_react_app \ue0b0 \ue0a0 master \ue0b0 docker build -f Dockerfile.dev . Sending build context to Docker daemon 716.8kB Step 1/6 : FROM node:alpine ---> ebbf98230a82 Step 2/6 : WORKDIR /app ---> Using cache ---> 93b2648262c0 Step 3/6 : COPY package.json . ---> Using cache ---> a33dd4926364 Step 4/6 : RUN npm install ---> Using cache ---> 2f4e0c88fbee Step 5/6 : COPY . . ---> 387cee59068b Step 6/6 : CMD [\"npm\", \"run\", \"start\"] ---> Running in 8a7360864aba Removing intermediate container 8a7360864aba ---> 940d69d2b0af Successfully built 940d69d2b0af davis@davis-arch \ue0b0 ~/projects/docker/04_react_app \ue0b0 \ue0a0 master \ue0b0 docker run 940d69d2b0af npm run test > frontend@0.1.0 test /app > react-scripts test PASS src/App.test.js \u2713 renders without crashing (27ms) Test Suites: 1 passed, 1 total Tests: 1 passed, 1 total Snapshots: 0 total Time: 1.185s Ran all test suites. When running the command with -it flags, we get a fullscreen app with PASS src/App.test.js \u2713 renders without crashing (23ms) Test Suites: 1 passed, 1 total Tests: 1 passed, 1 total Snapshots: 0 total Time: 1.197s Ran all test suites. Watch Usage \u203a Press f to run only failed tests. \u203a Press o to only run tests related to changed files. \u203a Press q to quit watch mode. \u203a Press t to filter by a test name regex pattern. \u203a Press p to filter by a filename regex pattern. \u203a Press Enter to trigger a test run. But, if we make any changes, they will not reflect on the changes because we have no volumes mounted. We'll modify the docker-compose.yml file for tests. version: '3' services: web: build: context: . dockerfile: Dockerfile.dev ports: - \"3000:3000\" volumes: - /app/node_modules - .:/app tests: build: context: . dockerfile: Dockerfile.dev volumes: - /app/node_modules - .:/app command: [\"npm\", \"run\", \"test\"] Now we can run davis@davis-arch \ue0b0 ~/projects/docker/04_react_app \ue0b0 \ue0a0 master \ue0b0 docker-compose up --build Building web Step 1/6 : FROM node:alpine ---> ebbf98230a82 Step 2/6 : WORKDIR /app ---> Using cache ---> 93b2648262c0 Step 3/6 : COPY package.json . ---> Using cache ---> c1b398fa1193 Step 4/6 : RUN npm install ---> Using cache ---> f1a451e57b5f Step 5/6 : COPY . . ---> 8055397f2914 Step 6/6 : CMD [\"npm\", \"run\", \"start\"] ---> Running in 3929c0160587 Removing intermediate container 3929c0160587 ---> 8f8dc4a4edce Successfully built 8f8dc4a4edce Successfully tagged 04_react_app_web:latest Building tests Step 1/6 : FROM node:alpine ---> ebbf98230a82 Step 2/6 : WORKDIR /app ---> Using cache ---> 93b2648262c0 Step 3/6 : COPY package.json . ---> Using cache ---> c1b398fa1193 Step 4/6 : RUN npm install ---> Using cache ---> f1a451e57b5f Step 5/6 : COPY . . ---> Using cache ---> 8055397f2914 Step 6/6 : CMD [\"npm\", \"run\", \"start\"] ---> Using cache ---> 8f8dc4a4edce Successfully built 8f8dc4a4edce Successfully tagged 04_react_app_tests:latest Recreating 04_react_app_web_1 ... done Creating 04_react_app_tests_1 ... done Attaching to 04_react_app_web_1, 04_react_app_tests_1 web_1 | web_1 | > frontend@0.1.0 start /app web_1 | > react-scripts start web_1 | web_1 | Starting the development server... web_1 | tests_1 | tests_1 | > frontend@0.1.0 test /app tests_1 | > react-scripts test tests_1 | web_1 | Compiled successfully! web_1 | web_1 | You can now view frontend in the browser. web_1 | web_1 | Local: http://localhost:3000/ web_1 | On Your Network: http://172.21.0.2:3000/ web_1 | web_1 | Note that the development build is not optimized. web_1 | To create a production build, use npm run build. web_1 | tests_1 | PASS src/App.test.js tests_1 | \u2713 renders without crashing (24ms) tests_1 | tests_1 | Test Suites: 1 passed, 1 total tests_1 | Tests: 1 passed, 1 total tests_1 | Snapshots: 0 total tests_1 | Time: 1.197s tests_1 | Ran all test suites. tests_1 | If we change the tests: tests_1 | PASS src/App.test.js tests_1 | \u2713 renders without crashing (4ms) tests_1 | \u2713 is true (1ms) tests_1 | tests_1 | console.log src/App.test.js:12 tests_1 | true tests_1 | tests_1 | Test Suites: 1 passed, 1 total tests_1 | Tests: 2 passed, 2 total tests_1 | Snapshots: 0 total tests_1 | Time: 0.243s, estimated 1s tests_1 | Ran all test suites. tests_1 | But still, the workflow is not ideal, we cannot interact with the tests, but to do that you will need to do docker exec -it tests_1 sh and run the tests by hand.","title":"Executing tests"},{"location":"Docker%20%26%20Kubernetes/06_creating_a_production_grade_workflow/09_multistep_docker_builds/","text":"Multistep docker builds \u00b6 In development workflow we use the dev server, but in production we will not have it. So we'll need to setup an nginx server to serve the staticly generated files. So we'll make another dockerfile for production mode. The new production Dockerfile will have 2 phases: 1. Build phase 1. Use node:alpine 2. Copy the package.json 3. Install dependencies 4. Run npm run build 2. Run phase 1. Use nginx 2. Copy over the results of the npm run build 3. Start nginx These phases can be implemented by creating the Dockerfile davis@davis-arch \ue0b0 ~/projects/docker/04_react_app \ue0b0 \ue0a0 master \ue0b0 touch Dockerfile With content: FROM node:alpine AS builder WORKDIR /app COPY package.json . RUN npm install COPY . . RUN npm run build FROM nginx COPY --from=builder /app/build /usr/share/nginx/html Now when we build: davis@davis-arch \ue0b0 ~/projects/docker/04_react_app \ue0b0 \ue0a0 master \ue0b0 docker build . Sending build context to Docker daemon 717.8kB Step 1/8 : FROM node:alpine AS builder ---> ebbf98230a82 Step 2/8 : WORKDIR /app ---> Using cache ---> 93b2648262c0 Step 3/8 : COPY package.json . ---> Using cache ---> a33dd4926364 Step 4/8 : RUN npm install ---> Using cache ---> 2f4e0c88fbee Step 5/8 : COPY . . ---> e9002362daaa Step 6/8 : RUN npm run build ---> Running in 61cdb073c244 > frontend@0.1.0 build /app > react-scripts build Creating an optimized production build... Compiled successfully. File sizes after gzip: 34.71 KB build/static/js/1.fa92c112.chunk.js 763 B build/static/js/runtime~main.229c360f.js 716 B build/static/js/main.1cbe6fea.chunk.js 510 B build/static/css/main.00c0f591.chunk.css The project was built assuming it is hosted at the server root. You can control this with the homepage field in your package.json. For example, add this to build it for GitHub Pages: \"homepage\" : \"http://myname.github.io/myapp\", The build folder is ready to be deployed. You may serve it with a static server: npm install -g serve serve -s build Find out more about deployment here: http://bit.ly/CRA-deploy Removing intermediate container 61cdb073c244 ---> 1f0d8b362268 Step 7/8 : FROM nginx latest: Pulling from library/nginx 5e6ec7f28fb7: Pull complete ab804f9bbcbe: Pull complete 052b395f16bc: Pull complete Digest: sha256:56bcd35e8433343dbae0484ed5b740843dd8bff9479400990f251c13bbb94763 Status: Downloaded newer image for nginx:latest ---> 42b4762643dc Step 8/8 : COPY --from=builder /app/build /usr/share/nginx/html ---> 1eb220e606fb Successfully built 1eb220e606fb And run it, visit with browser: \u2718 davis@davis-arch \ue0b0 ~/projects/docker/04_react_app \ue0b0 \ue0a0 master \ue0b0 docker run -p 8080:80 1eb220e606fb 172.17.0.1 - - [02/Feb/2019:11:34:01 +0000] \"GET / HTTP/1.1\" 200 2062 \"-\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\" \"-\" 172.17.0.1 - - [02/Feb/2019:11:34:02 +0000] \"GET /static/css/main.00c0f591.chunk.css HTTP/1.1\" 200 984 \"http://localhost:8080/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\" \"-\" 172.17.0.1 - - [02/Feb/2019:11:34:02 +0000] \"GET /static/js/1.fa92c112.chunk.js HTTP/1.1\" 200 112436 \"http://localhost:8080/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\" \"-\" 172.17.0.1 - - [02/Feb/2019:11:34:02 +0000] \"GET /static/js/main.1cbe6fea.chunk.js HTTP/1.1\" 200 1349 \"http://localhost:8080/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\" \"-\" 172.17.0.1 - - [02/Feb/2019:11:34:02 +0000] \"GET /static/media/logo.5d5d9eef.svg HTTP/1.1\" 200 2671 \"http://localhost:8080/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\" \"-\" 172.17.0.1 - - [02/Feb/2019:11:34:02 +0000] \"GET /manifest.json HTTP/1.1\" 200 306 \"http://localhost:8080/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\" \"-\" 172.17.0.1 - - [02/Feb/2019:11:34:02 +0000] \"GET /favicon.ico HTTP/1.1\" 200 3870 \"http://localhost:8080/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\" \"-\"","title":"Multistep docker builds"},{"location":"Docker%20%26%20Kubernetes/06_creating_a_production_grade_workflow/09_multistep_docker_builds/#multistep-docker-builds","text":"In development workflow we use the dev server, but in production we will not have it. So we'll need to setup an nginx server to serve the staticly generated files. So we'll make another dockerfile for production mode. The new production Dockerfile will have 2 phases: 1. Build phase 1. Use node:alpine 2. Copy the package.json 3. Install dependencies 4. Run npm run build 2. Run phase 1. Use nginx 2. Copy over the results of the npm run build 3. Start nginx These phases can be implemented by creating the Dockerfile davis@davis-arch \ue0b0 ~/projects/docker/04_react_app \ue0b0 \ue0a0 master \ue0b0 touch Dockerfile With content: FROM node:alpine AS builder WORKDIR /app COPY package.json . RUN npm install COPY . . RUN npm run build FROM nginx COPY --from=builder /app/build /usr/share/nginx/html Now when we build: davis@davis-arch \ue0b0 ~/projects/docker/04_react_app \ue0b0 \ue0a0 master \ue0b0 docker build . Sending build context to Docker daemon 717.8kB Step 1/8 : FROM node:alpine AS builder ---> ebbf98230a82 Step 2/8 : WORKDIR /app ---> Using cache ---> 93b2648262c0 Step 3/8 : COPY package.json . ---> Using cache ---> a33dd4926364 Step 4/8 : RUN npm install ---> Using cache ---> 2f4e0c88fbee Step 5/8 : COPY . . ---> e9002362daaa Step 6/8 : RUN npm run build ---> Running in 61cdb073c244 > frontend@0.1.0 build /app > react-scripts build Creating an optimized production build... Compiled successfully. File sizes after gzip: 34.71 KB build/static/js/1.fa92c112.chunk.js 763 B build/static/js/runtime~main.229c360f.js 716 B build/static/js/main.1cbe6fea.chunk.js 510 B build/static/css/main.00c0f591.chunk.css The project was built assuming it is hosted at the server root. You can control this with the homepage field in your package.json. For example, add this to build it for GitHub Pages: \"homepage\" : \"http://myname.github.io/myapp\", The build folder is ready to be deployed. You may serve it with a static server: npm install -g serve serve -s build Find out more about deployment here: http://bit.ly/CRA-deploy Removing intermediate container 61cdb073c244 ---> 1f0d8b362268 Step 7/8 : FROM nginx latest: Pulling from library/nginx 5e6ec7f28fb7: Pull complete ab804f9bbcbe: Pull complete 052b395f16bc: Pull complete Digest: sha256:56bcd35e8433343dbae0484ed5b740843dd8bff9479400990f251c13bbb94763 Status: Downloaded newer image for nginx:latest ---> 42b4762643dc Step 8/8 : COPY --from=builder /app/build /usr/share/nginx/html ---> 1eb220e606fb Successfully built 1eb220e606fb And run it, visit with browser: \u2718 davis@davis-arch \ue0b0 ~/projects/docker/04_react_app \ue0b0 \ue0a0 master \ue0b0 docker run -p 8080:80 1eb220e606fb 172.17.0.1 - - [02/Feb/2019:11:34:01 +0000] \"GET / HTTP/1.1\" 200 2062 \"-\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\" \"-\" 172.17.0.1 - - [02/Feb/2019:11:34:02 +0000] \"GET /static/css/main.00c0f591.chunk.css HTTP/1.1\" 200 984 \"http://localhost:8080/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\" \"-\" 172.17.0.1 - - [02/Feb/2019:11:34:02 +0000] \"GET /static/js/1.fa92c112.chunk.js HTTP/1.1\" 200 112436 \"http://localhost:8080/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\" \"-\" 172.17.0.1 - - [02/Feb/2019:11:34:02 +0000] \"GET /static/js/main.1cbe6fea.chunk.js HTTP/1.1\" 200 1349 \"http://localhost:8080/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\" \"-\" 172.17.0.1 - - [02/Feb/2019:11:34:02 +0000] \"GET /static/media/logo.5d5d9eef.svg HTTP/1.1\" 200 2671 \"http://localhost:8080/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\" \"-\" 172.17.0.1 - - [02/Feb/2019:11:34:02 +0000] \"GET /manifest.json HTTP/1.1\" 200 306 \"http://localhost:8080/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\" \"-\" 172.17.0.1 - - [02/Feb/2019:11:34:02 +0000] \"GET /favicon.ico HTTP/1.1\" 200 3870 \"http://localhost:8080/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\" \"-\"","title":"Multistep docker builds"},{"location":"Docker%20%26%20Kubernetes/07_ci_and_deployments_with_aws/01_service_overview/","text":"Service overview \u00b6 In this project we will set up a GitHub repo where new features are made with feature branches. Once those changes are merged into master branch, those will automatically be deployed on AWS hosting. The project will use Github, Travis CI and AWS services.","title":"Service overview"},{"location":"Docker%20%26%20Kubernetes/07_ci_and_deployments_with_aws/01_service_overview/#service-overview","text":"In this project we will set up a GitHub repo where new features are made with feature branches. Once those changes are merged into master branch, those will automatically be deployed on AWS hosting. The project will use Github, Travis CI and AWS services.","title":"Service overview"},{"location":"Docker%20%26%20Kubernetes/07_ci_and_deployments_with_aws/02_github_setup/","text":"Github setup \u00b6 We create a new repository on GitHub, and push our code to it: https://github.com/daviskregers/docker-react","title":"Github setup"},{"location":"Docker%20%26%20Kubernetes/07_ci_and_deployments_with_aws/02_github_setup/#github-setup","text":"We create a new repository on GitHub, and push our code to it: https://github.com/daviskregers/docker-react","title":"Github setup"},{"location":"Docker%20%26%20Kubernetes/07_ci_and_deployments_with_aws/03_travis_CI_integration/","text":"Travis CI integration \u00b6 The idea of using Travis CI is that it watches whenever we push something onto our Github repository, then it pulls all the code and does some predefined tasks like testing, building and deployment. To get started, we navigate to https://travis-ci.com/ and create an account and activating Github Apps integration. When using creating the app, we select to use the repository we just made: And now when refreshing the \"Settings\" section, we will see an option to manage the settings of the repository we want to watch. Travis YML FIle Configuration \u00b6 Now we need to specify what exactly Travis needs to do with each push, for this we will create a .travis.yml file. It will: Tell travis we need a copy of docker running Build our image using Dockerfile.dev Tell Travis how to run our test suite Tell travis to deploy our code to AWS sudo: required services: - docker before_install: - docker build -t daviskregers/docker-react -f Dockerfile.dev . script: - docker run daviskregers/docker-react npm run test -- --coverage Now when we push it to github, we can see that Travis CI automatically picked it up: And in the build time, it executed all the tests and ran coverage:","title":"Travis CI integration"},{"location":"Docker%20%26%20Kubernetes/07_ci_and_deployments_with_aws/03_travis_CI_integration/#travis-ci-integration","text":"The idea of using Travis CI is that it watches whenever we push something onto our Github repository, then it pulls all the code and does some predefined tasks like testing, building and deployment. To get started, we navigate to https://travis-ci.com/ and create an account and activating Github Apps integration. When using creating the app, we select to use the repository we just made: And now when refreshing the \"Settings\" section, we will see an option to manage the settings of the repository we want to watch.","title":"Travis CI integration"},{"location":"Docker%20%26%20Kubernetes/07_ci_and_deployments_with_aws/03_travis_CI_integration/#travis-yml-file-configuration","text":"Now we need to specify what exactly Travis needs to do with each push, for this we will create a .travis.yml file. It will: Tell travis we need a copy of docker running Build our image using Dockerfile.dev Tell Travis how to run our test suite Tell travis to deploy our code to AWS sudo: required services: - docker before_install: - docker build -t daviskregers/docker-react -f Dockerfile.dev . script: - docker run daviskregers/docker-react npm run test -- --coverage Now when we push it to github, we can see that Travis CI automatically picked it up: And in the build time, it executed all the tests and ran coverage:","title":"Travis YML FIle Configuration"},{"location":"Docker%20%26%20Kubernetes/07_ci_and_deployments_with_aws/04_deployment_to_aws/","text":"Automatic deployment to AWS \u00b6 Now, when we have created a travis configuration file and executed all the tests, we can deploy it to AWS. In order to get started with AWS, we need to create an account at https://portal.aws.amazon.com/billing/signup#/start , it may require entering credit card even while applying for free tier isntances. Once AWS account is set up, we will create an \"Elastic Beanstalk\" application. To do that, we can go to \"Elastic Beanstalk\" section and click on \"Create New Application\". Now, we created a some kind of a workspace, where we'll need to create an environment. We will be asked to choose between a web server environment and worker environment. We are intending on running a web server environment. On the next step we can leave everything as is except for the \"Platform\" - select \"Docker\" under it: Now it will take some time to create the environment: Once it's done, you'll see something like this: When opening up the link that beanstalk has provided, you can see the sample page: And under health section, you can see that there is one instance running behind the load balancer: Configuring Travis to deploy to AWS \u00b6 Now we'll modify the travis configuration file to deploy to this Beanstalk environment. deploy: - provider: elasticbeanstalk - region: \"eu-central-1\" - app: \"docker-react\" - environment: \"DockerReact-env\" The region can be gotten from the link that the AWS created, app is the application name we created, environment is the environment we created. Now we need to specify an S3 bucket from where the deployment files are taken from. We can find it by going to the S3 section of AWS. - bucket_name: \"elasticbeanstalk-eu-central-1-334313735136\" - bucket_path: \"docker-react\" So, now the .travis.yml file looks like this: sudo: required services: - docker before_install: - docker build -t daviskregers/docker-react -f Dockerfile.dev . script: - docker run daviskregers/docker-react npm run test -- --coverage deploy: - provider: elasticbeanstalk - region: \"eu-central-1\" - app: \"docker-react\" - environment: \"DockerReact-env\" - bucket_name: - bucket_name: \"elasticbeanstalk-eu-central-1-334313735136\" - bucket_path: \"docker-react\" on: branch: master In order to deploy this successfully we'll need access, to get it, we can go to IAM -> Users and create a new user: In the next permission step, we'll provide full access to AWS Elastic Beanstalk: And once it's done, we'll get an access key and ID: These keys are required for the Travis CI to deploy the code, but we do not want to add it to the .yml file because anyone who can read the file, will have full access to the elastic beanstalk instances. We can overcome this by going to \"More options -> settings\" in Travis and there will be a section for Environment variable management. And we add both values like so: Now we'll tell the travis to use these environment variables: sudo: required services: - docker before_install: - docker build -t daviskregers/docker-react -f Dockerfile.dev . script: - docker run daviskregers/docker-react npm run test -- --coverage deploy: - provider: elasticbeanstalk - region: \"eu-central-1\" - app: \"docker-react\" - environment: \"DockerReact-env\" - bucket_name: - bucket_name: \"elasticbeanstalk-eu-central-1-334313735136\" - bucket_path: \"docker-react\" on: branch: master access_key_id: $AWS_ACCESS_KEY secret_access_key: secure: \"$AWS_SECRET_KEY\" Now when we push out travis configuration to github: At the end the deployment will fail. Now we need to do one more thing in order to make everything work - open up the port 80 in the production dockerfile. FROM node:alpine AS builder WORKDIR /app COPY package.json . RUN npm install COPY . . RUN npm run build EXPOSE 80 FROM nginx COPY --from=builder /app/build /usr/share/nginx/html And now when pushing and letting the travis to finish: When opening the ELB url:","title":"Automatic deployment to AWS"},{"location":"Docker%20%26%20Kubernetes/07_ci_and_deployments_with_aws/04_deployment_to_aws/#automatic-deployment-to-aws","text":"Now, when we have created a travis configuration file and executed all the tests, we can deploy it to AWS. In order to get started with AWS, we need to create an account at https://portal.aws.amazon.com/billing/signup#/start , it may require entering credit card even while applying for free tier isntances. Once AWS account is set up, we will create an \"Elastic Beanstalk\" application. To do that, we can go to \"Elastic Beanstalk\" section and click on \"Create New Application\". Now, we created a some kind of a workspace, where we'll need to create an environment. We will be asked to choose between a web server environment and worker environment. We are intending on running a web server environment. On the next step we can leave everything as is except for the \"Platform\" - select \"Docker\" under it: Now it will take some time to create the environment: Once it's done, you'll see something like this: When opening up the link that beanstalk has provided, you can see the sample page: And under health section, you can see that there is one instance running behind the load balancer:","title":"Automatic deployment to AWS"},{"location":"Docker%20%26%20Kubernetes/07_ci_and_deployments_with_aws/04_deployment_to_aws/#configuring-travis-to-deploy-to-aws","text":"Now we'll modify the travis configuration file to deploy to this Beanstalk environment. deploy: - provider: elasticbeanstalk - region: \"eu-central-1\" - app: \"docker-react\" - environment: \"DockerReact-env\" The region can be gotten from the link that the AWS created, app is the application name we created, environment is the environment we created. Now we need to specify an S3 bucket from where the deployment files are taken from. We can find it by going to the S3 section of AWS. - bucket_name: \"elasticbeanstalk-eu-central-1-334313735136\" - bucket_path: \"docker-react\" So, now the .travis.yml file looks like this: sudo: required services: - docker before_install: - docker build -t daviskregers/docker-react -f Dockerfile.dev . script: - docker run daviskregers/docker-react npm run test -- --coverage deploy: - provider: elasticbeanstalk - region: \"eu-central-1\" - app: \"docker-react\" - environment: \"DockerReact-env\" - bucket_name: - bucket_name: \"elasticbeanstalk-eu-central-1-334313735136\" - bucket_path: \"docker-react\" on: branch: master In order to deploy this successfully we'll need access, to get it, we can go to IAM -> Users and create a new user: In the next permission step, we'll provide full access to AWS Elastic Beanstalk: And once it's done, we'll get an access key and ID: These keys are required for the Travis CI to deploy the code, but we do not want to add it to the .yml file because anyone who can read the file, will have full access to the elastic beanstalk instances. We can overcome this by going to \"More options -> settings\" in Travis and there will be a section for Environment variable management. And we add both values like so: Now we'll tell the travis to use these environment variables: sudo: required services: - docker before_install: - docker build -t daviskregers/docker-react -f Dockerfile.dev . script: - docker run daviskregers/docker-react npm run test -- --coverage deploy: - provider: elasticbeanstalk - region: \"eu-central-1\" - app: \"docker-react\" - environment: \"DockerReact-env\" - bucket_name: - bucket_name: \"elasticbeanstalk-eu-central-1-334313735136\" - bucket_path: \"docker-react\" on: branch: master access_key_id: $AWS_ACCESS_KEY secret_access_key: secure: \"$AWS_SECRET_KEY\" Now when we push out travis configuration to github: At the end the deployment will fail. Now we need to do one more thing in order to make everything work - open up the port 80 in the production dockerfile. FROM node:alpine AS builder WORKDIR /app COPY package.json . RUN npm install COPY . . RUN npm run build EXPOSE 80 FROM nginx COPY --from=builder /app/build /usr/share/nginx/html And now when pushing and letting the travis to finish: When opening the ELB url:","title":"Configuring Travis to deploy to AWS"},{"location":"Docker%20%26%20Kubernetes/07_ci_and_deployments_with_aws/05_workflow_with_github/","text":"Workflow with GitHub \u00b6 In the service overview, we said that we will use feature branches for the development workflow. davis@davis-arch \ue0b0 ~/projects/docker/05_aws_auto_deployment \ue0b0 \ue0a0 master \ue0b0 git checkout -b feature Switched to a new branch 'feature' Now we'll make a change. diff --git a/src/App.js b/src/App.js index 4076e46..2a2814b 100755 --- a/src/App.js +++ b/src/App.js @@ -9,8 +9,7 @@ class App extends Component { <header className=\"App-header\"> <img src={logo} className=\"App-logo\" alt=\"logo\" /> <p> - Edit <code>src/App.js</code> and save to reload. - I did! + I was changed on the feature branch </p> <a className=\"App-link\" Push it: davis@davis-arch \ue0b0 ~/projects/docker/05_aws_auto_deployment \ue0b0 \ue0a0 feature \u25cf \ue0b0 git commit -am \"changed app text\" [feature 7f7ce3c] changed app text 1 file changed, 1 insertion(+), 2 deletions(-) davis@davis-arch \ue0b0 ~/projects/docker/05_aws_auto_deployment \ue0b0 \ue0a0 feature \ue0b0 git push origin feature Enumerating objects: 7, done. Counting objects: 100% (7/7), done. Delta compression using up to 8 threads Compressing objects: 100% (4/4), done. Writing objects: 100% (4/4), 400 bytes | 400.00 KiB/s, done. Total 4 (delta 3), reused 0 (delta 0) remote: Resolving deltas: 100% (3/3), completed with 3 local objects. remote: remote: Create a pull request for 'feature' on GitHub by visiting: remote: https://github.com/daviskregers/docker-react/pull/new/feature remote: To github.com:daviskregers/docker-react.git * [new branch] feature -> feature Now, when visiting the repository on github, we'll see a popup like this: Once clicking on this putton, we'll attempt to merge it into the master branch: It will automatically do checks whether the changes can be merged into the master, travis will check whether it can build the project, but will not deploy it yet. Now we can click on the green button to confirm the merge: Now the Travis will automatically deploy the merged code to AWS:","title":"Workflow with GitHub"},{"location":"Docker%20%26%20Kubernetes/07_ci_and_deployments_with_aws/05_workflow_with_github/#workflow-with-github","text":"In the service overview, we said that we will use feature branches for the development workflow. davis@davis-arch \ue0b0 ~/projects/docker/05_aws_auto_deployment \ue0b0 \ue0a0 master \ue0b0 git checkout -b feature Switched to a new branch 'feature' Now we'll make a change. diff --git a/src/App.js b/src/App.js index 4076e46..2a2814b 100755 --- a/src/App.js +++ b/src/App.js @@ -9,8 +9,7 @@ class App extends Component { <header className=\"App-header\"> <img src={logo} className=\"App-logo\" alt=\"logo\" /> <p> - Edit <code>src/App.js</code> and save to reload. - I did! + I was changed on the feature branch </p> <a className=\"App-link\" Push it: davis@davis-arch \ue0b0 ~/projects/docker/05_aws_auto_deployment \ue0b0 \ue0a0 feature \u25cf \ue0b0 git commit -am \"changed app text\" [feature 7f7ce3c] changed app text 1 file changed, 1 insertion(+), 2 deletions(-) davis@davis-arch \ue0b0 ~/projects/docker/05_aws_auto_deployment \ue0b0 \ue0a0 feature \ue0b0 git push origin feature Enumerating objects: 7, done. Counting objects: 100% (7/7), done. Delta compression using up to 8 threads Compressing objects: 100% (4/4), done. Writing objects: 100% (4/4), 400 bytes | 400.00 KiB/s, done. Total 4 (delta 3), reused 0 (delta 0) remote: Resolving deltas: 100% (3/3), completed with 3 local objects. remote: remote: Create a pull request for 'feature' on GitHub by visiting: remote: https://github.com/daviskregers/docker-react/pull/new/feature remote: To github.com:daviskregers/docker-react.git * [new branch] feature -> feature Now, when visiting the repository on github, we'll see a popup like this: Once clicking on this putton, we'll attempt to merge it into the master branch: It will automatically do checks whether the changes can be merged into the master, travis will check whether it can build the project, but will not deploy it yet. Now we can click on the green button to confirm the merge: Now the Travis will automatically deploy the merged code to AWS:","title":"Workflow with GitHub"},{"location":"Docker%20%26%20Kubernetes/07_ci_and_deployments_with_aws/06_environment_cleanup/","text":"Environment cleanup \u00b6 At the end of this project you might want to delete the AWS environment in order not to get billed for it. You can open up the environment and click Actions -> terminate environment . After some time it will completely disappear. Also, you might want to disable building the project on Travis CI side, since you only have 100 free builds before you need to start paying.","title":"Environment cleanup"},{"location":"Docker%20%26%20Kubernetes/07_ci_and_deployments_with_aws/06_environment_cleanup/#environment-cleanup","text":"At the end of this project you might want to delete the AWS environment in order not to get billed for it. You can open up the environment and click Actions -> terminate environment . After some time it will completely disappear. Also, you might want to disable building the project on Travis CI side, since you only have 100 free builds before you need to start paying.","title":"Environment cleanup"},{"location":"Docker%20%26%20Kubernetes/08_building_a_multicontainer_application/01_single_container_deployment_issues/","text":"Single Container Deployment Issues \u00b6 In the previous section, there were a couple issues that were not addressed: 1. The app was simple - no outside dependencies 2. Our image was build multiple times 3. We did not use any databases In this section we'll make a multi-container application that will use multiple databases, that is tied together with docker-compose and deployed to AWS ELB.","title":"Single Container Deployment Issues"},{"location":"Docker%20%26%20Kubernetes/08_building_a_multicontainer_application/01_single_container_deployment_issues/#single-container-deployment-issues","text":"In the previous section, there were a couple issues that were not addressed: 1. The app was simple - no outside dependencies 2. Our image was build multiple times 3. We did not use any databases In this section we'll make a multi-container application that will use multiple databases, that is tied together with docker-compose and deployed to AWS ELB.","title":"Single Container Deployment Issues"},{"location":"Docker%20%26%20Kubernetes/08_building_a_multicontainer_application/02_application_architecture/","text":"Application architecture \u00b6 We will create an application for fibonacci calculator. It will consist of multiple containers: - Nginx - routing, decides whether browser wants to access the react or express (API) server - React server - Express (API) server - Redis server - caches indices submitted. - Worker - watches redis for new indicies, whenever a new value shows up - calculates and puts back into redis. - Postgres server - stores indices submitted.","title":"Application architecture"},{"location":"Docker%20%26%20Kubernetes/08_building_a_multicontainer_application/02_application_architecture/#application-architecture","text":"We will create an application for fibonacci calculator. It will consist of multiple containers: - Nginx - routing, decides whether browser wants to access the react or express (API) server - React server - Express (API) server - Redis server - caches indices submitted. - Worker - watches redis for new indicies, whenever a new value shows up - calculates and puts back into redis. - Postgres server - stores indices submitted.","title":"Application architecture"},{"location":"Docker%20%26%20Kubernetes/08_building_a_multicontainer_application/03_worker_process_setup/","text":"Worker process setup \u00b6 We'll create a new project directory and a worker directory in it: davis@davis-arch \ue0b0 ~/projects/docker \ue0b0 \ue0a0 master \ue0b0 mkdir 06_building_a_multicontainer_application davis@davis-arch \ue0b0 ~/projects/docker \ue0b0 \ue0a0 master \ue0b0 cd 06_building_a_multicontainer_application davis@davis-arch \ue0b0 ~/projects/docker/06_building_a_multicontainer_application \ue0b0 \ue0a0 master \ue0b0 mkdir worker In the worker directory we create a package.json file: { \"dependencies\": { \"nodemon\": \"1.18.10\", \"redis\": \"2.8.0\" }, \"scripts\": { \"start\": \"node index.js\", \"dev\": \"nodemon\" } } And index.js file: const keys = require('./keys'); const redis = require('redis'); const redisClient = redis.createClient({ host: keys.redisHost, port: keys.redisPort, retry_strategy: () => 1000 // automatically reconnect once every 1 second if connection lost }); const sub = redisClient.duplicate(); function fib(index) { if( index < 2) return 1; return fib(index - 1) + fib(index - 2); } sub.on('message', (channel, message) => { redisClient.hset('values', message, fib(parseInt(message))); }); sub.subscribe('insert'); // subscribe to insert messages And keys.js file: module.exports = { redisHost: process.env.REDIS_HOST, redisPort: process.env.REDIS_PORT }","title":"Worker process setup"},{"location":"Docker%20%26%20Kubernetes/08_building_a_multicontainer_application/03_worker_process_setup/#worker-process-setup","text":"We'll create a new project directory and a worker directory in it: davis@davis-arch \ue0b0 ~/projects/docker \ue0b0 \ue0a0 master \ue0b0 mkdir 06_building_a_multicontainer_application davis@davis-arch \ue0b0 ~/projects/docker \ue0b0 \ue0a0 master \ue0b0 cd 06_building_a_multicontainer_application davis@davis-arch \ue0b0 ~/projects/docker/06_building_a_multicontainer_application \ue0b0 \ue0a0 master \ue0b0 mkdir worker In the worker directory we create a package.json file: { \"dependencies\": { \"nodemon\": \"1.18.10\", \"redis\": \"2.8.0\" }, \"scripts\": { \"start\": \"node index.js\", \"dev\": \"nodemon\" } } And index.js file: const keys = require('./keys'); const redis = require('redis'); const redisClient = redis.createClient({ host: keys.redisHost, port: keys.redisPort, retry_strategy: () => 1000 // automatically reconnect once every 1 second if connection lost }); const sub = redisClient.duplicate(); function fib(index) { if( index < 2) return 1; return fib(index - 1) + fib(index - 2); } sub.on('message', (channel, message) => { redisClient.hset('values', message, fib(parseInt(message))); }); sub.subscribe('insert'); // subscribe to insert messages And keys.js file: module.exports = { redisHost: process.env.REDIS_HOST, redisPort: process.env.REDIS_PORT }","title":"Worker process setup"},{"location":"Docker%20%26%20Kubernetes/08_building_a_multicontainer_application/04_express_api_setup/","text":"Express API setup \u00b6 We'll make another directory in the project directory. davis@davis-arch \ue0b0 ~/projects/docker/06_building_a_multicontainer_application \ue0b0 \ue0a0 master \ue0b0 mkdir server davis@davis-arch \ue0b0 ~/projects/docker/06_building_a_multicontainer_application \ue0b0 \ue0a0 master \ue0b0 cd server davis@davis-arch \ue0b0 ~/projects/docker/06_building_a_multicontainer_application/server \ue0b0 \ue0a0 master \ue0b0 touch package.json davis@davis-arch \ue0b0 ~/projects/docker/06_building_a_multicontainer_application/server \ue0b0 \ue0a0 master \ue0b0 touch index.js davis@davis-arch \ue0b0 ~/projects/docker/06_building_a_multicontainer_application/server \ue0b0 \ue0a0 master \ue0b0 touch keys.js The package.json { \"dependencies\": { \"express\": \"4.16.4\", \"pg\": \"7.8.0\", \"redis\": \"2.8.0\", \"cors\":\"2.8.5\", \"nodemon\": \"1.18.10\" }, \"scripts\": { \"dev\": \"nodemon\", \"start\": \"node index.js\" } } The keys.js module.exports = { redisHost: process.env.REDIS_HOST, redisPort: process.env.REDIS_PORT, pgHost: process.env.PGHOST, pgPort: process.env.PGPORT, pgDatabase: process.env.PGDATABASE, pgUser: process.env.PGUSER, pgPassword: process.env.PGPASSWORD, } The index.js const keys = require('./keys'); // Express App Setup const express = require('express'); const bodyParser = require('body-parser'); const cors = require('cors'); const app = express(); app.use(cors()); app.use(bodyParser.json()); // Postgres Client Setup const { Pool } = require('pg'); const pgClient = new Pool({ user: keys.pgUser, password: keys.pgPassword, database: keys.pgDatabase, host: keys.pgHost, port: keys.pgPort }); pgClient.on('error', () => console.log('Lost PG connection')); pgClient.query('CREATE TABLE IF NOT EXISTS values (number INT);').catch((err) => console.log(err)); // Redis Client Setup const redis = require('redis'); const redisClient = redis.createClient({ host: keys.redisHost, port: keys.redisPort, retry_strategy: () => 1000, // If connection lost, reconnect once every second }); const redisPublisher = redisClient.duplicate(); // Express route handlers app.get('/', (req, res) => { res.send('Hi'); }); app.get('/values/all', async (req ,res) => { const values = await pgClient.query('SELECT * FROM values'); res.send(values.rows); }); app.get('/values/current', async (req, res) => { redisClient.hgetall('values', (err, values) => { res.send(values); }) ; }); app.post('/values', async (req, res) => { const index = req.body.index; if( parseInt(index) > 40 ) { return res.status(422).send('Index too high'); } redisClient.hset('values', index, 'Nothing yet!'); redisPublisher.publish('insert', index); pgClient.query('INSERT INTO values(number) VALUES($1)', [index]); res.send({working: true}); }); app.listen(5000, err => { console.log('Listening'); })","title":"Express API setup"},{"location":"Docker%20%26%20Kubernetes/08_building_a_multicontainer_application/04_express_api_setup/#express-api-setup","text":"We'll make another directory in the project directory. davis@davis-arch \ue0b0 ~/projects/docker/06_building_a_multicontainer_application \ue0b0 \ue0a0 master \ue0b0 mkdir server davis@davis-arch \ue0b0 ~/projects/docker/06_building_a_multicontainer_application \ue0b0 \ue0a0 master \ue0b0 cd server davis@davis-arch \ue0b0 ~/projects/docker/06_building_a_multicontainer_application/server \ue0b0 \ue0a0 master \ue0b0 touch package.json davis@davis-arch \ue0b0 ~/projects/docker/06_building_a_multicontainer_application/server \ue0b0 \ue0a0 master \ue0b0 touch index.js davis@davis-arch \ue0b0 ~/projects/docker/06_building_a_multicontainer_application/server \ue0b0 \ue0a0 master \ue0b0 touch keys.js The package.json { \"dependencies\": { \"express\": \"4.16.4\", \"pg\": \"7.8.0\", \"redis\": \"2.8.0\", \"cors\":\"2.8.5\", \"nodemon\": \"1.18.10\" }, \"scripts\": { \"dev\": \"nodemon\", \"start\": \"node index.js\" } } The keys.js module.exports = { redisHost: process.env.REDIS_HOST, redisPort: process.env.REDIS_PORT, pgHost: process.env.PGHOST, pgPort: process.env.PGPORT, pgDatabase: process.env.PGDATABASE, pgUser: process.env.PGUSER, pgPassword: process.env.PGPASSWORD, } The index.js const keys = require('./keys'); // Express App Setup const express = require('express'); const bodyParser = require('body-parser'); const cors = require('cors'); const app = express(); app.use(cors()); app.use(bodyParser.json()); // Postgres Client Setup const { Pool } = require('pg'); const pgClient = new Pool({ user: keys.pgUser, password: keys.pgPassword, database: keys.pgDatabase, host: keys.pgHost, port: keys.pgPort }); pgClient.on('error', () => console.log('Lost PG connection')); pgClient.query('CREATE TABLE IF NOT EXISTS values (number INT);').catch((err) => console.log(err)); // Redis Client Setup const redis = require('redis'); const redisClient = redis.createClient({ host: keys.redisHost, port: keys.redisPort, retry_strategy: () => 1000, // If connection lost, reconnect once every second }); const redisPublisher = redisClient.duplicate(); // Express route handlers app.get('/', (req, res) => { res.send('Hi'); }); app.get('/values/all', async (req ,res) => { const values = await pgClient.query('SELECT * FROM values'); res.send(values.rows); }); app.get('/values/current', async (req, res) => { redisClient.hgetall('values', (err, values) => { res.send(values); }) ; }); app.post('/values', async (req, res) => { const index = req.body.index; if( parseInt(index) > 40 ) { return res.status(422).send('Index too high'); } redisClient.hset('values', index, 'Nothing yet!'); redisPublisher.publish('insert', index); pgClient.query('INSERT INTO values(number) VALUES($1)', [index]); res.send({working: true}); }); app.listen(5000, err => { console.log('Listening'); })","title":"Express API setup"},{"location":"Docker%20%26%20Kubernetes/08_building_a_multicontainer_application/05_generating_the_react_app/","text":"Generating the React app \u00b6 davis@davis-arch \ue0b0 ~/projects/docker/06_building_a_multicontainer_application \ue0b0 \ue0a0 master \ue0b0 create-react-app client Creating a new React app in /home/davis/projects/docker/06_building_a_multicontainer_application/client. Installing packages. This might take a couple of minutes. Installing react, react-dom, and react-scripts... + react-dom@16.8.2 + react@16.8.2 + react-scripts@2.1.5 added 1847 packages from 724 contributors and audited 36349 packages in 57.322s found 63 low severity vulnerabilities run `npm audit fix` to fix them, or `npm audit` for details Success! Created client at /home/davis/projects/docker/06_building_a_multicontainer_application/client Inside that directory, you can run several commands: npm start Starts the development server. npm run build Bundles the app into static files for production. npm test Starts the test runner. npm run eject Removes this tool and copies build dependencies, configuration files and scripts into the app directory. If you do this, you can\u2019t go back! We suggest that you begin by typing: cd client npm start Happy hacking! Modify package.json { \"name\": \"client\", \"version\": \"0.1.0\", \"private\": true, \"dependencies\": { \"react\": \"^16.8.2\", \"react-dom\": \"^16.8.2\", \"react-scripts\": \"2.1.5\", \"react-router-dom\": \"4.3.1\", \"axios\": \"0.18.0\" }, \"scripts\": { \"start\": \"react-scripts start\", \"build\": \"react-scripts build\", \"test\": \"react-scripts test\", \"eject\": \"react-scripts eject\" }, \"eslintConfig\": { \"extends\": \"react-app\" }, \"browserslist\": [ \">0.2%\", \"not dead\", \"not ie <= 11\", \"not op_mini all\" ] } Create src/OtherPage.js : import React from 'react'; import {Link} from 'react-router-dom'; export default () => { return ( <div> Im some other page! <link to=\"/\">Go back home</link> </div> ) } Create src/Fib.js : import React, {Component} from 'react'; import axios from 'axios'; class Fib extends Component { state = { seenIndexes: [], values: {}, index: '' }; componentDidMount() { this.fetchValues(); this.fetchIndexes(); } async fetchValues() { const values = await axios.get('/api/values/current'); this.setState({values: values.data}); } async fetchIndexes() { const seenIndexes = await axios.get('/api/values/all'); this.setState({seenndexes: seenIndexes.data}); } handleSubmit = async (event) => { event.preventDefault(); await axios.post('/api/values', { index: this.state.index }); this.setState({index: ''}) } renderSeenIndexes() { return this.state.seenIndexes.map( ({number}) => number).join(', '); } renderValues() { const entries = []; for ( let key in this.state.values ) { entries.push( <div key={key}> For index {key} I calculated {this.state.values[key]} </div> ) } } render() { return ( <div> <form onSubmit={this.handleSubmit}> <label >Enter your index:</label> <input value={this.state.index} onChange={event => this.setState({index: event.target.value})} /> <button>Submit</button> </form> <h3>Indexes I have seen</h3> { this.renderSeenIndexes() } <h3>Calculated values:</h3> { this.renderValues() } </div> ) } } export default Fib; App.js import React, { Component } from 'react'; import logo from './logo.svg'; import './App.css'; import {BrowserRouter as Router, Route, Link} from 'react-router-dom'; import OtherPage from './OtherPage'; import Fib from './Fib'; class App extends Component { render() { return ( <Router> <div className=\"App\"> <header className=\"App-header\"> <img src={logo} className=\"App-logo\" alt=\"logo\" /> <Link to=\"/\">Home</Link> <Link to=\"/otherpage\">Other Page</Link> </header> <div> <Route exact path = \"/\" component={Fib}/> <Route path=\"/otherpage\" component={OtherPage} />} </div> </div> </Router> ); } } export default App;","title":"Generating the React app"},{"location":"Docker%20%26%20Kubernetes/08_building_a_multicontainer_application/05_generating_the_react_app/#generating-the-react-app","text":"davis@davis-arch \ue0b0 ~/projects/docker/06_building_a_multicontainer_application \ue0b0 \ue0a0 master \ue0b0 create-react-app client Creating a new React app in /home/davis/projects/docker/06_building_a_multicontainer_application/client. Installing packages. This might take a couple of minutes. Installing react, react-dom, and react-scripts... + react-dom@16.8.2 + react@16.8.2 + react-scripts@2.1.5 added 1847 packages from 724 contributors and audited 36349 packages in 57.322s found 63 low severity vulnerabilities run `npm audit fix` to fix them, or `npm audit` for details Success! Created client at /home/davis/projects/docker/06_building_a_multicontainer_application/client Inside that directory, you can run several commands: npm start Starts the development server. npm run build Bundles the app into static files for production. npm test Starts the test runner. npm run eject Removes this tool and copies build dependencies, configuration files and scripts into the app directory. If you do this, you can\u2019t go back! We suggest that you begin by typing: cd client npm start Happy hacking! Modify package.json { \"name\": \"client\", \"version\": \"0.1.0\", \"private\": true, \"dependencies\": { \"react\": \"^16.8.2\", \"react-dom\": \"^16.8.2\", \"react-scripts\": \"2.1.5\", \"react-router-dom\": \"4.3.1\", \"axios\": \"0.18.0\" }, \"scripts\": { \"start\": \"react-scripts start\", \"build\": \"react-scripts build\", \"test\": \"react-scripts test\", \"eject\": \"react-scripts eject\" }, \"eslintConfig\": { \"extends\": \"react-app\" }, \"browserslist\": [ \">0.2%\", \"not dead\", \"not ie <= 11\", \"not op_mini all\" ] } Create src/OtherPage.js : import React from 'react'; import {Link} from 'react-router-dom'; export default () => { return ( <div> Im some other page! <link to=\"/\">Go back home</link> </div> ) } Create src/Fib.js : import React, {Component} from 'react'; import axios from 'axios'; class Fib extends Component { state = { seenIndexes: [], values: {}, index: '' }; componentDidMount() { this.fetchValues(); this.fetchIndexes(); } async fetchValues() { const values = await axios.get('/api/values/current'); this.setState({values: values.data}); } async fetchIndexes() { const seenIndexes = await axios.get('/api/values/all'); this.setState({seenndexes: seenIndexes.data}); } handleSubmit = async (event) => { event.preventDefault(); await axios.post('/api/values', { index: this.state.index }); this.setState({index: ''}) } renderSeenIndexes() { return this.state.seenIndexes.map( ({number}) => number).join(', '); } renderValues() { const entries = []; for ( let key in this.state.values ) { entries.push( <div key={key}> For index {key} I calculated {this.state.values[key]} </div> ) } } render() { return ( <div> <form onSubmit={this.handleSubmit}> <label >Enter your index:</label> <input value={this.state.index} onChange={event => this.setState({index: event.target.value})} /> <button>Submit</button> </form> <h3>Indexes I have seen</h3> { this.renderSeenIndexes() } <h3>Calculated values:</h3> { this.renderValues() } </div> ) } } export default Fib; App.js import React, { Component } from 'react'; import logo from './logo.svg'; import './App.css'; import {BrowserRouter as Router, Route, Link} from 'react-router-dom'; import OtherPage from './OtherPage'; import Fib from './Fib'; class App extends Component { render() { return ( <Router> <div className=\"App\"> <header className=\"App-header\"> <img src={logo} className=\"App-logo\" alt=\"logo\" /> <Link to=\"/\">Home</Link> <Link to=\"/otherpage\">Other Page</Link> </header> <div> <Route exact path = \"/\" component={Fib}/> <Route path=\"/otherpage\" component={OtherPage} />} </div> </div> </Router> ); } } export default App;","title":"Generating the React app"},{"location":"Docker%20%26%20Kubernetes/09_dockerizing_multiple_services/01_dockerizing_react_app/","text":"Dockerizing React app \u00b6 We'll dockerize the react app that we created in previous section. davis@davis-arch \ue0b0 ~/projects/docker/06_building_a_multicontainer_application \ue0b0 \ue0a0 master \ue0b0 touch client/Dockerfile.dev FROM node:alpine WORKDIR '/app' COPY ./package.json ./ RUN npm install COPY . . CMD [\"npm\", \"run\", \"start\"] davis@davis-arch \ue0b0 ~/projects/docker/06_building_a_multicontainer_application \ue0b0 \ue0a0 master \ue0b0 cd client davis@davis-arch \ue0b0 ~/projects/docker/06_building_a_multicontainer_application/client \ue0b0 \ue0a0 master \ue0b0 docker build -f Dockerfile.dev . Sending build context to Docker daemon 233.9MB Step 1/6 : FROM node:alpine ---> ebbf98230a82 Step 2/6 : WORKDIR '/app' ---> Using cache ---> 93b2648262c0 Step 3/6 : COPY ./package.json ./ ---> 89997607ce86 Step 4/6 : RUN npm install ---> Running in 61f78a9feb95 npm WARN deprecated kleur@2.0.2: Please upgrade to kleur@3 or migrate to 'ansi-colors' if you prefer the old syntax. Visit <https://github.com/lukeed/kleur/releases/tag/v3.0.0\\> for migration path(s). npm WARN deprecated circular-json@0.3.3: CircularJSON is in maintenance only, flatted is its successor. npm notice created a lockfile as package-lock.json. You should commit this file. npm WARN ts-pnp@1.0.0 requires a peer of typescript@* but none is installed. You must install peer dependencies yourself. npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules/fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"}) npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.7 (node_modules/chokidar/node_modules/fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.7: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"}) added 1853 packages from 729 contributors and audited 36407 packages in 222.036s found 63 low severity vulnerabilities run `npm audit fix` to fix them, or `npm audit` for details Removing intermediate container 61f78a9feb95 ---> c323bbd27a3d Step 5/6 : COPY . . ---> d8a0e0b9092f Step 6/6 : CMD [\"npm\", \"run\", \"start\"] ---> Running in 6a35d272baca Removing intermediate container 6a35d272baca ---> 0cfda483865f Successfully built 0cfda483865f","title":"Dockerizing React app"},{"location":"Docker%20%26%20Kubernetes/09_dockerizing_multiple_services/01_dockerizing_react_app/#dockerizing-react-app","text":"We'll dockerize the react app that we created in previous section. davis@davis-arch \ue0b0 ~/projects/docker/06_building_a_multicontainer_application \ue0b0 \ue0a0 master \ue0b0 touch client/Dockerfile.dev FROM node:alpine WORKDIR '/app' COPY ./package.json ./ RUN npm install COPY . . CMD [\"npm\", \"run\", \"start\"] davis@davis-arch \ue0b0 ~/projects/docker/06_building_a_multicontainer_application \ue0b0 \ue0a0 master \ue0b0 cd client davis@davis-arch \ue0b0 ~/projects/docker/06_building_a_multicontainer_application/client \ue0b0 \ue0a0 master \ue0b0 docker build -f Dockerfile.dev . Sending build context to Docker daemon 233.9MB Step 1/6 : FROM node:alpine ---> ebbf98230a82 Step 2/6 : WORKDIR '/app' ---> Using cache ---> 93b2648262c0 Step 3/6 : COPY ./package.json ./ ---> 89997607ce86 Step 4/6 : RUN npm install ---> Running in 61f78a9feb95 npm WARN deprecated kleur@2.0.2: Please upgrade to kleur@3 or migrate to 'ansi-colors' if you prefer the old syntax. Visit <https://github.com/lukeed/kleur/releases/tag/v3.0.0\\> for migration path(s). npm WARN deprecated circular-json@0.3.3: CircularJSON is in maintenance only, flatted is its successor. npm notice created a lockfile as package-lock.json. You should commit this file. npm WARN ts-pnp@1.0.0 requires a peer of typescript@* but none is installed. You must install peer dependencies yourself. npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules/fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"}) npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.7 (node_modules/chokidar/node_modules/fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.7: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"}) added 1853 packages from 729 contributors and audited 36407 packages in 222.036s found 63 low severity vulnerabilities run `npm audit fix` to fix them, or `npm audit` for details Removing intermediate container 61f78a9feb95 ---> c323bbd27a3d Step 5/6 : COPY . . ---> d8a0e0b9092f Step 6/6 : CMD [\"npm\", \"run\", \"start\"] ---> Running in 6a35d272baca Removing intermediate container 6a35d272baca ---> 0cfda483865f Successfully built 0cfda483865f","title":"Dockerizing React app"},{"location":"Docker%20%26%20Kubernetes/09_dockerizing_multiple_services/02_dockerizing_generic_node_apps/","text":"Dockerizing generic node apps \u00b6 davis@davis-arch \ue0b0 ~/projects/docker/06_building_a_multicontainer_application \ue0b0 \ue0a0 master \ue0b0 touch server/Dockerfile.dev davis@davis-arch \ue0b0 ~/projects/docker/06_building_a_multicontainer_application \ue0b0 \ue0a0 master \ue0b0 touch worker/Dockerfile.dev For both Dockerfile.dev : FROM node:alpine WORKDIR \"/app\" COPY ./package.json ./ RUN npm install COPY . . CMD [\"npm\", \"run\", \"dev\"]","title":"Dockerizing generic node apps"},{"location":"Docker%20%26%20Kubernetes/09_dockerizing_multiple_services/02_dockerizing_generic_node_apps/#dockerizing-generic-node-apps","text":"davis@davis-arch \ue0b0 ~/projects/docker/06_building_a_multicontainer_application \ue0b0 \ue0a0 master \ue0b0 touch server/Dockerfile.dev davis@davis-arch \ue0b0 ~/projects/docker/06_building_a_multicontainer_application \ue0b0 \ue0a0 master \ue0b0 touch worker/Dockerfile.dev For both Dockerfile.dev : FROM node:alpine WORKDIR \"/app\" COPY ./package.json ./ RUN npm install COPY . . CMD [\"npm\", \"run\", \"dev\"]","title":"Dockerizing generic node apps"},{"location":"Docker%20%26%20Kubernetes/09_dockerizing_multiple_services/03_nginx_routing/","text":"Nginx routing \u00b6 Create a new service directory in the project called nginx and add a default.conf file: upstream client { server client: 3000; } upstream api { server api:5000; } server { listen 80; location / { proxy_pass http://client; } location /api { rewrite /api/(.*) /$1 break; proxy_pass http://api; } } Create Dockerfile.dev : FROM nginx COPY ./default.conf /etc/nginx/conf.d/default.conf","title":"Nginx routing"},{"location":"Docker%20%26%20Kubernetes/09_dockerizing_multiple_services/03_nginx_routing/#nginx-routing","text":"Create a new service directory in the project called nginx and add a default.conf file: upstream client { server client: 3000; } upstream api { server api:5000; } server { listen 80; location / { proxy_pass http://client; } location /api { rewrite /api/(.*) /$1 break; proxy_pass http://api; } } Create Dockerfile.dev : FROM nginx COPY ./default.conf /etc/nginx/conf.d/default.conf","title":"Nginx routing"},{"location":"Docker%20%26%20Kubernetes/09_dockerizing_multiple_services/04_docker_compose_config/","text":"Docker compose config \u00b6 We'll add the docker-compose.yml file in the project directory. version: '3' services: postgres: image: 'postgres:latest' redis: image: 'redis:latest' nginx: restart: always build: dockerfile: Dockerfile.dev context: ./nginx ports: - \"3050:80\" api: build: dockerfile: Dockerfile.dev context: ./server volumes: - /app/node_modules - ./server:/app environment: - REDIS_HOST=redis - REDIS_PORT=6379 - PGUSER=postgres - PGHOST=postgres - PGDATABASE=postgres - PGPASSWORD=postgres_password - PGPORT=5432 client: build: dockerfile: Dockerfile.dev context: ./client volumes: - /app/node_modules - ./client:/app worker: build: dockerfile: Dockerfile.dev context: ./worker volumes: - /app/node_modules - ./worker:/app environment: - REDIS_HOST=redis - REDIS_PORT=6379 And now, when starting docker-compose up --build we'll rebuild all the images and start the service on port 3050 . When visiting the http://localhost:3050 :","title":"Docker compose config"},{"location":"Docker%20%26%20Kubernetes/09_dockerizing_multiple_services/04_docker_compose_config/#docker-compose-config","text":"We'll add the docker-compose.yml file in the project directory. version: '3' services: postgres: image: 'postgres:latest' redis: image: 'redis:latest' nginx: restart: always build: dockerfile: Dockerfile.dev context: ./nginx ports: - \"3050:80\" api: build: dockerfile: Dockerfile.dev context: ./server volumes: - /app/node_modules - ./server:/app environment: - REDIS_HOST=redis - REDIS_PORT=6379 - PGUSER=postgres - PGHOST=postgres - PGDATABASE=postgres - PGPASSWORD=postgres_password - PGPORT=5432 client: build: dockerfile: Dockerfile.dev context: ./client volumes: - /app/node_modules - ./client:/app worker: build: dockerfile: Dockerfile.dev context: ./worker volumes: - /app/node_modules - ./worker:/app environment: - REDIS_HOST=redis - REDIS_PORT=6379 And now, when starting docker-compose up --build we'll rebuild all the images and start the service on port 3050 . When visiting the http://localhost:3050 :","title":"Docker compose config"},{"location":"Docker%20%26%20Kubernetes/09_dockerizing_multiple_services/05_opening_websocket_connections/","text":"Opening websocket connections \u00b6 React development server requires websocket connection, but our nginx does not allow it. We'll modify the default.conf file to allow it: upstream client { server client:3000; } upstream api { server api:5000; } server { listen 80; location / { proxy_pass http://client; } location /sockjs-node { proxy_pass http://client; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"Upgrade\"; } location /api { rewrite /api/(.*) /$1 break; proxy_pass http://api; } } Now we can run docker-compose up --build .","title":"Opening websocket connections"},{"location":"Docker%20%26%20Kubernetes/09_dockerizing_multiple_services/05_opening_websocket_connections/#opening-websocket-connections","text":"React development server requires websocket connection, but our nginx does not allow it. We'll modify the default.conf file to allow it: upstream client { server client:3000; } upstream api { server api:5000; } server { listen 80; location / { proxy_pass http://client; } location /sockjs-node { proxy_pass http://client; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"Upgrade\"; } location /api { rewrite /api/(.*) /$1 break; proxy_pass http://api; } } Now we can run docker-compose up --build .","title":"Opening websocket connections"},{"location":"Docker%20%26%20Kubernetes/10_CI_integration_workflow_for_multiple_images/01_production_multicontainer_deployments/","text":"Production multi-container deployments \u00b6 Previously on our single container setup we were: 1. Pushing code to github 2. TravisCI automatically pulled repo 3. TravisCI built an image, tested the code 4. TravisCI pushed the code to AWS ELB 5. ELB built image, deployed it In our multi-container setup: 1. We push to github 2. TravisCI automatically pulls repo 3. Travis CI builds a test image, tests the code 4. TravisCI builds prod images 5. TravisCI pushes built prod images to docker hub 6. Travis pushes project to AWS ELB 7. ELB pulls images from Docker Hub, deploys","title":"Production multi-container deployments"},{"location":"Docker%20%26%20Kubernetes/10_CI_integration_workflow_for_multiple_images/01_production_multicontainer_deployments/#production-multi-container-deployments","text":"Previously on our single container setup we were: 1. Pushing code to github 2. TravisCI automatically pulled repo 3. TravisCI built an image, tested the code 4. TravisCI pushed the code to AWS ELB 5. ELB built image, deployed it In our multi-container setup: 1. We push to github 2. TravisCI automatically pulls repo 3. Travis CI builds a test image, tests the code 4. TravisCI builds prod images 5. TravisCI pushes built prod images to docker hub 6. Travis pushes project to AWS ELB 7. ELB pulls images from Docker Hub, deploys","title":"Production multi-container deployments"},{"location":"Docker%20%26%20Kubernetes/10_CI_integration_workflow_for_multiple_images/02_production_dockerfiles/","text":"Production dockerfiles \u00b6 worker/Dockerfile and server/Dockerfile : FROM node:alpine WORKDIR \"/app\" COPY ./package.json ./ RUN npm install COPY . . CMD [\"node\", \"run\", \"start\"] nginx/Dockerfile : FROM nginx COPY ./default.conf /etc/nginx/conf.d/default.conf client/Dockerfile : FROM node:alpine as builder WORKDIR '/app' COPY package*.json ./ RUN npm install COPY . . RUN npm run build FROM nginx EXPOSE 3000 COPY ./nginx/default.conf /etc/nginx/conf.d/default.conf COPY --from=builder /app/build /usr/share/nginx/html client/nginx/default.conf server { listen 3000; location / { root /usr/share/nginx/html; index index.html index.htm; try_files $uri $uri/ /index.html; } }","title":"Production dockerfiles"},{"location":"Docker%20%26%20Kubernetes/10_CI_integration_workflow_for_multiple_images/02_production_dockerfiles/#production-dockerfiles","text":"worker/Dockerfile and server/Dockerfile : FROM node:alpine WORKDIR \"/app\" COPY ./package.json ./ RUN npm install COPY . . CMD [\"node\", \"run\", \"start\"] nginx/Dockerfile : FROM nginx COPY ./default.conf /etc/nginx/conf.d/default.conf client/Dockerfile : FROM node:alpine as builder WORKDIR '/app' COPY package*.json ./ RUN npm install COPY . . RUN npm run build FROM nginx EXPOSE 3000 COPY ./nginx/default.conf /etc/nginx/conf.d/default.conf COPY --from=builder /app/build /usr/share/nginx/html client/nginx/default.conf server { listen 3000; location / { root /usr/share/nginx/html; index index.html index.htm; try_files $uri $uri/ /index.html; } }","title":"Production dockerfiles"},{"location":"Docker%20%26%20Kubernetes/10_CI_integration_workflow_for_multiple_images/03_cleaning_up_tests/","text":"Cleaning up tests \u00b6 The App.test.js file runs a test that when creating the component, nothing will crash. Since at the test time it calls the Fib component which makes api calls, it most likely will crash, so we'll remove this test: import React from 'react'; import ReactDOM from 'react-dom'; import App from './App'; it('renders without crashing', () => { });","title":"Cleaning up tests"},{"location":"Docker%20%26%20Kubernetes/10_CI_integration_workflow_for_multiple_images/03_cleaning_up_tests/#cleaning-up-tests","text":"The App.test.js file runs a test that when creating the component, nothing will crash. Since at the test time it calls the Fib component which makes api calls, it most likely will crash, so we'll remove this test: import React from 'react'; import ReactDOM from 'react-dom'; import App from './App'; it('renders without crashing', () => { });","title":"Cleaning up tests"},{"location":"Docker%20%26%20Kubernetes/10_CI_integration_workflow_for_multiple_images/04_github_and_travis_ci/","text":"Github and travis CI \u00b6 Now we push the project to the GitHub. In order to do so, we will create a new repository at github called multi-docker and run following commands: cd ~/projects/learning/docker_06/building_a_multicontainer_application git init git add . git commit -m \"first commit\" git remote add origin git@github.com:daviskregers/multi-docker.git git push -u origin master Now that the project has been pushed to GitHub, we will go to TravisCI. Because the repository is freshly created, we'll press the Sync account button in the Settings section of TravisCI. Not you should see it in GitHub Apps Integration section. If not, then check the Manage repositories on GitHub section. We'll click on the settings for multi-docker , now it should be visible in the left side build watcher.","title":"Github and travis CI"},{"location":"Docker%20%26%20Kubernetes/10_CI_integration_workflow_for_multiple_images/04_github_and_travis_ci/#github-and-travis-ci","text":"Now we push the project to the GitHub. In order to do so, we will create a new repository at github called multi-docker and run following commands: cd ~/projects/learning/docker_06/building_a_multicontainer_application git init git add . git commit -m \"first commit\" git remote add origin git@github.com:daviskregers/multi-docker.git git push -u origin master Now that the project has been pushed to GitHub, we will go to TravisCI. Because the repository is freshly created, we'll press the Sync account button in the Settings section of TravisCI. Not you should see it in GitHub Apps Integration section. If not, then check the Manage repositories on GitHub section. We'll click on the settings for multi-docker , now it should be visible in the left side build watcher.","title":"Github and travis CI"},{"location":"Docker%20%26%20Kubernetes/10_CI_integration_workflow_for_multiple_images/05_travis_configuration/","text":"Travis CI Configuration \u00b6 Previously we created a github repository and added it to travis ci for building. But since there is no actual travis configuration, nothing will be ran. So, we'll create configuration that: 1. Specifies docker as a dependency 2. Builds test version of react projkect 3. Runs tests 4. Build prod versions of all projects 5. Pushes all containers to docker hub 6. Tells Elastic Beanstalk to update Because pushing to docker hub will require credentials, we'll go to multi-docker -> settings in TravisCI and set up environment variables. Not we'll create .travis.yml in the project: sudo: required services: - docker before_install: # bulid tests - docker build -t $DOCKER_ID/react-test -f ./client/Dockerfile.dev ./client script: # run tests - docker run $DOCKER_ID/react-test npm test -- --coverage after_success: # build production images - docker build -t $DOCKER_ID/multi-client ./client - docker build -t $DOCKER_ID/multi-nginx ./nginx - docker build -t $DOCKER_ID/multi-server ./server - docker build -t $DOCKER_ID/multi-worker ./worker # Login to docker CLI - echo \"$DOCKER_PASSWORD\" | docker login -u \"$DOCKER_ID\" --password-stdin # Take images and push to docker hub - docker push $DOCKER_ID/multi-client - docker push $DOCKER_ID/multi-nginx - docker push $DOCKER_ID/multi-server - docker push $DOCKER_ID/multi-worker And push it to github. Now we can see that Travis CI is building the project: After some time it finishes with success: And the images have been pushed to docker hub","title":"Travis CI Configuration"},{"location":"Docker%20%26%20Kubernetes/10_CI_integration_workflow_for_multiple_images/05_travis_configuration/#travis-ci-configuration","text":"Previously we created a github repository and added it to travis ci for building. But since there is no actual travis configuration, nothing will be ran. So, we'll create configuration that: 1. Specifies docker as a dependency 2. Builds test version of react projkect 3. Runs tests 4. Build prod versions of all projects 5. Pushes all containers to docker hub 6. Tells Elastic Beanstalk to update Because pushing to docker hub will require credentials, we'll go to multi-docker -> settings in TravisCI and set up environment variables. Not we'll create .travis.yml in the project: sudo: required services: - docker before_install: # bulid tests - docker build -t $DOCKER_ID/react-test -f ./client/Dockerfile.dev ./client script: # run tests - docker run $DOCKER_ID/react-test npm test -- --coverage after_success: # build production images - docker build -t $DOCKER_ID/multi-client ./client - docker build -t $DOCKER_ID/multi-nginx ./nginx - docker build -t $DOCKER_ID/multi-server ./server - docker build -t $DOCKER_ID/multi-worker ./worker # Login to docker CLI - echo \"$DOCKER_PASSWORD\" | docker login -u \"$DOCKER_ID\" --password-stdin # Take images and push to docker hub - docker push $DOCKER_ID/multi-client - docker push $DOCKER_ID/multi-nginx - docker push $DOCKER_ID/multi-server - docker push $DOCKER_ID/multi-worker And push it to github. Now we can see that Travis CI is building the project: After some time it finishes with success: And the images have been pushed to docker hub","title":"Travis CI Configuration"},{"location":"Docker%20%26%20Kubernetes/11_multi-container_deployments_to_AWS/01_multi-container_definition_files/","text":"Multi-container definition files \u00b6 Previously when we deployed to AWS, we had a single Docker image to be ran. Now we have multiple images and we'll need to specify what exactly we need to do. We'll need to create a Dockerrun.aws.json file, where we'll make container definitions, which is similar to docker-compose.yml file. The Elastic Beanstalk doesn't really know on how to run containers itself, whenever it needs to do so, it delegates it to Amazon Elastic Container Service (ECS) . The Parameters on how to create the Dockerrun.aws.json file can be found here https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html . So, we'll create Dockerrun.aws.json file in the project directory: { \"AWSEBDockerrunVersion\": 2, \"containerDefinitions\": [ { \"name\": \"client\", \"image\": \"deiveris/multi-client\", \"hostname\": \"client\", \"essential\": false, }, { \"name\": \"server\", \"image\": \"deiveris/multi-server\", \"hostname\": \"api\", \"essential\": false }, { \"name\": \"worker\", \"image\": \"deiveris/multi-worker\", \"hostname\": \"worker\", \"essential\": false }, { \"name\": \"nginx\", \"image\": \"deiveris/multi-nginx\", \"hostname\": \"nginx\", \"essential\": true, \"portMappings\": [ { \"hostPort\": 80, \"containerPort\": 80 } ], \"links\": [\"client\", \"server\"] } ] } The essential marks the container to be essential, meaning when it crashes - all other containers are closed down as well. At least 1 container must be marked as essential.","title":"Multi-container definition files"},{"location":"Docker%20%26%20Kubernetes/11_multi-container_deployments_to_AWS/01_multi-container_definition_files/#multi-container-definition-files","text":"Previously when we deployed to AWS, we had a single Docker image to be ran. Now we have multiple images and we'll need to specify what exactly we need to do. We'll need to create a Dockerrun.aws.json file, where we'll make container definitions, which is similar to docker-compose.yml file. The Elastic Beanstalk doesn't really know on how to run containers itself, whenever it needs to do so, it delegates it to Amazon Elastic Container Service (ECS) . The Parameters on how to create the Dockerrun.aws.json file can be found here https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html . So, we'll create Dockerrun.aws.json file in the project directory: { \"AWSEBDockerrunVersion\": 2, \"containerDefinitions\": [ { \"name\": \"client\", \"image\": \"deiveris/multi-client\", \"hostname\": \"client\", \"essential\": false, }, { \"name\": \"server\", \"image\": \"deiveris/multi-server\", \"hostname\": \"api\", \"essential\": false }, { \"name\": \"worker\", \"image\": \"deiveris/multi-worker\", \"hostname\": \"worker\", \"essential\": false }, { \"name\": \"nginx\", \"image\": \"deiveris/multi-nginx\", \"hostname\": \"nginx\", \"essential\": true, \"portMappings\": [ { \"hostPort\": 80, \"containerPort\": 80 } ], \"links\": [\"client\", \"server\"] } ] } The essential marks the container to be essential, meaning when it crashes - all other containers are closed down as well. At least 1 container must be marked as essential.","title":"Multi-container definition files"},{"location":"Docker%20%26%20Kubernetes/11_multi-container_deployments_to_AWS/02_creating_the_EB_environment/","text":"Creating the EB environment \u00b6 Now that the Dockerrun.aws.json is set up, we'll need to create the Elastic Beanstalk environment. To do that we'll log into the AWS Management Console and go to the Services -> Elastic Beanstalk and click on the Create New Application . Now we'll create a new environment Select the Web server environemnt: In the next form, everything can stay as it is except for the Platform section. Select thge Multi-container Docker option. And click on the Create Environment . Now it will take some time to create the environment: When it's finished, you'll be redirected to a page like this: And when visiting the environment URL, you'll see the sample app.","title":"Creating the EB environment"},{"location":"Docker%20%26%20Kubernetes/11_multi-container_deployments_to_AWS/02_creating_the_EB_environment/#creating-the-eb-environment","text":"Now that the Dockerrun.aws.json is set up, we'll need to create the Elastic Beanstalk environment. To do that we'll log into the AWS Management Console and go to the Services -> Elastic Beanstalk and click on the Create New Application . Now we'll create a new environment Select the Web server environemnt: In the next form, everything can stay as it is except for the Platform section. Select thge Multi-container Docker option. And click on the Create Environment . Now it will take some time to create the environment: When it's finished, you'll be redirected to a page like this: And when visiting the environment URL, you'll see the sample app.","title":"Creating the EB environment"},{"location":"Docker%20%26%20Kubernetes/11_multi-container_deployments_to_AWS/03_managed_data_service_providers/","text":"Managed Data Service providers \u00b6 If you noticed, that docker-compose.yml file contains databases - redis and postgres , while Dockerrun.aws.json does not. This is made because it will use the AWS Elastic Cache and AWS Relational Database services. The reason behind using amazon services instead of dockerized images are: 1. Automatically creates and maintains instances for you 2. Easy to scale 3. Built-in logging and maintenance 4. Probably better security than what we can do 5. Easier to migrate off EB with 6. Automated backups and rollbacks","title":"Managed Data Service providers"},{"location":"Docker%20%26%20Kubernetes/11_multi-container_deployments_to_AWS/03_managed_data_service_providers/#managed-data-service-providers","text":"If you noticed, that docker-compose.yml file contains databases - redis and postgres , while Dockerrun.aws.json does not. This is made because it will use the AWS Elastic Cache and AWS Relational Database services. The reason behind using amazon services instead of dockerized images are: 1. Automatically creates and maintains instances for you 2. Easy to scale 3. Built-in logging and maintenance 4. Probably better security than what we can do 5. Easier to migrate off EB with 6. Automated backups and rollbacks","title":"Managed Data Service providers"},{"location":"Docker%20%26%20Kubernetes/11_multi-container_deployments_to_AWS/04_overview_of_aws_vpc_and_security_groups/","text":"Overview of AWS VPC's and Security Groups \u00b6 We'll want to connect our EB instance with the RDS (Postgres) and EC (Redis) which by default is not allowed. The VPC stands for Virtual Private Cloud which is created for each account on each region, for example eu-central-1 . This is done to isolate containers, they are accessible only to your account. This VPC comes with set of security rules. To view them, we can go to Services -> VPC Dashboard -> VPCs We'll need to specify following security rules: 1. Allow any incoming traffic on port 80 from any IP 2. Allow any traffic from any other AWS service that has this security group To set those up, we can go to Services -> VPC Dashboard -> Security Groups and there we should see a security group that is named same as our EB environment MultiDocker-env . There, under the Inbound Rules we can see that we already allow incoming traffic on port 80 from any IP","title":"Overview of AWS VPC's and Security Groups"},{"location":"Docker%20%26%20Kubernetes/11_multi-container_deployments_to_AWS/04_overview_of_aws_vpc_and_security_groups/#overview-of-aws-vpcs-and-security-groups","text":"We'll want to connect our EB instance with the RDS (Postgres) and EC (Redis) which by default is not allowed. The VPC stands for Virtual Private Cloud which is created for each account on each region, for example eu-central-1 . This is done to isolate containers, they are accessible only to your account. This VPC comes with set of security rules. To view them, we can go to Services -> VPC Dashboard -> VPCs We'll need to specify following security rules: 1. Allow any incoming traffic on port 80 from any IP 2. Allow any traffic from any other AWS service that has this security group To set those up, we can go to Services -> VPC Dashboard -> Security Groups and there we should see a security group that is named same as our EB environment MultiDocker-env . There, under the Inbound Rules we can see that we already allow incoming traffic on port 80 from any IP","title":"Overview of AWS VPC's and Security Groups"},{"location":"Docker%20%26%20Kubernetes/11_multi-container_deployments_to_AWS/05_rds_database_creation/","text":"RDS Database creation \u00b6 We'll create the PostgreSQL server in RDS. To do this, we'll go to Services -> RDS and find a section called Create database and click on it, then we'll be able to choose the database we want. We'll select the PostgreSQL Now you'll be able to configure it as you want, but the default settings should suffice. If you checked the Only enable options eligible for RDS Free Usage Tier in the previous step, your only option will be to use db.t2.micro instance class. We'll need to fill out the settings though: Now, on the next step, we'll want to put the database on our default VPC and select Public accessibility as No . Specify the database name: You can also setup more things like backups, monitoring, performance insights etc, but currently we don't need that. Now, when clicking on Create database , it will redirect to a page like this and it will take a few minutes to create the database. After some time you'll see that it is available:","title":"RDS Database creation"},{"location":"Docker%20%26%20Kubernetes/11_multi-container_deployments_to_AWS/05_rds_database_creation/#rds-database-creation","text":"We'll create the PostgreSQL server in RDS. To do this, we'll go to Services -> RDS and find a section called Create database and click on it, then we'll be able to choose the database we want. We'll select the PostgreSQL Now you'll be able to configure it as you want, but the default settings should suffice. If you checked the Only enable options eligible for RDS Free Usage Tier in the previous step, your only option will be to use db.t2.micro instance class. We'll need to fill out the settings though: Now, on the next step, we'll want to put the database on our default VPC and select Public accessibility as No . Specify the database name: You can also setup more things like backups, monitoring, performance insights etc, but currently we don't need that. Now, when clicking on Create database , it will redirect to a page like this and it will take a few minutes to create the database. After some time you'll see that it is available:","title":"RDS Database creation"},{"location":"Docker%20%26%20Kubernetes/11_multi-container_deployments_to_AWS/06_elasticache_redis_creation/","text":"Elasticache redis creation \u00b6 To create Elasticache Redis we'll go to Services -> Elasticache -> Redis and click on Create . We'll leave the Cluster mode disabled and set up the name and change the note type since the default one is quite expensive. Set Number of Replicas to 0. Also, we'll make a subnet called redis-group and target it to the default VPC, check all the subnets. Now the instance is creating: After some time, it will change status as available:","title":"Elasticache redis creation"},{"location":"Docker%20%26%20Kubernetes/11_multi-container_deployments_to_AWS/06_elasticache_redis_creation/#elasticache-redis-creation","text":"To create Elasticache Redis we'll go to Services -> Elasticache -> Redis and click on Create . We'll leave the Cluster mode disabled and set up the name and change the note type since the default one is quite expensive. Set Number of Replicas to 0. Also, we'll make a subnet called redis-group and target it to the default VPC, check all the subnets. Now the instance is creating: After some time, it will change status as available:","title":"Elasticache redis creation"},{"location":"Docker%20%26%20Kubernetes/11_multi-container_deployments_to_AWS/07_creating_a_custom_security_group/","text":"Creating a custom security group \u00b6 To create a new security group , we'll go to Services -> VPC Dashboard -> Security -> Security Groups . Now we'll create a new security group to allow communication between EB, RDS and EC. We'll click on create and then we'll see it in the security group list. We'll select the multi-docker security group, go to Inbound Rules tab and click on edit . Create inbound ports for postgres and redis, source will be the security group ID.","title":"Creating a custom security group"},{"location":"Docker%20%26%20Kubernetes/11_multi-container_deployments_to_AWS/07_creating_a_custom_security_group/#creating-a-custom-security-group","text":"To create a new security group , we'll go to Services -> VPC Dashboard -> Security -> Security Groups . Now we'll create a new security group to allow communication between EB, RDS and EC. We'll click on create and then we'll see it in the security group list. We'll select the multi-docker security group, go to Inbound Rules tab and click on edit . Create inbound ports for postgres and redis, source will be the security group ID.","title":"Creating a custom security group"},{"location":"Docker%20%26%20Kubernetes/11_multi-container_deployments_to_AWS/08_applying_security_groups_to_resources/","text":"Applying security groups to resources \u00b6 Now, when we created a security group, we'll need to add the resources to this group. Elasticache Redis \u00b6 First, we can go to Services -> Elasticache -> Redis , select our previously created instance and click on modify . Then we'll modify the VPC security groups : And apply the changes. RDS Postgres \u00b6 Go to Services -> RDS , check the created instance and click on Modify . Under the Network & Security box, we can apply a security group: Elastic Beanstalk \u00b6 Go to Services -> Elastic Beanstalk -> Multidocker-env -> Configuration -> Instances -> EC2 security groups","title":"Applying security groups to resources"},{"location":"Docker%20%26%20Kubernetes/11_multi-container_deployments_to_AWS/08_applying_security_groups_to_resources/#applying-security-groups-to-resources","text":"Now, when we created a security group, we'll need to add the resources to this group.","title":"Applying security groups to resources"},{"location":"Docker%20%26%20Kubernetes/11_multi-container_deployments_to_AWS/08_applying_security_groups_to_resources/#elasticache-redis","text":"First, we can go to Services -> Elasticache -> Redis , select our previously created instance and click on modify . Then we'll modify the VPC security groups : And apply the changes.","title":"Elasticache Redis"},{"location":"Docker%20%26%20Kubernetes/11_multi-container_deployments_to_AWS/08_applying_security_groups_to_resources/#rds-postgres","text":"Go to Services -> RDS , check the created instance and click on Modify . Under the Network & Security box, we can apply a security group:","title":"RDS Postgres"},{"location":"Docker%20%26%20Kubernetes/11_multi-container_deployments_to_AWS/08_applying_security_groups_to_resources/#elastic-beanstalk","text":"Go to Services -> Elastic Beanstalk -> Multidocker-env -> Configuration -> Instances -> EC2 security groups","title":"Elastic Beanstalk"},{"location":"Docker%20%26%20Kubernetes/11_multi-container_deployments_to_AWS/09_setting_environment_variables/","text":"Setting environment variables \u00b6 To setup ELB environment variables, we can go to Services -> Elastic Beanstalk -> MultiDocker-env -> Configuration -> Software and set up the Environment properties . And when hitting on apply, it will take some time to apply them:","title":"Setting environment variables"},{"location":"Docker%20%26%20Kubernetes/11_multi-container_deployments_to_AWS/09_setting_environment_variables/#setting-environment-variables","text":"To setup ELB environment variables, we can go to Services -> Elastic Beanstalk -> MultiDocker-env -> Configuration -> Software and set up the Environment properties . And when hitting on apply, it will take some time to apply them:","title":"Setting environment variables"},{"location":"Docker%20%26%20Kubernetes/11_multi-container_deployments_to_AWS/10_iam_keys_for_deployment/","text":"IAM Keys for deployment \u00b6 Now, when everything is ready, we'll go to Services -> IAM -> Users and create a new user for deploying the application. Make sure to check the Programmatic access box. We'll attach existing policies that are related to beanstalk: And create the user: Now, we'll go to Travis CI , open up the repository settings and set them up as environment variables:","title":"IAM Keys for deployment"},{"location":"Docker%20%26%20Kubernetes/11_multi-container_deployments_to_AWS/10_iam_keys_for_deployment/#iam-keys-for-deployment","text":"Now, when everything is ready, we'll go to Services -> IAM -> Users and create a new user for deploying the application. Make sure to check the Programmatic access box. We'll attach existing policies that are related to beanstalk: And create the user: Now, we'll go to Travis CI , open up the repository settings and set them up as environment variables:","title":"IAM Keys for deployment"},{"location":"Docker%20%26%20Kubernetes/11_multi-container_deployments_to_AWS/11_travis_deploy_script/","text":"Travis Deploy script \u00b6 We'll modify the previously created .travis.yml file to deploy the application to AWS.","title":"Travis Deploy script"},{"location":"Docker%20%26%20Kubernetes/11_multi-container_deployments_to_AWS/11_travis_deploy_script/#travis-deploy-script","text":"We'll modify the previously created .travis.yml file to deploy the application to AWS.","title":"Travis Deploy script"},{"location":"Docker%20%26%20Kubernetes/11_multi-container_deployments_to_AWS/12_container_memory_allocations/","text":"Container memory allocations \u00b6 Now, when pushing the project to AWS, it might fail with a following error: This is complaining that we need to specify an option called memory . This can be done in the Dockerrun.aws.json file. { \"AWSEBDockerrunVersion\": 2, \"containerDefinitions\": [ { \"name\": \"client\", \"image\": \"deiveris/multi-client\", \"hostname\": \"client\", \"essential\": false, \"memory\": 128 }, { \"name\": \"server\", \"image\": \"deiveris/multi-server\", \"hostname\": \"api\", \"essential\": false, \"memory\": 128 }, { \"name\": \"worker\", \"image\": \"deiveris/multi-worker\", \"hostname\": \"worker\", \"essential\": false, \"memory\": 128 }, { \"name\": \"nginx\", \"image\": \"deiveris/multi-nginx\", \"hostname\": \"nginx\", \"essential\": true, \"portMappings\": [ { \"hostPort\": 80, \"containerPort\": 80 } ], \"links\": [\"client\", \"server\"], \"memory\": 128 } ] } Every container has now 128mb of memory allocation, which might not be the best case, but it should work for a test.","title":"Container memory allocations"},{"location":"Docker%20%26%20Kubernetes/11_multi-container_deployments_to_AWS/12_container_memory_allocations/#container-memory-allocations","text":"Now, when pushing the project to AWS, it might fail with a following error: This is complaining that we need to specify an option called memory . This can be done in the Dockerrun.aws.json file. { \"AWSEBDockerrunVersion\": 2, \"containerDefinitions\": [ { \"name\": \"client\", \"image\": \"deiveris/multi-client\", \"hostname\": \"client\", \"essential\": false, \"memory\": 128 }, { \"name\": \"server\", \"image\": \"deiveris/multi-server\", \"hostname\": \"api\", \"essential\": false, \"memory\": 128 }, { \"name\": \"worker\", \"image\": \"deiveris/multi-worker\", \"hostname\": \"worker\", \"essential\": false, \"memory\": 128 }, { \"name\": \"nginx\", \"image\": \"deiveris/multi-nginx\", \"hostname\": \"nginx\", \"essential\": true, \"portMappings\": [ { \"hostPort\": 80, \"containerPort\": 80 } ], \"links\": [\"client\", \"server\"], \"memory\": 128 } ] } Every container has now 128mb of memory allocation, which might not be the best case, but it should work for a test.","title":"Container memory allocations"},{"location":"Docker%20%26%20Kubernetes/11_multi-container_deployments_to_AWS/13_verify_deployment/","text":"Verify deployment \u00b6 After memory allocation, there might be another error when deployment fails due to docker push creating a private repository. I made them public. Now, when deployed and opened up the aws url:","title":"Verify deployment"},{"location":"Docker%20%26%20Kubernetes/11_multi-container_deployments_to_AWS/13_verify_deployment/#verify-deployment","text":"After memory allocation, there might be another error when deployment fails due to docker push creating a private repository. I made them public. Now, when deployed and opened up the aws url:","title":"Verify deployment"},{"location":"Docker%20%26%20Kubernetes/12_kubernetes/01_what_is_kubernetes/","text":"What is Kubernetes \u00b6 Previously we built an application with 4 containers running at the same time - nginx, server, client and a worker container. But, if we would want it to scale using the AWS ELB, it would spawn new instances that runs all 4 of those containers - doesn't matter that the actual load is only on one of those containers. We can use kubernetes to solve this. Kubernetes creates a cluster that has one or multiple nodes (virtual or phisical machines) assigned to it. The master controls what each Node does. Each node runs a set of containers that master has told them to run. In summary \u00b6 Kubernetes is a system for running many different containers over multiple different machines. You might want to use it when you need to run many different containers with different images.","title":"What is Kubernetes"},{"location":"Docker%20%26%20Kubernetes/12_kubernetes/01_what_is_kubernetes/#what-is-kubernetes","text":"Previously we built an application with 4 containers running at the same time - nginx, server, client and a worker container. But, if we would want it to scale using the AWS ELB, it would spawn new instances that runs all 4 of those containers - doesn't matter that the actual load is only on one of those containers. We can use kubernetes to solve this. Kubernetes creates a cluster that has one or multiple nodes (virtual or phisical machines) assigned to it. The master controls what each Node does. Each node runs a set of containers that master has told them to run.","title":"What is Kubernetes"},{"location":"Docker%20%26%20Kubernetes/12_kubernetes/01_what_is_kubernetes/#in-summary","text":"Kubernetes is a system for running many different containers over multiple different machines. You might want to use it when you need to run many different containers with different images.","title":"In summary"},{"location":"Docker%20%26%20Kubernetes/12_kubernetes/02_kubernetes_developement_and_production/","text":"Kubernetes development and production \u00b6 When using kubernetes, there is a very large distinction when using it on development environment and on production environment. In a development environment, we make use kubernetes by using an application like minikube . In production environment, we most likely will use managed solutions like: Amazon Elastic Container Service for Kubernetes (EKS) Google Cloud Kubernetes Engine (GKE) Other self-hosted solutions","title":"Kubernetes development and production"},{"location":"Docker%20%26%20Kubernetes/12_kubernetes/02_kubernetes_developement_and_production/#kubernetes-development-and-production","text":"When using kubernetes, there is a very large distinction when using it on development environment and on production environment. In a development environment, we make use kubernetes by using an application like minikube . In production environment, we most likely will use managed solutions like: Amazon Elastic Container Service for Kubernetes (EKS) Google Cloud Kubernetes Engine (GKE) Other self-hosted solutions","title":"Kubernetes development and production"},{"location":"Docker%20%26%20Kubernetes/12_kubernetes/03_kubernetes_setup/","text":"Installation \u00b6 We can follow the guide on https://kubernetes.io/docs/tasks/tools/install-minikube/ . Before you begin \u00b6 Check required vmx / svm support. davis@davis-arch \ue0b0 /tmp \ue0b0 egrep --color 'vmx|svm' /proc/cpuinfo flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm cpuid_fault epb pti tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms xsaveopt dtherm ida arat pln pts flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm cpuid_fault epb pti tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms xsaveopt dtherm ida arat pln pts flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm cpuid_fault epb pti tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms xsaveopt dtherm ida arat pln pts flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm cpuid_fault epb pti tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms xsaveopt dtherm ida arat pln pts flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm cpuid_fault epb pti tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms xsaveopt dtherm ida arat pln pts flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm cpuid_fault epb pti tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms xsaveopt dtherm ida arat pln pts flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm cpuid_fault epb pti tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms xsaveopt dtherm ida arat pln pts flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm cpuid_fault epb pti tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms xsaveopt dtherm ida arat pln pts Install a Hypervisor \u00b6 sudo pacman -Syu davis@davis-arch \ue0b0 /tmp \ue0b0 sudo pacman -S virtualbox resolving dependencies... :: There are 2 providers available for VIRTUALBOX-HOST-MODULES: :: Repository community 1) virtualbox-host-dkms 2) virtualbox-host-modules-arch Enter a number (default=1): 2 looking for conflicting packages... Packages (4) qt5-x11extras-5.12.1-1 sdl-1.2.15-10 virtualbox-host-modules-arch-6.0.4-12 virtualbox-6.0.4-4 Total Download Size: 62.05 MiB Total Installed Size: 178.07 MiB :: Proceed with installation? [Y/n] :: Retrieving packages... sdl-1.2.15-10-x86_64 341.3 KiB 1219K/s 00:00 [########################################] 100% qt5-x11extras-5.12.1-1-x86_64 13.0 KiB 4.24M/s 00:00 [########################################] 100% virtualbox-host-modules-arch-6.0.4-12-x86_64 162.0 KiB 1742K/s 00:00 [########################################] 100% virtualbox-6.0.4-4-x86_64 61.6 MiB 1402K/s 00:45 [########################################] 100% (4/4) checking keys in keyring [########################################] 100% (4/4) checking package integrity [########################################] 100% (4/4) loading package files [########################################] 100% (4/4) checking for file conflicts [########################################] 100% (4/4) checking available disk space [########################################] 100% :: Processing package changes... (1/4) installing sdl [########################################] 100% Optional dependencies for sdl alsa-lib: ALSA audio driver [installed] libpulse: PulseAudio audio driver [installed] (2/4) installing qt5-x11extras [########################################] 100% (3/4) installing virtualbox-host-modules-arch [########################################] 100% (4/4) installing virtualbox [########################################] 100% Optional dependencies for virtualbox vde2: Virtual Distributed Ethernet support virtualbox-guest-iso: Guest Additions CD image virtualbox-ext-vnc: VNC server support virtualbox-sdk: Developer kit :: Running post-transaction hooks... (1/8) Updating linux module dependencies... (2/8) Updating icon theme caches... (3/8) Reloading system manager configuration... (4/8) Creating system user accounts... (5/8) Reloading device manager configuration... (6/8) Arming ConditionNeedsUpdate... (7/8) Updating the desktop file MIME type cache... (8/8) Updating the MIME type database... davis@davis-arch \ue0b0 /tmp \ue0b0 virtualbox WARNING: The vboxdrv kernel module is not loaded. Either there is no module available for the current kernel (5.0.0-arch1-1-ARCH) or it failed to load. Please recompile the kernel module and install it by sudo /sbin/vboxconfig You will not be able to start VMs until this problem is fixed. davis@davis-arch \ue0b0 /tmp \ue0b0 sudo modprobe vboxdrv davis@davis-arch \ue0b0 /tmp \ue0b0 virtualbox davis@davis-arch \ue0b0 /tmp \ue0b0 Install kubectl \u00b6 davis@davis-arch \ue0b0 /tmp \ue0b0 curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.13.4/bin/linux/amd64/kubectl % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 37.4M 100 37.4M 0 0 1662k 0 0:00:23 0:00:23 --:--:-- 1634k davis@davis-arch \ue0b0 /tmp \ue0b0 chmod +x ./kubectl davis@davis-arch \ue0b0 /tmp \ue0b0 sudo mv ./kubectl /usr/local/bin/kubectl davis@davis-arch \ue0b0 /tmp \ue0b0 kubectl version Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.4\", GitCommit:\"c27b913fddd1a6c480c229191a087698aa92f0b1\", GitTreeState:\"clean\", BuildDate:\"2019-02-28T13:37:52Z\", GoVersion:\"go1.11.5\", Compiler:\"gc\", Platform:\"linux/amd64\"} The connection to the server localhost:8080 was refused - did you specify the right host or port? Install minikube \u00b6 davis@davis-arch \ue0b0 /tmp \ue0b0 curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 \\ && chmod +x minikube % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 38.2M 100 38.2M 0 0 1714k 0 0:00:22 0:00:22 --:--:-- 1523k davis@davis-arch \ue0b0 /tmp \ue0b0 sudo cp minikube /usr/local/bin && rm minikube davis@davis-arch \ue0b0 /tmp \ue0b0 minikube version minikube version: v0.35.0 Start minikube \u00b6 davis@davis-arch \ue0b0 ~ \ue0b0 minikube start o minikube v0.35.0 on linux (amd64) > Creating virtualbox VM (CPUs=2, Memory=2048MB, Disk=20000MB) ... - \"minikube\" IP address is 192.168.99.101 - Configuring Docker as the container runtime ... - Preparing Kubernetes environment ... - Pulling images required by Kubernetes v1.13.4 ... - Launching Kubernetes v1.13.4 using kubeadm ... : Waiting for pods: apiserver proxy etcd scheduler controller addon-manager dns - Configuring cluster permissions ... - Verifying component health ..... + kubectl is now configured to use \"minikube\" = Done! Thank you for using minikube! davis@davis-arch \ue0b0 ~ \ue0b0 minikube status host: Running kubelet: Running apiserver: Running kubectl: Correctly Configured: pointing to minikube-vm at 192.168.99.101 davis@davis-arch \ue0b0 ~ \ue0b0 kubectl cluster-info Kubernetes master is running at https://192.168.99.101:8443 KubeDNS is running at https://192.168.99.101:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.","title":"Installation"},{"location":"Docker%20%26%20Kubernetes/12_kubernetes/03_kubernetes_setup/#installation","text":"We can follow the guide on https://kubernetes.io/docs/tasks/tools/install-minikube/ .","title":"Installation"},{"location":"Docker%20%26%20Kubernetes/12_kubernetes/03_kubernetes_setup/#before-you-begin","text":"Check required vmx / svm support. davis@davis-arch \ue0b0 /tmp \ue0b0 egrep --color 'vmx|svm' /proc/cpuinfo flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm cpuid_fault epb pti tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms xsaveopt dtherm ida arat pln pts flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm cpuid_fault epb pti tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms xsaveopt dtherm ida arat pln pts flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm cpuid_fault epb pti tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms xsaveopt dtherm ida arat pln pts flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm cpuid_fault epb pti tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms xsaveopt dtherm ida arat pln pts flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm cpuid_fault epb pti tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms xsaveopt dtherm ida arat pln pts flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm cpuid_fault epb pti tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms xsaveopt dtherm ida arat pln pts flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm cpuid_fault epb pti tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms xsaveopt dtherm ida arat pln pts flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm cpuid_fault epb pti tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms xsaveopt dtherm ida arat pln pts","title":"Before you begin"},{"location":"Docker%20%26%20Kubernetes/12_kubernetes/03_kubernetes_setup/#install-a-hypervisor","text":"sudo pacman -Syu davis@davis-arch \ue0b0 /tmp \ue0b0 sudo pacman -S virtualbox resolving dependencies... :: There are 2 providers available for VIRTUALBOX-HOST-MODULES: :: Repository community 1) virtualbox-host-dkms 2) virtualbox-host-modules-arch Enter a number (default=1): 2 looking for conflicting packages... Packages (4) qt5-x11extras-5.12.1-1 sdl-1.2.15-10 virtualbox-host-modules-arch-6.0.4-12 virtualbox-6.0.4-4 Total Download Size: 62.05 MiB Total Installed Size: 178.07 MiB :: Proceed with installation? [Y/n] :: Retrieving packages... sdl-1.2.15-10-x86_64 341.3 KiB 1219K/s 00:00 [########################################] 100% qt5-x11extras-5.12.1-1-x86_64 13.0 KiB 4.24M/s 00:00 [########################################] 100% virtualbox-host-modules-arch-6.0.4-12-x86_64 162.0 KiB 1742K/s 00:00 [########################################] 100% virtualbox-6.0.4-4-x86_64 61.6 MiB 1402K/s 00:45 [########################################] 100% (4/4) checking keys in keyring [########################################] 100% (4/4) checking package integrity [########################################] 100% (4/4) loading package files [########################################] 100% (4/4) checking for file conflicts [########################################] 100% (4/4) checking available disk space [########################################] 100% :: Processing package changes... (1/4) installing sdl [########################################] 100% Optional dependencies for sdl alsa-lib: ALSA audio driver [installed] libpulse: PulseAudio audio driver [installed] (2/4) installing qt5-x11extras [########################################] 100% (3/4) installing virtualbox-host-modules-arch [########################################] 100% (4/4) installing virtualbox [########################################] 100% Optional dependencies for virtualbox vde2: Virtual Distributed Ethernet support virtualbox-guest-iso: Guest Additions CD image virtualbox-ext-vnc: VNC server support virtualbox-sdk: Developer kit :: Running post-transaction hooks... (1/8) Updating linux module dependencies... (2/8) Updating icon theme caches... (3/8) Reloading system manager configuration... (4/8) Creating system user accounts... (5/8) Reloading device manager configuration... (6/8) Arming ConditionNeedsUpdate... (7/8) Updating the desktop file MIME type cache... (8/8) Updating the MIME type database... davis@davis-arch \ue0b0 /tmp \ue0b0 virtualbox WARNING: The vboxdrv kernel module is not loaded. Either there is no module available for the current kernel (5.0.0-arch1-1-ARCH) or it failed to load. Please recompile the kernel module and install it by sudo /sbin/vboxconfig You will not be able to start VMs until this problem is fixed. davis@davis-arch \ue0b0 /tmp \ue0b0 sudo modprobe vboxdrv davis@davis-arch \ue0b0 /tmp \ue0b0 virtualbox davis@davis-arch \ue0b0 /tmp \ue0b0","title":"Install a Hypervisor"},{"location":"Docker%20%26%20Kubernetes/12_kubernetes/03_kubernetes_setup/#install-kubectl","text":"davis@davis-arch \ue0b0 /tmp \ue0b0 curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.13.4/bin/linux/amd64/kubectl % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 37.4M 100 37.4M 0 0 1662k 0 0:00:23 0:00:23 --:--:-- 1634k davis@davis-arch \ue0b0 /tmp \ue0b0 chmod +x ./kubectl davis@davis-arch \ue0b0 /tmp \ue0b0 sudo mv ./kubectl /usr/local/bin/kubectl davis@davis-arch \ue0b0 /tmp \ue0b0 kubectl version Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.4\", GitCommit:\"c27b913fddd1a6c480c229191a087698aa92f0b1\", GitTreeState:\"clean\", BuildDate:\"2019-02-28T13:37:52Z\", GoVersion:\"go1.11.5\", Compiler:\"gc\", Platform:\"linux/amd64\"} The connection to the server localhost:8080 was refused - did you specify the right host or port?","title":"Install kubectl"},{"location":"Docker%20%26%20Kubernetes/12_kubernetes/03_kubernetes_setup/#install-minikube","text":"davis@davis-arch \ue0b0 /tmp \ue0b0 curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 \\ && chmod +x minikube % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 38.2M 100 38.2M 0 0 1714k 0 0:00:22 0:00:22 --:--:-- 1523k davis@davis-arch \ue0b0 /tmp \ue0b0 sudo cp minikube /usr/local/bin && rm minikube davis@davis-arch \ue0b0 /tmp \ue0b0 minikube version minikube version: v0.35.0","title":"Install minikube"},{"location":"Docker%20%26%20Kubernetes/12_kubernetes/03_kubernetes_setup/#start-minikube","text":"davis@davis-arch \ue0b0 ~ \ue0b0 minikube start o minikube v0.35.0 on linux (amd64) > Creating virtualbox VM (CPUs=2, Memory=2048MB, Disk=20000MB) ... - \"minikube\" IP address is 192.168.99.101 - Configuring Docker as the container runtime ... - Preparing Kubernetes environment ... - Pulling images required by Kubernetes v1.13.4 ... - Launching Kubernetes v1.13.4 using kubeadm ... : Waiting for pods: apiserver proxy etcd scheduler controller addon-manager dns - Configuring cluster permissions ... - Verifying component health ..... + kubectl is now configured to use \"minikube\" = Done! Thank you for using minikube! davis@davis-arch \ue0b0 ~ \ue0b0 minikube status host: Running kubelet: Running apiserver: Running kubectl: Correctly Configured: pointing to minikube-vm at 192.168.99.101 davis@davis-arch \ue0b0 ~ \ue0b0 kubectl cluster-info Kubernetes master is running at https://192.168.99.101:8443 KubeDNS is running at https://192.168.99.101:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.","title":"Start minikube"},{"location":"Docker%20%26%20Kubernetes/12_kubernetes/04_mapping_existing_knowledge/","text":"Mapping existing knowledge \u00b6 Goal \u00b6 Get the multi-client image running on our local Kubernetes Cluster running as a container. Compose \u00b6 The multi-client docker-compose.yml file: Each service can optionally get docker-compose to build the image Each service represents a container we want to create Each entry defines the networking requirements, ports In Kubernetes : Expects all images to be already built One config per object we want to create We have to manually set up all the networking Config \u00b6 Get a simple container running on our local Kubernetes running: Make sure our image is hosted on docker hub Make one config file to create the container Make one config file to set up networking","title":"Mapping existing knowledge"},{"location":"Docker%20%26%20Kubernetes/12_kubernetes/04_mapping_existing_knowledge/#mapping-existing-knowledge","text":"","title":"Mapping existing knowledge"},{"location":"Docker%20%26%20Kubernetes/12_kubernetes/04_mapping_existing_knowledge/#goal","text":"Get the multi-client image running on our local Kubernetes Cluster running as a container.","title":"Goal"},{"location":"Docker%20%26%20Kubernetes/12_kubernetes/04_mapping_existing_knowledge/#compose","text":"The multi-client docker-compose.yml file: Each service can optionally get docker-compose to build the image Each service represents a container we want to create Each entry defines the networking requirements, ports In Kubernetes : Expects all images to be already built One config per object we want to create We have to manually set up all the networking","title":"Compose"},{"location":"Docker%20%26%20Kubernetes/12_kubernetes/04_mapping_existing_knowledge/#config","text":"Get a simple container running on our local Kubernetes running: Make sure our image is hosted on docker hub Make one config file to create the container Make one config file to set up networking","title":"Config"},{"location":"Docker%20%26%20Kubernetes/12_kubernetes/05_adding_configuration_files/","text":"Adding configuration files \u00b6 Before starting to configuring, first we'll go to https://hub.docker.com and make sure that we still have our images there. Now we'll create a new project: davis@davis-arch \ue0b0 ~/projects/learning/docker \ue0b0 \ue0a0 master \ue0b0 mkdir 07_simplek8s davis@davis-arch \ue0b0 ~/projects/learning/docker \ue0b0 \ue0a0 master \ue0b0 cd 07_simplek8s And make a new client-pod.yaml file: apiVersion: v1 kind: Pod metadata: name: client-pod labels: component: web spec: containers: - name: client image: deiveris/multi-client ports: - containerPort: 3000 And create client-node-port.yaml : apiVersion: v1 kind: Service metadata: name: client-node-port spec: type: NodePort ports: - port: 3050 targetPort: 3000 nodePort: 31515 selector: component: web","title":"Adding configuration files"},{"location":"Docker%20%26%20Kubernetes/12_kubernetes/05_adding_configuration_files/#adding-configuration-files","text":"Before starting to configuring, first we'll go to https://hub.docker.com and make sure that we still have our images there. Now we'll create a new project: davis@davis-arch \ue0b0 ~/projects/learning/docker \ue0b0 \ue0a0 master \ue0b0 mkdir 07_simplek8s davis@davis-arch \ue0b0 ~/projects/learning/docker \ue0b0 \ue0a0 master \ue0b0 cd 07_simplek8s And make a new client-pod.yaml file: apiVersion: v1 kind: Pod metadata: name: client-pod labels: component: web spec: containers: - name: client image: deiveris/multi-client ports: - containerPort: 3000 And create client-node-port.yaml : apiVersion: v1 kind: Service metadata: name: client-node-port spec: type: NodePort ports: - port: 3050 targetPort: 3000 nodePort: 31515 selector: component: web","title":"Adding configuration files"},{"location":"Docker%20%26%20Kubernetes/12_kubernetes/06_object_types_and_API_versions/","text":"Object types and API versions \u00b6 Previously made configuration files are used to make kubernetes objects which are slightly different than docker containers. The term object references a thing that exists in the kubernetes cluster that can be one of many types like: StatefulSet ReplicaController Pod Service etc Objects serve different purposes - running a container, monitoring container, setting up networking etc. When we specify apiVersion in the configuration file scopes types of objects we want to create in the configuration file: apiVersion: v1 componentStatus configMap Endpoints Event Namespace Pod ... apiVersion: apps/v1 ConvrollerRevision StatefulSet ...","title":"Object types and API versions"},{"location":"Docker%20%26%20Kubernetes/12_kubernetes/06_object_types_and_API_versions/#object-types-and-api-versions","text":"Previously made configuration files are used to make kubernetes objects which are slightly different than docker containers. The term object references a thing that exists in the kubernetes cluster that can be one of many types like: StatefulSet ReplicaController Pod Service etc Objects serve different purposes - running a container, monitoring container, setting up networking etc. When we specify apiVersion in the configuration file scopes types of objects we want to create in the configuration file: apiVersion: v1 componentStatus configMap Endpoints Event Namespace Pod ... apiVersion: apps/v1 ConvrollerRevision StatefulSet ...","title":"Object types and API versions"},{"location":"Docker%20%26%20Kubernetes/12_kubernetes/07_running_containers_in_pods/","text":"Running containers in pods \u00b6 When we installed kubectl and minikube , we ran a command minikube start . It created a new VM on our computer that is running as a Node . It will be used by kubernetes to run different objects. One of the most basic objects are known as a Pod for which we previously made a configuration file. It will group containers for the nginx image. Pod must run 1 or more containers within it. Usually when there are multiple containers defined in the Pod , it is because they are tighlt coupled. Like a database backup service would most certaily need a database in order for it to run. The metadata section is mostly used for logging purposes and object coupling.","title":"Running containers in pods"},{"location":"Docker%20%26%20Kubernetes/12_kubernetes/07_running_containers_in_pods/#running-containers-in-pods","text":"When we installed kubectl and minikube , we ran a command minikube start . It created a new VM on our computer that is running as a Node . It will be used by kubernetes to run different objects. One of the most basic objects are known as a Pod for which we previously made a configuration file. It will group containers for the nginx image. Pod must run 1 or more containers within it. Usually when there are multiple containers defined in the Pod , it is because they are tighlt coupled. Like a database backup service would most certaily need a database in order for it to run. The metadata section is mostly used for logging purposes and object coupling.","title":"Running containers in pods"},{"location":"Docker%20%26%20Kubernetes/12_kubernetes/08_service_config_files_in_depth/","text":"Service config files in depth \u00b6 Previously we looked at Pods which are used for running one or more closely related containers. Now, we'll look at Services which are used to set up networking in a kubernetes cluster. There are 4 subtypes for Service : - ClusterIP - NodePort - LoadBalancer - Ingress In our configuration file we chose NodePort type which is only good for development purposes, it is not to be used in production environment with exceptions. So, basically, our current configuration looks like this: We can use this diagram to describe the configuration we previously made: And the whole configuration will look like this:","title":"Service config files in depth"},{"location":"Docker%20%26%20Kubernetes/12_kubernetes/08_service_config_files_in_depth/#service-config-files-in-depth","text":"Previously we looked at Pods which are used for running one or more closely related containers. Now, we'll look at Services which are used to set up networking in a kubernetes cluster. There are 4 subtypes for Service : - ClusterIP - NodePort - LoadBalancer - Ingress In our configuration file we chose NodePort type which is only good for development purposes, it is not to be used in production environment with exceptions. So, basically, our current configuration looks like this: We can use this diagram to describe the configuration we previously made: And the whole configuration will look like this:","title":"Service config files in depth"},{"location":"Docker%20%26%20Kubernetes/12_kubernetes/09_connecting_to_running_containers/","text":"Connecting to running containers \u00b6 Now we are ready to load the configuration in our kubernetes cluster. We can do that by doing kubectl apply -f <filename> $ ls client-node-port.yaml client-pod.yaml $ kubectl apply -f client-pod.yaml pod/client-pod created $ kubectl apply -f client-node-port.yaml service/client-node-port created To check the status of the object: kubectl get <object type> $ kubectl get pods NAME READY STATUS RESTARTS AGE client-pod 1/1 Running 0 2m51s $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE client-node-port NodePort 10.106.105.100 <none> 3050:31515/TCP 2m26s kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 49m So, everything seems to be running. We can verify that we can load the site. Note that the minikube VM has it's own IP and will not respond to localhost , so we can run a command: $ minikube ip 192.168.99.101 And visit http://192.168.99.101:31515/","title":"Connecting to running containers"},{"location":"Docker%20%26%20Kubernetes/12_kubernetes/09_connecting_to_running_containers/#connecting-to-running-containers","text":"Now we are ready to load the configuration in our kubernetes cluster. We can do that by doing kubectl apply -f <filename> $ ls client-node-port.yaml client-pod.yaml $ kubectl apply -f client-pod.yaml pod/client-pod created $ kubectl apply -f client-node-port.yaml service/client-node-port created To check the status of the object: kubectl get <object type> $ kubectl get pods NAME READY STATUS RESTARTS AGE client-pod 1/1 Running 0 2m51s $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE client-node-port NodePort 10.106.105.100 <none> 3050:31515/TCP 2m26s kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 49m So, everything seems to be running. We can verify that we can load the site. Note that the minikube VM has it's own IP and will not respond to localhost , so we can run a command: $ minikube ip 192.168.99.101 And visit http://192.168.99.101:31515/","title":"Connecting to running containers"},{"location":"Docker%20%26%20Kubernetes/12_kubernetes/10_the_entire_deployment_flow/","text":"The entire deployment flow \u00b6 In this section we'll look into what really happened when we applied the configration to the kubernetes cluster. When run the command kubectl apply , the file is taken and passed of to a Master that controls the kubernetes cluster, one of which is kube-apiserver which is responsible for monitoring the status of all the different nodes in the cluster and making sure that their are doing the correct things. The kube-apiserver holds a table of it's state and updates it with the configuration passed. Then, based on it's state, it issues commands to nodes. For example, if we tell the master that we want 4 copies of multi-worker , it will update it's state table and will see that it needs to be running 4 copies of multi-worker , but has 0. It will issue commands to nodes to start up those containers. Then, the node will download the image from docker hub and start up the containers. Then the Master will update poll the nodes and see that there are 4 containers of multi-worker running, update it's state table. If for some reason one of these containers would be killed, the master would notice it and issue a command to the node to start it up again.","title":"The entire deployment flow"},{"location":"Docker%20%26%20Kubernetes/12_kubernetes/10_the_entire_deployment_flow/#the-entire-deployment-flow","text":"In this section we'll look into what really happened when we applied the configration to the kubernetes cluster. When run the command kubectl apply , the file is taken and passed of to a Master that controls the kubernetes cluster, one of which is kube-apiserver which is responsible for monitoring the status of all the different nodes in the cluster and making sure that their are doing the correct things. The kube-apiserver holds a table of it's state and updates it with the configuration passed. Then, based on it's state, it issues commands to nodes. For example, if we tell the master that we want 4 copies of multi-worker , it will update it's state table and will see that it needs to be running 4 copies of multi-worker , but has 0. It will issue commands to nodes to start up those containers. Then, the node will download the image from docker hub and start up the containers. Then the Master will update poll the nodes and see that there are 4 containers of multi-worker running, update it's state table. If for some reason one of these containers would be killed, the master would notice it and issue a command to the node to start it up again.","title":"The entire deployment flow"},{"location":"Docker%20%26%20Kubernetes/12_kubernetes/11_imperative_vs_declaritive_deployments/","text":"Imperative vs declarative deployments \u00b6 Important takeaways \u00b6 Kubernetes is a system to deploy containerized apps Nodes are individual machines (or vm's) that run containers Masters are machines (or vm's) with a set of programs to manage the nodes Kubernetes didn't build our images - it got them from somewhere else Kubernetes (the master) decided where to run each container - each node can run a dissimilar set of containers. (we can specify on which node to run which containers, but by default the master decides itself). To deploy something, we update the desired state of the master with a config file The master works constantly to meet your desired state Imperative and Declaritive deployments \u00b6 There are 2 ways to manage deployments: Imperative \"Do exactly these steps to arrive at this container setup\" \"Create this container, delete that one ...\" Declarative \"Our container setup should look like this, make it hapen\" While kubernetes support both ways of doing things, for production environment it is recommended to use the Declaritive approach.","title":"Imperative vs declarative deployments"},{"location":"Docker%20%26%20Kubernetes/12_kubernetes/11_imperative_vs_declaritive_deployments/#imperative-vs-declarative-deployments","text":"","title":"Imperative vs declarative deployments"},{"location":"Docker%20%26%20Kubernetes/12_kubernetes/11_imperative_vs_declaritive_deployments/#important-takeaways","text":"Kubernetes is a system to deploy containerized apps Nodes are individual machines (or vm's) that run containers Masters are machines (or vm's) with a set of programs to manage the nodes Kubernetes didn't build our images - it got them from somewhere else Kubernetes (the master) decided where to run each container - each node can run a dissimilar set of containers. (we can specify on which node to run which containers, but by default the master decides itself). To deploy something, we update the desired state of the master with a config file The master works constantly to meet your desired state","title":"Important takeaways"},{"location":"Docker%20%26%20Kubernetes/12_kubernetes/11_imperative_vs_declaritive_deployments/#imperative-and-declaritive-deployments","text":"There are 2 ways to manage deployments: Imperative \"Do exactly these steps to arrive at this container setup\" \"Create this container, delete that one ...\" Declarative \"Our container setup should look like this, make it hapen\" While kubernetes support both ways of doing things, for production environment it is recommended to use the Declaritive approach.","title":"Imperative and Declaritive deployments"},{"location":"Docker%20%26%20Kubernetes/13_maintaining_sets_of_containers_with_deployments/01_updating_existing_objects/","text":"Updating existing objects \u00b6 Goal \u00b6 We'll update our existing multi-client pod to use the multi-worker image. Imperative approach \u00b6 Run a command to list our all current running pods Run a command to update the current pod to use a new image Declaritive approach \u00b6 Update our config file that orginally created the pod Throw the updated config file into kubectl","title":"Updating existing objects"},{"location":"Docker%20%26%20Kubernetes/13_maintaining_sets_of_containers_with_deployments/01_updating_existing_objects/#updating-existing-objects","text":"","title":"Updating existing objects"},{"location":"Docker%20%26%20Kubernetes/13_maintaining_sets_of_containers_with_deployments/01_updating_existing_objects/#goal","text":"We'll update our existing multi-client pod to use the multi-worker image.","title":"Goal"},{"location":"Docker%20%26%20Kubernetes/13_maintaining_sets_of_containers_with_deployments/01_updating_existing_objects/#imperative-approach","text":"Run a command to list our all current running pods Run a command to update the current pod to use a new image","title":"Imperative approach"},{"location":"Docker%20%26%20Kubernetes/13_maintaining_sets_of_containers_with_deployments/01_updating_existing_objects/#declaritive-approach","text":"Update our config file that orginally created the pod Throw the updated config file into kubectl","title":"Declaritive approach"},{"location":"Docker%20%26%20Kubernetes/13_maintaining_sets_of_containers_with_deployments/02_declarative_updates_in_action/","text":"Declarative updates in action \u00b6 We are going to find the client-pod.yaml file: apiVersion: v1 kind: Pod metadata: name: client-pod labels: component: web spec: containers: - name: client image: deiveris/multi-client ports: - containerPort: 3000 And modify it: apiVersion: v1 kind: Pod metadata: name: client-pod labels: component: web spec: containers: - name: client image: deiveris/multi-worker ports: - containerPort: 3000 Then run the command: $ kubectl apply -f client-pod.yaml pod/client-pod configured We can run the command to verify the change kubectl get pods We can use the command to get detailed info: kubectl describe <object type> <object name>","title":"Declarative updates in action"},{"location":"Docker%20%26%20Kubernetes/13_maintaining_sets_of_containers_with_deployments/02_declarative_updates_in_action/#declarative-updates-in-action","text":"We are going to find the client-pod.yaml file: apiVersion: v1 kind: Pod metadata: name: client-pod labels: component: web spec: containers: - name: client image: deiveris/multi-client ports: - containerPort: 3000 And modify it: apiVersion: v1 kind: Pod metadata: name: client-pod labels: component: web spec: containers: - name: client image: deiveris/multi-worker ports: - containerPort: 3000 Then run the command: $ kubectl apply -f client-pod.yaml pod/client-pod configured We can run the command to verify the change kubectl get pods We can use the command to get detailed info: kubectl describe <object type> <object name>","title":"Declarative updates in action"},{"location":"Docker%20%26%20Kubernetes/13_maintaining_sets_of_containers_with_deployments/03_limitations_in_config_updates/","text":"Limitations in config updates \u00b6 If we change the client-pod.yaml from: apiVersion: v1 kind: Pod metadata: name: client-pod labels: component: web spec: containers: - name: client image: deiveris/multi-worker ports: - containerPort: 3000 to apiVersion: v1 kind: Pod metadata: name: client-pod labels: component: web spec: containers: - name: client image: deiveris/multi-worker ports: - containerPort: 9999 And update: kubectl apply -f client-pod.yaml It will throw an error saying: The Pod \"client-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations) So, we are only allowed to change these 4 different properties.","title":"Limitations in config updates"},{"location":"Docker%20%26%20Kubernetes/13_maintaining_sets_of_containers_with_deployments/03_limitations_in_config_updates/#limitations-in-config-updates","text":"If we change the client-pod.yaml from: apiVersion: v1 kind: Pod metadata: name: client-pod labels: component: web spec: containers: - name: client image: deiveris/multi-worker ports: - containerPort: 3000 to apiVersion: v1 kind: Pod metadata: name: client-pod labels: component: web spec: containers: - name: client image: deiveris/multi-worker ports: - containerPort: 9999 And update: kubectl apply -f client-pod.yaml It will throw an error saying: The Pod \"client-pod\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations) So, we are only allowed to change these 4 different properties.","title":"Limitations in config updates"},{"location":"Docker%20%26%20Kubernetes/13_maintaining_sets_of_containers_with_deployments/04_running_containers_with_deployments/","text":"Running containers with deployments \u00b6 Previously we changed a port on the client-pod configuration and it threw an error saying that the property cannot be updated once the pod is created. To solve this issue, we can make use of a different object called Deployment . It maintains a set of identical pods, ensuring that they have correct config and that the right number exists. The pod \u00b6 Runs a single set of containers Good for one-off dev purposes Rarely used directly in production Deployment \u00b6 Runs a set of identical pods (one or more) Monitors the state of each pod, updating as necessary Good for dev Good for production When we create a deployment, it has an attached pod template that describes how the pods should look like. If we change the template, for example, the port. The deployment will attempt to change it in the pod or kill and recreate the pod.","title":"Running containers with deployments"},{"location":"Docker%20%26%20Kubernetes/13_maintaining_sets_of_containers_with_deployments/04_running_containers_with_deployments/#running-containers-with-deployments","text":"Previously we changed a port on the client-pod configuration and it threw an error saying that the property cannot be updated once the pod is created. To solve this issue, we can make use of a different object called Deployment . It maintains a set of identical pods, ensuring that they have correct config and that the right number exists.","title":"Running containers with deployments"},{"location":"Docker%20%26%20Kubernetes/13_maintaining_sets_of_containers_with_deployments/04_running_containers_with_deployments/#the-pod","text":"Runs a single set of containers Good for one-off dev purposes Rarely used directly in production","title":"The pod"},{"location":"Docker%20%26%20Kubernetes/13_maintaining_sets_of_containers_with_deployments/04_running_containers_with_deployments/#deployment","text":"Runs a set of identical pods (one or more) Monitors the state of each pod, updating as necessary Good for dev Good for production When we create a deployment, it has an attached pod template that describes how the pods should look like. If we change the template, for example, the port. The deployment will attempt to change it in the pod or kill and recreate the pod.","title":"Deployment"},{"location":"Docker%20%26%20Kubernetes/13_maintaining_sets_of_containers_with_deployments/05_deployment_configuration_files/","text":"Deployment configuration files \u00b6 In order to create the deployment configuration file, we'll create a new client-deployment.yml file : apiVersion: apps/v1 kind: Deployment metadata: name: client-deployment spec: replicas: 1 selector: matchLabels: component: web template: metadata: labels: component: web spec: containers: - name: client image: deiveris/multi-client ports: - containerPort: 3000 The template describes the pod to be created. The replicas describes the number of pods to make. The selector describes a handle for the deployment to find the pod after it's created.","title":"Deployment configuration files"},{"location":"Docker%20%26%20Kubernetes/13_maintaining_sets_of_containers_with_deployments/05_deployment_configuration_files/#deployment-configuration-files","text":"In order to create the deployment configuration file, we'll create a new client-deployment.yml file : apiVersion: apps/v1 kind: Deployment metadata: name: client-deployment spec: replicas: 1 selector: matchLabels: component: web template: metadata: labels: component: web spec: containers: - name: client image: deiveris/multi-client ports: - containerPort: 3000 The template describes the pod to be created. The replicas describes the number of pods to make. The selector describes a handle for the deployment to find the pod after it's created.","title":"Deployment configuration files"},{"location":"Docker%20%26%20Kubernetes/13_maintaining_sets_of_containers_with_deployments/06_applying_deployment/","text":"Applying deployment \u00b6 Since we still have the pod running from previous attempts, we are going to remove it: kubectl delete -f <config file> $ kubectl delete -f client-pod.yaml pod \"client-pod\" deleted $ kubectl get pods No resources found. Now we'll apply the deployment: $ kubectl apply -f client-deployment.yaml deployment.apps/client-deployment created $ kubectl get pods NAME READY STATUS RESTARTS AGE client-deployment-7bf8c9b5c5-48b4v 1/1 Running 0 5s $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE client-deployment 1/1 1 1 40s And we can verify that we can still access the nginx site: $ minikube ip 192.168.99.101 Open up http://192.168.99.101:31515/","title":"Applying deployment"},{"location":"Docker%20%26%20Kubernetes/13_maintaining_sets_of_containers_with_deployments/06_applying_deployment/#applying-deployment","text":"Since we still have the pod running from previous attempts, we are going to remove it: kubectl delete -f <config file> $ kubectl delete -f client-pod.yaml pod \"client-pod\" deleted $ kubectl get pods No resources found. Now we'll apply the deployment: $ kubectl apply -f client-deployment.yaml deployment.apps/client-deployment created $ kubectl get pods NAME READY STATUS RESTARTS AGE client-deployment-7bf8c9b5c5-48b4v 1/1 Running 0 5s $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE client-deployment 1/1 1 1 40s And we can verify that we can still access the nginx site: $ minikube ip 192.168.99.101 Open up http://192.168.99.101:31515/","title":"Applying deployment"},{"location":"Docker%20%26%20Kubernetes/13_maintaining_sets_of_containers_with_deployments/07_why_use_services/","text":"Why use services? \u00b6 Now when we have an idea on how deployments work, we can try to understand the need of services. $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES client-deployment-7bf8c9b5c5-48b4v 1/1 Running 0 4m37s 172.17.0.4 minikube <none> <none> As we can see from the table, every single pod gets a seperate IP address. When it is updated, it can get an entirely differnt IP address. The service will look at every pod that matches it's selector and then automatically route traffic to it.","title":"Why use services?"},{"location":"Docker%20%26%20Kubernetes/13_maintaining_sets_of_containers_with_deployments/07_why_use_services/#why-use-services","text":"Now when we have an idea on how deployments work, we can try to understand the need of services. $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES client-deployment-7bf8c9b5c5-48b4v 1/1 Running 0 4m37s 172.17.0.4 minikube <none> <none> As we can see from the table, every single pod gets a seperate IP address. When it is updated, it can get an entirely differnt IP address. The service will look at every pod that matches it's selector and then automatically route traffic to it.","title":"Why use services?"},{"location":"Docker%20%26%20Kubernetes/13_maintaining_sets_of_containers_with_deployments/08_scaling_and_changing_deployments/","text":"Scaling and changing deployments \u00b6 We are going to open up the client-deployment.yaml file: apiVersion: apps/v1 kind: Deployment metadata: name: client-deployment spec: replicas: 1 selector: matchLabels: component: web template: metadata: labels: component: web spec: containers: - name: client image: deiveris/multi-client ports: - containerPort: 3000 And modify the port that previously failed on pod update: apiVersion: apps/v1 kind: Deployment metadata: name: client-deployment spec: replicas: 1 selector: matchLabels: component: web template: metadata: labels: component: web spec: containers: - name: client image: deiveris/multi-client ports: - containerPort: 9999 $ kubectl apply -f client-deployment.yaml deployment.apps/client-deployment configured $ kubectl get pods NAME READY STATUS RESTARTS AGE client-deployment-69f476d99b-s4trm 1/1 Running 0 18s $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE client-deployment 1/1 1 1 10m $ kubectl describe pods Name: client-deployment-69f476d99b-s4trm Namespace: default Priority: 0 PriorityClassName: <none> Node: minikube/10.0.2.15 Start Time: Sun, 17 Mar 2019 17:08:31 +0200 Labels: component=web pod-template-hash=69f476d99b Annotations: <none> Status: Running IP: 172.17.0.5 Controlled By: ReplicaSet/client-deployment-69f476d99b Containers: client: Container ID: docker://7c59d43a6cd56c8eabfca5230f1fe2929b52debf49b58ff7d0022c6e8031ac93 Image: deiveris/multi-client Image ID: docker-pullable://deiveris/multi-client@sha256:51c71c0006401fcb25f15b0bf809b58e0f5a3885271b3d655fe1ed685a2f64f9 Port: 9999/TCP Host Port: 0/TCP State: Running Started: Sun, 17 Mar 2019 17:08:34 +0200 Ready: True Restart Count: 0 Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-vq8qh (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: default-token-vq8qh: Type: Secret (a volume populated by a Secret) SecretName: default-token-vq8qh Optional: false QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 85s default-scheduler Successfully assigned default/client-deployment-69f476d99b-s4trm to minikube Normal Pulling 84s kubelet, minikube pulling image \"deiveris/multi-client\" Normal Pulled 83s kubelet, minikube Successfully pulled image \"deiveris/multi-client\" Normal Created 83s kubelet, minikube Created container Normal Started 82s kubelet, minikube Started container We can see that the pod is working on the port 9999 . If we change the replicas: 5 $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE client-deployment 5/5 5 5 13m $ kubectl get pods NAME READY STATUS RESTARTS AGE client-deployment-69f476d99b-9dzd5 1/1 Running 0 25s client-deployment-69f476d99b-ct94h 1/1 Running 0 25s client-deployment-69f476d99b-lwbcj 1/1 Running 0 25s client-deployment-69f476d99b-nt5mz 1/1 Running 0 25s client-deployment-69f476d99b-s4trm 1/1 Running 0 3m7s","title":"Scaling and changing deployments"},{"location":"Docker%20%26%20Kubernetes/13_maintaining_sets_of_containers_with_deployments/08_scaling_and_changing_deployments/#scaling-and-changing-deployments","text":"We are going to open up the client-deployment.yaml file: apiVersion: apps/v1 kind: Deployment metadata: name: client-deployment spec: replicas: 1 selector: matchLabels: component: web template: metadata: labels: component: web spec: containers: - name: client image: deiveris/multi-client ports: - containerPort: 3000 And modify the port that previously failed on pod update: apiVersion: apps/v1 kind: Deployment metadata: name: client-deployment spec: replicas: 1 selector: matchLabels: component: web template: metadata: labels: component: web spec: containers: - name: client image: deiveris/multi-client ports: - containerPort: 9999 $ kubectl apply -f client-deployment.yaml deployment.apps/client-deployment configured $ kubectl get pods NAME READY STATUS RESTARTS AGE client-deployment-69f476d99b-s4trm 1/1 Running 0 18s $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE client-deployment 1/1 1 1 10m $ kubectl describe pods Name: client-deployment-69f476d99b-s4trm Namespace: default Priority: 0 PriorityClassName: <none> Node: minikube/10.0.2.15 Start Time: Sun, 17 Mar 2019 17:08:31 +0200 Labels: component=web pod-template-hash=69f476d99b Annotations: <none> Status: Running IP: 172.17.0.5 Controlled By: ReplicaSet/client-deployment-69f476d99b Containers: client: Container ID: docker://7c59d43a6cd56c8eabfca5230f1fe2929b52debf49b58ff7d0022c6e8031ac93 Image: deiveris/multi-client Image ID: docker-pullable://deiveris/multi-client@sha256:51c71c0006401fcb25f15b0bf809b58e0f5a3885271b3d655fe1ed685a2f64f9 Port: 9999/TCP Host Port: 0/TCP State: Running Started: Sun, 17 Mar 2019 17:08:34 +0200 Ready: True Restart Count: 0 Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-vq8qh (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: default-token-vq8qh: Type: Secret (a volume populated by a Secret) SecretName: default-token-vq8qh Optional: false QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 85s default-scheduler Successfully assigned default/client-deployment-69f476d99b-s4trm to minikube Normal Pulling 84s kubelet, minikube pulling image \"deiveris/multi-client\" Normal Pulled 83s kubelet, minikube Successfully pulled image \"deiveris/multi-client\" Normal Created 83s kubelet, minikube Created container Normal Started 82s kubelet, minikube Started container We can see that the pod is working on the port 9999 . If we change the replicas: 5 $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE client-deployment 5/5 5 5 13m $ kubectl get pods NAME READY STATUS RESTARTS AGE client-deployment-69f476d99b-9dzd5 1/1 Running 0 25s client-deployment-69f476d99b-ct94h 1/1 Running 0 25s client-deployment-69f476d99b-lwbcj 1/1 Running 0 25s client-deployment-69f476d99b-nt5mz 1/1 Running 0 25s client-deployment-69f476d99b-s4trm 1/1 Running 0 3m7s","title":"Scaling and changing deployments"},{"location":"Docker%20%26%20Kubernetes/13_maintaining_sets_of_containers_with_deployments/09_updating_deployment_images/","text":"Updating deployment images \u00b6 In the next sections we are going to see on how to make a realistic deployments. First, we'll revert the changes in the deployment to use the multi-client We are going to update and push a new image to docker hub Get the deployment to recreate our pods with the latest verision of the image So, we are changing the client-deployment.yaml file back to: apiVersion: apps/v1 kind: Deployment metadata: name: client-deployment spec: replicas: 1 selector: matchLabels: component: web template: metadata: labels: component: web spec: containers: - name: client image: deiveris/multi-client ports: - containerPort: 3000 $ kubectl apply -f client-deployment.yaml deployment.apps/client-deployment configured $ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE client-deployment 1/1 1 1 20m $ kubectl get pods NAME READY STATUS RESTARTS AGE client-deployment-69f476d99b-s4trm 0/1 Terminating 0 10m client-deployment-7bf8c9b5c5-ptj4h 1/1 Running 0 11s And check that we can still access it: $ minikube ip 192.168.99.101 And visit http://192.168.99.101:31515/ .","title":"Updating deployment images"},{"location":"Docker%20%26%20Kubernetes/13_maintaining_sets_of_containers_with_deployments/09_updating_deployment_images/#updating-deployment-images","text":"In the next sections we are going to see on how to make a realistic deployments. First, we'll revert the changes in the deployment to use the multi-client We are going to update and push a new image to docker hub Get the deployment to recreate our pods with the latest verision of the image So, we are changing the client-deployment.yaml file back to: apiVersion: apps/v1 kind: Deployment metadata: name: client-deployment spec: replicas: 1 selector: matchLabels: component: web template: metadata: labels: component: web spec: containers: - name: client image: deiveris/multi-client ports: - containerPort: 3000 $ kubectl apply -f client-deployment.yaml deployment.apps/client-deployment configured $ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE client-deployment 1/1 1 1 20m $ kubectl get pods NAME READY STATUS RESTARTS AGE client-deployment-69f476d99b-s4trm 0/1 Terminating 0 10m client-deployment-7bf8c9b5c5-ptj4h 1/1 Running 0 11s And check that we can still access it: $ minikube ip 192.168.99.101 And visit http://192.168.99.101:31515/ .","title":"Updating deployment images"},{"location":"Docker%20%26%20Kubernetes/13_maintaining_sets_of_containers_with_deployments/10_rebuilding_the_client_image/","text":"Rebuilding the client image \u00b6 Now as per mention in previous section, we are going to change and update the multi-client image, so we can push a new version on docker hub. We'll open up 06_building_a_multicontainer_application/client/src/App.js and modify it: import React, { Component } from 'react'; import logo from './logo.svg'; import './App.css'; import { BrowserRouter as Router, Route, Link } from 'react-router-dom'; import OtherPage from './OtherPage'; import Fib from './Fib'; class App extends Component { render() { return ( <Router> <div className=\"App\"> <header className=\"App-header\"> <img src={logo} className=\"App-logo\" alt=\"logo\" /> <h1 className=\"App-title\">Fib Calculator v2</h1> <Link to=\"/\">Home</Link> <Link to=\"/otherpage\">Other Page</Link> </header> <div> <Route exact path=\"/\" component={Fib} /> <Route path=\"/otherpage\" component={OtherPage} /> </div> </div> </Router> ); } } export default App; And we'll rebuild the image: $ cd ../06_building_a_multicontainer_application/client $ docker build -it deiveris/multi-client . $ docker login $ docker push deiveris/multi-client And we can see that the image has been updated and pushed to docker hub :","title":"Rebuilding the client image"},{"location":"Docker%20%26%20Kubernetes/13_maintaining_sets_of_containers_with_deployments/10_rebuilding_the_client_image/#rebuilding-the-client-image","text":"Now as per mention in previous section, we are going to change and update the multi-client image, so we can push a new version on docker hub. We'll open up 06_building_a_multicontainer_application/client/src/App.js and modify it: import React, { Component } from 'react'; import logo from './logo.svg'; import './App.css'; import { BrowserRouter as Router, Route, Link } from 'react-router-dom'; import OtherPage from './OtherPage'; import Fib from './Fib'; class App extends Component { render() { return ( <Router> <div className=\"App\"> <header className=\"App-header\"> <img src={logo} className=\"App-logo\" alt=\"logo\" /> <h1 className=\"App-title\">Fib Calculator v2</h1> <Link to=\"/\">Home</Link> <Link to=\"/otherpage\">Other Page</Link> </header> <div> <Route exact path=\"/\" component={Fib} /> <Route path=\"/otherpage\" component={OtherPage} /> </div> </div> </Router> ); } } export default App; And we'll rebuild the image: $ cd ../06_building_a_multicontainer_application/client $ docker build -it deiveris/multi-client . $ docker login $ docker push deiveris/multi-client And we can see that the image has been updated and pushed to docker hub :","title":"Rebuilding the client image"},{"location":"Docker%20%26%20Kubernetes/13_maintaining_sets_of_containers_with_deployments/11_triggering_deployment_updates/","text":"Triggering deployment updates \u00b6 Now, when we have made a new image and deployed to docker hub, we need to apply it to the kubernetes cluster. Currently the issue with our configuration is that if we try to make changes by applying deployment: $ kubectl apply -f client-deployment.yaml deployment.apps/client-deployment unchanged This is because the configuration file has not changed. There are 3 solutions on how to get around this (none of which are great): - Manually delete pods to get the deployment to recreate them with the latest version - Tag build timages with a real version number and specify that version in the config file - Use an imperative command to update the image version the deployment should use","title":"Triggering deployment updates"},{"location":"Docker%20%26%20Kubernetes/13_maintaining_sets_of_containers_with_deployments/11_triggering_deployment_updates/#triggering-deployment-updates","text":"Now, when we have made a new image and deployed to docker hub, we need to apply it to the kubernetes cluster. Currently the issue with our configuration is that if we try to make changes by applying deployment: $ kubectl apply -f client-deployment.yaml deployment.apps/client-deployment unchanged This is because the configuration file has not changed. There are 3 solutions on how to get around this (none of which are great): - Manually delete pods to get the deployment to recreate them with the latest version - Tag build timages with a real version number and specify that version in the config file - Use an imperative command to update the image version the deployment should use","title":"Triggering deployment updates"},{"location":"Docker%20%26%20Kubernetes/13_maintaining_sets_of_containers_with_deployments/12_imperatively_updating_a_deployment_image/","text":"Imperatively updating a deployment image \u00b6 We'll rebuild the image once more with a tag v5 : $ docker build -t deiveris/multi-client:v5 . Sending build context to Docker daemon 233.9MB Step 1/10 : FROM node:11.10.1-alpine as builder ---> 842caa90d45b Step 2/10 : WORKDIR '/app' ---> Using cache ---> 38570625fbce Step 3/10 : COPY package*.json ./ ---> Using cache ---> 37c5b6da3bf9 Step 4/10 : RUN npm install ---> Using cache ---> 0f5c82488b91 Step 5/10 : COPY . . ---> Using cache ---> 5f86605194af Step 6/10 : RUN npm run build ---> Using cache ---> eece77ba22ec Step 7/10 : FROM nginx ---> 881bd08c0b08 Step 8/10 : EXPOSE 3000 ---> Using cache ---> f71625d87e8d Step 9/10 : COPY ./nginx/default.conf /etc/nginx/conf.d/default.conf ---> Using cache ---> 2212a0b54395 Step 10/10 : COPY --from=builder /app/build /usr/share/nginx/html ---> Using cache ---> 1019422ed773 Successfully built 1019422ed773 Successfully tagged deiveris/multi-client:v5 $ docker push deiveris/multi-client:v5 The push refers to repository [docker.io/deiveris/multi-client] d244568552e6: Layer already exists c6e5f4e5b580: Layer already exists 3e9eb35b1c23: Layer already exists c59b3ca455e3: Layer already exists 6744ca1b1190: Layer already exists v5: digest: sha256:5dff734172a3ea0d1fd8f6fa91bb140f537f823afb7ac86cabc0bade2e9d1066 size: 1365 Now we are going to use an imperative command to update the image: kubectl set image <object type>/<object name> <container name> = <new image to use> $ kubectl apply -f client-deployment.yaml deployment.apps/client-deployment unchanged $ kubectl set image deployment/client-deployment client=deiveris/multi-client:v5 deployment.extensions/client-deployment image updated $ kubectl get pods NAME READY STATUS RESTARTS AGE client-deployment-7bf8c9b5c5-ptj4h 0/1 Terminating 0 21m client-deployment-7fbcbb74f6-x66wp 1/1 Running 0 8s","title":"Imperatively updating a deployment image"},{"location":"Docker%20%26%20Kubernetes/13_maintaining_sets_of_containers_with_deployments/12_imperatively_updating_a_deployment_image/#imperatively-updating-a-deployment-image","text":"We'll rebuild the image once more with a tag v5 : $ docker build -t deiveris/multi-client:v5 . Sending build context to Docker daemon 233.9MB Step 1/10 : FROM node:11.10.1-alpine as builder ---> 842caa90d45b Step 2/10 : WORKDIR '/app' ---> Using cache ---> 38570625fbce Step 3/10 : COPY package*.json ./ ---> Using cache ---> 37c5b6da3bf9 Step 4/10 : RUN npm install ---> Using cache ---> 0f5c82488b91 Step 5/10 : COPY . . ---> Using cache ---> 5f86605194af Step 6/10 : RUN npm run build ---> Using cache ---> eece77ba22ec Step 7/10 : FROM nginx ---> 881bd08c0b08 Step 8/10 : EXPOSE 3000 ---> Using cache ---> f71625d87e8d Step 9/10 : COPY ./nginx/default.conf /etc/nginx/conf.d/default.conf ---> Using cache ---> 2212a0b54395 Step 10/10 : COPY --from=builder /app/build /usr/share/nginx/html ---> Using cache ---> 1019422ed773 Successfully built 1019422ed773 Successfully tagged deiveris/multi-client:v5 $ docker push deiveris/multi-client:v5 The push refers to repository [docker.io/deiveris/multi-client] d244568552e6: Layer already exists c6e5f4e5b580: Layer already exists 3e9eb35b1c23: Layer already exists c59b3ca455e3: Layer already exists 6744ca1b1190: Layer already exists v5: digest: sha256:5dff734172a3ea0d1fd8f6fa91bb140f537f823afb7ac86cabc0bade2e9d1066 size: 1365 Now we are going to use an imperative command to update the image: kubectl set image <object type>/<object name> <container name> = <new image to use> $ kubectl apply -f client-deployment.yaml deployment.apps/client-deployment unchanged $ kubectl set image deployment/client-deployment client=deiveris/multi-client:v5 deployment.extensions/client-deployment image updated $ kubectl get pods NAME READY STATUS RESTARTS AGE client-deployment-7bf8c9b5c5-ptj4h 0/1 Terminating 0 21m client-deployment-7fbcbb74f6-x66wp 1/1 Running 0 8s","title":"Imperatively updating a deployment image"},{"location":"Docker%20%26%20Kubernetes/13_maintaining_sets_of_containers_with_deployments/13_multiple_docker_installations/","text":"Multiple docker installations \u00b6 We can use docker cli to reach into the kubernetes node to see what exactly is going on. Currently, we have 2 docker installations on our local machine - one is installed normalle, one is inside the virtual machine for the node .","title":"Multiple docker installations"},{"location":"Docker%20%26%20Kubernetes/13_maintaining_sets_of_containers_with_deployments/13_multiple_docker_installations/#multiple-docker-installations","text":"We can use docker cli to reach into the kubernetes node to see what exactly is going on. Currently, we have 2 docker installations on our local machine - one is installed normalle, one is inside the virtual machine for the node .","title":"Multiple docker installations"},{"location":"Docker%20%26%20Kubernetes/13_maintaining_sets_of_containers_with_deployments/14_reconfiguring_docker_cli/","text":"Reconfiguring docker cli \u00b6 We can use this command to tell our docker to communicate to the one that is in the node: eval $(minikube docker-env) Note that this configures only your current terminal window. $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES $ eval $(minikube docker-env) $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 08e1f3633220 deiveris/multi-client \"nginx -g 'daemon of\u2026\" 7 minutes ago Up 7 minutes k8s_client_client-deployment-7fbcbb74f6-6b9zp_default_07dfcefa-48ce-11e9-9347-080027ac9b62_0 2cad7cd16d67 k8s.gcr.io/pause:3.1 \"/pause\" 7 minutes ago Up 7 minutes k8s_POD_client-deployment-7fbcbb74f6-6b9zp_default_07dfcefa-48ce-11e9-9347-080027ac9b62_0 70abe7496757 gcr.io/k8s-minikube/storage-provisioner \"/storage-provisioner\" 3 hours ago Up 3 hours k8s_storage-provisioner_storage-provisioner_kube-system_cad7848a-48b3-11e9-9347-080027ac9b62_0 c77fc24db6ed k8s.gcr.io/pause:3.1 \"/pause\" 3 hours ago Up 3 hours k8s_POD_storage-provisioner_kube-system_cad7848a-48b3-11e9-9347-080027ac9b62_0 db2a72d8a368 f59dcacceff4 \"/coredns -conf /etc\u2026\" 3 hours ago Up 3 hours k8s_coredns_coredns-86c58d9df4-6vwjn_kube-system_ca149085-48b3-11e9-9347-080027ac9b62_0 6ec0a646cc54 f59dcacceff4 \"/coredns -conf /etc\u2026\" 3 hours ago Up 3 hours k8s_coredns_coredns-86c58d9df4-tf5qs_kube-system_ca1377bd-48b3-11e9-9347-080027ac9b62_0 678de1a66ed9 k8s.gcr.io/pause:3.1 \"/pause\" 3 hours ago Up 3 hours k8s_POD_coredns-86c58d9df4-6vwjn_kube-system_ca149085-48b3-11e9-9347-080027ac9b62_0 cc9a879cf62e k8s.gcr.io/pause:3.1 \"/pause\" 3 hours ago Up 3 hours k8s_POD_coredns-86c58d9df4-tf5qs_kube-system_ca1377bd-48b3-11e9-9347-080027ac9b62_0 69a36af4889d fadcc5d2b066 \"/usr/local/bin/kube\u2026\" 3 hours ago Up 3 hours k8s_kube-proxy_kube-proxy-8fl92_kube-system_c9cf3515-48b3-11e9-9347-080027ac9b62_0 ce6025e1d4fe k8s.gcr.io/pause:3.1 \"/pause\" 3 hours ago Up 3 hours k8s_POD_kube-proxy-8fl92_kube-system_c9cf3515-48b3-11e9-9347-080027ac9b62_0 d2346064cd2c k8s.gcr.io/kube-addon-manager \"/opt/kube-addons.sh\" 3 hours ago Up 3 hours k8s_kube-addon-manager_kube-addon-manager-minikube_kube-system_5c72fb06dcdda608211b70d63c0ca488_0 44477cd9854b 3cab8e1b9802 \"etcd --advertise-cl\u2026\" 3 hours ago Up 3 hours k8s_etcd_etcd-minikube_kube-system_5d1dfac2685c9f6c638fb24ff7b526ae_0 30608e848d4b dd862b749309 \"kube-scheduler --ad\u2026\" 3 hours ago Up 3 hours k8s_kube-scheduler_kube-scheduler-minikube_kube-system_4b52d75cab61380f07c0c5a69fb371d4_0 f0e707d067b0 40a817357014 \"kube-controller-man\u2026\" 3 hours ago Up 3 hours k8s_kube-controller-manager_kube-controller-manager-minikube_kube-system_17eea6fd9342634d7d40a04d577641fd_0 5af97a55a8ee fc3801f0fc54 \"kube-apiserver --au\u2026\" 3 hours ago Up 3 hours k8s_kube-apiserver_kube-apiserver-minikube_kube-system_c43b7638a36a24e7ce3a86415f505a74_0 881e39c9c8d6 k8s.gcr.io/pause:3.1 \"/pause\" 3 hours ago Up 3 hours k8s_POD_kube-controller-manager-minikube_kube-system_17eea6fd9342634d7d40a04d577641fd_0 183b32b7c30f k8s.gcr.io/pause:3.1 \"/pause\" 3 hours ago Up 3 hours k8s_POD_kube-scheduler-minikube_kube-system_4b52d75cab61380f07c0c5a69fb371d4_0 bb07dcd126f7 k8s.gcr.io/pause:3.1 \"/pause\" 3 hours ago Up 3 hours k8s_POD_etcd-minikube_kube-system_5d1dfac2685c9f6c638fb24ff7b526ae_0 b7cad0807aa2 k8s.gcr.io/pause:3.1 \"/pause\" 3 hours ago Up 3 hours k8s_POD_kube-apiserver-minikube_kube-system_c43b7638a36a24e7ce3a86415f505a74_0 091482821db9 k8s.gcr.io/pause:3.1 \"/pause\" 3 hours ago Up 3 hours k8s_POD_kube-addon-manager-minikube_kube-system_5c72fb06dcdda608211b70d63c0ca488_0 If we view the command executed by eval, we can see that it exports some environment variables: $ minikube docker-env export DOCKER_TLS_VERIFY=\"1\" export DOCKER_HOST=\"tcp://192.168.99.101:2376\" export DOCKER_CERT_PATH=\"/home/davis/.minikube/certs\" export DOCKER_API_VERSION=\"1.35\" # Run this command to configure your shell: # eval $(minikube docker-env)","title":"Reconfiguring docker cli"},{"location":"Docker%20%26%20Kubernetes/13_maintaining_sets_of_containers_with_deployments/14_reconfiguring_docker_cli/#reconfiguring-docker-cli","text":"We can use this command to tell our docker to communicate to the one that is in the node: eval $(minikube docker-env) Note that this configures only your current terminal window. $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES $ eval $(minikube docker-env) $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 08e1f3633220 deiveris/multi-client \"nginx -g 'daemon of\u2026\" 7 minutes ago Up 7 minutes k8s_client_client-deployment-7fbcbb74f6-6b9zp_default_07dfcefa-48ce-11e9-9347-080027ac9b62_0 2cad7cd16d67 k8s.gcr.io/pause:3.1 \"/pause\" 7 minutes ago Up 7 minutes k8s_POD_client-deployment-7fbcbb74f6-6b9zp_default_07dfcefa-48ce-11e9-9347-080027ac9b62_0 70abe7496757 gcr.io/k8s-minikube/storage-provisioner \"/storage-provisioner\" 3 hours ago Up 3 hours k8s_storage-provisioner_storage-provisioner_kube-system_cad7848a-48b3-11e9-9347-080027ac9b62_0 c77fc24db6ed k8s.gcr.io/pause:3.1 \"/pause\" 3 hours ago Up 3 hours k8s_POD_storage-provisioner_kube-system_cad7848a-48b3-11e9-9347-080027ac9b62_0 db2a72d8a368 f59dcacceff4 \"/coredns -conf /etc\u2026\" 3 hours ago Up 3 hours k8s_coredns_coredns-86c58d9df4-6vwjn_kube-system_ca149085-48b3-11e9-9347-080027ac9b62_0 6ec0a646cc54 f59dcacceff4 \"/coredns -conf /etc\u2026\" 3 hours ago Up 3 hours k8s_coredns_coredns-86c58d9df4-tf5qs_kube-system_ca1377bd-48b3-11e9-9347-080027ac9b62_0 678de1a66ed9 k8s.gcr.io/pause:3.1 \"/pause\" 3 hours ago Up 3 hours k8s_POD_coredns-86c58d9df4-6vwjn_kube-system_ca149085-48b3-11e9-9347-080027ac9b62_0 cc9a879cf62e k8s.gcr.io/pause:3.1 \"/pause\" 3 hours ago Up 3 hours k8s_POD_coredns-86c58d9df4-tf5qs_kube-system_ca1377bd-48b3-11e9-9347-080027ac9b62_0 69a36af4889d fadcc5d2b066 \"/usr/local/bin/kube\u2026\" 3 hours ago Up 3 hours k8s_kube-proxy_kube-proxy-8fl92_kube-system_c9cf3515-48b3-11e9-9347-080027ac9b62_0 ce6025e1d4fe k8s.gcr.io/pause:3.1 \"/pause\" 3 hours ago Up 3 hours k8s_POD_kube-proxy-8fl92_kube-system_c9cf3515-48b3-11e9-9347-080027ac9b62_0 d2346064cd2c k8s.gcr.io/kube-addon-manager \"/opt/kube-addons.sh\" 3 hours ago Up 3 hours k8s_kube-addon-manager_kube-addon-manager-minikube_kube-system_5c72fb06dcdda608211b70d63c0ca488_0 44477cd9854b 3cab8e1b9802 \"etcd --advertise-cl\u2026\" 3 hours ago Up 3 hours k8s_etcd_etcd-minikube_kube-system_5d1dfac2685c9f6c638fb24ff7b526ae_0 30608e848d4b dd862b749309 \"kube-scheduler --ad\u2026\" 3 hours ago Up 3 hours k8s_kube-scheduler_kube-scheduler-minikube_kube-system_4b52d75cab61380f07c0c5a69fb371d4_0 f0e707d067b0 40a817357014 \"kube-controller-man\u2026\" 3 hours ago Up 3 hours k8s_kube-controller-manager_kube-controller-manager-minikube_kube-system_17eea6fd9342634d7d40a04d577641fd_0 5af97a55a8ee fc3801f0fc54 \"kube-apiserver --au\u2026\" 3 hours ago Up 3 hours k8s_kube-apiserver_kube-apiserver-minikube_kube-system_c43b7638a36a24e7ce3a86415f505a74_0 881e39c9c8d6 k8s.gcr.io/pause:3.1 \"/pause\" 3 hours ago Up 3 hours k8s_POD_kube-controller-manager-minikube_kube-system_17eea6fd9342634d7d40a04d577641fd_0 183b32b7c30f k8s.gcr.io/pause:3.1 \"/pause\" 3 hours ago Up 3 hours k8s_POD_kube-scheduler-minikube_kube-system_4b52d75cab61380f07c0c5a69fb371d4_0 bb07dcd126f7 k8s.gcr.io/pause:3.1 \"/pause\" 3 hours ago Up 3 hours k8s_POD_etcd-minikube_kube-system_5d1dfac2685c9f6c638fb24ff7b526ae_0 b7cad0807aa2 k8s.gcr.io/pause:3.1 \"/pause\" 3 hours ago Up 3 hours k8s_POD_kube-apiserver-minikube_kube-system_c43b7638a36a24e7ce3a86415f505a74_0 091482821db9 k8s.gcr.io/pause:3.1 \"/pause\" 3 hours ago Up 3 hours k8s_POD_kube-addon-manager-minikube_kube-system_5c72fb06dcdda608211b70d63c0ca488_0 If we view the command executed by eval, we can see that it exports some environment variables: $ minikube docker-env export DOCKER_TLS_VERIFY=\"1\" export DOCKER_HOST=\"tcp://192.168.99.101:2376\" export DOCKER_CERT_PATH=\"/home/davis/.minikube/certs\" export DOCKER_API_VERSION=\"1.35\" # Run this command to configure your shell: # eval $(minikube docker-env)","title":"Reconfiguring docker cli"},{"location":"Docker%20%26%20Kubernetes/13_maintaining_sets_of_containers_with_deployments/15_why_mess_with_docker_in_the_node/","text":"Why mess with docker in the node? \u00b6 You can use the same debugging techniques we learned with docker CLI docker logs container_id docker exec -it container_id sh ... Manually kill containers to test kubernetes ability to self-heal docker kill container_id Delete cached images in the node docker system prune -a","title":"Why mess with docker in the node?"},{"location":"Docker%20%26%20Kubernetes/13_maintaining_sets_of_containers_with_deployments/15_why_mess_with_docker_in_the_node/#why-mess-with-docker-in-the-node","text":"You can use the same debugging techniques we learned with docker CLI docker logs container_id docker exec -it container_id sh ... Manually kill containers to test kubernetes ability to self-heal docker kill container_id Delete cached images in the node docker system prune -a","title":"Why mess with docker in the node?"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/01_the_path_to_production/","text":"The path to production \u00b6 In this section we will take the previously created application and make it with use of kubernetes. It will look like this: First we will set all of this on local machine, then we will push it on a service provider like AWS or Google Cloud. So, the steps we are going to take: - Create config files for each service and deployment - Test locally on minikube - Create a Github/Travis flow to build images and deploy - Deploy app to a cloud provider","title":"The path to production"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/01_the_path_to_production/#the-path-to-production","text":"In this section we will take the previously created application and make it with use of kubernetes. It will look like this: First we will set all of this on local machine, then we will push it on a service provider like AWS or Google Cloud. So, the steps we are going to take: - Create config files for each service and deployment - Test locally on minikube - Create a Github/Travis flow to build images and deploy - Deploy app to a cloud provider","title":"The path to production"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/02_quick_checkpoint/","text":"Quick checkpoint \u00b6 Because we are using the same application as the previous project, we are going to copy it: $ cp -r 06_building_a_multicontainer_application 08_multicontainer_application_with_kubernetes $ cd 08_multicontainer_application_with_kubernetes $ rm -rf .git $ git init And verify that it is still working: $ docker-compose up And visiting http://localhost:3050 .","title":"Quick checkpoint"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/02_quick_checkpoint/#quick-checkpoint","text":"Because we are using the same application as the previous project, we are going to copy it: $ cp -r 06_building_a_multicontainer_application 08_multicontainer_application_with_kubernetes $ cd 08_multicontainer_application_with_kubernetes $ rm -rf .git $ git init And verify that it is still working: $ docker-compose up And visiting http://localhost:3050 .","title":"Quick checkpoint"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/03_recreating_the_deployment/","text":"Recreating the deployment \u00b6 We are going to clean up the previously copied project removing files: $ rm .travis.yml $ rm docker-compose.yml $ rm -r .elasticbeanstalk $ rm Dockerrun.aws.json $ rm -r nginx $ mkdir k8s Now we'll create a new deployment k8s/client-deployment.yaml : apiVersion: apps/v1 kind: Deployment metadata: name: client-deployment spec: replicas: 3 selector: matchLabels: component: web template: metadata: labels: component: web spec: containers: - name: client image: deiveris/multi-client ports: - containerPort: 3000","title":"Recreating the deployment"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/03_recreating_the_deployment/#recreating-the-deployment","text":"We are going to clean up the previously copied project removing files: $ rm .travis.yml $ rm docker-compose.yml $ rm -r .elasticbeanstalk $ rm Dockerrun.aws.json $ rm -r nginx $ mkdir k8s Now we'll create a new deployment k8s/client-deployment.yaml : apiVersion: apps/v1 kind: Deployment metadata: name: client-deployment spec: replicas: 3 selector: matchLabels: component: web template: metadata: labels: component: web spec: containers: - name: client image: deiveris/multi-client ports: - containerPort: 3000","title":"Recreating the deployment"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/04_nodePortvsClusterIP_services/","text":"NodePort vs ClusterIP Services \u00b6 Now, when we have a client-deployment , we are going to create the ClusterIP Service that is before it in the schema. Previously we used a NodePort service. The difference between these object types is that ClusterIP exposes a set of pods to other objects in the cluster while NodePort exposes a set of pods to the outside world (only good for dev purposes). So, we are goin to create a new file k8s/client-cluster-ip-service.yaml : apiVersion: v1 kind: Service metadata: name: client-cluster-ip-service spec: type: ClusterIP selector: component: web ports: - port: 3000 targetPort: 3000","title":"NodePort vs ClusterIP Services"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/04_nodePortvsClusterIP_services/#nodeport-vs-clusterip-services","text":"Now, when we have a client-deployment , we are going to create the ClusterIP Service that is before it in the schema. Previously we used a NodePort service. The difference between these object types is that ClusterIP exposes a set of pods to other objects in the cluster while NodePort exposes a set of pods to the outside world (only good for dev purposes). So, we are goin to create a new file k8s/client-cluster-ip-service.yaml : apiVersion: v1 kind: Service metadata: name: client-cluster-ip-service spec: type: ClusterIP selector: component: web ports: - port: 3000 targetPort: 3000","title":"NodePort vs ClusterIP Services"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/05_applying_multiple_files_with_kubectl/","text":"Applying multiple files with Kubectl \u00b6 Just to make sure everything is fine, we'll load both of previously created files. First, we are going to delete deployments from previous section. $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE client-deployment 1/1 1 1 114m $ kubectl delete deployment client-deployment deployment.extensions \"client-deployment\" deleted $ kubectl get deploymens error: the server doesn't have a resource type \"deploymens\" $ kubectl get deployments No resources found. $ kubectl get pods No resources found. $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE client-node-port NodePort 10.106.105.100 <none> 3050:31515/TCP 3h12m kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 4h $ kubectl delete service client-node-port service \"client-node-port\" deleted $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 4h Now, when applying, instead of specifying each file separately, we can specify the k8s directory. $ kubectl apply -f k8s service/client-cluster-ip-service created deployment.apps/client-deployment created $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE client-deployment 3/3 3 3 32s $ kubectl get pods NAME READY STATUS RESTARTS AGE client-deployment-7bf8c9b5c5-7gcc7 1/1 Running 0 35s client-deployment-7bf8c9b5c5-96g47 1/1 Running 0 35s client-deployment-7bf8c9b5c5-qr64m 1/1 Running 0 35s $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE client-cluster-ip-service ClusterIP 10.101.240.238 <none> 3000/TCP 39s kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 4h2m","title":"Applying multiple files with Kubectl"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/05_applying_multiple_files_with_kubectl/#applying-multiple-files-with-kubectl","text":"Just to make sure everything is fine, we'll load both of previously created files. First, we are going to delete deployments from previous section. $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE client-deployment 1/1 1 1 114m $ kubectl delete deployment client-deployment deployment.extensions \"client-deployment\" deleted $ kubectl get deploymens error: the server doesn't have a resource type \"deploymens\" $ kubectl get deployments No resources found. $ kubectl get pods No resources found. $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE client-node-port NodePort 10.106.105.100 <none> 3050:31515/TCP 3h12m kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 4h $ kubectl delete service client-node-port service \"client-node-port\" deleted $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 4h Now, when applying, instead of specifying each file separately, we can specify the k8s directory. $ kubectl apply -f k8s service/client-cluster-ip-service created deployment.apps/client-deployment created $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE client-deployment 3/3 3 3 32s $ kubectl get pods NAME READY STATUS RESTARTS AGE client-deployment-7bf8c9b5c5-7gcc7 1/1 Running 0 35s client-deployment-7bf8c9b5c5-96g47 1/1 Running 0 35s client-deployment-7bf8c9b5c5-qr64m 1/1 Running 0 35s $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE client-cluster-ip-service ClusterIP 10.101.240.238 <none> 3000/TCP 39s kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 4h2m","title":"Applying multiple files with Kubectl"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/06_express_API_deployment_config/","text":"Express API deployment config \u00b6 Now we are going to add the deployment for the express API. It should look like this: So, like before, we create a k8s/server-deployment.yaml file: apiVersion: apps/v1 kind: Deployment metadata: name: server-deployment spec: replicas: 3 selector: matchLabels: component: server template: metadata: labels: component: server spec: containers: - name: server image: deiveris/multi-server ports: - containerPort: 5000 Note that the image expects multiple environment variables to be specified, which will be looked at later.","title":"Express API deployment config"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/06_express_API_deployment_config/#express-api-deployment-config","text":"Now we are going to add the deployment for the express API. It should look like this: So, like before, we create a k8s/server-deployment.yaml file: apiVersion: apps/v1 kind: Deployment metadata: name: server-deployment spec: replicas: 3 selector: matchLabels: component: server template: metadata: labels: component: server spec: containers: - name: server image: deiveris/multi-server ports: - containerPort: 5000 Note that the image expects multiple environment variables to be specified, which will be looked at later.","title":"Express API deployment config"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/07_clusterIP_for_api/","text":"Cluster IP for the Express API \u00b6 Now we are going to make a configuration file k8s/server-cluseter-ip-service.yaml . apiVersion: v1 kind: Service metadata: name: server-cluster-ip-service spec: type: ClusterIP selector: component: server ports: - port: 5000 targetPort: 5000","title":"Cluster IP for the Express API"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/07_clusterIP_for_api/#cluster-ip-for-the-express-api","text":"Now we are going to make a configuration file k8s/server-cluseter-ip-service.yaml . apiVersion: v1 kind: Service metadata: name: server-cluster-ip-service spec: type: ClusterIP selector: component: server ports: - port: 5000 targetPort: 5000","title":"Cluster IP for the Express API"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/08_combining_config_into_single_files/","text":"Combining config into single files \u00b6 We can also combine all the previously created configuration files into a one file: config1 --- config2 --- config3","title":"Combining config into single files"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/08_combining_config_into_single_files/#combining-config-into-single-files","text":"We can also combine all the previously created configuration files into a one file: config1 --- config2 --- config3","title":"Combining config into single files"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/09_worker_deployment/","text":"The worker deployment \u00b6 We are going to create a new file k8s/worker-deployment.yaml : apiVersion: apps/v1 kind: Deployment metadata: name: worker-deployment spec: replicas: 1 selector: matchLabels: component: web template: metadata: labels: component: web spec: containers: - name: worker image: deiveris/multi-worker","title":"The worker deployment"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/09_worker_deployment/#the-worker-deployment","text":"We are going to create a new file k8s/worker-deployment.yaml : apiVersion: apps/v1 kind: Deployment metadata: name: worker-deployment spec: replicas: 1 selector: matchLabels: component: web template: metadata: labels: component: web spec: containers: - name: worker image: deiveris/multi-worker","title":"The worker deployment"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/10_reapplying_a_batch_of_config_files/","text":"Reapplying a batch of config files \u00b6 $ kubectl apply -f k8s service/client-cluster-ip-service unchanged deployment.apps/client-deployment unchanged service/server-cluster-ip-service created deployment.apps/server-deployment created deployment.apps/worker-deployment created $ kubectl get pods NAME READY STATUS RESTARTS AGE client-deployment-7bf8c9b5c5-7gcc7 1/1 Running 0 21m client-deployment-7bf8c9b5c5-96g47 1/1 Running 0 21m client-deployment-7bf8c9b5c5-qr64m 1/1 Running 0 21m server-deployment-d9cb6bcf4-2v75c 1/1 Running 0 26s server-deployment-d9cb6bcf4-4z7n4 1/1 Running 0 26s server-deployment-d9cb6bcf4-kp84s 1/1 Running 0 26s worker-deployment-85896bdb7-cs52k 0/1 CrashLoopBackOff 1 26s $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE client-deployment 3/3 3 3 22m server-deployment 3/3 3 3 67s worker-deployment 0/1 1 0 67s $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE client-cluster-ip-service ClusterIP 10.101.240.238 <none> 3000/TCP 22m kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 4h25m server-cluster-ip-service ClusterIP 10.99.73.109 <none> 5000/TCP 86s We can check logs of pods by using: $kubectl logs server-deployment-d9cb6bcf4-4z7n4 > @ start /app > node index.js Listening { Error: connect ECONNREFUSED 127.0.0.1:5432 at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1083:14) errno: 'ECONNREFUSED', code: 'ECONNREFUSED', syscall: 'connect', address: '127.0.0.1', port: 5432 }","title":"Reapplying a batch of config files"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/10_reapplying_a_batch_of_config_files/#reapplying-a-batch-of-config-files","text":"$ kubectl apply -f k8s service/client-cluster-ip-service unchanged deployment.apps/client-deployment unchanged service/server-cluster-ip-service created deployment.apps/server-deployment created deployment.apps/worker-deployment created $ kubectl get pods NAME READY STATUS RESTARTS AGE client-deployment-7bf8c9b5c5-7gcc7 1/1 Running 0 21m client-deployment-7bf8c9b5c5-96g47 1/1 Running 0 21m client-deployment-7bf8c9b5c5-qr64m 1/1 Running 0 21m server-deployment-d9cb6bcf4-2v75c 1/1 Running 0 26s server-deployment-d9cb6bcf4-4z7n4 1/1 Running 0 26s server-deployment-d9cb6bcf4-kp84s 1/1 Running 0 26s worker-deployment-85896bdb7-cs52k 0/1 CrashLoopBackOff 1 26s $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE client-deployment 3/3 3 3 22m server-deployment 3/3 3 3 67s worker-deployment 0/1 1 0 67s $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE client-cluster-ip-service ClusterIP 10.101.240.238 <none> 3000/TCP 22m kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 4h25m server-cluster-ip-service ClusterIP 10.99.73.109 <none> 5000/TCP 86s We can check logs of pods by using: $kubectl logs server-deployment-d9cb6bcf4-4z7n4 > @ start /app > node index.js Listening { Error: connect ECONNREFUSED 127.0.0.1:5432 at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1083:14) errno: 'ECONNREFUSED', code: 'ECONNREFUSED', syscall: 'connect', address: '127.0.0.1', port: 5432 }","title":"Reapplying a batch of config files"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/11_creating_and_applying_redis_config/","text":"Creating and applying redis config \u00b6 We are going to create a deployment for redis k8s/redis-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: redis-deployment spec: replicas: 1 selector: matchLabels: component: redis template: metadata: labels: component: redis spec: containers: - name: redis image: redis ports: - containerPort: 6379 And the ClusterIP at k8s/redis-cluster-ip-service.yaml apiVersion: v1 kind: Service metadata: name: redis-cluster-ip-service spec: type: ClusterIP selector: component: redis ports: - port: 6379 targetPort: 6379 And now apply: $ kubectl apply -f k8s service/client-cluster-ip-service unchanged deployment.apps/client-deployment unchanged service/redis-cluster-ip-service created deployment.apps/redis-deployment created service/server-cluster-ip-service unchanged deployment.apps/server-deployment unchanged deployment.apps/worker-deployment unchanged","title":"Creating and applying redis config"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/11_creating_and_applying_redis_config/#creating-and-applying-redis-config","text":"We are going to create a deployment for redis k8s/redis-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: redis-deployment spec: replicas: 1 selector: matchLabels: component: redis template: metadata: labels: component: redis spec: containers: - name: redis image: redis ports: - containerPort: 6379 And the ClusterIP at k8s/redis-cluster-ip-service.yaml apiVersion: v1 kind: Service metadata: name: redis-cluster-ip-service spec: type: ClusterIP selector: component: redis ports: - port: 6379 targetPort: 6379 And now apply: $ kubectl apply -f k8s service/client-cluster-ip-service unchanged deployment.apps/client-deployment unchanged service/redis-cluster-ip-service created deployment.apps/redis-deployment created service/server-cluster-ip-service unchanged deployment.apps/server-deployment unchanged deployment.apps/worker-deployment unchanged","title":"Creating and applying redis config"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/12_postgres_config/","text":"Postgres config \u00b6 We are going to create a deployment for redis k8s/postgres-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: postgres-deployment spec: replicas: 1 selector: matchLabels: component: postgres template: metadata: labels: component: postgres spec: containers: - name: postgres image: postgres ports: - containerPort: 5432 And the ClusterIP at k8s/postgres-cluster-ip-service.yaml apiVersion: v1 kind: Service metadata: name: postgres-cluster-ip-service spec: type: ClusterIP selector: component: postgres ports: - port: 5432 targetPort: 5432 And now apply: $ kubectl apply -f k8s service/client-cluster-ip-service unchanged deployment.apps/client-deployment unchanged service/postgres-cluster-ip-service created deployment.apps/postgres-deployment created service/redis-cluster-ip-service unchanged deployment.apps/redis-deployment unchanged service/server-cluster-ip-service unchanged deployment.apps/server-deployment unchanged deployment.apps/worker-deployment unchanged","title":"Postgres config"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/12_postgres_config/#postgres-config","text":"We are going to create a deployment for redis k8s/postgres-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: postgres-deployment spec: replicas: 1 selector: matchLabels: component: postgres template: metadata: labels: component: postgres spec: containers: - name: postgres image: postgres ports: - containerPort: 5432 And the ClusterIP at k8s/postgres-cluster-ip-service.yaml apiVersion: v1 kind: Service metadata: name: postgres-cluster-ip-service spec: type: ClusterIP selector: component: postgres ports: - port: 5432 targetPort: 5432 And now apply: $ kubectl apply -f k8s service/client-cluster-ip-service unchanged deployment.apps/client-deployment unchanged service/postgres-cluster-ip-service created deployment.apps/postgres-deployment created service/redis-cluster-ip-service unchanged deployment.apps/redis-deployment unchanged service/server-cluster-ip-service unchanged deployment.apps/server-deployment unchanged deployment.apps/worker-deployment unchanged","title":"Postgres config"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/13_the_need_for_volumes_with_databases/","text":"The need for volumes with databases \u00b6 We are going to set up the Postgre PVC in the previous schema, which stands for Persistent Volume Claim . If we would have a pod that crashes for the postgres, kubernetes would remove it and create a new clean one. If we would not have a volume, all data that was written to it would be lost. So, we use volumes to host the data on the host machine.","title":"The need for volumes with databases"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/13_the_need_for_volumes_with_databases/#the-need-for-volumes-with-databases","text":"We are going to set up the Postgre PVC in the previous schema, which stands for Persistent Volume Claim . If we would have a pod that crashes for the postgres, kubernetes would remove it and create a new clean one. If we would not have a volume, all data that was written to it would be lost. So, we use volumes to host the data on the host machine.","title":"The need for volumes with databases"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/14_kubernetes_volumes/","text":"Kubernetes volumes \u00b6 A volume in kubernetes is an object that allows a container to store data at the pod level.","title":"Kubernetes volumes"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/14_kubernetes_volumes/#kubernetes-volumes","text":"A volume in kubernetes is an object that allows a container to store data at the pod level.","title":"Kubernetes volumes"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/15_volumes_vs_persistent_volumes/","text":"Volumes vs Persistent volumes \u00b6 The volume object is stored into the pod, if the postgres container crashes, the data persists and a new container can read it. However, if entiire pod crashes, the data is gone. The Persistent Volume is not tied to any specific pod.","title":"Volumes vs Persistent volumes"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/15_volumes_vs_persistent_volumes/#volumes-vs-persistent-volumes","text":"The volume object is stored into the pod, if the postgres container crashes, the data persists and a new container can read it. However, if entiire pod crashes, the data is gone. The Persistent Volume is not tied to any specific pod.","title":"Volumes vs Persistent volumes"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/16_persistent_volumes_vs_peristent_volume_claims/","text":"Persistent Volumes vs Persistent Volume Claims \u00b6 A persistent volume claim is not a volume, but an options of storages instead. When a volume gets claimed, kubernetes checks statically provisioned peristent volumes, if none are found, kubernetes creates a dynamically provisioned persistent volume. Basically, PVC looks at existing volumes, if there are none, it creates them on the fly.","title":"Persistent Volumes vs Persistent Volume Claims"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/16_persistent_volumes_vs_peristent_volume_claims/#persistent-volumes-vs-persistent-volume-claims","text":"A persistent volume claim is not a volume, but an options of storages instead. When a volume gets claimed, kubernetes checks statically provisioned peristent volumes, if none are found, kubernetes creates a dynamically provisioned persistent volume. Basically, PVC looks at existing volumes, if there are none, it creates them on the fly.","title":"Persistent Volumes vs Persistent Volume Claims"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/17_claim_config_files/","text":"Claim config files \u00b6 We are going to make a new file k8s/database-persistent-volume-claim.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: database-persistent-volume-claim spec: accessModes: - ReadWriteOnce resources: requests: storage: 2Gi","title":"Claim config files"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/17_claim_config_files/#claim-config-files","text":"We are going to make a new file k8s/database-persistent-volume-claim.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: database-persistent-volume-claim spec: accessModes: - ReadWriteOnce resources: requests: storage: 2Gi","title":"Claim config files"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/18_persistent_volume_access_modes/","text":"Persistent VOlume Access Modes \u00b6 Once more, a volume claim is something we attach to a pod config. So, previously created PVC tells kubernetes that it must find a volume with the given requirements. We have 3 different access modes: - ReadWriteOnce - can be used by a single node at a time - ReadOnlyMany - multiple nodes at the same time can read from this volume - ReadWriteMany - can be read and written to by many nodes And the storage has to be 2G of space.","title":"Persistent VOlume Access Modes"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/18_persistent_volume_access_modes/#persistent-volume-access-modes","text":"Once more, a volume claim is something we attach to a pod config. So, previously created PVC tells kubernetes that it must find a volume with the given requirements. We have 3 different access modes: - ReadWriteOnce - can be used by a single node at a time - ReadOnlyMany - multiple nodes at the same time can read from this volume - ReadWriteMany - can be read and written to by many nodes And the storage has to be 2G of space.","title":"Persistent VOlume Access Modes"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/19_where_does_kubernetes_allocate_persistent_volumes/","text":"Where does kubernetes allocate persistent volumes? \u00b6 To get all the options for storage, we can see: $ kubectl get storageclass NAME PROVISIONER AGE standard (default) k8s.io/minikube-hostpath 5h8m $ kubectl describe storageclass Name: standard IsDefaultClass: Yes Annotations: storageclass.beta.kubernetes.io/is-default-class=true Provisioner: k8s.io/minikube-hostpath Parameters: <none> AllowVolumeExpansion: <unset> MountOptions: <none> ReclaimPolicy: Delete VolumeBindingMode: Immediate Events: <none> So, on our computer there is only one option - to store on hard drive. But when we would host it on cloud, we would get a large amount of options. All the available storage classes are available here https://kubernetes.io/docs/concepts/storage/storage-classes/ .","title":"Where does kubernetes allocate persistent volumes?"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/19_where_does_kubernetes_allocate_persistent_volumes/#where-does-kubernetes-allocate-persistent-volumes","text":"To get all the options for storage, we can see: $ kubectl get storageclass NAME PROVISIONER AGE standard (default) k8s.io/minikube-hostpath 5h8m $ kubectl describe storageclass Name: standard IsDefaultClass: Yes Annotations: storageclass.beta.kubernetes.io/is-default-class=true Provisioner: k8s.io/minikube-hostpath Parameters: <none> AllowVolumeExpansion: <unset> MountOptions: <none> ReclaimPolicy: Delete VolumeBindingMode: Immediate Events: <none> So, on our computer there is only one option - to store on hard drive. But when we would host it on cloud, we would get a large amount of options. All the available storage classes are available here https://kubernetes.io/docs/concepts/storage/storage-classes/ .","title":"Where does kubernetes allocate persistent volumes?"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/20_designating_a_pvc_in_a_pod_template/","text":"Designating a PVC in a Pod template \u00b6 We are going to modify the k8s/postgres-deployment.yaml config to have the PVC: apiVersion: apps/v1 kind: Deployment metadata: name: postgres-deployment spec: replicas: 1 selector: matchLabels: component: postgres template: metadata: labels: component: postgres spec: volumes: - name: postgres-storage persistentVolumeClaim: claimName: database-persistent-volume-claim containers: - name: postgres image: postgres ports: - containerPort: 5432 volumeMounts: - name: postgres-storage mountPath: /var/lib/postgresql/data subPath: postgres","title":"Designating a PVC in a Pod template"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/20_designating_a_pvc_in_a_pod_template/#designating-a-pvc-in-a-pod-template","text":"We are going to modify the k8s/postgres-deployment.yaml config to have the PVC: apiVersion: apps/v1 kind: Deployment metadata: name: postgres-deployment spec: replicas: 1 selector: matchLabels: component: postgres template: metadata: labels: component: postgres spec: volumes: - name: postgres-storage persistentVolumeClaim: claimName: database-persistent-volume-claim containers: - name: postgres image: postgres ports: - containerPort: 5432 volumeMounts: - name: postgres-storage mountPath: /var/lib/postgresql/data subPath: postgres","title":"Designating a PVC in a Pod template"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/21_applying_a_pvc/","text":"Applying a PVC \u00b6 Now the last thing is to apply the PVC to our local cluster. $ kubectl apply -f k8s service/client-cluster-ip-service unchanged deployment.apps/client-deployment unchanged persistentvolumeclaim/database-persistent-volume-claim created service/postgres-cluster-ip-service unchanged deployment.apps/postgres-deployment configured service/redis-cluster-ip-service unchanged deployment.apps/redis-deployment unchanged service/server-cluster-ip-service unchanged deployment.apps/server-deployment unchanged deployment.apps/worker-deployment unchanged $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-6fb13666-48e0-11e9-9347-080027ac9b62 2Gi RWO Delete Bound default/database-persistent-volume-claim standard 61s $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE database-persistent-volume-claim Bound pvc-6fb13666-48e0-11e9-9347-080027ac9b62 2Gi RWO standard 86s","title":"Applying a PVC"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/21_applying_a_pvc/#applying-a-pvc","text":"Now the last thing is to apply the PVC to our local cluster. $ kubectl apply -f k8s service/client-cluster-ip-service unchanged deployment.apps/client-deployment unchanged persistentvolumeclaim/database-persistent-volume-claim created service/postgres-cluster-ip-service unchanged deployment.apps/postgres-deployment configured service/redis-cluster-ip-service unchanged deployment.apps/redis-deployment unchanged service/server-cluster-ip-service unchanged deployment.apps/server-deployment unchanged deployment.apps/worker-deployment unchanged $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-6fb13666-48e0-11e9-9347-080027ac9b62 2Gi RWO Delete Bound default/database-persistent-volume-claim standard 61s $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE database-persistent-volume-claim Bound pvc-6fb13666-48e0-11e9-9347-080027ac9b62 2Gi RWO standard 86s","title":"Applying a PVC"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/22_defining_environment_variables/","text":"Defining environment variables \u00b6 One of the last things to set up for the application to work with kubernetes is to set up the environment variables.","title":"Defining environment variables"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/22_defining_environment_variables/#defining-environment-variables","text":"One of the last things to set up for the application to work with kubernetes is to set up the environment variables.","title":"Defining environment variables"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/23_adding_environment_variables_to_config/","text":"Adding environment variables to config \u00b6 We are going to modify the k8s/worker-deployment.yaml file: apiVersion: apps/v1 kind: Deployment metadata: name: worker-deployment spec: replicas: 1 selector: matchLabels: component: web template: metadata: labels: component: web spec: containers: - name: worker image: deiveris/multi-worker env: - name: REDIS_HOST value: redis-cluster-ip-service - name: REDIS_PORT value: 6379 And modify k8s/server-deployment.yaml : apiVersion: apps/v1 kind: Deployment metadata: name: server-deployment spec: replicas: 3 selector: matchLabels: component: server template: metadata: labels: component: server spec: containers: - name: server image: deiveris/multi-server ports: - containerPort: 5000 env: - name: REDIS_HOST value: redis-cluster-ip-service - name: REDIS_PORT value: 6379 - name: PGUSER value: postgres - name: PGHOST value: postgres-cluster-ip-service - name: PGPORT value: 5432 - name: PGDATABASE value: postgres","title":"Adding environment variables to config"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/23_adding_environment_variables_to_config/#adding-environment-variables-to-config","text":"We are going to modify the k8s/worker-deployment.yaml file: apiVersion: apps/v1 kind: Deployment metadata: name: worker-deployment spec: replicas: 1 selector: matchLabels: component: web template: metadata: labels: component: web spec: containers: - name: worker image: deiveris/multi-worker env: - name: REDIS_HOST value: redis-cluster-ip-service - name: REDIS_PORT value: 6379 And modify k8s/server-deployment.yaml : apiVersion: apps/v1 kind: Deployment metadata: name: server-deployment spec: replicas: 3 selector: matchLabels: component: server template: metadata: labels: component: server spec: containers: - name: server image: deiveris/multi-server ports: - containerPort: 5000 env: - name: REDIS_HOST value: redis-cluster-ip-service - name: REDIS_PORT value: 6379 - name: PGUSER value: postgres - name: PGHOST value: postgres-cluster-ip-service - name: PGPORT value: 5432 - name: PGDATABASE value: postgres","title":"Adding environment variables to config"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/24_creating_an_encoded_secret/","text":"Creating an encoded secret \u00b6 We dont want to store the last environment variable the PGPASSWORD as a plain text. We can store it as a secret object which securely stores a piece of information in the cluster such as passwords. In order to create it: kubectl create secret <generic|docker-registry|tls> <secret name> --from-literalkey=value $ kubectl create secret generic pgpassword --from-litraal PGPASSWORD=12345asd secret/pgpassword created $ kubectl get secrets NAME TYPE DATA AGE default-token-vq8qh kubernetes.io/service-account-token 3 5h37m pgpassword Opaque 1 26s","title":"Creating an encoded secret"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/24_creating_an_encoded_secret/#creating-an-encoded-secret","text":"We dont want to store the last environment variable the PGPASSWORD as a plain text. We can store it as a secret object which securely stores a piece of information in the cluster such as passwords. In order to create it: kubectl create secret <generic|docker-registry|tls> <secret name> --from-literalkey=value $ kubectl create secret generic pgpassword --from-litraal PGPASSWORD=12345asd secret/pgpassword created $ kubectl get secrets NAME TYPE DATA AGE default-token-vq8qh kubernetes.io/service-account-token 3 5h37m pgpassword Opaque 1 26s","title":"Creating an encoded secret"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/25_passing_secrets_as_environment_variables/","text":"Passing secrets as environment variables \u00b6 Now the last thing is to hook up the k8s/server-deployment.yaml file with the secret: apiVersion: apps/v1 kind: Deployment metadata: name: server-deployment spec: replicas: 3 selector: matchLabels: component: server template: metadata: labels: component: server spec: containers: - name: server image: deiveris/multi-server ports: - containerPort: 5000 env: - name: REDIS_HOST value: redis-cluster-ip-service - name: REDIS_PORT value: 6379 - name: PGUSER value: postgres - name: PGHOST value: postgres-cluster-ip-service - name: PGPORT value: 5432 - name: PGDATABASE value: postgres - name: PGPASSWORD valueFrom: secretKeyRef: name: pgpassword key: PGPASSWORD Also, we'll need to set up the secret password in k8s/postgres-deployment.yaml file: apiVersion: apps/v1 kind: Deployment metadata: name: postgres-deployment spec: replicas: 1 selector: matchLabels: component: postgres template: metadata: labels: component: postgres spec: volumes: - name: postgres-storage persistentVolumeClaim: claimName: database-persistent-volume-claim containers: - name: postgres image: postgres ports: - containerPort: 5432 volumeMounts: - name: postgres-storage mountPath: /var/lib/postgresql/data subPath: postgres env: - name: PGPASSWORD valueFrom: secretKeyRef: name: pgpassword key: PGPASSWORD Now, when we apply: $ kubectl apply -f k8s service/client-cluster-ip-service unchanged deployment.apps/client-deployment unchanged persistentvolumeclaim/database-persistent-volume-claim unchanged service/postgres-cluster-ip-service unchanged deployment.apps/postgres-deployment unchanged service/redis-cluster-ip-service unchanged deployment.apps/redis-deployment unchanged service/server-cluster-ip-service unchanged deployment.apps/server-deployment configured deployment.apps/worker-deployment configured","title":"Passing secrets as environment variables"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/25_passing_secrets_as_environment_variables/#passing-secrets-as-environment-variables","text":"Now the last thing is to hook up the k8s/server-deployment.yaml file with the secret: apiVersion: apps/v1 kind: Deployment metadata: name: server-deployment spec: replicas: 3 selector: matchLabels: component: server template: metadata: labels: component: server spec: containers: - name: server image: deiveris/multi-server ports: - containerPort: 5000 env: - name: REDIS_HOST value: redis-cluster-ip-service - name: REDIS_PORT value: 6379 - name: PGUSER value: postgres - name: PGHOST value: postgres-cluster-ip-service - name: PGPORT value: 5432 - name: PGDATABASE value: postgres - name: PGPASSWORD valueFrom: secretKeyRef: name: pgpassword key: PGPASSWORD Also, we'll need to set up the secret password in k8s/postgres-deployment.yaml file: apiVersion: apps/v1 kind: Deployment metadata: name: postgres-deployment spec: replicas: 1 selector: matchLabels: component: postgres template: metadata: labels: component: postgres spec: volumes: - name: postgres-storage persistentVolumeClaim: claimName: database-persistent-volume-claim containers: - name: postgres image: postgres ports: - containerPort: 5432 volumeMounts: - name: postgres-storage mountPath: /var/lib/postgresql/data subPath: postgres env: - name: PGPASSWORD valueFrom: secretKeyRef: name: pgpassword key: PGPASSWORD Now, when we apply: $ kubectl apply -f k8s service/client-cluster-ip-service unchanged deployment.apps/client-deployment unchanged persistentvolumeclaim/database-persistent-volume-claim unchanged service/postgres-cluster-ip-service unchanged deployment.apps/postgres-deployment unchanged service/redis-cluster-ip-service unchanged deployment.apps/redis-deployment unchanged service/server-cluster-ip-service unchanged deployment.apps/server-deployment configured deployment.apps/worker-deployment configured","title":"Passing secrets as environment variables"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/26_environment_variables_as_strings/","text":"Environment variables as strings \u00b6 When applying the configuration to kubectl now, it throws an error: \"k8s/worker-deployment.yaml\": cannot convert int64 to string To fix this, all we have to do is to wrap the environment variables, that are numbers, with quotes: value: '6379' So the k8s/server-deploymant.yaml is now: apiVersion: apps/v1 kind: Deployment metadata: name: server-deployment spec: replicas: 3 selector: matchLabels: component: server template: metadata: labels: component: server spec: containers: - name: server image: deiveris/multi-server ports: - containerPort: 5000 env: - name: REDIS_HOST value: redis-cluster-ip-service - name: REDIS_PORT value: '6379' - name: PGUSER value: postgres - name: PGHOST value: postgres-cluster-ip-service - name: PGPORT value: '5432' - name: PGDATABASE value: postgres - name: PGPASSWORD valueFrom: secretKeyRef: name: pgpassword key: PGPASSWORD And k8s/worker-deployment.yaml is now: apiVersion: apps/v1 kind: Deployment metadata: name: worker-deployment spec: replicas: 1 selector: matchLabels: component: web template: metadata: labels: component: web spec: containers: - name: worker image: deiveris/multi-worker env: - name: REDIS_HOST value: redis-cluster-ip-service - name: REDIS_PORT value: '6379'","title":"Environment variables as strings"},{"location":"Docker%20%26%20Kubernetes/14_a_multi-container_app_with_kubernetes/26_environment_variables_as_strings/#environment-variables-as-strings","text":"When applying the configuration to kubectl now, it throws an error: \"k8s/worker-deployment.yaml\": cannot convert int64 to string To fix this, all we have to do is to wrap the environment variables, that are numbers, with quotes: value: '6379' So the k8s/server-deploymant.yaml is now: apiVersion: apps/v1 kind: Deployment metadata: name: server-deployment spec: replicas: 3 selector: matchLabels: component: server template: metadata: labels: component: server spec: containers: - name: server image: deiveris/multi-server ports: - containerPort: 5000 env: - name: REDIS_HOST value: redis-cluster-ip-service - name: REDIS_PORT value: '6379' - name: PGUSER value: postgres - name: PGHOST value: postgres-cluster-ip-service - name: PGPORT value: '5432' - name: PGDATABASE value: postgres - name: PGPASSWORD valueFrom: secretKeyRef: name: pgpassword key: PGPASSWORD And k8s/worker-deployment.yaml is now: apiVersion: apps/v1 kind: Deployment metadata: name: worker-deployment spec: replicas: 1 selector: matchLabels: component: web template: metadata: labels: component: web spec: containers: - name: worker image: deiveris/multi-worker env: - name: REDIS_HOST value: redis-cluster-ip-service - name: REDIS_PORT value: '6379'","title":"Environment variables as strings"},{"location":"Docker%20%26%20Kubernetes/15_handling_traffic_with_ingress_controllers/01_load_balancer_services/","text":"Load balancer services \u00b6 Previously we set up all the Deployments and ClusterIP services, but we still need to somehow route all the traffic to these services. We are starting to wire up an Ingress Service . In this section we will look at the LoadBalancer service and why we are not going to use it. It is a legacy way of getting network traffic into a cluster. It does two separate things: - Allow to access only one set of pods - Kubernetes will reach out to cloud provider and create a load balancer using their configuration","title":"Load balancer services"},{"location":"Docker%20%26%20Kubernetes/15_handling_traffic_with_ingress_controllers/01_load_balancer_services/#load-balancer-services","text":"Previously we set up all the Deployments and ClusterIP services, but we still need to somehow route all the traffic to these services. We are starting to wire up an Ingress Service . In this section we will look at the LoadBalancer service and why we are not going to use it. It is a legacy way of getting network traffic into a cluster. It does two separate things: - Allow to access only one set of pods - Kubernetes will reach out to cloud provider and create a load balancer using their configuration","title":"Load balancer services"},{"location":"Docker%20%26%20Kubernetes/15_handling_traffic_with_ingress_controllers/02_quick_note_on_ingresses/","text":"A quck note on ingresses \u00b6 In kubernetes there are several different implementations of ingress. We are going to use ingress-nginx . Which is a community led project, not the official kubernetes-ingress which is a separate project that is also nginx based. Setup of ingress-nginx changes depending on your environment (local, Google Cloud, AWS, Azure). We are going to set up ingress-nginx on local and Google Cloud environments.","title":"A quck note on ingresses"},{"location":"Docker%20%26%20Kubernetes/15_handling_traffic_with_ingress_controllers/02_quick_note_on_ingresses/#a-quck-note-on-ingresses","text":"In kubernetes there are several different implementations of ingress. We are going to use ingress-nginx . Which is a community led project, not the official kubernetes-ingress which is a separate project that is also nginx based. Setup of ingress-nginx changes depending on your environment (local, Google Cloud, AWS, Azure). We are going to set up ingress-nginx on local and Google Cloud environments.","title":"A quck note on ingresses"},{"location":"Docker%20%26%20Kubernetes/15_handling_traffic_with_ingress_controllers/03_behind_the_scenes_of_ingress/","text":"Behind the scenes of Ingress \u00b6 We are going to set up an ingress controller, which is using nginx behind the scenes. This will create a pod running nginx that sends traffic further into the cluster based on the config. We are making use of the ingres-nginx instead of configuring our own nginx pod because it is more aware that it is in a kubernetes cluster. Instead of sending requests to ClusterIP services, it will send them straight to the pods, which can be a must in situations where we need sticky sessions etc. More reading on ingress-nginx is available at https://www.joyfulbikeshedding.com/blog/2018-03-26-studying-the-kubernetes-ingress-system.html .","title":"Behind the scenes of Ingress"},{"location":"Docker%20%26%20Kubernetes/15_handling_traffic_with_ingress_controllers/03_behind_the_scenes_of_ingress/#behind-the-scenes-of-ingress","text":"We are going to set up an ingress controller, which is using nginx behind the scenes. This will create a pod running nginx that sends traffic further into the cluster based on the config. We are making use of the ingres-nginx instead of configuring our own nginx pod because it is more aware that it is in a kubernetes cluster. Instead of sending requests to ClusterIP services, it will send them straight to the pods, which can be a must in situations where we need sticky sessions etc. More reading on ingress-nginx is available at https://www.joyfulbikeshedding.com/blog/2018-03-26-studying-the-kubernetes-ingress-system.html .","title":"Behind the scenes of Ingress"},{"location":"Docker%20%26%20Kubernetes/15_handling_traffic_with_ingress_controllers/04_setting_up_ingress_locally/","text":"Setting up ingress locally \u00b6 First we are going to https://kubernetes.github.io/ingress-nginx/deploy/#generic-deployment . And copy the Mandatory command and run it: kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml This will create a health check pod called default-backend , configs and the deployment of ingress controller . $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml namespace/ingress-nginx created configmap/nginx-configuration created configmap/tcp-services created configmap/udp-services created serviceaccount/nginx-ingress-serviceaccount created clusterrole.rbac.authorization.k8s.io/nginx-ingress-clusterrole created role.rbac.authorization.k8s.io/nginx-ingress-role created rolebinding.rbac.authorization.k8s.io/nginx-ingress-role-nisa-binding created clusterrolebinding.rbac.authorization.k8s.io/nginx-ingress-clusterrole-nisa-binding created deployment.apps/nginx-ingress-controller created $ minikube addons enable ingress - ingress was successfully enabled","title":"Setting up ingress locally"},{"location":"Docker%20%26%20Kubernetes/15_handling_traffic_with_ingress_controllers/04_setting_up_ingress_locally/#setting-up-ingress-locally","text":"First we are going to https://kubernetes.github.io/ingress-nginx/deploy/#generic-deployment . And copy the Mandatory command and run it: kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml This will create a health check pod called default-backend , configs and the deployment of ingress controller . $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml namespace/ingress-nginx created configmap/nginx-configuration created configmap/tcp-services created configmap/udp-services created serviceaccount/nginx-ingress-serviceaccount created clusterrole.rbac.authorization.k8s.io/nginx-ingress-clusterrole created role.rbac.authorization.k8s.io/nginx-ingress-role created rolebinding.rbac.authorization.k8s.io/nginx-ingress-role-nisa-binding created clusterrolebinding.rbac.authorization.k8s.io/nginx-ingress-clusterrole-nisa-binding created deployment.apps/nginx-ingress-controller created $ minikube addons enable ingress - ingress was successfully enabled","title":"Setting up ingress locally"},{"location":"Docker%20%26%20Kubernetes/15_handling_traffic_with_ingress_controllers/05_creating_the_ingress_config/","text":"Creating the ingress config \u00b6 Now the last thing we need to do is to create ingress config simillary like we did when we created our own nginx server with ELB. Create k8s/ingress-service.yaml file: apiVersion: extensions/v1beta1 kind: Ingress metadata: name: ingress-service annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/rewrite-target: /$1 spec: rules: http: paths: - path: /?(.*) backend: serviceName: client-cluster-ip-service servicePort: 3000 - path: /api/?(.*) backend: serviceName: server-cluster-ip-service servicePort: 5000 Now apply: $ kubectl apply -f k8s service/client-cluster-ip-service unchanged deployment.apps/client-deployment unchanged persistentvolumeclaim/database-persistent-volume-claim unchanged ingress.extensions/ingress-service created service/postgres-cluster-ip-service unchanged deployment.apps/postgres-deployment unchanged service/redis-cluster-ip-service unchanged deployment.apps/redis-deployment unchanged service/server-cluster-ip-service unchanged deployment.apps/server-deployment unchanged deployment.apps/worker-deployment unchanged","title":"Creating the ingress config"},{"location":"Docker%20%26%20Kubernetes/15_handling_traffic_with_ingress_controllers/05_creating_the_ingress_config/#creating-the-ingress-config","text":"Now the last thing we need to do is to create ingress config simillary like we did when we created our own nginx server with ELB. Create k8s/ingress-service.yaml file: apiVersion: extensions/v1beta1 kind: Ingress metadata: name: ingress-service annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/rewrite-target: /$1 spec: rules: http: paths: - path: /?(.*) backend: serviceName: client-cluster-ip-service servicePort: 3000 - path: /api/?(.*) backend: serviceName: server-cluster-ip-service servicePort: 5000 Now apply: $ kubectl apply -f k8s service/client-cluster-ip-service unchanged deployment.apps/client-deployment unchanged persistentvolumeclaim/database-persistent-volume-claim unchanged ingress.extensions/ingress-service created service/postgres-cluster-ip-service unchanged deployment.apps/postgres-deployment unchanged service/redis-cluster-ip-service unchanged deployment.apps/redis-deployment unchanged service/server-cluster-ip-service unchanged deployment.apps/server-deployment unchanged deployment.apps/worker-deployment unchanged","title":"Creating the ingress config"},{"location":"Docker%20%26%20Kubernetes/15_handling_traffic_with_ingress_controllers/06_testing_ingress_locally/","text":"Testing ingress locally \u00b6 To test that everything is set up properlly, first we are going to get the IP of our cluster: $ minikube ip 192.168.99.101 And now we can visit this IP directly: https://192.168.99.101 The nginx will force https, which will appear not secure. Ignore the sertificate. See that all the services are working as they should be:","title":"Testing ingress locally"},{"location":"Docker%20%26%20Kubernetes/15_handling_traffic_with_ingress_controllers/06_testing_ingress_locally/#testing-ingress-locally","text":"To test that everything is set up properlly, first we are going to get the IP of our cluster: $ minikube ip 192.168.99.101 And now we can visit this IP directly: https://192.168.99.101 The nginx will force https, which will appear not secure. Ignore the sertificate. See that all the services are working as they should be:","title":"Testing ingress locally"},{"location":"Docker%20%26%20Kubernetes/15_handling_traffic_with_ingress_controllers/07_the_minikube_dashboard/","text":"The minikube dashboard \u00b6 We can run this command: $ minikube dashboard - Enabling dashboard ... - Verifying dashboard health ... - Launching proxy ... - Verifying proxy health ... - Opening http://127.0.0.1:37975/api/v1/namespaces/kube-system/services/http:kubernetes-dashboard:/proxy/ in your default browser... [27329:27349:0317/214654.391943:ERROR:browser_process_sub_thread.cc(209)] Waited 4 ms for network service Opening in existing browser session. It will open up a dashboard that shows exactly what is going on in our cluster. We can also make changes but they will not be reflected to our configration files we made.","title":"The minikube dashboard"},{"location":"Docker%20%26%20Kubernetes/15_handling_traffic_with_ingress_controllers/07_the_minikube_dashboard/#the-minikube-dashboard","text":"We can run this command: $ minikube dashboard - Enabling dashboard ... - Verifying dashboard health ... - Launching proxy ... - Verifying proxy health ... - Opening http://127.0.0.1:37975/api/v1/namespaces/kube-system/services/http:kubernetes-dashboard:/proxy/ in your default browser... [27329:27349:0317/214654.391943:ERROR:browser_process_sub_thread.cc(209)] Waited 4 ms for network service Opening in existing browser session. It will open up a dashboard that shows exactly what is going on in our cluster. We can also make changes but they will not be reflected to our configration files we made.","title":"The minikube dashboard"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/01_the_deployment_process/","text":"The deployment process \u00b6 Create Github repo Tie repo to Travis CI Create Google Cloud project Enable billing for the project Add deployment scripts to the repo","title":"The deployment process"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/01_the_deployment_process/#the-deployment-process","text":"Create Github repo Tie repo to Travis CI Create Google Cloud project Enable billing for the project Add deployment scripts to the repo","title":"The deployment process"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/02_Google_Cloud_vs_AWS_for_kubernetes/","text":"Google Cloud vs AWS for kubernetes \u00b6 Why Google Cloud? \u00b6 Google created kubernetes AWS only \"recently\" got kubernetes support It is easier to work with kubernetes on Google Cloud Excellent documentation for beginners","title":"Google Cloud vs AWS for kubernetes"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/02_Google_Cloud_vs_AWS_for_kubernetes/#google-cloud-vs-aws-for-kubernetes","text":"","title":"Google Cloud vs AWS for kubernetes"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/02_Google_Cloud_vs_AWS_for_kubernetes/#why-google-cloud","text":"Google created kubernetes AWS only \"recently\" got kubernetes support It is easier to work with kubernetes on Google Cloud Excellent documentation for beginners","title":"Why Google Cloud?"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/03_creating_git_repo/","text":"Creating Git repo \u00b6 We are going to create a repository on GitHub: https://github.com/daviskregers/multi-k8s Then cd into it and push the code: $ git status On branch master No commits yet Untracked files: (use \"git add <file>...\" to include in what will be committed) .gitignore README.md client/ k8s/ server/ worker/ nothing added to commit but untracked files present (use \"git add\" to track) $ git add . $ git commit -m \"add files\" [master (root-commit) acadafe] add files 43 files changed, 17403 insertions(+) create mode 100644 .gitignore create mode 100644 README.md create mode 100755 client/.gitignore create mode 100644 client/Dockerfile create mode 100644 client/Dockerfile.dev create mode 100755 client/README.md create mode 100644 client/nginx/default.conf create mode 100644 client/package-lock.json create mode 100644 client/package.json create mode 100755 client/public/favicon.ico create mode 100755 client/public/index.html create mode 100755 client/public/manifest.json create mode 100644 client/src/App.css create mode 100644 client/src/App.js create mode 100644 client/src/App.test.js create mode 100644 client/src/Fib.js create mode 100644 client/src/OtherPage.js create mode 100644 client/src/index.css create mode 100644 client/src/index.js create mode 100644 client/src/logo.svg create mode 100644 client/src/registerServiceWorker.js create mode 100755 client/src/serviceWorker.js create mode 100644 k8s/client-cluster-ip-service.yaml create mode 100644 k8s/client-deployment.yaml create mode 100644 k8s/database-persistent-volume-claim.yaml create mode 100644 k8s/ingress-service.yaml create mode 100644 k8s/postgres-cluster-ip-service.yaml create mode 100644 k8s/postgres-deployment.yaml create mode 100644 k8s/redis-cluster-ip-service.yaml create mode 100644 k8s/redis-deployment.yaml create mode 100644 k8s/server-cluseter-ip-service.yaml create mode 100644 k8s/server-deployment.yaml create mode 100644 k8s/worker-deployment.yaml create mode 100644 server/Dockerfile create mode 100644 server/Dockerfile.dev create mode 100644 server/index.js create mode 100644 server/keys.js create mode 100644 server/package.json create mode 100644 worker/Dockerfile create mode 100644 worker/Dockerfile.dev create mode 100644 worker/index.js create mode 100644 worker/keys.js create mode 100644 worker/package.json $ git remote add origin git@github.com:daviskregers/multi-k8s.git $ git push -u origin master Enumerating objects: 50, done. Counting objects: 100% (50/50), done. Delta compression using up to 8 threads Compressing objects: 100% (49/49), done. Writing objects: 100% (50/50), 150.76 KiB | 1.17 MiB/s, done. Total 50 (delta 8), reused 0 (delta 0) remote: Resolving deltas: 100% (8/8), done. To github.com:daviskregers/multi-k8s.git * [new branch] master -> master Branch 'master' set up to track remote branch 'master' from 'origin'.","title":"Creating Git repo"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/03_creating_git_repo/#creating-git-repo","text":"We are going to create a repository on GitHub: https://github.com/daviskregers/multi-k8s Then cd into it and push the code: $ git status On branch master No commits yet Untracked files: (use \"git add <file>...\" to include in what will be committed) .gitignore README.md client/ k8s/ server/ worker/ nothing added to commit but untracked files present (use \"git add\" to track) $ git add . $ git commit -m \"add files\" [master (root-commit) acadafe] add files 43 files changed, 17403 insertions(+) create mode 100644 .gitignore create mode 100644 README.md create mode 100755 client/.gitignore create mode 100644 client/Dockerfile create mode 100644 client/Dockerfile.dev create mode 100755 client/README.md create mode 100644 client/nginx/default.conf create mode 100644 client/package-lock.json create mode 100644 client/package.json create mode 100755 client/public/favicon.ico create mode 100755 client/public/index.html create mode 100755 client/public/manifest.json create mode 100644 client/src/App.css create mode 100644 client/src/App.js create mode 100644 client/src/App.test.js create mode 100644 client/src/Fib.js create mode 100644 client/src/OtherPage.js create mode 100644 client/src/index.css create mode 100644 client/src/index.js create mode 100644 client/src/logo.svg create mode 100644 client/src/registerServiceWorker.js create mode 100755 client/src/serviceWorker.js create mode 100644 k8s/client-cluster-ip-service.yaml create mode 100644 k8s/client-deployment.yaml create mode 100644 k8s/database-persistent-volume-claim.yaml create mode 100644 k8s/ingress-service.yaml create mode 100644 k8s/postgres-cluster-ip-service.yaml create mode 100644 k8s/postgres-deployment.yaml create mode 100644 k8s/redis-cluster-ip-service.yaml create mode 100644 k8s/redis-deployment.yaml create mode 100644 k8s/server-cluseter-ip-service.yaml create mode 100644 k8s/server-deployment.yaml create mode 100644 k8s/worker-deployment.yaml create mode 100644 server/Dockerfile create mode 100644 server/Dockerfile.dev create mode 100644 server/index.js create mode 100644 server/keys.js create mode 100644 server/package.json create mode 100644 worker/Dockerfile create mode 100644 worker/Dockerfile.dev create mode 100644 worker/index.js create mode 100644 worker/keys.js create mode 100644 worker/package.json $ git remote add origin git@github.com:daviskregers/multi-k8s.git $ git push -u origin master Enumerating objects: 50, done. Counting objects: 100% (50/50), done. Delta compression using up to 8 threads Compressing objects: 100% (49/49), done. Writing objects: 100% (50/50), 150.76 KiB | 1.17 MiB/s, done. Total 50 (delta 8), reused 0 (delta 0) remote: Resolving deltas: 100% (8/8), done. To github.com:daviskregers/multi-k8s.git * [new branch] master -> master Branch 'master' set up to track remote branch 'master' from 'origin'.","title":"Creating Git repo"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/04_linking_the_github_repo_to_travis/","text":"Linking the Github Repo to Travis CI \u00b6 Now that we have created the github repo and pushed the code to it, we can link travis CI to it. We are going to https://travis-ci.com/account/repositories First, we are going to push the Sync account button on the left side. Then we are going to add the repo in Manage repositories on Github . Click on save and then the multi-k8s building should be enabled. You can click on Settings to verify it.","title":"Linking the Github Repo to Travis CI"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/04_linking_the_github_repo_to_travis/#linking-the-github-repo-to-travis-ci","text":"Now that we have created the github repo and pushed the code to it, we can link travis CI to it. We are going to https://travis-ci.com/account/repositories First, we are going to push the Sync account button on the left side. Then we are going to add the repo in Manage repositories on Github . Click on save and then the multi-k8s building should be enabled. You can click on Settings to verify it.","title":"Linking the Github Repo to Travis CI"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/05_free_google_cloud_credits/","text":"Free google cloud credits \u00b6 You can try using this link https://console.cloud.google.com/freetrial/signup/tos for receiving free Google Cloud credits. You will need to create a new account to receive them.","title":"Free google cloud credits"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/05_free_google_cloud_credits/#free-google-cloud-credits","text":"You can try using this link https://console.cloud.google.com/freetrial/signup/tos for receiving free Google Cloud credits. You will need to create a new account to receive them.","title":"Free google cloud credits"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/06_creating_a_google_cloud_project/","text":"Creating a Google Cloud project \u00b6 In order to create the new project, we can navigate to https://console.cloud.google.com . On the top left corner, there is a project selector: We are going to create a new project and call it multi-k8s .","title":"Creating a Google Cloud project"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/06_creating_a_google_cloud_project/#creating-a-google-cloud-project","text":"In order to create the new project, we can navigate to https://console.cloud.google.com . On the top left corner, there is a project selector: We are going to create a new project and call it multi-k8s .","title":"Creating a Google Cloud project"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/07_linking_a_billing_account/","text":"Linking a Billing account \u00b6 When we have created a new project and selected it, we must make sure to add billing to it in order to use kubernetes clusters. That can be set up in the left hand side menu, under Billing .","title":"Linking a Billing account"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/07_linking_a_billing_account/#linking-a-billing-account","text":"When we have created a new project and selected it, we must make sure to add billing to it in order to use kubernetes clusters. That can be set up in the left hand side menu, under Billing .","title":"Linking a Billing account"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/08_kuberenetes_engine_init/","text":"Kubernetes Engine init \u00b6 Now we are going to create a new kubernetes cluster. We are gong to Compute -> Kubernetes Engine -> Clusters . Now it will enable kubernetes engine:","title":"Kubernetes Engine init"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/08_kuberenetes_engine_init/#kubernetes-engine-init","text":"Now we are going to create a new kubernetes cluster. We are gong to Compute -> Kubernetes Engine -> Clusters . Now it will enable kubernetes engine:","title":"Kubernetes Engine init"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/09_creating_a_cluster_with_google_cloud/","text":"Creating a cluster with Google Cloud \u00b6 Now when the APIs have been initialized, we can create a new cluster: We are going to click on the Create cluster button. We are going to set the name as multi-cluster , set the location as Zonal and select whatever is closer to us. Leave the master version as the default setting. For node pools we are going to select 3 number of nodes , 1 vCPU machine type . Now we are going to click on Create .","title":"Creating a cluster with Google Cloud"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/09_creating_a_cluster_with_google_cloud/#creating-a-cluster-with-google-cloud","text":"Now when the APIs have been initialized, we can create a new cluster: We are going to click on the Create cluster button. We are going to set the name as multi-cluster , set the location as Zonal and select whatever is closer to us. Leave the master version as the default setting. For node pools we are going to select 3 number of nodes , 1 vCPU machine type . Now we are going to click on Create .","title":"Creating a cluster with Google Cloud"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/10_kubernetes_dashboard_on_Google_Cloud/","text":"Kubernetes dashboard on Google Cloud \u00b6 When the cluster has been created, it will look like this: We can click on the multi-cluster and it will display information for it. In the left menu there are multiple other sections like: Workloads - display all the pods and deployments that belong to the application Services - all the services in the cluster Applications - different plugins / 3rd party software to install in our cluster Configuration - all environment configuration / secrets Storage - list all persistent volumes and persistent volume claims.","title":"Kubernetes dashboard on Google Cloud"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/10_kubernetes_dashboard_on_Google_Cloud/#kubernetes-dashboard-on-google-cloud","text":"When the cluster has been created, it will look like this: We can click on the multi-cluster and it will display information for it. In the left menu there are multiple other sections like: Workloads - display all the pods and deployments that belong to the application Services - all the services in the cluster Applications - different plugins / 3rd party software to install in our cluster Configuration - all environment configuration / secrets Storage - list all persistent volumes and persistent volume claims.","title":"Kubernetes dashboard on Google Cloud"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/11_travis_deployment_overview/","text":"Travis deployment overview \u00b6 We are going to do following steps to deploy the application to Google Cloud using Travis CI. Install Gogle Cloud SDK CLI Configure the SDK with out Google Cloud auth info Login to Docker CLI Build the test version of multi-client Run tests If tests pass, run a script to deploy newest images Build all our images, tag each one, push each to docker hub Apply all configs in k8s folder Imperatively set latest images on each deployment","title":"Travis deployment overview"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/11_travis_deployment_overview/#travis-deployment-overview","text":"We are going to do following steps to deploy the application to Google Cloud using Travis CI. Install Gogle Cloud SDK CLI Configure the SDK with out Google Cloud auth info Login to Docker CLI Build the test version of multi-client Run tests If tests pass, run a script to deploy newest images Build all our images, tag each one, push each to docker hub Apply all configs in k8s folder Imperatively set latest images on each deployment","title":"Travis deployment overview"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/12_installing_the_Google_Cloud_SDK/","text":"Installing the Google Cloud SDK \u00b6 We are going to create a new .travis.yaml file ine the project directory. touch .travis.yaml Then we are going to tell it to install Google Cloud SDK and log into it. sudo: required services: - docker before_install: - curl https://sdk.cloud.google.com | bash > /dev/null; - source $HOME/google-cloud-sdk/path.bash.inc - gcloud components update kubectl - gcloud auth activate-service-account --key-file service-account.json","title":"Installing the Google Cloud SDK"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/12_installing_the_Google_Cloud_SDK/#installing-the-google-cloud-sdk","text":"We are going to create a new .travis.yaml file ine the project directory. touch .travis.yaml Then we are going to tell it to install Google Cloud SDK and log into it. sudo: required services: - docker before_install: - curl https://sdk.cloud.google.com | bash > /dev/null; - source $HOME/google-cloud-sdk/path.bash.inc - gcloud components update kubectl - gcloud auth activate-service-account --key-file service-account.json","title":"Installing the Google Cloud SDK"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/13_generating_a_service_account/","text":"Generating a service account \u00b6 Now, when we have configuration on how to install google cloud and authorize in it, we need to create credentials for it to authorize with. We are going to: Create a Service account Download service account credentials in a json file Download and install Travis CLI Encrypt and upload the json file to our travis account In travis.yml, add code to unencrypt the json file and load into GCloud SDK So, first, we are going to IAM & admin section in google cloud: Then go to Service accounts and then click on Create Service account . We are name the service account travis-deployer : In the roles section, we are going to make it as a Kubernetes Engine Admin : On the final step, we want to click on the Create key and select JSON type. Once created, it will you to download it. We can click on Done and see that the account has been created.","title":"Generating a service account"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/13_generating_a_service_account/#generating-a-service-account","text":"Now, when we have configuration on how to install google cloud and authorize in it, we need to create credentials for it to authorize with. We are going to: Create a Service account Download service account credentials in a json file Download and install Travis CLI Encrypt and upload the json file to our travis account In travis.yml, add code to unencrypt the json file and load into GCloud SDK So, first, we are going to IAM & admin section in google cloud: Then go to Service accounts and then click on Create Service account . We are name the service account travis-deployer : In the roles section, we are going to make it as a Kubernetes Engine Admin : On the final step, we want to click on the Create key and select JSON type. Once created, it will you to download it. We can click on Done and see that the account has been created.","title":"Generating a service account"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/14_running_travis_CLI_in_a_container/","text":"Running Travis CLI in a container \u00b6 Since Travis CLI requires rubu installed on a local machine, we don't really want to install it. We can use docker to install the Travis CLI. $ docker run -it -v $(pwd):/app ruby:2.3 sh Unable to find image 'ruby:2.3' locally 2.3: Pulling from library/ruby e79bb959ec00: Pull complete d4b7902036fe: Pull complete 1b2a72d4e030: Pull complete d54db43011fd: Pull complete 69d473365bb3: Pull complete 84ed2a0dc034: Pull complete 8952ca0665c5: Pull complete ef485f36c624: Pull complete Digest: sha256:78cc821d95c48621e577b6b0d44c9d509f0f2a4e089b9fd0ca2ae86f274773a8 Status: Downloaded newer image for ruby:2.3 # cd app # gem install travis Fetching multipart-post-2.0.0.gem Fetching faraday-0.15.4.gem Fetching faraday_middleware-0.13.1.gem Fetching highline-1.7.10.gem Fetching backports-3.13.0.gem Fetching multi_json-1.13.1.gem Fetching addressable-2.4.0.gem Fetching net-http-persistent-2.9.4.gem Fetching net-http-pipeline-1.0.1.gem Fetching gh-0.15.1.gem Fetching launchy-2.4.3.gem Fetching ffi-1.10.0.gem Fetching ethon-0.12.0.gem Fetching typhoeus-0.8.0.gem Fetching websocket-1.2.8.gem Fetching pusher-client-0.6.2.gem Fetching travis-1.8.9.gem Successfully installed multipart-post-2.0.0 Successfully installed faraday-0.15.4 Successfully installed faraday_middleware-0.13.1 Successfully installed highline-1.7.10 Successfully installed backports-3.13.0 Successfully installed multi_json-1.13.1 Successfully installed addressable-2.4.0 Successfully installed net-http-persistent-2.9.4 Successfully installed net-http-pipeline-1.0.1 Successfully installed gh-0.15.1 Successfully installed launchy-2.4.3 Building native extensions. This could take a while... Successfully installed ffi-1.10.0 Successfully installed ethon-0.12.0 Successfully installed typhoeus-0.8.0 Successfully installed websocket-1.2.8 Successfully installed pusher-client-0.6.2 Successfully installed travis-1.8.9 # travis Shell completion not installed. Would you like to install it now? |y| n Usage: travis COMMAND ... Available commands: accounts displays accounts and their subscription status branches displays the most recent build for each branch cache lists or deletes repository caches cancel cancels a job or build console interactive shell disable disables a project enable enables a project encrypt encrypts values for the .travis.yml encrypt-file encrypts a file and adds decryption steps to .travis.yml endpoint displays or changes the API endpoint env show or modify build environment variables help helps you out when in dire need of information history displays a projects build history init generates a .travis.yml and enables the project lint display warnings for a .travis.yml login authenticates against the API and stores the token logout deletes the stored API token logs streams test logs monitor live monitor for what's going on open opens a build or job in the browser pubkey prints out a repository's public key raw makes an (authenticated) API call and prints out the result report generates a report useful for filing issues repos lists repositories the user has certain permissions on requests lists recent requests restart restarts a build or job settings access repository settings setup sets up an addon or deploy target show displays a build or job sshkey checks, updates or deletes an SSH key status checks status of the latest build sync triggers a new sync with GitHub token outputs the secret API token version outputs the client version whatsup lists most recent builds whoami outputs the current user run `/usr/local/bundle/bin/travis help COMMAND` for more infos","title":"Running Travis CLI in a container"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/14_running_travis_CLI_in_a_container/#running-travis-cli-in-a-container","text":"Since Travis CLI requires rubu installed on a local machine, we don't really want to install it. We can use docker to install the Travis CLI. $ docker run -it -v $(pwd):/app ruby:2.3 sh Unable to find image 'ruby:2.3' locally 2.3: Pulling from library/ruby e79bb959ec00: Pull complete d4b7902036fe: Pull complete 1b2a72d4e030: Pull complete d54db43011fd: Pull complete 69d473365bb3: Pull complete 84ed2a0dc034: Pull complete 8952ca0665c5: Pull complete ef485f36c624: Pull complete Digest: sha256:78cc821d95c48621e577b6b0d44c9d509f0f2a4e089b9fd0ca2ae86f274773a8 Status: Downloaded newer image for ruby:2.3 # cd app # gem install travis Fetching multipart-post-2.0.0.gem Fetching faraday-0.15.4.gem Fetching faraday_middleware-0.13.1.gem Fetching highline-1.7.10.gem Fetching backports-3.13.0.gem Fetching multi_json-1.13.1.gem Fetching addressable-2.4.0.gem Fetching net-http-persistent-2.9.4.gem Fetching net-http-pipeline-1.0.1.gem Fetching gh-0.15.1.gem Fetching launchy-2.4.3.gem Fetching ffi-1.10.0.gem Fetching ethon-0.12.0.gem Fetching typhoeus-0.8.0.gem Fetching websocket-1.2.8.gem Fetching pusher-client-0.6.2.gem Fetching travis-1.8.9.gem Successfully installed multipart-post-2.0.0 Successfully installed faraday-0.15.4 Successfully installed faraday_middleware-0.13.1 Successfully installed highline-1.7.10 Successfully installed backports-3.13.0 Successfully installed multi_json-1.13.1 Successfully installed addressable-2.4.0 Successfully installed net-http-persistent-2.9.4 Successfully installed net-http-pipeline-1.0.1 Successfully installed gh-0.15.1 Successfully installed launchy-2.4.3 Building native extensions. This could take a while... Successfully installed ffi-1.10.0 Successfully installed ethon-0.12.0 Successfully installed typhoeus-0.8.0 Successfully installed websocket-1.2.8 Successfully installed pusher-client-0.6.2 Successfully installed travis-1.8.9 # travis Shell completion not installed. Would you like to install it now? |y| n Usage: travis COMMAND ... Available commands: accounts displays accounts and their subscription status branches displays the most recent build for each branch cache lists or deletes repository caches cancel cancels a job or build console interactive shell disable disables a project enable enables a project encrypt encrypts values for the .travis.yml encrypt-file encrypts a file and adds decryption steps to .travis.yml endpoint displays or changes the API endpoint env show or modify build environment variables help helps you out when in dire need of information history displays a projects build history init generates a .travis.yml and enables the project lint display warnings for a .travis.yml login authenticates against the API and stores the token logout deletes the stored API token logs streams test logs monitor live monitor for what's going on open opens a build or job in the browser pubkey prints out a repository's public key raw makes an (authenticated) API call and prints out the result report generates a report useful for filing issues repos lists repositories the user has certain permissions on requests lists recent requests restart restarts a build or job settings access repository settings setup sets up an addon or deploy target show displays a build or job sshkey checks, updates or deletes an SSH key status checks status of the latest build sync triggers a new sync with GitHub token outputs the secret API token version outputs the client version whatsup lists most recent builds whoami outputs the current user run `/usr/local/bundle/bin/travis help COMMAND` for more infos","title":"Running Travis CLI in a container"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/15_encrypting_a_service_account_file/","text":"Encrypting a service account file \u00b6 We are continuing working inside the Travis CLI docker container from previous section: # travis login --pro Make sure that you have copied the service account credential json file to the project directory. # ls README.md client k8s multi-k8s-236808-e22ce5e7f920.json server worker # mv multi-k8s-236808-e22ce5e7f920.json service-account.json # ls README.md client k8s server service-account.json worker Now we encrypt the file and specify that we want it to tie it up to our repository (note that it is case sensitive) : # travis encrypt-file --pro service-account.json encrypting service-account.json for daviskregers/multi-k8s storing result as service-account.json.enc storing secure env variables for decryption Please add the following to your build script (before_install stage in your .travis.yml, for instance): openssl aes-256-cbc -K $encrypted_0c35eebf403c_key -iv $encrypted_0c35eebf403c_iv -in service-account.json.enc -out service-account.json -d Pro Tip: You can add it automatically by running with --add. Make sure to add service-account.json.enc to the git repository. Make sure not to add service-account.json to the git repository. Commit all changes to your .travis.yml. We are going to copy the command: openssl aes-256-cbc -K $encrypted_0c35eebf403c_key -iv $encrypted_0c35eebf403c_iv -in service-account.json.enc -out service-account.json -d and copy it into the .travis.yaml sudo: required services: - docker before_install: - openssl aes-256-cbc -K $encrypted_0c35eebf403c_key -iv $encrypted_0c35eebf403c_iv -in service-account.json.enc -out service-account.json -d - curl https://sdk.cloud.google.com | bash > /dev/null; - source $HOME/google-cloud-sdk/path.bash.inc - gcloud components update kubectl - gcloud auth activate-service-account --key-file service-account.json # ls README.md client k8s server service-account.json service-account.json.enc worker Now we need to delete the orginal json file and commit the travis.yml and the encrypted account. $ rm service-account.json $ git status On branch master Your branch is up to date with 'origin/master'. Untracked files: (use \"git add <file>...\" to include in what will be committed) .travis.yaml service-account.json.enc nothing added to commit but untracked files present (use \"git add\" to track) $ git add . $ git commit -m \"travis\" [master 7114a57] travis 2 files changed, 9 insertions(+) create mode 100644 .travis.yaml create mode 100644 service-account.json.enc","title":"Encrypting a service account file"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/15_encrypting_a_service_account_file/#encrypting-a-service-account-file","text":"We are continuing working inside the Travis CLI docker container from previous section: # travis login --pro Make sure that you have copied the service account credential json file to the project directory. # ls README.md client k8s multi-k8s-236808-e22ce5e7f920.json server worker # mv multi-k8s-236808-e22ce5e7f920.json service-account.json # ls README.md client k8s server service-account.json worker Now we encrypt the file and specify that we want it to tie it up to our repository (note that it is case sensitive) : # travis encrypt-file --pro service-account.json encrypting service-account.json for daviskregers/multi-k8s storing result as service-account.json.enc storing secure env variables for decryption Please add the following to your build script (before_install stage in your .travis.yml, for instance): openssl aes-256-cbc -K $encrypted_0c35eebf403c_key -iv $encrypted_0c35eebf403c_iv -in service-account.json.enc -out service-account.json -d Pro Tip: You can add it automatically by running with --add. Make sure to add service-account.json.enc to the git repository. Make sure not to add service-account.json to the git repository. Commit all changes to your .travis.yml. We are going to copy the command: openssl aes-256-cbc -K $encrypted_0c35eebf403c_key -iv $encrypted_0c35eebf403c_iv -in service-account.json.enc -out service-account.json -d and copy it into the .travis.yaml sudo: required services: - docker before_install: - openssl aes-256-cbc -K $encrypted_0c35eebf403c_key -iv $encrypted_0c35eebf403c_iv -in service-account.json.enc -out service-account.json -d - curl https://sdk.cloud.google.com | bash > /dev/null; - source $HOME/google-cloud-sdk/path.bash.inc - gcloud components update kubectl - gcloud auth activate-service-account --key-file service-account.json # ls README.md client k8s server service-account.json service-account.json.enc worker Now we need to delete the orginal json file and commit the travis.yml and the encrypted account. $ rm service-account.json $ git status On branch master Your branch is up to date with 'origin/master'. Untracked files: (use \"git add <file>...\" to include in what will be committed) .travis.yaml service-account.json.enc nothing added to commit but untracked files present (use \"git add\" to track) $ git add . $ git commit -m \"travis\" [master 7114a57] travis 2 files changed, 9 insertions(+) create mode 100644 .travis.yaml create mode 100644 service-account.json.enc","title":"Encrypting a service account file"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/16_more_google_cloud_cli_config/","text":"More Google Cloud CLI Config \u00b6 Previously we created an encrypted account service file that unencrypted, placed into the project directory and fed to Google Cloud CLI once it is installed on Travis CLI. Now we can set more configuration: sudo: required services: - docker before_install: - openssl aes-256-cbc -K $encrypted_0c35eebf403c_key -iv $encrypted_0c35eebf403c_iv -in service-account.json.enc -out service-account.json -d - curl https://sdk.cloud.google.com | bash > /dev/null; - source $HOME/google-cloud-sdk/path.bash.inc - gcloud components update kubectl - gcloud auth activate-service-account --key-file service-account.json - gcloud config set project multi-k8s-236808 - gcloud config set compute/zone europe-west1-c - gcloud container clusters get-credentials multi-cluster You can get the project ID from the project selector: You can get the container name and zone from Compute -> Kubernetes Engine -> Clusters","title":"More Google Cloud CLI Config"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/16_more_google_cloud_cli_config/#more-google-cloud-cli-config","text":"Previously we created an encrypted account service file that unencrypted, placed into the project directory and fed to Google Cloud CLI once it is installed on Travis CLI. Now we can set more configuration: sudo: required services: - docker before_install: - openssl aes-256-cbc -K $encrypted_0c35eebf403c_key -iv $encrypted_0c35eebf403c_iv -in service-account.json.enc -out service-account.json -d - curl https://sdk.cloud.google.com | bash > /dev/null; - source $HOME/google-cloud-sdk/path.bash.inc - gcloud components update kubectl - gcloud auth activate-service-account --key-file service-account.json - gcloud config set project multi-k8s-236808 - gcloud config set compute/zone europe-west1-c - gcloud container clusters get-credentials multi-cluster You can get the project ID from the project selector: You can get the container name and zone from Compute -> Kubernetes Engine -> Clusters","title":"More Google Cloud CLI Config"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/17_running_tests_with_travis/","text":"Running tests with Travis \u00b6 We are going to modify the .travis.yaml file to include tests. The docker credentials have been set up like Travis Configuration . The encrypted variables were automatically generated when encrypting the service account file. sudo: required services: - docker before_install: - openssl aes-256-cbc -K $encrypted_0c35eebf403c_key -iv $encrypted_0c35eebf403c_iv -in service-account.json.enc -out service-account.json -d - curl https://sdk.cloud.google.com | bash > /dev/null; - source $HOME/google-cloud-sdk/path.bash.inc - gcloud components update kubectl - gcloud auth activate-service-account --key-file service-account.json - gcloud config set project multi-k8s-236808 - gcloud config set compute/zone europe-west1-c - gcloud container clusters get-credentials multi-cluster - echo \"$DOCKER_PASSWORD\" | docker login -u \"$DOCKER_ID\" --password-stdin - docker build -t deiveris/react-test -f ./client/Dockerfile.dev ./client script: - docker run deiveris/react-test npm test -- --coverage","title":"Running tests with Travis"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/17_running_tests_with_travis/#running-tests-with-travis","text":"We are going to modify the .travis.yaml file to include tests. The docker credentials have been set up like Travis Configuration . The encrypted variables were automatically generated when encrypting the service account file. sudo: required services: - docker before_install: - openssl aes-256-cbc -K $encrypted_0c35eebf403c_key -iv $encrypted_0c35eebf403c_iv -in service-account.json.enc -out service-account.json -d - curl https://sdk.cloud.google.com | bash > /dev/null; - source $HOME/google-cloud-sdk/path.bash.inc - gcloud components update kubectl - gcloud auth activate-service-account --key-file service-account.json - gcloud config set project multi-k8s-236808 - gcloud config set compute/zone europe-west1-c - gcloud container clusters get-credentials multi-cluster - echo \"$DOCKER_PASSWORD\" | docker login -u \"$DOCKER_ID\" --password-stdin - docker build -t deiveris/react-test -f ./client/Dockerfile.dev ./client script: - docker run deiveris/react-test npm test -- --coverage","title":"Running tests with Travis"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/17_unique_tags_for_built_images/","text":"Unique tags for build images \u00b6 In previous section we stumbled upon an issue that we need to create unique tags for images so we can apply them to the kubernetes cluster. We can use git SHA for the tag.","title":"Unique tags for build images"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/17_unique_tags_for_built_images/#unique-tags-for-build-images","text":"In previous section we stumbled upon an issue that we need to create unique tags for images so we can apply them to the kubernetes cluster. We can use git SHA for the tag.","title":"Unique tags for build images"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/18_custom_deployment_providers/","text":"Custom deployment providers \u00b6 We are going to write a custom script if the tests have passed: Build all our images, tag each one, push to docker hub Apply all configs in k8s folder Imperatively set latest images on each deployment First, we modify the .travis.yml file: sudo: required services: - docker before_install: - openssl aes-256-cbc -K $encrypted_0c35eebf403c_key -iv $encrypted_0c35eebf403c_iv -in service-account.json.enc -out service-account.json -d - curl https://sdk.cloud.google.com | bash > /dev/null; - source $HOME/google-cloud-sdk/path.bash.inc - gcloud components update kubectl - gcloud auth activate-service-account --key-file service-account.json - gcloud config set project multi-k8s-236808 - gcloud config set compute/zone europe-west1-c - gcloud container clusters get-credentials multi-cluster - echo \"$DOCKER_PASSWORD\" | docker login -u \"$DOCKER_ID\" --password-stdin - docker build -t deiveris/react-test -f ./client/Dockerfile.dev ./client script: - docker run deiveris/react-test npm test -- --coverage deploy: provider: script script: bash ./deploy.sh on: branch: master","title":"Custom deployment providers"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/18_custom_deployment_providers/#custom-deployment-providers","text":"We are going to write a custom script if the tests have passed: Build all our images, tag each one, push to docker hub Apply all configs in k8s folder Imperatively set latest images on each deployment First, we modify the .travis.yml file: sudo: required services: - docker before_install: - openssl aes-256-cbc -K $encrypted_0c35eebf403c_key -iv $encrypted_0c35eebf403c_iv -in service-account.json.enc -out service-account.json -d - curl https://sdk.cloud.google.com | bash > /dev/null; - source $HOME/google-cloud-sdk/path.bash.inc - gcloud components update kubectl - gcloud auth activate-service-account --key-file service-account.json - gcloud config set project multi-k8s-236808 - gcloud config set compute/zone europe-west1-c - gcloud container clusters get-credentials multi-cluster - echo \"$DOCKER_PASSWORD\" | docker login -u \"$DOCKER_ID\" --password-stdin - docker build -t deiveris/react-test -f ./client/Dockerfile.dev ./client script: - docker run deiveris/react-test npm test -- --coverage deploy: provider: script script: bash ./deploy.sh on: branch: master","title":"Custom deployment providers"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/19_unique_deployment_images/","text":"Unique deployment images \u00b6 We are create the deploy.sh file that Travis CI is going to execute: # Build images docker build -t deiveris/multi-client -f ./client/Dockerfile ./client docker build -t deiveris/multi-server -f ./server/Dockerfile ./server docker build -t deiveris/multi-worker -f ./worker/Dockerfile ./worker # Push to Docker Hub docker push deiveris/multi-client docker push deiveris/multi-server docker push deiveris/multi-woker # Apply configrations kubectl apply -f k8s # Imperatively set latest images on each deployment kubectl set image deployments/server-deployment server=deiveris/multi-server This deployment will not work perfectly, because it will set the latest tag to the deployments. Since the latest was already previously set, there will be no changes. This can be fixed by using a git commit SHA checksum. Before we apply this, we will create new environment variables for travis: sudo: required services: - docker env: global: - SHA = $(git rev-parse HEAD) - CLOUDSDK_CORE_DISABLE_PROMPTS=1 before_install: - openssl aes-256-cbc -K $encrypted_0c35eebf403c_key -iv $encrypted_0c35eebf403c_iv -in service-account.json.enc -out service-account.json -d - curl https://sdk.cloud.google.com | bash > /dev/null; - source $HOME/google-cloud-sdk/path.bash.inc - gcloud components update kubectl - gcloud auth activate-service-account --key-file service-account.json - gcloud config set project multi-k8s-236808 - gcloud config set compute/zone europe-west1-c - gcloud container clusters get-credentials multi-cluster - echo \"$DOCKER_PASSWORD\" | docker login -u \"$DOCKER_ID\" --password-stdin - docker build -t deiveris/react-test -f ./client/Dockerfile.dev ./client script: - docker run deiveris/react-test npm test -- --coverage deploy: provider: script script: bash ./deploy.sh on: branch: master Now the deployment script will look like this: # Build images docker build -t deiveris/multi-client:latest -t deiveris/multi-client:$SHA -f ./client/Dockerfile ./client docker build -t deiveris/multi-server:latest -t deiveris/multi-server:$SHA -f ./server/Dockerfile ./server docker build -t deiveris/multi-worker:latest -t deiveris/multi-worker:$SHA -f ./worker/Dockerfile ./worker # Push to Docker Hub docker push deiveris/multi-client:latest docker push deiveris/multi-server:latest docker push deiveris/multi-woker:latest docker push deiveris/multi-client:$SHA docker push deiveris/multi-server:$SHA docker push deiveris/multi-woker:$SHA # Apply configrations kubectl apply -f k8s # Imperatively set latest images on each deployment kubectl set image deployments/server-deployment server=deiveris/multi-server:$SHA kubectl set image deployments/client-deployment client=deiveris/multi-client:$SHA kubectl set image deployments/worker-deployment worker=deiveris/multi-worker:$SHA","title":"Unique deployment images"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/19_unique_deployment_images/#unique-deployment-images","text":"We are create the deploy.sh file that Travis CI is going to execute: # Build images docker build -t deiveris/multi-client -f ./client/Dockerfile ./client docker build -t deiveris/multi-server -f ./server/Dockerfile ./server docker build -t deiveris/multi-worker -f ./worker/Dockerfile ./worker # Push to Docker Hub docker push deiveris/multi-client docker push deiveris/multi-server docker push deiveris/multi-woker # Apply configrations kubectl apply -f k8s # Imperatively set latest images on each deployment kubectl set image deployments/server-deployment server=deiveris/multi-server This deployment will not work perfectly, because it will set the latest tag to the deployments. Since the latest was already previously set, there will be no changes. This can be fixed by using a git commit SHA checksum. Before we apply this, we will create new environment variables for travis: sudo: required services: - docker env: global: - SHA = $(git rev-parse HEAD) - CLOUDSDK_CORE_DISABLE_PROMPTS=1 before_install: - openssl aes-256-cbc -K $encrypted_0c35eebf403c_key -iv $encrypted_0c35eebf403c_iv -in service-account.json.enc -out service-account.json -d - curl https://sdk.cloud.google.com | bash > /dev/null; - source $HOME/google-cloud-sdk/path.bash.inc - gcloud components update kubectl - gcloud auth activate-service-account --key-file service-account.json - gcloud config set project multi-k8s-236808 - gcloud config set compute/zone europe-west1-c - gcloud container clusters get-credentials multi-cluster - echo \"$DOCKER_PASSWORD\" | docker login -u \"$DOCKER_ID\" --password-stdin - docker build -t deiveris/react-test -f ./client/Dockerfile.dev ./client script: - docker run deiveris/react-test npm test -- --coverage deploy: provider: script script: bash ./deploy.sh on: branch: master Now the deployment script will look like this: # Build images docker build -t deiveris/multi-client:latest -t deiveris/multi-client:$SHA -f ./client/Dockerfile ./client docker build -t deiveris/multi-server:latest -t deiveris/multi-server:$SHA -f ./server/Dockerfile ./server docker build -t deiveris/multi-worker:latest -t deiveris/multi-worker:$SHA -f ./worker/Dockerfile ./worker # Push to Docker Hub docker push deiveris/multi-client:latest docker push deiveris/multi-server:latest docker push deiveris/multi-woker:latest docker push deiveris/multi-client:$SHA docker push deiveris/multi-server:$SHA docker push deiveris/multi-woker:$SHA # Apply configrations kubectl apply -f k8s # Imperatively set latest images on each deployment kubectl set image deployments/server-deployment server=deiveris/multi-server:$SHA kubectl set image deployments/client-deployment client=deiveris/multi-client:$SHA kubectl set image deployments/worker-deployment worker=deiveris/multi-worker:$SHA","title":"Unique deployment images"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/20_configuring_gcloud_cli_on_cloud_console/","text":"Configuring GCloud CLI on Cloud Console \u00b6 Before we push the project to travis and deploy it, we will need to set everything up on Google Cloud side, like PGPASSWORD secret which we previously created locally We can run Cloud Console by clicking on this icon in Google Cloud . Now we can run the commands: gcloud config set project multi-k8s-236808 gcloud config set compute/zone europe-west1-c gcloud container clusters get-credentials multi-cluster","title":"Configuring GCloud CLI on Cloud Console"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/20_configuring_gcloud_cli_on_cloud_console/#configuring-gcloud-cli-on-cloud-console","text":"Before we push the project to travis and deploy it, we will need to set everything up on Google Cloud side, like PGPASSWORD secret which we previously created locally We can run Cloud Console by clicking on this icon in Google Cloud . Now we can run the commands: gcloud config set project multi-k8s-236808 gcloud config set compute/zone europe-west1-c gcloud container clusters get-credentials multi-cluster","title":"Configuring GCloud CLI on Cloud Console"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/21_creating_a_secret_on_google_cloud/","text":"Creating a secret on google cloud \u00b6 Previously we connected to the Google Cloud CLI and selected the container we want to work with. Now we are going to set up the secrets. kubectl create secret generic pgpassword --from-literal PGPASSWORD=thepassword123 After running the command, we can see the secret under the Configuration tab.","title":"Creating a secret on google cloud"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/21_creating_a_secret_on_google_cloud/#creating-a-secret-on-google-cloud","text":"Previously we connected to the Google Cloud CLI and selected the container we want to work with. Now we are going to set up the secrets. kubectl create secret generic pgpassword --from-literal PGPASSWORD=thepassword123 After running the command, we can see the secret under the Configuration tab.","title":"Creating a secret on google cloud"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/22_helm_setup/","text":"Helm setup \u00b6 Previously we had setup an ingress service in the k8s directory. It relies on the ingress-nginx project. 15_handling_traffic_with_ingress_controllers/04_setting_up_ingress_locally We have to set it up on Google Cloud service now. We can find the commands for this at https://kubernetes.github.io/ingress-nginx/deploy/ . But, different than the last time, we are going to use Helm. It is a programm that can be used to administer 3rd party software inside our kubernetes cluster. It uses 2 separate pieces of software - helm and tiller . The helm is a client to tiller that sits inside the cluster and issues commands to modify objects in it. The documentation for it can be found at helm.sh/ . We are going to use the Quckstart Guide . And find the Installing Helm -> From script section. $ curl https://raw.githubusercontent.com/helm/helm/master/scripts/get > get_helm.sh $ chmod 700 get_helm.sh $ ./get_helm.sh And paste it into the Google Cloud Shell . The Google Cloud Console now is saying that we should run helm init , but do not run it yet as we need to add some extra setup first.","title":"Helm setup"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/22_helm_setup/#helm-setup","text":"Previously we had setup an ingress service in the k8s directory. It relies on the ingress-nginx project. 15_handling_traffic_with_ingress_controllers/04_setting_up_ingress_locally We have to set it up on Google Cloud service now. We can find the commands for this at https://kubernetes.github.io/ingress-nginx/deploy/ . But, different than the last time, we are going to use Helm. It is a programm that can be used to administer 3rd party software inside our kubernetes cluster. It uses 2 separate pieces of software - helm and tiller . The helm is a client to tiller that sits inside the cluster and issues commands to modify objects in it. The documentation for it can be found at helm.sh/ . We are going to use the Quckstart Guide . And find the Installing Helm -> From script section. $ curl https://raw.githubusercontent.com/helm/helm/master/scripts/get > get_helm.sh $ chmod 700 get_helm.sh $ ./get_helm.sh And paste it into the Google Cloud Shell . The Google Cloud Console now is saying that we should run helm init , but do not run it yet as we need to add some extra setup first.","title":"Helm setup"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/23_kubernetes_security_with_RBAC/","text":"Kubernetes security with RBAC \u00b6 RBAC stands for Role Based Access Control : - Limits who can access and modify objects in our cluster - Enabled on Google Cloud by default - Tiller wants to make changes to our cluster, so it needs to get some permissions set There are 4 different security roles: - User Accounts - identifies as a person administering our cluster - Service Accounts - identifies a pod administering a cluster - ClusterRoleBinding - Authorizes an account to do a certain set of actions accros the entire cluster - RoleBinding - Authorizes an account to do a certain set of actions in a single namespace.","title":"Kubernetes security with RBAC"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/23_kubernetes_security_with_RBAC/#kubernetes-security-with-rbac","text":"RBAC stands for Role Based Access Control : - Limits who can access and modify objects in our cluster - Enabled on Google Cloud by default - Tiller wants to make changes to our cluster, so it needs to get some permissions set There are 4 different security roles: - User Accounts - identifies as a person administering our cluster - Service Accounts - identifies a pod administering a cluster - ClusterRoleBinding - Authorizes an account to do a certain set of actions accros the entire cluster - RoleBinding - Authorizes an account to do a certain set of actions in a single namespace.","title":"Kubernetes security with RBAC"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/24_assigning_tiller_a_service_account/","text":"Assigning Tiller a Service account \u00b6 We can run the following commands in Google Cloud Shell to create a service account for Tiller . kubectl create serviceaccount --namespace kube-system tiller kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller Now, we can finally run the helm init: helm init --service-account tiller --upgrade Now, finally, we have Helm installed.","title":"Assigning Tiller a Service account"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/24_assigning_tiller_a_service_account/#assigning-tiller-a-service-account","text":"We can run the following commands in Google Cloud Shell to create a service account for Tiller . kubectl create serviceaccount --namespace kube-system tiller kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller Now, we can finally run the helm init: helm init --service-account tiller --upgrade Now, finally, we have Helm installed.","title":"Assigning Tiller a Service account"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/25_ingress_nginx_with_helm/","text":"Ingress-Nginx with Helm \u00b6 Now we can install the ingress-nginx on Google Cloud by using Helm. The commands for this are available at https://kubernetes.github.io/ingress-nginx/deploy/#using-helm . helm install stable/nginx-ingress --name my-nginx --set rbac.create=true","title":"Ingress-Nginx with Helm"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/25_ingress_nginx_with_helm/#ingress-nginx-with-helm","text":"Now we can install the ingress-nginx on Google Cloud by using Helm. The commands for this are available at https://kubernetes.github.io/ingress-nginx/deploy/#using-helm . helm install stable/nginx-ingress --name my-nginx --set rbac.create=true","title":"Ingress-Nginx with Helm"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/26_the_result_of_ingress_nginx/","text":"The Result of ingress-nginx \u00b6 Now, after when we have run the helm initialization, we can go to Kubernetes Engine -> Workloads and see that there is an nginx-ingress deployments. Also, under Services section, there should be a Load Balancer with two endpoints: We can go to Network Services -> Load Balancing , we can find a load balancer that is linked to our 3 nodes: So, now, the final step is to actually deploy our application.","title":"The Result of ingress-nginx"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/26_the_result_of_ingress_nginx/#the-result-of-ingress-nginx","text":"Now, after when we have run the helm initialization, we can go to Kubernetes Engine -> Workloads and see that there is an nginx-ingress deployments. Also, under Services section, there should be a Load Balancer with two endpoints: We can go to Network Services -> Load Balancing , we can find a load balancer that is linked to our 3 nodes: So, now, the final step is to actually deploy our application.","title":"The Result of ingress-nginx"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/27_deployment/","text":"Deployment \u00b6 Now, we can go to the project and push the changes: $ git status On branch master Your branch is ahead of 'origin/master' by 2 commits. (use \"git push\" to publish your local commits) nothing to commit, working tree clean $ git push origin master Enumerating objects: 9, done. Counting objects: 100% (9/9), done. Delta compression using up to 8 threads Compressing objects: 100% (8/8), done. Writing objects: 100% (8/8), 3.83 KiB | 3.83 MiB/s, done. Total 8 (delta 2), reused 0 (delta 0) remote: Resolving deltas: 100% (2/2), done. To github.com:daviskregers/multi-k8s.git acadafe..0f85f84 master -> master Now the Travis CI has been started: And after some time: We can verify that the image was pushed to the Docker Hub: It has pushed both the latest and the SHA tags to docker hub. And if we visit the url of the Load Balancer at https://35.240.34.60/ , we will see that the application has been deployed, but it does not work properly because of the unsigned certificate. Also, under the Kubernetes Engine we can see the new */ and */api/ routes. Also Workloads section: And the Persistent Storage for postgres:","title":"Deployment"},{"location":"Docker%20%26%20Kubernetes/16_kubernetes_production_deployment/27_deployment/#deployment","text":"Now, we can go to the project and push the changes: $ git status On branch master Your branch is ahead of 'origin/master' by 2 commits. (use \"git push\" to publish your local commits) nothing to commit, working tree clean $ git push origin master Enumerating objects: 9, done. Counting objects: 100% (9/9), done. Delta compression using up to 8 threads Compressing objects: 100% (8/8), done. Writing objects: 100% (8/8), 3.83 KiB | 3.83 MiB/s, done. Total 8 (delta 2), reused 0 (delta 0) remote: Resolving deltas: 100% (2/2), done. To github.com:daviskregers/multi-k8s.git acadafe..0f85f84 master -> master Now the Travis CI has been started: And after some time: We can verify that the image was pushed to the Docker Hub: It has pushed both the latest and the SHA tags to docker hub. And if we visit the url of the Load Balancer at https://35.240.34.60/ , we will see that the application has been deployed, but it does not work properly because of the unsigned certificate. Also, under the Kubernetes Engine we can see the new */ and */api/ routes. Also Workloads section: And the Persistent Storage for postgres:","title":"Deployment"},{"location":"Docker%20%26%20Kubernetes/17_HTTPS_with_kubernetes/01_https_setup_overview/","text":"HTTPS setup overview \u00b6 In order to setup an HTTPS certificate in our kubernetes cluster, we'll need a domain name. This will be used to setup letsencrypt and pass it's validation.","title":"HTTPS setup overview"},{"location":"Docker%20%26%20Kubernetes/17_HTTPS_with_kubernetes/01_https_setup_overview/#https-setup-overview","text":"In order to setup an HTTPS certificate in our kubernetes cluster, we'll need a domain name. This will be used to setup letsencrypt and pass it's validation.","title":"HTTPS setup overview"},{"location":"Docker%20%26%20Kubernetes/17_HTTPS_with_kubernetes/02_domain_setup/","text":"Domain setup \u00b6 Since I already have a domain at deiveris.lv , I will create a subdomain at k8s.deiveris.lv . In domain settings, I am going to create an A Record that points to the Load Balancer in Google Cloud .","title":"Domain setup"},{"location":"Docker%20%26%20Kubernetes/17_HTTPS_with_kubernetes/02_domain_setup/#domain-setup","text":"Since I already have a domain at deiveris.lv , I will create a subdomain at k8s.deiveris.lv . In domain settings, I am going to create an A Record that points to the Load Balancer in Google Cloud .","title":"Domain setup"},{"location":"Docker%20%26%20Kubernetes/17_HTTPS_with_kubernetes/03_cert_manager_install/","text":"Cert Manager installation \u00b6 We are going to use cert-manager package to manage the certificates. The documentation is available at https://docs.cert-manager.io/en/latest/ . We can install it by using Helm : https://docs.cert-manager.io/en/latest/getting-started/install.html#installing-with-helm . So, we'll run the command in our Google Cloud Shell . # Install the CustomResourceDefinition resources separately kubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.7/deploy/manifests/00-crds.yaml # Create the namespace for cert-manager kubectl create namespace cert-manager # Label the cert-manager namespace to disable resource validation kubectl label namespace cert-manager certmanager.k8s.io/disable-validation=true # Add the Jetstack Helm repository helm repo add jetstack https://charts.jetstack.io # Update your local Helm chart repository cache helm repo update # Install the cert-manager Helm chart helm install \\ --name cert-manager \\ --namespace cert-manager \\ --version v0.7.0 \\ jetstack/cert-manager Verify Installation:","title":"Cert Manager installation"},{"location":"Docker%20%26%20Kubernetes/17_HTTPS_with_kubernetes/03_cert_manager_install/#cert-manager-installation","text":"We are going to use cert-manager package to manage the certificates. The documentation is available at https://docs.cert-manager.io/en/latest/ . We can install it by using Helm : https://docs.cert-manager.io/en/latest/getting-started/install.html#installing-with-helm . So, we'll run the command in our Google Cloud Shell . # Install the CustomResourceDefinition resources separately kubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.7/deploy/manifests/00-crds.yaml # Create the namespace for cert-manager kubectl create namespace cert-manager # Label the cert-manager namespace to disable resource validation kubectl label namespace cert-manager certmanager.k8s.io/disable-validation=true # Add the Jetstack Helm repository helm repo add jetstack https://charts.jetstack.io # Update your local Helm chart repository cache helm repo update # Install the cert-manager Helm chart helm install \\ --name cert-manager \\ --namespace cert-manager \\ --version v0.7.0 \\ jetstack/cert-manager Verify Installation:","title":"Cert Manager installation"},{"location":"Docker%20%26%20Kubernetes/17_HTTPS_with_kubernetes/04_issuer_config_fire/","text":"Wire up Cert Manager \u00b6 We are going to create k8s/issuer.yaml file: apiVersion: certmanager.k8s.io/v1alpha1 kind: ClusterIssuer metadata: name: letsencrypt-prod spec: acme: server: https://acme-v02.api.letsencrypt.org/directory email: 'kregers.davis@gmail.com' privateKeySecretRef: name: letsencrypt-prod http01: {}","title":"Wire up Cert Manager"},{"location":"Docker%20%26%20Kubernetes/17_HTTPS_with_kubernetes/04_issuer_config_fire/#wire-up-cert-manager","text":"We are going to create k8s/issuer.yaml file: apiVersion: certmanager.k8s.io/v1alpha1 kind: ClusterIssuer metadata: name: letsencrypt-prod spec: acme: server: https://acme-v02.api.letsencrypt.org/directory email: 'kregers.davis@gmail.com' privateKeySecretRef: name: letsencrypt-prod http01: {}","title":"Wire up Cert Manager"},{"location":"Docker%20%26%20Kubernetes/17_HTTPS_with_kubernetes/05_certificate_config_file/","text":"Certificate Config File \u00b6 We are going to create k8s/certificate.yaml : apiVersion: certmanager.k8s.io/v1alpha1 kind: Certificate metadata: name: k8s-deiveris-lv-tls spec: secretName: k8s-deiveris-lv issuerRef: name: letsencrypt-prod kind: ClusterIssuer commonName: k8s.deiveris.lv dnsNames: - k8s.deiveris.lv acme: config: - http01: ingressClass: nginx domains: - k8s.deiveris.lv","title":"Certificate Config File"},{"location":"Docker%20%26%20Kubernetes/17_HTTPS_with_kubernetes/05_certificate_config_file/#certificate-config-file","text":"We are going to create k8s/certificate.yaml : apiVersion: certmanager.k8s.io/v1alpha1 kind: Certificate metadata: name: k8s-deiveris-lv-tls spec: secretName: k8s-deiveris-lv issuerRef: name: letsencrypt-prod kind: ClusterIssuer commonName: k8s.deiveris.lv dnsNames: - k8s.deiveris.lv acme: config: - http01: ingressClass: nginx domains: - k8s.deiveris.lv","title":"Certificate Config File"},{"location":"Docker%20%26%20Kubernetes/17_HTTPS_with_kubernetes/06_deploy_changes/","text":"Deploy changes \u00b6 $ git status On branch master Your branch is up to date with 'origin/master'. Untracked files: (use \"git add <file>...\" to include in what will be committed) k8s/certificate.yaml k8s/issuer.yaml nothing added to commit but untracked files present (use \"git add\" to track) $ git add . $ git commit -m \"added certificateand issuer\" [master 56fb459] added certificate and issuer 2 files changed, 29 insertions(+) create mode 100644 k8s/certificate.yaml create mode 100644 k8s/issuer.yaml $ git push origin master Enumerating objects: 7, done. Counting objects: 100% (7/7), done. Delta compression using up to 8 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (5/5), 794 bytes | 794.00 KiB/s, done. Total 5 (delta 2), reused 0 (delta 0) remote: Resolving deltas: 100% (2/2), completed with 2 local objects. To github.com:daviskregers/multi-k8s.git 3dee337..56fb459 master -> master Now the travis will update the cluster.","title":"Deploy changes"},{"location":"Docker%20%26%20Kubernetes/17_HTTPS_with_kubernetes/06_deploy_changes/#deploy-changes","text":"$ git status On branch master Your branch is up to date with 'origin/master'. Untracked files: (use \"git add <file>...\" to include in what will be committed) k8s/certificate.yaml k8s/issuer.yaml nothing added to commit but untracked files present (use \"git add\" to track) $ git add . $ git commit -m \"added certificateand issuer\" [master 56fb459] added certificate and issuer 2 files changed, 29 insertions(+) create mode 100644 k8s/certificate.yaml create mode 100644 k8s/issuer.yaml $ git push origin master Enumerating objects: 7, done. Counting objects: 100% (7/7), done. Delta compression using up to 8 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (5/5), 794 bytes | 794.00 KiB/s, done. Total 5 (delta 2), reused 0 (delta 0) remote: Resolving deltas: 100% (2/2), completed with 2 local objects. To github.com:daviskregers/multi-k8s.git 3dee337..56fb459 master -> master Now the travis will update the cluster.","title":"Deploy changes"},{"location":"Docker%20%26%20Kubernetes/17_HTTPS_with_kubernetes/07_verifying_the_certificate/","text":"Verifying the certificate \u00b6 We can check if everything was set up correctly by running a command: kregers_davis@cloudshell:~ (multi-k8s-236808)$ kubectl get certificates NAME k8s-deiveris-lv-tls kregers_davis@cloudshell:~ (multi-k8s-236808)$ kubectl describe certificates Name: k8s-deiveris-lv-tls Namespace: default Labels: <none> Annotations: kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"certmanager.k8s.io/v1alpha1\",\"kind\":\"Certificate\",\"metadata\":{\"annotations\":{},\"name\":\"k8s-deiveris-lv-tls\",\"namespace\":\"default\"},\"spec... API Version: certmanager.k8s.io/v1alpha1 Kind: Certificate Metadata: Creation Timestamp: 2019-04-06T12:16:17Z Generation: 1 Resource Version: 36905 Self Link: /apis/certmanager.k8s.io/v1alpha1/namespaces/default/certificates/k8s-deiveris-lv-tls UID: c7cb696e-5865-11e9-ac17-42010a840032 Spec: Acme: Config: Domains: k8s.deiveris.lv Http 01: Ingress Class: nginx Common Name: k8s.deiveris.lv Dns Names: k8s.deiveris.lv Issuer Ref: Kind: ClusterIssuer Name: letsencrypt-prod Secret Name: k8s-deiveris-lv Status: Conditions: Last Transition Time: 2019-04-06T12:16:52Z Message: Certificate is up to date and has not expired Reason: Ready Status: True Type: Ready Not After: 2019-07-05T11:16:51Z Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning IssuerNotFound 3m (x2 over 3m) cert-manager clusterissuer.certmanager.k8s.io \"letsencrypt-prod\" not found Warning IssuerNotReady 3m cert-manager Issuer letsencrypt-prod not ready Normal Generated 3m cert-manager Generated new private key Normal GenerateSelfSigned 3m cert-manager Generated temporary self signed certificate Normal OrderCreated 3m cert-manager Created Order resource \"k8s-deiveris-lv-tls-2662049116\" Normal OrderComplete 2m cert-manager Order \"k8s-deiveris-lv-tls-2662049116\" completed successfully Normal CertIssued 2m cert-manager Certificate issued successfully Check that secrets are created: kregers_davis@cloudshell:~ (multi-k8s-236808)$ kubectl get secrets NAME TYPE DATA AGE default-token-6vq4q kubernetes.io/service-account-token 3 3h k8s-deiveris-lv kubernetes.io/tls 3 5m my-nginx-nginx-ingress-token-mrtj5 kubernetes.io/service-account-token 3 1h pgpassword Opaque 1 2h kregers_davis@cloudshell:~ (multi-k8s-236808)$","title":"Verifying the certificate"},{"location":"Docker%20%26%20Kubernetes/17_HTTPS_with_kubernetes/07_verifying_the_certificate/#verifying-the-certificate","text":"We can check if everything was set up correctly by running a command: kregers_davis@cloudshell:~ (multi-k8s-236808)$ kubectl get certificates NAME k8s-deiveris-lv-tls kregers_davis@cloudshell:~ (multi-k8s-236808)$ kubectl describe certificates Name: k8s-deiveris-lv-tls Namespace: default Labels: <none> Annotations: kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"certmanager.k8s.io/v1alpha1\",\"kind\":\"Certificate\",\"metadata\":{\"annotations\":{},\"name\":\"k8s-deiveris-lv-tls\",\"namespace\":\"default\"},\"spec... API Version: certmanager.k8s.io/v1alpha1 Kind: Certificate Metadata: Creation Timestamp: 2019-04-06T12:16:17Z Generation: 1 Resource Version: 36905 Self Link: /apis/certmanager.k8s.io/v1alpha1/namespaces/default/certificates/k8s-deiveris-lv-tls UID: c7cb696e-5865-11e9-ac17-42010a840032 Spec: Acme: Config: Domains: k8s.deiveris.lv Http 01: Ingress Class: nginx Common Name: k8s.deiveris.lv Dns Names: k8s.deiveris.lv Issuer Ref: Kind: ClusterIssuer Name: letsencrypt-prod Secret Name: k8s-deiveris-lv Status: Conditions: Last Transition Time: 2019-04-06T12:16:52Z Message: Certificate is up to date and has not expired Reason: Ready Status: True Type: Ready Not After: 2019-07-05T11:16:51Z Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning IssuerNotFound 3m (x2 over 3m) cert-manager clusterissuer.certmanager.k8s.io \"letsencrypt-prod\" not found Warning IssuerNotReady 3m cert-manager Issuer letsencrypt-prod not ready Normal Generated 3m cert-manager Generated new private key Normal GenerateSelfSigned 3m cert-manager Generated temporary self signed certificate Normal OrderCreated 3m cert-manager Created Order resource \"k8s-deiveris-lv-tls-2662049116\" Normal OrderComplete 2m cert-manager Order \"k8s-deiveris-lv-tls-2662049116\" completed successfully Normal CertIssued 2m cert-manager Certificate issued successfully Check that secrets are created: kregers_davis@cloudshell:~ (multi-k8s-236808)$ kubectl get secrets NAME TYPE DATA AGE default-token-6vq4q kubernetes.io/service-account-token 3 3h k8s-deiveris-lv kubernetes.io/tls 3 5m my-nginx-nginx-ingress-token-mrtj5 kubernetes.io/service-account-token 3 1h pgpassword Opaque 1 2h kregers_davis@cloudshell:~ (multi-k8s-236808)$","title":"Verifying the certificate"},{"location":"Docker%20%26%20Kubernetes/17_HTTPS_with_kubernetes/08_ingress_config_for_https/","text":"Ingress config for HTTPS \u00b6 When we have issued a new certificate, we do need to tell the ingress service to use it. We are going to modify the k8s/ingress-service.yaml file: apiVersion: extensions/v1beta1 kind: Ingress metadata: name: ingress-service annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/rewrite-target: /$1 certManager.k8s.io/cluster-issuer: 'letsencrypt-prod' nginx.ingress.kubernetes.io/ssl-redirect: 'true' spec: tls: - hosts: - k8s.deiveris.lv secretName: k8s-deiveris-lv rules: - host: k8s.deiveris.lv http: paths: - path: /?(.*) backend: serviceName: client-cluster-ip-service servicePort: 3000 - path: /api/?(.*) backend: serviceName: server-cluster-ip-service servicePort: 5000","title":"Ingress config for HTTPS"},{"location":"Docker%20%26%20Kubernetes/17_HTTPS_with_kubernetes/08_ingress_config_for_https/#ingress-config-for-https","text":"When we have issued a new certificate, we do need to tell the ingress service to use it. We are going to modify the k8s/ingress-service.yaml file: apiVersion: extensions/v1beta1 kind: Ingress metadata: name: ingress-service annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/rewrite-target: /$1 certManager.k8s.io/cluster-issuer: 'letsencrypt-prod' nginx.ingress.kubernetes.io/ssl-redirect: 'true' spec: tls: - hosts: - k8s.deiveris.lv secretName: k8s-deiveris-lv rules: - host: k8s.deiveris.lv http: paths: - path: /?(.*) backend: serviceName: client-cluster-ip-service servicePort: 3000 - path: /api/?(.*) backend: serviceName: server-cluster-ip-service servicePort: 5000","title":"Ingress config for HTTPS"},{"location":"Docker%20%26%20Kubernetes/17_HTTPS_with_kubernetes/09_verify_changes/","text":"Vertify changes \u00b6 We are going to push it now: $ git add . $ git commit -m \"ingress config fo certificate\" [master fac2595] ingress config for certificate 1 file changed, 8 insertions(+), 1 deletion(-) $ git push Enumerating objects: 7, done. Counting objects: 100% (7/7), done. Delta compression using up to 8 threads Compressing objects: 100% (4/4), done. Writing objects: 100% (4/4), 523 bytes | 523.00 KiB/s, done. Total 4 (delta 3), reused 0 (delta 0) remote: Resolving deltas: 100% (3/3), completed with 3 local objects. To github.com:daviskregers/multi-k8s.git 56fb459..fac2595 master -> master And wait for the Travis to apply changes. After some time, we can see that the domain has been set. When visiting the site, we can see that everything is working fine:","title":"Vertify changes"},{"location":"Docker%20%26%20Kubernetes/17_HTTPS_with_kubernetes/09_verify_changes/#vertify-changes","text":"We are going to push it now: $ git add . $ git commit -m \"ingress config fo certificate\" [master fac2595] ingress config for certificate 1 file changed, 8 insertions(+), 1 deletion(-) $ git push Enumerating objects: 7, done. Counting objects: 100% (7/7), done. Delta compression using up to 8 threads Compressing objects: 100% (4/4), done. Writing objects: 100% (4/4), 523 bytes | 523.00 KiB/s, done. Total 4 (delta 3), reused 0 (delta 0) remote: Resolving deltas: 100% (3/3), completed with 3 local objects. To github.com:daviskregers/multi-k8s.git 56fb459..fac2595 master -> master And wait for the Travis to apply changes. After some time, we can see that the domain has been set. When visiting the site, we can see that everything is working fine:","title":"Vertify changes"},{"location":"Docker%20%26%20Kubernetes/18_kubernetes_cleanup/01_google_cloud_cleanup/","text":"Google Cloud cleanup \u00b6 Click on the Project Selector on the Top Left corner of the page Click on the gear icon on the right: Find your project in the list of projects that is presented, then click the three dots on the far right hand side Click Delete Enter the project ID and shut down The project will be deleted after 30 days","title":"Google Cloud cleanup"},{"location":"Docker%20%26%20Kubernetes/18_kubernetes_cleanup/01_google_cloud_cleanup/#google-cloud-cleanup","text":"Click on the Project Selector on the Top Left corner of the page Click on the gear icon on the right: Find your project in the list of projects that is presented, then click the three dots on the far right hand side Click Delete Enter the project ID and shut down The project will be deleted after 30 days","title":"Google Cloud cleanup"},{"location":"Docker%20%26%20Kubernetes/18_kubernetes_cleanup/02_local_cleanup/","text":"Local Environment Cleanup \u00b6 Stopping minikube \u00b6 To stop Minikube, and the VM that it runs, run minikube stop . You can bring your local cluster back online at any time by running minikube start Stopping running containers \u00b6 You might still have some containers running on your machine. Try a docker ps . You can then run docker stop <container_id> to clean up any running containers. Clearing the Build cache \u00b6 All the images that we built and ran during the course are cached on your local machine - they might be taking up to around 1GB of space. You can clean these up by running docker system prune .","title":"Local Environment Cleanup"},{"location":"Docker%20%26%20Kubernetes/18_kubernetes_cleanup/02_local_cleanup/#local-environment-cleanup","text":"","title":"Local Environment Cleanup"},{"location":"Docker%20%26%20Kubernetes/18_kubernetes_cleanup/02_local_cleanup/#stopping-minikube","text":"To stop Minikube, and the VM that it runs, run minikube stop . You can bring your local cluster back online at any time by running minikube start","title":"Stopping minikube"},{"location":"Docker%20%26%20Kubernetes/18_kubernetes_cleanup/02_local_cleanup/#stopping-running-containers","text":"You might still have some containers running on your machine. Try a docker ps . You can then run docker stop <container_id> to clean up any running containers.","title":"Stopping running containers"},{"location":"Docker%20%26%20Kubernetes/18_kubernetes_cleanup/02_local_cleanup/#clearing-the-build-cache","text":"All the images that we built and ran during the course are cached on your local machine - they might be taking up to around 1GB of space. You can clean these up by running docker system prune .","title":"Clearing the Build cache"},{"location":"Elixir/","text":"Learning elixir Sources: - The Complete Elixir and Phoenix Bootcamp on Udemy The code for these notes is available at https://github.com/daviskregers/elixir-and-phoenix-bootcamp","title":"Index"},{"location":"Elixir/01_installing/","text":"Installing elixir \u00b6 In order to install elixir you can go to https://elixir-lang.org/install.html and follow the steps listed there. OSX \u00b6 brew install elixir Arch \u00b6 pacman -S elixir Now when running the command elixir you should see list of options that the elixir binary has.","title":"Installing elixir"},{"location":"Elixir/01_installing/#installing-elixir","text":"In order to install elixir you can go to https://elixir-lang.org/install.html and follow the steps listed there.","title":"Installing elixir"},{"location":"Elixir/01_installing/#osx","text":"brew install elixir","title":"OSX"},{"location":"Elixir/01_installing/#arch","text":"pacman -S elixir Now when running the command elixir you should see list of options that the elixir binary has.","title":"Arch"},{"location":"Elixir/02_generating_project/","text":"Generating project \u00b6 Elixir includes mix that is a CLI . It is used to do tasks like: - Generating projects - Compiling projects - Running tasks - Managing dependencies - Running tests - Generating documentation To generate new project, go into the directory you want to create the project in and use the command: cd ~/Projects/elixir mix new cards This will generate several files like the main module, config, tests etc.","title":"Generating project"},{"location":"Elixir/02_generating_project/#generating-project","text":"Elixir includes mix that is a CLI . It is used to do tasks like: - Generating projects - Compiling projects - Running tasks - Managing dependencies - Running tests - Generating documentation To generate new project, go into the directory you want to create the project in and use the command: cd ~/Projects/elixir mix new cards This will generate several files like the main module, config, tests etc.","title":"Generating project"},{"location":"Elixir/03_modules_and_methods/","text":"Modules and methods \u00b6 When generating a project, there will be a module generated, that is located in lib directory. It will contain something like this. defmodule Cards do end This file defines a single module in the code, using the defmodule keyword, it is called Cards . Nearly all code in elixir is organized in various modules. You can add a method using following syntax: def hello do \"hi there!\" end The def keyword defines a new method called hello that returns hi there! string. The elixir has a thing called implicit return which means that whenever a function runs, whatever the last value is - it will get returned, so the keyword return is not used. You can run the method by going into your terminal and running: iex -S mix Cards.hello The iex is called Interactive Elixir Shell where you can interactively work with elixir.","title":"Modules and methods"},{"location":"Elixir/03_modules_and_methods/#modules-and-methods","text":"When generating a project, there will be a module generated, that is located in lib directory. It will contain something like this. defmodule Cards do end This file defines a single module in the code, using the defmodule keyword, it is called Cards . Nearly all code in elixir is organized in various modules. You can add a method using following syntax: def hello do \"hi there!\" end The def keyword defines a new method called hello that returns hi there! string. The elixir has a thing called implicit return which means that whenever a function runs, whatever the last value is - it will get returned, so the keyword return is not used. You can run the method by going into your terminal and running: iex -S mix Cards.hello The iex is called Interactive Elixir Shell where you can interactively work with elixir.","title":"Modules and methods"},{"location":"Elixir/04_lists_and_strings/","text":"Lists and strings \u00b6 Now we create a method that generates a list of playing cards. The list can be defined by using [] . def create_deck do [\"Ace\", \"Two\", \"Three\"] end The convention is to use double quotes \" all though the code. Although using single quotes ' is supported. Now, when opening up the terminal again, if it was not closed - the Cards.create_deck will not be found, since the compiled version does not contain it. In order to fix this, we can use the recompile command.","title":"Lists and strings"},{"location":"Elixir/04_lists_and_strings/#lists-and-strings","text":"Now we create a method that generates a list of playing cards. The list can be defined by using [] . def create_deck do [\"Ace\", \"Two\", \"Three\"] end The convention is to use double quotes \" all though the code. Although using single quotes ' is supported. Now, when opening up the terminal again, if it was not closed - the Cards.create_deck will not be found, since the compiled version does not contain it. In order to fix this, we can use the recompile command.","title":"Lists and strings"},{"location":"Elixir/05_oop_vs_functional_programming/","text":"OOP vs functional programming \u00b6 Object Oriented Programming \u00b6 In OOP approuch the code is organized into classes where each has has class has several methods. Each of these methods will operate on their local instance variables (properties). For example of a deck of cards, there would be 2 classes - one will be Deck and one will be Card . The deck instance would hold a list of Card instances and provide several methods of managing them. Functional programming \u00b6 In functional programming, the functions are organized into modules, there is no concept of a class or it's instance. The modules cannot be copied or instantiated, they are used only as collections of methods. For example, there might be a method in the Cards module that deals with shuffleing cards. It will accept input of list of strings that represent cards and return this list shuffled.","title":"OOP vs functional programming"},{"location":"Elixir/05_oop_vs_functional_programming/#oop-vs-functional-programming","text":"","title":"OOP vs functional programming"},{"location":"Elixir/05_oop_vs_functional_programming/#object-oriented-programming","text":"In OOP approuch the code is organized into classes where each has has class has several methods. Each of these methods will operate on their local instance variables (properties). For example of a deck of cards, there would be 2 classes - one will be Deck and one will be Card . The deck instance would hold a list of Card instances and provide several methods of managing them.","title":"Object Oriented Programming"},{"location":"Elixir/05_oop_vs_functional_programming/#functional-programming","text":"In functional programming, the functions are organized into modules, there is no concept of a class or it's instance. The modules cannot be copied or instantiated, they are used only as collections of methods. For example, there might be a method in the Cards module that deals with shuffleing cards. It will accept input of list of strings that represent cards and return this list shuffled.","title":"Functional programming"},{"location":"Elixir/06_method_arguments/","text":"Method arguments \u00b6 When defining arguments of methods, they are defined after the function name, listing all names of the arguments accepted. def shuffle(deck) do end When compiling the code, elixir will track whether the arguments are used or not and give feedback on it. For example, currently it will give a warning that the deck argument is not being used. In elixir, we can have multiple methods with the same name, each of these methods can accept different numbers and types of arguments. For example, if we call the shuffle method without any arguments, there will be an error thrown that will say that the module does not contain a shuffle method with no arguments, but it does contain shuffle method with 1 argument. Same, if we provide 2 arguments.","title":"Method arguments"},{"location":"Elixir/06_method_arguments/#method-arguments","text":"When defining arguments of methods, they are defined after the function name, listing all names of the arguments accepted. def shuffle(deck) do end When compiling the code, elixir will track whether the arguments are used or not and give feedback on it. For example, currently it will give a warning that the deck argument is not being used. In elixir, we can have multiple methods with the same name, each of these methods can accept different numbers and types of arguments. For example, if we call the shuffle method without any arguments, there will be an error thrown that will say that the module does not contain a shuffle method with no arguments, but it does contain shuffle method with 1 argument. Same, if we provide 2 arguments.","title":"Method arguments"},{"location":"Elixir/07_enum_mode/","text":"The Enum mode \u00b6 The previously created shuffle method will use the elixir standard library . For learning more on it, the documentation can be found at https://elixir-lang.org/docs.html Specifically, it will use the Enum module https://hexdocs.pm/elixir/Enum.html , the shuffle method. To call it, we can reference it as Enum.shuffle() . def shuffle(deck) do Enum.shuffle(dec) end And now, we can run it.","title":"The Enum mode"},{"location":"Elixir/07_enum_mode/#the-enum-mode","text":"The previously created shuffle method will use the elixir standard library . For learning more on it, the documentation can be found at https://elixir-lang.org/docs.html Specifically, it will use the Enum module https://hexdocs.pm/elixir/Enum.html , the shuffle method. To call it, we can reference it as Enum.shuffle() . def shuffle(deck) do Enum.shuffle(dec) end And now, we can run it.","title":"The Enum mode"},{"location":"Elixir/08_immutability_in_elixir/","text":"Immutability in elixir \u00b6 When working with elixir, we never modify an existing data structure. For example, when passing a list of method, instead of modifying it, it creates a copy of it, modifies it and returns the copy. This concept is called Immutability . So, in previous section, when calling the shuffle method on a deck. It doesn't actually shuffles the existing deck, it creates a copy of it and then shuffles it.","title":"Immutability in elixir"},{"location":"Elixir/08_immutability_in_elixir/#immutability-in-elixir","text":"When working with elixir, we never modify an existing data structure. For example, when passing a list of method, instead of modifying it, it creates a copy of it, modifies it and returns the copy. This concept is called Immutability . So, in previous section, when calling the shuffle method on a deck. It doesn't actually shuffles the existing deck, it creates a copy of it and then shuffles it.","title":"Immutability in elixir"},{"location":"Elixir/09_searching_a_list/","text":"Searching a list \u00b6 The next method in the Cards module will be contains? method. That checks wether the list has a specific element, return either true or false . By convention, whenever function has a question mark in it ? it will return a boolean value. To check whether a card is in the deck we can use the Enum.member? method. def contains?(deck, card) do Enum.member?(deck, card) end","title":"Searching a list"},{"location":"Elixir/09_searching_a_list/#searching-a-list","text":"The next method in the Cards module will be contains? method. That checks wether the list has a specific element, return either true or false . By convention, whenever function has a question mark in it ? it will return a boolean value. To check whether a card is in the deck we can use the Enum.member? method. def contains?(deck, card) do Enum.member?(deck, card) end","title":"Searching a list"},{"location":"Elixir/10_comprehensions_over_lists/","text":"Comprehensions over lists \u00b6 Previously we used only elixir standard library, specifically the Enum module to handle the cards. But we will require some custom logic. For example, currently the create_deck method only returns a list of Ace , Two and Three . It does not have any concept of suits. The goal is to create two lists - values and suits and create every combination of those two lists. This can be done by using something list comprehension. A list comprehension is a mapping function. It uses the for keyword and iterates over all the elements in the list. Whatever is returned from the do block, gets added to a newly created list. for suit <- suits do suit end will return [\"Ace\", \"Two\", \"Three\"] Since we have two lists, we need to solve a comprehension for nested arrays. Using a for in a for comprehension will return a 2D list . This can be solved by using List.flatten method. def create_deck do values = [\"Ace\", \"Two\", \"Three\", \"Four\", \"Five\", \"Six\", \"Seven\", \"Eight\", \"Nine\", \"Jack\", \"Queen\", \"King\"] suits = [\"Spades\", \"Clubs\", \"Hearts\", \"Diamonds\"] cards = for suit <- suits do for value <- values do \"#{value} of #{suit}\" end end List.flatten(cards) end This can be optimized, you can have multiple comprehensions running the same time, which will eliminate the extra steps like the List.flatten that is a unnecessary computation step. def create_deck do values = [\"Ace\", \"Two\", \"Three\", \"Four\", \"Five\", \"Six\", \"Seven\", \"Eight\", \"Nine\", \"Jack\", \"Queen\", \"King\"] suits = [\"Spades\", \"Clubs\", \"Hearts\", \"Diamonds\"] for suit <- suits, value <- values do \"#{value} of #{suit}\" end end","title":"Comprehensions over lists"},{"location":"Elixir/10_comprehensions_over_lists/#comprehensions-over-lists","text":"Previously we used only elixir standard library, specifically the Enum module to handle the cards. But we will require some custom logic. For example, currently the create_deck method only returns a list of Ace , Two and Three . It does not have any concept of suits. The goal is to create two lists - values and suits and create every combination of those two lists. This can be done by using something list comprehension. A list comprehension is a mapping function. It uses the for keyword and iterates over all the elements in the list. Whatever is returned from the do block, gets added to a newly created list. for suit <- suits do suit end will return [\"Ace\", \"Two\", \"Three\"] Since we have two lists, we need to solve a comprehension for nested arrays. Using a for in a for comprehension will return a 2D list . This can be solved by using List.flatten method. def create_deck do values = [\"Ace\", \"Two\", \"Three\", \"Four\", \"Five\", \"Six\", \"Seven\", \"Eight\", \"Nine\", \"Jack\", \"Queen\", \"King\"] suits = [\"Spades\", \"Clubs\", \"Hearts\", \"Diamonds\"] cards = for suit <- suits do for value <- values do \"#{value} of #{suit}\" end end List.flatten(cards) end This can be optimized, you can have multiple comprehensions running the same time, which will eliminate the extra steps like the List.flatten that is a unnecessary computation step. def create_deck do values = [\"Ace\", \"Two\", \"Three\", \"Four\", \"Five\", \"Six\", \"Seven\", \"Eight\", \"Nine\", \"Jack\", \"Queen\", \"King\"] suits = [\"Spades\", \"Clubs\", \"Hearts\", \"Diamonds\"] for suit <- suits, value <- values do \"#{value} of #{suit}\" end end","title":"Comprehensions over lists"},{"location":"Elixir/11_importance_of_index_with_tuples/","text":"Importance of Index with Tuples \u00b6 Next step in the Cards module is to implement a deal method. That deals n cards from a deck. This can be done by using the Enum.split method. def deal(deck, hand_size) do Enum.split(deck, hand_size) end You can see that the output of the method is wrapped with {} , the first element is the cards dealt and the second list is the cards left. The {} represents a data structure called a tuple . The hand is always at the index of 0 , the rest of the cards are at index 1 . Essentially, a list is for a list of similar records , meanwhile the tuple is for stuff that can be in different types, but the order has some specific meaning .","title":"Importance of Index with Tuples"},{"location":"Elixir/11_importance_of_index_with_tuples/#importance-of-index-with-tuples","text":"Next step in the Cards module is to implement a deal method. That deals n cards from a deck. This can be done by using the Enum.split method. def deal(deck, hand_size) do Enum.split(deck, hand_size) end You can see that the output of the method is wrapped with {} , the first element is the cards dealt and the second list is the cards left. The {} represents a data structure called a tuple . The hand is always at the index of 0 , the rest of the cards are at index 1 . Essentially, a list is for a list of similar records , meanwhile the tuple is for stuff that can be in different types, but the order has some specific meaning .","title":"Importance of Index with Tuples"},{"location":"Elixir/12_pattern_matching/","text":"Pattern matching \u00b6 In previous section we got a tuple that contained a list of dealt hand cards and a list of the cards left in a deck. iex(32)> Cards.deal(deck, 5) {[\"Ace of Spades\", \"Two of Spades\", \"Three of Spades\", \"Four of Spades\", \"Five of Spades\"], [\"Six of Spades\", \"Seven of Spades\", \"Eight of Spades\", \"Nine of Spades\", \"Jack of Spades\", \"Queen of Spades\", \"King of Spades\", \"Ace of Clubs\", \"Two of Clubs\", \"Three of Clubs\", \"Four of Clubs\", \"Five of Clubs\", \"Six of Clubs\", \"Seven of Clubs\", \"Eight of Clubs\", \"Nine of Clubs\", \"Jack of Clubs\", \"Queen of Clubs\", \"King of Clubs\", \"Ace of Hearts\", \"Two of Hearts\", \"Three of Hearts\", \"Four of Hearts\", \"Five of Hearts\", \"Six of Hearts\", \"Seven of Hearts\", \"Eight of Hearts\", \"Nine of Hearts\", \"Jack of Hearts\", \"Queen of Hearts\", \"King of Hearts\", \"Ace of Diamonds\", \"Two of Diamonds\", \"Three of Diamonds\", \"Four of Diamonds\", \"Five of Diamonds\", \"Six of Diamonds\", \"Seven of Diamonds\", \"Eight of Diamonds\", \"Nine of Diamonds\", \"Jack of Diamonds\", \"Queen of Diamonds\", \"King of Diamonds\"]} In order to get the first index from the tuple, we cannot use something like Cards.deal(deck,4)[0] that can be seen in other languages - it will throw an error. Instead, we can use pattern matching. Pattern matching is elixir's replacement for variable assignment. { hand, rest_of_deck } = Cards.deal(deck, 5) We write out a tuple on the left hand side, which matches the output of the Cards.deal() method, match the number of elements in the tuple - the first element is the hand , the second - rest_of_the_deck . So, we create a structure that matches the output of the method. When running this, the elements are assigned to the variables defined in the pattern matching assignment. color1 = [\"red\"] # color1 = [\"red\"] [color1] = [\"red\"] # color1 = \"red\" [color1, color2] = [\"red\", \"blue\"] # color1 = \"red\", color2 = \"blue\" [color1, color2, color3] = [\"red\", \"blue\"] # MatchError Some more advanced usages of pattern matching is in ./15_loading_from_filesystem.md , ./21_identicon_generation_and_structs.md","title":"Pattern matching"},{"location":"Elixir/12_pattern_matching/#pattern-matching","text":"In previous section we got a tuple that contained a list of dealt hand cards and a list of the cards left in a deck. iex(32)> Cards.deal(deck, 5) {[\"Ace of Spades\", \"Two of Spades\", \"Three of Spades\", \"Four of Spades\", \"Five of Spades\"], [\"Six of Spades\", \"Seven of Spades\", \"Eight of Spades\", \"Nine of Spades\", \"Jack of Spades\", \"Queen of Spades\", \"King of Spades\", \"Ace of Clubs\", \"Two of Clubs\", \"Three of Clubs\", \"Four of Clubs\", \"Five of Clubs\", \"Six of Clubs\", \"Seven of Clubs\", \"Eight of Clubs\", \"Nine of Clubs\", \"Jack of Clubs\", \"Queen of Clubs\", \"King of Clubs\", \"Ace of Hearts\", \"Two of Hearts\", \"Three of Hearts\", \"Four of Hearts\", \"Five of Hearts\", \"Six of Hearts\", \"Seven of Hearts\", \"Eight of Hearts\", \"Nine of Hearts\", \"Jack of Hearts\", \"Queen of Hearts\", \"King of Hearts\", \"Ace of Diamonds\", \"Two of Diamonds\", \"Three of Diamonds\", \"Four of Diamonds\", \"Five of Diamonds\", \"Six of Diamonds\", \"Seven of Diamonds\", \"Eight of Diamonds\", \"Nine of Diamonds\", \"Jack of Diamonds\", \"Queen of Diamonds\", \"King of Diamonds\"]} In order to get the first index from the tuple, we cannot use something like Cards.deal(deck,4)[0] that can be seen in other languages - it will throw an error. Instead, we can use pattern matching. Pattern matching is elixir's replacement for variable assignment. { hand, rest_of_deck } = Cards.deal(deck, 5) We write out a tuple on the left hand side, which matches the output of the Cards.deal() method, match the number of elements in the tuple - the first element is the hand , the second - rest_of_the_deck . So, we create a structure that matches the output of the method. When running this, the elements are assigned to the variables defined in the pattern matching assignment. color1 = [\"red\"] # color1 = [\"red\"] [color1] = [\"red\"] # color1 = \"red\" [color1, color2] = [\"red\", \"blue\"] # color1 = \"red\", color2 = \"blue\" [color1, color2, color3] = [\"red\", \"blue\"] # MatchError Some more advanced usages of pattern matching is in ./15_loading_from_filesystem.md , ./21_identicon_generation_and_structs.md","title":"Pattern matching"},{"location":"Elixir/13_elixirs_relation_with_erlang/","text":"Elixir's relation with erlang \u00b6 Elixir is not a standalone programming language. All the code that we write is not executed as elixir code. The code that we write gets fed into Elixir runtime and is transpiled into Erlang which then compiles into something caled BEAM and executes it. Erlang is a standalone programming language. It has different style of syntax than the elixir, but it has the same underlying concepts. Erlang was developed about 30 years ago for handling telecom networks. It is a wonderful language, but is notorious for it's hard to understand syntax. So, that is where elixir comes in. You can think of elixir as a dialect of Erlang , where you can get away from the annoying parts of Erlang . The endproduct of is the BEAM which stands for Bogdan/Bj\u00f6rn's Erlang Abstract Machine . It is a virtual machine where all the Erlang code is executed in. It's like the JVM in `Java.","title":"Elixir's relation with erlang"},{"location":"Elixir/13_elixirs_relation_with_erlang/#elixirs-relation-with-erlang","text":"Elixir is not a standalone programming language. All the code that we write is not executed as elixir code. The code that we write gets fed into Elixir runtime and is transpiled into Erlang which then compiles into something caled BEAM and executes it. Erlang is a standalone programming language. It has different style of syntax than the elixir, but it has the same underlying concepts. Erlang was developed about 30 years ago for handling telecom networks. It is a wonderful language, but is notorious for it's hard to understand syntax. So, that is where elixir comes in. You can think of elixir as a dialect of Erlang , where you can get away from the annoying parts of Erlang . The endproduct of is the BEAM which stands for Bogdan/Bj\u00f6rn's Erlang Abstract Machine . It is a virtual machine where all the Erlang code is executed in. It's like the JVM in `Java.","title":"Elixir's relation with erlang"},{"location":"Elixir/14_saving_info_filesystem/","text":"Saving into filesystem \u00b6 We can implement a saving method that saves the deck to the filesystem. def save(deck, filename) do binary = :erlang.term_to_binary(deck) File.write(filename, binary) end The :erlang invokes an erlang method called term_to_binary which basically converts the deck into an object that can be written onto a filesystem. It saves it onto the filesystem in a binary mode.","title":"Saving into filesystem"},{"location":"Elixir/14_saving_info_filesystem/#saving-into-filesystem","text":"We can implement a saving method that saves the deck to the filesystem. def save(deck, filename) do binary = :erlang.term_to_binary(deck) File.write(filename, binary) end The :erlang invokes an erlang method called term_to_binary which basically converts the deck into an object that can be written onto a filesystem. It saves it onto the filesystem in a binary mode.","title":"Saving into filesystem"},{"location":"Elixir/15_loading_from_filesystem/","text":"Loading from filesystem \u00b6 We can use File.read to read the file from the filesystem. It will return a tuple where the first element is operation status atom and the second is the data . So we can use pattern matching and reverse the previously created save method. def load(filename) do {status, binary} = File.read(filename) case status do :ok -> :erlang.binary_to_term binary :error -> \"That file does not exist\" end end We use the :ok and :error atoms to control the flow of the program. You can consider the atoms as string like response codes. The previous load function can be condensed into a smaller form using pattern matching. def load(filename) do case File.read(filename) do {:ok, binary} -> :erlang.binary_to_term binary {:error, _reason} -> \"That file does not exist\" end end We use the _ symbol in the pattern matching to tell the compiler that we know that there is an element there, but we don't care about it. It will prevent the warning messages about unused variables.","title":"Loading from filesystem"},{"location":"Elixir/15_loading_from_filesystem/#loading-from-filesystem","text":"We can use File.read to read the file from the filesystem. It will return a tuple where the first element is operation status atom and the second is the data . So we can use pattern matching and reverse the previously created save method. def load(filename) do {status, binary} = File.read(filename) case status do :ok -> :erlang.binary_to_term binary :error -> \"That file does not exist\" end end We use the :ok and :error atoms to control the flow of the program. You can consider the atoms as string like response codes. The previous load function can be condensed into a smaller form using pattern matching. def load(filename) do case File.read(filename) do {:ok, binary} -> :erlang.binary_to_term binary {:error, _reason} -> \"That file does not exist\" end end We use the _ symbol in the pattern matching to tell the compiler that we know that there is an element there, but we don't care about it. It will prevent the warning messages about unused variables.","title":"Loading from filesystem"},{"location":"Elixir/16_pipe_operator/","text":"The Pipe operator \u00b6 The pipe operator is used to setup a chain of method calls that pass results from one to other. For example, we might have a function that uses 3 of the previously created methods: def create_hand(hand_size) do deck = Cards.create_deck deck = Cards.shuffle(deck) hand = Cards.deal(deck, hand_size) end This code contains repetitive code and can be refactored by using the pipe operator: def create_hand(hand_size) do Cards.create_deck |> Cards.shuffle |> Cards.deal(hand_size) end So the create_deck method is executed and fed into the shuffle method. The result of the shuffle method is fed into the deal method. Notice that the deal method accepts 2 arguments, only one is provided, it feeds the deck argument as well. Provided this, it requires that you create methods with consistend arguments. If the method had different order of arguments like hand_size and then deal , it wouldn't know that.","title":"The Pipe operator"},{"location":"Elixir/16_pipe_operator/#the-pipe-operator","text":"The pipe operator is used to setup a chain of method calls that pass results from one to other. For example, we might have a function that uses 3 of the previously created methods: def create_hand(hand_size) do deck = Cards.create_deck deck = Cards.shuffle(deck) hand = Cards.deal(deck, hand_size) end This code contains repetitive code and can be refactored by using the pipe operator: def create_hand(hand_size) do Cards.create_deck |> Cards.shuffle |> Cards.deal(hand_size) end So the create_deck method is executed and fed into the shuffle method. The result of the shuffle method is fed into the deal method. Notice that the deal method accepts 2 arguments, only one is provided, it feeds the deck argument as well. Provided this, it requires that you create methods with consistend arguments. If the method had different order of arguments like hand_size and then deal , it wouldn't know that.","title":"The Pipe operator"},{"location":"Elixir/17_writing_documentation/","text":"Module documentation \u00b6 We can add documentation to our project in two steps. The first step is to install a ex_doc package. That can be done by modifying the mix.exs file: defp deps do [ {:ex_doc, \"~> 0.19.2\"} ] end And running the mix deps.get command. There are two types of documentation - module documentation and function documentation. The module documentation is a summary-like documentation of saying what the module does overall. Then the function is used to document the usage of individual functions. The module documentation can be added in a following manner: defmodule Cards do @moduledoc \"\"\" Provides methods for creating and handling a deck of cards \"\"\" ... end The function documentation can be added like this: @doc \"\"\" Divides a deck into a hand and the remainder of the deck. The `hand_size` argument indicates how many cards should be in the hand. ## Examples iex> deck = Cards.create_deck iex> {hand,deck} = Cards.deal(deck,1) iex> hand [\"Ace of Spades\"] \"\"\" def deal(deck, hand_size) do Enum.split(deck, hand_size) end The documentation can be generated by using the mix docs command. It will generate a static html in the doc directory.","title":"Module documentation"},{"location":"Elixir/17_writing_documentation/#module-documentation","text":"We can add documentation to our project in two steps. The first step is to install a ex_doc package. That can be done by modifying the mix.exs file: defp deps do [ {:ex_doc, \"~> 0.19.2\"} ] end And running the mix deps.get command. There are two types of documentation - module documentation and function documentation. The module documentation is a summary-like documentation of saying what the module does overall. Then the function is used to document the usage of individual functions. The module documentation can be added in a following manner: defmodule Cards do @moduledoc \"\"\" Provides methods for creating and handling a deck of cards \"\"\" ... end The function documentation can be added like this: @doc \"\"\" Divides a deck into a hand and the remainder of the deck. The `hand_size` argument indicates how many cards should be in the hand. ## Examples iex> deck = Cards.create_deck iex> {hand,deck} = Cards.deal(deck,1) iex> hand [\"Ace of Spades\"] \"\"\" def deal(deck, hand_size) do Enum.split(deck, hand_size) end The documentation can be generated by using the mix docs command. It will generate a static html in the doc directory.","title":"Module documentation"},{"location":"Elixir/18_writing_tests/","text":"Writing tests \u00b6 Testing in elixir comes out of the box. You don't need additional packages to do it. The tests are located into the test directory. There is a test/cards_test.exs file generated with the cards module. The tests can be ran with the mix test command. defmodule CardsTest do use ExUnit.Case doctest Cards test \"greets the world\" do assert Cards.hello() == :world end end In elixir tests there are two distinct types of tests - case tests and doctests . The unit tests are written in the test/cards_test.exs , meanwhile doctests are written in the lib/cards.ex in the @doc section. @doc \"\"\" Divides a deck into a hand and the remainder of the deck. The `hand_size` argument indicates how many cards should be in the hand. ## Examples iex> deck = Cards.create_deck iex> {hand,_deck} = Cards.deal(deck,1) iex> hand [\"Ace of Spades\"] \"\"\" The doctests use the Examples section. Writing the doctest must follow the exact syntax: Starts with the ## Examples The examples must be indented with 3 tabs (6 spaces) The commands to be executed starts with iex> , the ones without are assertions. There must be doctest Cards in the test/cards_test.exs file. In Case Tests you can use refute to invert the assert operation: test \"shuffling a deck randomizes it\" do deck = Cards.create_deck refute deck == Cards.shuffle(deck) assert deck != Cards.shuffle(deck) end","title":"Writing tests"},{"location":"Elixir/18_writing_tests/#writing-tests","text":"Testing in elixir comes out of the box. You don't need additional packages to do it. The tests are located into the test directory. There is a test/cards_test.exs file generated with the cards module. The tests can be ran with the mix test command. defmodule CardsTest do use ExUnit.Case doctest Cards test \"greets the world\" do assert Cards.hello() == :world end end In elixir tests there are two distinct types of tests - case tests and doctests . The unit tests are written in the test/cards_test.exs , meanwhile doctests are written in the lib/cards.ex in the @doc section. @doc \"\"\" Divides a deck into a hand and the remainder of the deck. The `hand_size` argument indicates how many cards should be in the hand. ## Examples iex> deck = Cards.create_deck iex> {hand,_deck} = Cards.deal(deck,1) iex> hand [\"Ace of Spades\"] \"\"\" The doctests use the Examples section. Writing the doctest must follow the exact syntax: Starts with the ## Examples The examples must be indented with 3 tabs (6 spaces) The commands to be executed starts with iex> , the ones without are assertions. There must be doctest Cards in the test/cards_test.exs file. In Case Tests you can use refute to invert the assert operation: test \"shuffling a deck randomizes it\" do deck = Cards.create_deck refute deck == Cards.shuffle(deck) assert deck != Cards.shuffle(deck) end","title":"Writing tests"},{"location":"Elixir/19_introduction_to_maps/","text":"Introduction to maps \u00b6 Maps are collections of key-value pairs. They are just like hashes in ruby, nearly identical to objects in javascript. iex(1)> colors = %{primary: \"red\", secondary: \"blue\"} %{primary: \"red\", secondary: \"blue\"} iex(2)> colors.primary \"red\" iex(3)> %{secondary: secondary_color} = %{primary: \"red\", secondary: \"blue\"} %{primary: \"red\", secondary: \"blue\"} iex(4)> secondary_color \"blue\" When we need to update a value in a map, we do not modify the map, we create a new one. iex(1)> colors = %{primary: \"red\", secondary: \"blue\"} %{primary: \"red\", secondary: \"blue\"} iex(2)> Map.put(colors, :primary, \"green\") %{primary: \"green\", secondary: \"blue\"} This creates a new map with the updated value. The second way to do this is: iex(1)> colors = %{primary: \"red\", secondary: \"blue\"} %{primary: \"red\", secondary: \"blue\"} iex(2)> %{ colors | primary: \"green\" } %{primary: \"green\", secondary: \"blue\"} This will only work when we are trying to update a property, if the property does not exist - it will throw an error.","title":"Introduction to maps"},{"location":"Elixir/19_introduction_to_maps/#introduction-to-maps","text":"Maps are collections of key-value pairs. They are just like hashes in ruby, nearly identical to objects in javascript. iex(1)> colors = %{primary: \"red\", secondary: \"blue\"} %{primary: \"red\", secondary: \"blue\"} iex(2)> colors.primary \"red\" iex(3)> %{secondary: secondary_color} = %{primary: \"red\", secondary: \"blue\"} %{primary: \"red\", secondary: \"blue\"} iex(4)> secondary_color \"blue\" When we need to update a value in a map, we do not modify the map, we create a new one. iex(1)> colors = %{primary: \"red\", secondary: \"blue\"} %{primary: \"red\", secondary: \"blue\"} iex(2)> Map.put(colors, :primary, \"green\") %{primary: \"green\", secondary: \"blue\"} This creates a new map with the updated value. The second way to do this is: iex(1)> colors = %{primary: \"red\", secondary: \"blue\"} %{primary: \"red\", secondary: \"blue\"} iex(2)> %{ colors | primary: \"green\" } %{primary: \"green\", secondary: \"blue\"} This will only work when we are trying to update a property, if the property does not exist - it will throw an error.","title":"Introduction to maps"},{"location":"Elixir/20_keyword_lists/","text":"Keyword lists \u00b6 Keyword lists is a merged structure between lists and tuples. iex(1)> color = [{:primary, \"red\"}, {:secondary, \"green\"}] [primary: \"red\", secondary: \"green\"] To access the elements, you can use: iex(2)> color[:primary] \"red\" You can also use the following syntax: iex(3)> colors = [primary: \"red\", secondary: \"green\"] [primary: \"red\", secondary: \"green\"]","title":"Keyword lists"},{"location":"Elixir/20_keyword_lists/#keyword-lists","text":"Keyword lists is a merged structure between lists and tuples. iex(1)> color = [{:primary, \"red\"}, {:secondary, \"green\"}] [primary: \"red\", secondary: \"green\"] To access the elements, you can use: iex(2)> color[:primary] \"red\" You can also use the following syntax: iex(3)> colors = [primary: \"red\", secondary: \"green\"] [primary: \"red\", secondary: \"green\"]","title":"Keyword lists"},{"location":"Elixir/21_identicon_generation_and_structs/","text":"Identicon generation process \u00b6 An indenticon is a 250x250px image which is splitted into a 5x5 grid - each 50x50px. The squares are mirrored around the center axis. The identicons are not generated randomly, but based on it's input. So the Identicon generator will accept a string as an input, from it, will generate the identicon and save it as an image into a filesystem. The actual process of building an image \u00b6 We take a string Compute MD5 hash of the string List of numbers based on the string Pick color Build grid of squares Convert grid into image Save image The process \u00b6 We create a new project called identicon : davis@davis-arch \ue0b0 ~/projects/elixir \ue0b0 mix new identicon * creating README.md * creating .formatter.exs * creating .gitignore * creating mix.exs * creating config * creating config/config.exs * creating lib * creating lib/identicon.ex * creating test * creating test/test_helper.exs * creating test/identicon_test.exs Your Mix project was created successfully. You can use \"mix\" to compile it, test it, and more: cd identicon mix test Run \"mix help\" for more commands. Add the main method that will handle the input of the string. This main function will call all the necessary steps for the program. Then we add a method called hash_input that handles the hashing the string and return it as a list of 16 numbers. def hash_input(input) do :crypto.hash(:md5, input) |> :binary.bin_to_list end The numerical values will range from 0-255 in this list, so we can use the first 3 values in the list to the next step - picking the color. So the next step is to build a grid, which needs to be 5x5 and symetrical by the center axis. 1 2 3 2 1 4 5 6 5 4 7 8 9 8 7 10 11 12 11 10 13 14 15 14 13 Then apply each number in the list to a square. Color in the squares if the number is even, leave blank squares where number is odd. We can use struct data structure in elixir to construct this grid. These are similar to maps, but they enforce that the only properties in the struct are previously defined. By convention, if we previously know the properties of the data structure, we use a struct instead of a map. To define a struct, we need to make a new module, so in the we create a new file called lib/image.ex . In the struct, the list of numbers will be called hex . defmodule Identicon.Image do defstruct hex: nil end And we can modify the hash_input to return the struct instead. def hash_input(input) do hex = :crypto.hash(:md5, input) |> :binary.bin_to_list %Identicon.Image{hex: hex} end Now we implement the pick_color method. It will accept the %Identicon.Image struct. def pick_color(image) do %Identicon.Image{ hex: [r, g, b | _tail] } = image %Identicon.Image{ image | color: {r,g,b} } end We can refactor the code into a following structure: def pick_color(%Identicon.Image{ hex: [r, g, b | _tail] } = image) do %Identicon.Image{ image | color: {r,g,b} } end Now we implement the build_grid method which will chunk the input list by 3 elements and mirror the rows. def build_grid(%Identicon.Image{hex: hex} = image) do grid = hex |> Enum.chunk(3) |> Enum.map(&mirror_row/1) |> List.flatten |> Enum.with_index %Identicon.Image{ image | grid: grid } end def mirror_row(row) do [first, second | _tail] = row row ++ [second, first] end The &mirror_row/1 is a reference to a function mirror_row . The next step is to filter out the odd squares. def filter_odd_squares(%Identicon.Image{grid: grid} = image) do grid = Enum.filtergrid, fn({code, _index}) -> # calculate reminder by 2, if 0 - keep in list rem(code, 2) == 0 end %Identicon.Image{image | grid: grid} end Now we can implement function for creating the actual image by using the Erlang egd since elixir itself does not provide any image manipulation libraries. To install it, we add a dependency in mix.exs {:egd, github: \"erlang/egd\"} Then restart the terminal and run the iex -S mix , it will ask for installing rebar3 , accept and continue. The first step is to create a pixel map for the for telling the Erlang egd how to draw the grid. def build_pixel_map(%Identicon.Image{grid: grid} = image) do Enum.map grid, fn({_code, index}) -> horizontal = rem(index, 5) * 50 vertical = div(index, 5) * 50 top_left = {horizontal, vertical} bottom_right = {horizontal + 50, vertical + 50} {top_left, bottom_right} end end And now, draw the image def draw_image(%Identicon.Image{color: color, pixel_map: pixel_map}) do image = :egd.create(250, 250) fill = :egd.color(color) Enum.each pixel_map, fn({start, stop}) -> :egd.filledRectangle(image, start, stop, fill) end :egd.render(image) end Now create a method for saving into the filesystem def save_image(image, filename) do File.write(\"#{filename}.png\", image) end And bootstrap everything in the main function def main(input) do input |> hash_input |> pick_color |> build_grid |> filter_odd_squares |> build_pixel_map |> draw_image |> save_image(input) end Now, we can run the Identicon.main and get the following images: Identicon.main(\"identicon\") Identicon.main(\"davis\")","title":"Identicon generation process"},{"location":"Elixir/21_identicon_generation_and_structs/#identicon-generation-process","text":"An indenticon is a 250x250px image which is splitted into a 5x5 grid - each 50x50px. The squares are mirrored around the center axis. The identicons are not generated randomly, but based on it's input. So the Identicon generator will accept a string as an input, from it, will generate the identicon and save it as an image into a filesystem.","title":"Identicon generation process"},{"location":"Elixir/21_identicon_generation_and_structs/#the-actual-process-of-building-an-image","text":"We take a string Compute MD5 hash of the string List of numbers based on the string Pick color Build grid of squares Convert grid into image Save image","title":"The actual process of building an image"},{"location":"Elixir/21_identicon_generation_and_structs/#the-process","text":"We create a new project called identicon : davis@davis-arch \ue0b0 ~/projects/elixir \ue0b0 mix new identicon * creating README.md * creating .formatter.exs * creating .gitignore * creating mix.exs * creating config * creating config/config.exs * creating lib * creating lib/identicon.ex * creating test * creating test/test_helper.exs * creating test/identicon_test.exs Your Mix project was created successfully. You can use \"mix\" to compile it, test it, and more: cd identicon mix test Run \"mix help\" for more commands. Add the main method that will handle the input of the string. This main function will call all the necessary steps for the program. Then we add a method called hash_input that handles the hashing the string and return it as a list of 16 numbers. def hash_input(input) do :crypto.hash(:md5, input) |> :binary.bin_to_list end The numerical values will range from 0-255 in this list, so we can use the first 3 values in the list to the next step - picking the color. So the next step is to build a grid, which needs to be 5x5 and symetrical by the center axis. 1 2 3 2 1 4 5 6 5 4 7 8 9 8 7 10 11 12 11 10 13 14 15 14 13 Then apply each number in the list to a square. Color in the squares if the number is even, leave blank squares where number is odd. We can use struct data structure in elixir to construct this grid. These are similar to maps, but they enforce that the only properties in the struct are previously defined. By convention, if we previously know the properties of the data structure, we use a struct instead of a map. To define a struct, we need to make a new module, so in the we create a new file called lib/image.ex . In the struct, the list of numbers will be called hex . defmodule Identicon.Image do defstruct hex: nil end And we can modify the hash_input to return the struct instead. def hash_input(input) do hex = :crypto.hash(:md5, input) |> :binary.bin_to_list %Identicon.Image{hex: hex} end Now we implement the pick_color method. It will accept the %Identicon.Image struct. def pick_color(image) do %Identicon.Image{ hex: [r, g, b | _tail] } = image %Identicon.Image{ image | color: {r,g,b} } end We can refactor the code into a following structure: def pick_color(%Identicon.Image{ hex: [r, g, b | _tail] } = image) do %Identicon.Image{ image | color: {r,g,b} } end Now we implement the build_grid method which will chunk the input list by 3 elements and mirror the rows. def build_grid(%Identicon.Image{hex: hex} = image) do grid = hex |> Enum.chunk(3) |> Enum.map(&mirror_row/1) |> List.flatten |> Enum.with_index %Identicon.Image{ image | grid: grid } end def mirror_row(row) do [first, second | _tail] = row row ++ [second, first] end The &mirror_row/1 is a reference to a function mirror_row . The next step is to filter out the odd squares. def filter_odd_squares(%Identicon.Image{grid: grid} = image) do grid = Enum.filtergrid, fn({code, _index}) -> # calculate reminder by 2, if 0 - keep in list rem(code, 2) == 0 end %Identicon.Image{image | grid: grid} end Now we can implement function for creating the actual image by using the Erlang egd since elixir itself does not provide any image manipulation libraries. To install it, we add a dependency in mix.exs {:egd, github: \"erlang/egd\"} Then restart the terminal and run the iex -S mix , it will ask for installing rebar3 , accept and continue. The first step is to create a pixel map for the for telling the Erlang egd how to draw the grid. def build_pixel_map(%Identicon.Image{grid: grid} = image) do Enum.map grid, fn({_code, index}) -> horizontal = rem(index, 5) * 50 vertical = div(index, 5) * 50 top_left = {horizontal, vertical} bottom_right = {horizontal + 50, vertical + 50} {top_left, bottom_right} end end And now, draw the image def draw_image(%Identicon.Image{color: color, pixel_map: pixel_map}) do image = :egd.create(250, 250) fill = :egd.color(color) Enum.each pixel_map, fn({start, stop}) -> :egd.filledRectangle(image, start, stop, fill) end :egd.render(image) end Now create a method for saving into the filesystem def save_image(image, filename) do File.write(\"#{filename}.png\", image) end And bootstrap everything in the main function def main(input) do input |> hash_input |> pick_color |> build_grid |> filter_odd_squares |> build_pixel_map |> draw_image |> save_image(input) end Now, we can run the Identicon.main and get the following images: Identicon.main(\"identicon\") Identicon.main(\"davis\")","title":"The process"},{"location":"Elixir/22_installing_phoenix_postgres/","text":"Installing phoenix \u00b6 The coure teaches phoenix v1.2 which can be installed by using the command mix archive.install https://github.com/phoenixframework/archives/raw/master/phoenix_new-1.2.5.ez The installation guide is available at https://hexdocs.pm/phoenix/installation.html . Part from elixir, erlang and phoenix, we need node.js (>=5.0.0) too. It can be installed using their website: https://nodejs.org/en/ . As the last step, we need to install PostgreSQL datatabase server, the course has a guide on how to install it directly on the machine. I will be using docker for this. docker run --name postgres -p 5432:5432 -e POSTGRES_PASSWORD=postgres -d postgres:11-alpine Now you can connect on localhost:5432 using user postgres and password postgres .","title":"Installing phoenix"},{"location":"Elixir/22_installing_phoenix_postgres/#installing-phoenix","text":"The coure teaches phoenix v1.2 which can be installed by using the command mix archive.install https://github.com/phoenixframework/archives/raw/master/phoenix_new-1.2.5.ez The installation guide is available at https://hexdocs.pm/phoenix/installation.html . Part from elixir, erlang and phoenix, we need node.js (>=5.0.0) too. It can be installed using their website: https://nodejs.org/en/ . As the last step, we need to install PostgreSQL datatabase server, the course has a guide on how to install it directly on the machine. I will be using docker for this. docker run --name postgres -p 5432:5432 -e POSTGRES_PASSWORD=postgres -d postgres:11-alpine Now you can connect on localhost:5432 using user postgres and password postgres .","title":"Installing phoenix"},{"location":"Elixir/23_phoenix/","text":"Phoenix \u00b6 The Phoenix framework is one of the fastest frameworks available. It is built with the Elixir programming language and is used for building low-latency, fault-tolerant, distributed systems, which are increasingly necessary qualities for modern web applications. Generating a Phoenix project \u00b6 mix phoenix.new discuss Not modify the mix.exs file and add {:ecto_sql, \"~> 3.0-rc.1\"} mix deps.get mix ecto.create Run the phoenix server with mix phoenix.server The project will be available at http://localhost:4000 . For interactive mode, you can run: iex -S mix phoenix.server","title":"Phoenix"},{"location":"Elixir/23_phoenix/#phoenix","text":"The Phoenix framework is one of the fastest frameworks available. It is built with the Elixir programming language and is used for building low-latency, fault-tolerant, distributed systems, which are increasingly necessary qualities for modern web applications.","title":"Phoenix"},{"location":"Elixir/23_phoenix/#generating-a-phoenix-project","text":"mix phoenix.new discuss Not modify the mix.exs file and add {:ecto_sql, \"~> 3.0-rc.1\"} mix deps.get mix ecto.create Run the phoenix server with mix phoenix.server The project will be available at http://localhost:4000 . For interactive mode, you can run: iex -S mix phoenix.server","title":"Generating a Phoenix project"},{"location":"Elixir/24_server_side_templating/","text":"Server side templating \u00b6 Server side templating is used when you want a collection of different web pages with distinct URLs that user can visit. It will serve a brand new HTML document. The templates are located into web/templates directory. There will be two folders called page and layout . The files will have extension of .eex . The pages folder is responsible for rendering separate pages, the templates folder is used to reuse different page layouts that the pages use. By default, phoenix installs bootstrap css framework.","title":"Server side templating"},{"location":"Elixir/24_server_side_templating/#server-side-templating","text":"Server side templating is used when you want a collection of different web pages with distinct URLs that user can visit. It will serve a brand new HTML document. The templates are located into web/templates directory. There will be two folders called page and layout . The files will have extension of .eex . The pages folder is responsible for rendering separate pages, the templates folder is used to reuse different page layouts that the pages use. By default, phoenix installs bootstrap css framework.","title":"Server side templating"},{"location":"Elixir/25_phoenix_MVC_model/","text":"Phoenix's MVC model \u00b6 The MVC paradigm is split into 3 components: - Model - processes the raw data - View - a template that takes the model to look nice - Controller - Figures out what the user is looking for, grabs the correct model, stuffs it into the view and returns to the user. The phoenix framework uses this MVC paradigm in its workflow. All 3 of these components in the MVC model is located in the web directory - web/controllers , web/models , web/views . So, in phoenix an incoming request works as follows: A request comes in It is processed by a router ( web/router.ex ), which routes it to a correct controller The controller takes a model, gets the data necessary and returns a view The view will get a template and compile the HTML of the response The response is sent to the user","title":"Phoenix's MVC model"},{"location":"Elixir/25_phoenix_MVC_model/#phoenixs-mvc-model","text":"The MVC paradigm is split into 3 components: - Model - processes the raw data - View - a template that takes the model to look nice - Controller - Figures out what the user is looking for, grabs the correct model, stuffs it into the view and returns to the user. The phoenix framework uses this MVC paradigm in its workflow. All 3 of these components in the MVC model is located in the web directory - web/controllers , web/models , web/views . So, in phoenix an incoming request works as follows: A request comes in It is processed by a router ( web/router.ex ), which routes it to a correct controller The controller takes a model, gets the data necessary and returns a view The view will get a template and compile the HTML of the response The response is sent to the user","title":"Phoenix's MVC model"},{"location":"Elixir/27_view_vs_template/","text":"View vs template \u00b6 In phoenix there are two folders in the web directory - web/views and web/templates , where the web/views is the MVC component. When phoenix first boots up, it looks at the web/views folder and looks up every module in it. It takes names of the view ( Discuss.PageView module's name will be Page ), then it will look at the templates directory for a folder that matches the name, in this example - the page folder. Because these folders match, we can call PageView.render(\"index.html\") which will return the web/templates/page/index.html template.","title":"View vs template"},{"location":"Elixir/27_view_vs_template/#view-vs-template","text":"In phoenix there are two folders in the web directory - web/views and web/templates , where the web/views is the MVC component. When phoenix first boots up, it looks at the web/views folder and looks up every module in it. It takes names of the view ( Discuss.PageView module's name will be Page ), then it will look at the templates directory for a folder that matches the name, in this example - the page folder. Because these folders match, we can call PageView.render(\"index.html\") which will return the web/templates/page/index.html template.","title":"View vs template"},{"location":"Elixir/28_phoenix_model_layer/","text":"Phoenix Model Layer \u00b6 The model layer is mostly used to get data from the database and interact with it. Migration files \u00b6 We can create migration files to tell phoenix to change the database. We can use a command to generate it: mix ecto.gen.migration add_topics The migration file will be stored in priv/repo/migrations directory. The migration file will contain a set of instructions for phoenix on how to change the database. So, if we want to create a table topics with a column title of type string we can use the following code: defmodule Discuss.Repo.Migrations.AddTopics do use Ecto.Migration def change do create table(:topics) do add :title, :string end end end Example on alter / reference def change do alter table(:topics) do add :user_id, references(:users) end end The migration process can be executed by running a command: mix ecto.migrate Model files \u00b6 The models are located into web/models directory. We can create a new Topic model by creating a web/models/topic.ex file with following content. defmodule Discuss.Topic do use Discuss.Web, :model schema \"topics\" do field :title, :string end def changeset(struct, params \\\\ %{}) do struct |> cast(params, [:title]) |> validate_required([:title]) end end The line use Discuss.Web, :model is used to imports the model method from web/web.ex which defines all the behaviours that controller have. Works is similar to class inheritance in OOP - extending the BaseModel class. The schema part describes which table and columns to use. The changeset function computes a changed model. It accepts the struct (current model) and the hash with all the properties to update. It casts it - produces the changeset, validates it - adds errors to the changeset if there are any Returns the changeset We can produce the changeset by doing following iex(3)> struct = %Discuss.Topic{} %Discuss.Topic{ __meta__: #Ecto.Schema.Metadata<:built, \"topics\">, id: nil, title: nil } iex(4)> params = %{title: \"A new Topic\"} %{title: \"A new Topic\"} iex(5)> Discuss.Topic.changeset(struct, params) #Ecto.Changeset< action: nil, changes: %{title: \"A new Topic\"}, errors: [], data: #Discuss.Topic<>, valid?: true >","title":"Phoenix Model Layer"},{"location":"Elixir/28_phoenix_model_layer/#phoenix-model-layer","text":"The model layer is mostly used to get data from the database and interact with it.","title":"Phoenix Model Layer"},{"location":"Elixir/28_phoenix_model_layer/#migration-files","text":"We can create migration files to tell phoenix to change the database. We can use a command to generate it: mix ecto.gen.migration add_topics The migration file will be stored in priv/repo/migrations directory. The migration file will contain a set of instructions for phoenix on how to change the database. So, if we want to create a table topics with a column title of type string we can use the following code: defmodule Discuss.Repo.Migrations.AddTopics do use Ecto.Migration def change do create table(:topics) do add :title, :string end end end Example on alter / reference def change do alter table(:topics) do add :user_id, references(:users) end end The migration process can be executed by running a command: mix ecto.migrate","title":"Migration files"},{"location":"Elixir/28_phoenix_model_layer/#model-files","text":"The models are located into web/models directory. We can create a new Topic model by creating a web/models/topic.ex file with following content. defmodule Discuss.Topic do use Discuss.Web, :model schema \"topics\" do field :title, :string end def changeset(struct, params \\\\ %{}) do struct |> cast(params, [:title]) |> validate_required([:title]) end end The line use Discuss.Web, :model is used to imports the model method from web/web.ex which defines all the behaviours that controller have. Works is similar to class inheritance in OOP - extending the BaseModel class. The schema part describes which table and columns to use. The changeset function computes a changed model. It accepts the struct (current model) and the hash with all the properties to update. It casts it - produces the changeset, validates it - adds errors to the changeset if there are any Returns the changeset We can produce the changeset by doing following iex(3)> struct = %Discuss.Topic{} %Discuss.Topic{ __meta__: #Ecto.Schema.Metadata<:built, \"topics\">, id: nil, title: nil } iex(4)> params = %{title: \"A new Topic\"} %{title: \"A new Topic\"} iex(5)> Discuss.Topic.changeset(struct, params) #Ecto.Changeset< action: nil, changes: %{title: \"A new Topic\"}, errors: [], data: #Discuss.Topic<>, valid?: true >","title":"Model files"},{"location":"Elixir/29_phoenix_router/","text":"Phoenix router \u00b6 The router file is located at web/router.ex . It is used to route a specific incoming request URL to a specific controller. The router file mostly consists of plug configuration and route configuration. The first thing is plugs. This defines what middleware / plugs are executed in what order through when a new request arrives. pipeline :browser do plug :accepts, [\"html\"] plug :fetch_session plug :fetch_flash plug :protect_from_forgery plug :put_secure_browser_headers end pipeline :api do plug :accepts, [\"json\"] end Then, you can define routes in a following manner: scope \"/\", Discuss do pipe_through :browser # Use the default browser stack get \"/\", TopicController, :index # get \"/topics/new\", TopicController, :new # post \"/topics\", TopicController, :create # get \"/topics/:id/edit\", TopicController, :edit # put \"/topics/:id\", TopicController, :update # delete \"/topics/:id\", TopicController, :delete resources \"/topics\", TopicController end You can define the routes one by one, but if you follow the RESTful approach, you can define them as resources.","title":"Phoenix router"},{"location":"Elixir/29_phoenix_router/#phoenix-router","text":"The router file is located at web/router.ex . It is used to route a specific incoming request URL to a specific controller. The router file mostly consists of plug configuration and route configuration. The first thing is plugs. This defines what middleware / plugs are executed in what order through when a new request arrives. pipeline :browser do plug :accepts, [\"html\"] plug :fetch_session plug :fetch_flash plug :protect_from_forgery plug :put_secure_browser_headers end pipeline :api do plug :accepts, [\"json\"] end Then, you can define routes in a following manner: scope \"/\", Discuss do pipe_through :browser # Use the default browser stack get \"/\", TopicController, :index # get \"/topics/new\", TopicController, :new # post \"/topics\", TopicController, :create # get \"/topics/:id/edit\", TopicController, :edit # put \"/topics/:id\", TopicController, :update # delete \"/topics/:id\", TopicController, :delete resources \"/topics\", TopicController end You can define the routes one by one, but if you follow the RESTful approach, you can define them as resources.","title":"Phoenix router"},{"location":"Elixir/30_phoenix_controller/","text":"Phoenix controller \u00b6 The controllers are located in web/controllers directory. We can create a new controller controller by adding web/controllers/topic_controller.ex With following content. defmodule Discuss.TopicController do use Discuss.Web, :controller alias Discuss.Topic def index(conn, _params) do render conn, \"index.html\", topics: Repo.all(Topic) end def show(conn, %{\"id\" => topic_id}) do render conn, \"show.html\", topic: Repo.get!(Topic, topic_id) end def new(conn, _params) do changeset = Topic.changeset(%Topic{}, %{}) render conn, \"new.html\", changeset: changeset end def create(conn, %{\"topic\" => topic}) do changeset = Topic.changeset(%Topic{}, topic) case Repo.insert(changeset) do {:ok, _post} -> conn |> put_flash(:info, \"Topic created\") |> redirect(to: topic_path(conn, :index)) {:error, changeset} -> render conn, \"new.html\", changeset: changeset end end def edit(conn, %{\"id\" => topic_id}) do topic = Repo.get(Topic, topic_id) changeset = Topic.changeset(topic) render conn, \"edit.html\", changeset: changeset, topic: topic end def update(conn, %{\"id\" => topic_id, \"topic\" => topic}) do changeset = Repo.get(Topic, topic_id) |> Topic.changeset(topic) case Repo.update(changeset) do {:ok, _topic} -> conn |> put_flash(:info, \"Topic updated\") |> redirect(to: topic_path(conn, :index)) {:error, changeset} -> render conn, \"edit.html\", changeset: changeset, topic: topic end end def delete(conn, %{\"id\" => topic_id}) do Repo.get!(Topic, topic_id) |> Repo.delete! conn |> put_flash(:info, \"Topic deleted\") |> redirect(to: topic_path(conn, :index)) end end The line use Discuss.Web, :controller is used to imports the controller method from web/web.ex which defines all the behaviours that controller have. Works is similar to class inheritance in OOP - extending the BaseController class. The conn argument is a Plug.Conn struct, that represents both incoming request and outgoing response. The params argument is a map that is used to parse the given URL. All methods must receive and return connection. Functions with ! like Repo.get! on failure will abort to 404 error .","title":"Phoenix controller"},{"location":"Elixir/30_phoenix_controller/#phoenix-controller","text":"The controllers are located in web/controllers directory. We can create a new controller controller by adding web/controllers/topic_controller.ex With following content. defmodule Discuss.TopicController do use Discuss.Web, :controller alias Discuss.Topic def index(conn, _params) do render conn, \"index.html\", topics: Repo.all(Topic) end def show(conn, %{\"id\" => topic_id}) do render conn, \"show.html\", topic: Repo.get!(Topic, topic_id) end def new(conn, _params) do changeset = Topic.changeset(%Topic{}, %{}) render conn, \"new.html\", changeset: changeset end def create(conn, %{\"topic\" => topic}) do changeset = Topic.changeset(%Topic{}, topic) case Repo.insert(changeset) do {:ok, _post} -> conn |> put_flash(:info, \"Topic created\") |> redirect(to: topic_path(conn, :index)) {:error, changeset} -> render conn, \"new.html\", changeset: changeset end end def edit(conn, %{\"id\" => topic_id}) do topic = Repo.get(Topic, topic_id) changeset = Topic.changeset(topic) render conn, \"edit.html\", changeset: changeset, topic: topic end def update(conn, %{\"id\" => topic_id, \"topic\" => topic}) do changeset = Repo.get(Topic, topic_id) |> Topic.changeset(topic) case Repo.update(changeset) do {:ok, _topic} -> conn |> put_flash(:info, \"Topic updated\") |> redirect(to: topic_path(conn, :index)) {:error, changeset} -> render conn, \"edit.html\", changeset: changeset, topic: topic end end def delete(conn, %{\"id\" => topic_id}) do Repo.get!(Topic, topic_id) |> Repo.delete! conn |> put_flash(:info, \"Topic deleted\") |> redirect(to: topic_path(conn, :index)) end end The line use Discuss.Web, :controller is used to imports the controller method from web/web.ex which defines all the behaviours that controller have. Works is similar to class inheritance in OOP - extending the BaseController class. The conn argument is a Plug.Conn struct, that represents both incoming request and outgoing response. The params argument is a map that is used to parse the given URL. All methods must receive and return connection. Functions with ! like Repo.get! on failure will abort to 404 error .","title":"Phoenix controller"},{"location":"Elixir/31_forms/","text":"Forms in Phoenix \u00b6 In order to get started with forms, we create a new view in web/views/topic_view.ex with the following content: defmodule Discuss.TopicView do use Discuss.Web, :view end And create templates in web/templates/topic/new.html.eex so it matches with the TopicController method new . <%= form_for @changeset, topic_path(@conn, :create), fn f -> %> <div class=\"form-group\"> <%= text_input f, :title, placeholder: \"Title\", class: \"form-control\" %> <%= error_tag f, :title %> </div> <%= submit \"Save Topic\", class: \"btn btn-primary\" %> <% end %> We can create web/templates/topic/index.html.eex with following: <h2>Topics</h2> <ul class=\"collection\"> <%= for topic <- @topics do %> <li class=\"collection-item\"> <%= link topic.title, to: topic_path(@conn, :show, topic) %> <div class=\"right\"> <%= link \"Edit\", to: topic_path(@conn, :edit, topic) %> <%= link \"Delete\", to: topic_path(@conn, :delete, topic), method: :delete %> </div> </li> <% end %> </ul> <div class=\"fixed-action-btn\"> <%= link to: topic_path(@conn, :new), class: \"btn-floating btn-large waves-effect waves-light red\" do %> <i class=\"material-icons\">add</i> <% end %> </div> Create web/templates/topic/edit.html.eex <%= form_for @changeset, topic_path(@conn, :update, @topic), fn f -> %> <div class=\"form-group\"> <%= text_input f, :title, placeholder: \"Title\", class: \"form-control\" %> <%= error_tag f, :title %> </div> <%= submit \"Save Topic\", class: \"btn btn-primary\" %> <% end %>","title":"Forms in Phoenix"},{"location":"Elixir/31_forms/#forms-in-phoenix","text":"In order to get started with forms, we create a new view in web/views/topic_view.ex with the following content: defmodule Discuss.TopicView do use Discuss.Web, :view end And create templates in web/templates/topic/new.html.eex so it matches with the TopicController method new . <%= form_for @changeset, topic_path(@conn, :create), fn f -> %> <div class=\"form-group\"> <%= text_input f, :title, placeholder: \"Title\", class: \"form-control\" %> <%= error_tag f, :title %> </div> <%= submit \"Save Topic\", class: \"btn btn-primary\" %> <% end %> We can create web/templates/topic/index.html.eex with following: <h2>Topics</h2> <ul class=\"collection\"> <%= for topic <- @topics do %> <li class=\"collection-item\"> <%= link topic.title, to: topic_path(@conn, :show, topic) %> <div class=\"right\"> <%= link \"Edit\", to: topic_path(@conn, :edit, topic) %> <%= link \"Delete\", to: topic_path(@conn, :delete, topic), method: :delete %> </div> </li> <% end %> </ul> <div class=\"fixed-action-btn\"> <%= link to: topic_path(@conn, :new), class: \"btn-floating btn-large waves-effect waves-light red\" do %> <i class=\"material-icons\">add</i> <% end %> </div> Create web/templates/topic/edit.html.eex <%= form_for @changeset, topic_path(@conn, :update, @topic), fn f -> %> <div class=\"form-group\"> <%= text_input f, :title, placeholder: \"Title\", class: \"form-control\" %> <%= error_tag f, :title %> </div> <%= submit \"Save Topic\", class: \"btn btn-primary\" %> <% end %>","title":"Forms in Phoenix"},{"location":"Elixir/32_phoenix_oauth/","text":"Oauth on Phoenix \u00b6 Elixir has a package Ueberauth that provides Oauth for phoenix. We will use github authentication strategy. To install it, we add the dependencies to mix.exs : {:ueberauth, \"~> 0.3\"}, {:ueberauth_github, \"~> 0.4\"} def application do [mod: {Discuss, []}, applications: [:phoenix, :phoenix_pubsub, :phoenix_html, :cowboy, :logger, :gettext, :phoenix_ecto, :postgrex, :ueberauth, :ueberauth_github]] end Next, well need to make a new OAuth application on github in https://github.com/settings/applications/new . As the Authorization callback URL we provide http://localhost:4000/auth/github/callback . Then, we get a Client ID and Client Secret keys. Add the following key to the config/config.exs . import_config \"oauth.secret.exs\" Create config/oauth.secret.exs with following content: use Mix.Config config :ueberauth, Ueberauth, providers: [ github: { Ueberauth.Strategy.Github, [default_scope: \"user\"]} ] config :ueberauth, Ueberauth.Strategy.Github.OAuth, client_id: \"the_id\", client_secret: \"the_secret\" Now we add new routes: scope \"/auth\", Discuss do pipe_through :browser get \"/signout\", AuthController, :signout get \"/:provider\", AuthController, :request get \"/:provider/callback\", AuthController, :callback end Add User model by using mix phoenix.gen.model User users email:string provider:string token:string It will generate the model as well as the migration. And now, add the web/controllers/auth_controller.ex defmodule Discuss.AuthController do use Discuss.Web, :controller alias Discuss.User plug Ueberauth def callback(%{assigns: %{ueberauth_auth: auth}} = conn, _params) do # IO.inspect(conn) user_params = %{token: auth.credentials.token, email: auth.info.email, provider: \"github\"} changeset = User.changeset(%User{}, user_params) signin(conn, changeset) end defp signin(conn, changeset) do case insert_or_update_user(changeset) do {:ok, user} -> conn |> put_flash(:info, \"Welcome back!\") |> put_session(:user_id, user.id) |> redirect(to: topic_path(conn, :index)) {:error, _reason} -> conn |> put_flash(:error, \"Error signing in\") |> redirect(to: topic_path(conn, :index)) end end defp insert_or_update_user(changeset) do case Repo.get_by(User, email: changeset.changes.email) do nil -> Repo.insert(changeset) user -> {:ok, user} end end def signout(conn, _params) do conn |> configure_session(drop: true) |> redirect(to: topic_path(conn, :index)) end end","title":"Oauth on Phoenix"},{"location":"Elixir/32_phoenix_oauth/#oauth-on-phoenix","text":"Elixir has a package Ueberauth that provides Oauth for phoenix. We will use github authentication strategy. To install it, we add the dependencies to mix.exs : {:ueberauth, \"~> 0.3\"}, {:ueberauth_github, \"~> 0.4\"} def application do [mod: {Discuss, []}, applications: [:phoenix, :phoenix_pubsub, :phoenix_html, :cowboy, :logger, :gettext, :phoenix_ecto, :postgrex, :ueberauth, :ueberauth_github]] end Next, well need to make a new OAuth application on github in https://github.com/settings/applications/new . As the Authorization callback URL we provide http://localhost:4000/auth/github/callback . Then, we get a Client ID and Client Secret keys. Add the following key to the config/config.exs . import_config \"oauth.secret.exs\" Create config/oauth.secret.exs with following content: use Mix.Config config :ueberauth, Ueberauth, providers: [ github: { Ueberauth.Strategy.Github, [default_scope: \"user\"]} ] config :ueberauth, Ueberauth.Strategy.Github.OAuth, client_id: \"the_id\", client_secret: \"the_secret\" Now we add new routes: scope \"/auth\", Discuss do pipe_through :browser get \"/signout\", AuthController, :signout get \"/:provider\", AuthController, :request get \"/:provider/callback\", AuthController, :callback end Add User model by using mix phoenix.gen.model User users email:string provider:string token:string It will generate the model as well as the migration. And now, add the web/controllers/auth_controller.ex defmodule Discuss.AuthController do use Discuss.Web, :controller alias Discuss.User plug Ueberauth def callback(%{assigns: %{ueberauth_auth: auth}} = conn, _params) do # IO.inspect(conn) user_params = %{token: auth.credentials.token, email: auth.info.email, provider: \"github\"} changeset = User.changeset(%User{}, user_params) signin(conn, changeset) end defp signin(conn, changeset) do case insert_or_update_user(changeset) do {:ok, user} -> conn |> put_flash(:info, \"Welcome back!\") |> put_session(:user_id, user.id) |> redirect(to: topic_path(conn, :index)) {:error, _reason} -> conn |> put_flash(:error, \"Error signing in\") |> redirect(to: topic_path(conn, :index)) end end defp insert_or_update_user(changeset) do case Repo.get_by(User, email: changeset.changes.email) do nil -> Repo.insert(changeset) user -> {:ok, user} end end def signout(conn, _params) do conn |> configure_session(drop: true) |> redirect(to: topic_path(conn, :index)) end end","title":"Oauth on Phoenix"},{"location":"Elixir/33_plugs/","text":"Phoenix plugs \u00b6 There are 2 times of plugs - module plugs and function plugs. The module plugs are an actual module that has an init function and call function. The function plugs is a function stored inside a controller. Currently we have stored the user id in the session, but we have to check whether we are signed in. We can add a plug that helps with this authentication process by creating web/controllers/plugs/set_user.ex file. defmodule Discuss.Plugs.SetUser do import Plug.Conn import Phoenix.Controller alias Discuss.Repo alias Discuss.User def init(_params) do end def call(conn, _params) do user_id = get_session(conn, :user_id) cond do user = user_id && Repo.get(User, user_id) -> assign(conn, :user, user) true -> assign(conn, :user, nil) end end end And add it to the web/router.ex : plug Discuss.Plugs.SetUser Now we can modify the web/templates/layouts/app.html.eex and add right in the header <%= if @conn.assigns[:user] && @conn.assigns.user.id == topic.user_id do %> <div class=\"right\"> <%= link \"Edit\", to: topic_path(@conn, :edit, topic) %> <%= link \"Delete\", to: topic_path(@conn, :delete, topic), method: :delete %> </div> <% end %> We can add another plug to require authorized user: defmodule Discuss.Plugs.RequireAuth do import Plug.Conn import Phoenix.Controller alias Discuss.Router.Helpers def init(_params) do end def call(conn, _params) do if conn.assigns[:user] do conn else conn |> put_flash(:error, \"You must be logged in.\") |> redirect(to: Helpers.topic_path(conn, :index)) |> halt() end end end And modify the TopicController: plug Discuss.Plugs.RequireAuth when action in [:new, :create, :edit, :update, :delete]","title":"Phoenix plugs"},{"location":"Elixir/33_plugs/#phoenix-plugs","text":"There are 2 times of plugs - module plugs and function plugs. The module plugs are an actual module that has an init function and call function. The function plugs is a function stored inside a controller. Currently we have stored the user id in the session, but we have to check whether we are signed in. We can add a plug that helps with this authentication process by creating web/controllers/plugs/set_user.ex file. defmodule Discuss.Plugs.SetUser do import Plug.Conn import Phoenix.Controller alias Discuss.Repo alias Discuss.User def init(_params) do end def call(conn, _params) do user_id = get_session(conn, :user_id) cond do user = user_id && Repo.get(User, user_id) -> assign(conn, :user, user) true -> assign(conn, :user, nil) end end end And add it to the web/router.ex : plug Discuss.Plugs.SetUser Now we can modify the web/templates/layouts/app.html.eex and add right in the header <%= if @conn.assigns[:user] && @conn.assigns.user.id == topic.user_id do %> <div class=\"right\"> <%= link \"Edit\", to: topic_path(@conn, :edit, topic) %> <%= link \"Delete\", to: topic_path(@conn, :delete, topic), method: :delete %> </div> <% end %> We can add another plug to require authorized user: defmodule Discuss.Plugs.RequireAuth do import Plug.Conn import Phoenix.Controller alias Discuss.Router.Helpers def init(_params) do end def call(conn, _params) do if conn.assigns[:user] do conn else conn |> put_flash(:error, \"You must be logged in.\") |> redirect(to: Helpers.topic_path(conn, :index)) |> halt() end end end And modify the TopicController: plug Discuss.Plugs.RequireAuth when action in [:new, :create, :edit, :update, :delete]","title":"Phoenix plugs"},{"location":"Elixir/34_adding_relations/","text":"Adding relations \u00b6 We will add an user_id to the topics table. def change do alter table(:topics) do add :user_id, references(:users) end end Then call mix ecto.migrate and modify the web/models/topic.ex model: schema \"topics\" do field :title, :string belongs_to :user, Discuss.User end As well as the user model: schema \"users\" do field :email, :string field :provider, :string field :token, :string has_many :topics, Discuss.Topic timestamps() end Now to test it iex(11)> Discuss.Repo.get(Discuss.Topic, 1) [debug] QUERY OK source=\"topics\" db=3.9ms queue=0.1ms SELECT t0.\"id\", t0.\"title\", t0.\"user_id\" FROM \"topics\" AS t0 WHERE (t0.\"id\" = $1) [1] %Discuss.Topic{ __meta__: #Ecto.Schema.Metadata<:loaded, \"topics\">, id: 1, title: \"New topic\", user: #Ecto.Association.NotLoaded<association :user is not loaded>, user_id: nil } iex(10)> Discuss.Repo.get(Discuss.User, 1) [debug] QUERY OK source=\"users\" db=1.6ms SELECT u0.\"id\", u0.\"email\", u0.\"provider\", u0.\"token\", u0.\"inserted_at\", u0.\"updated_at\" FROM \"users\" AS u0 WHERE (u0.\"id\" = $1) [1] %Discuss.User{ __meta__: #Ecto.Schema.Metadata<:loaded, \"users\">, id: 1, ... topics: #Ecto.Association.NotLoaded<association :topics is not loaded>, } Now we can modify the TopicController at the create method to include the user_id: # changeset = Topic.changeset(%Topic{}, topic) changeset = conn.assigns.user |> build_assoc(:topics) |> Topic.changeset(topic) We also might need to check whether the topic belogs to the user before editing, updating and deleting by using a function plug: plug :check_post_owner when action in [:update, :edit, :delete] def check_topic_owner(conn, _params) do %{params: %{\"id\" => topic_id}} = conn if Repo.get(Topic, topic_id).user_id == conn.assigns.user.id do conn else conn |> put_flash(:error, \"You cannot edit that\") |> redirect(to: topic_path(conn, :index)) |> halt() end end Also, edit the web/templates/topic/index.html.eex <%= if @conn.assigns.user.id == topic.user_id do %> <div class=\"right\"> <%= link \"Edit\", to: topic_path(@conn, :edit, topic) %> <%= link \"Delete\", to: topic_path(@conn, :delete, topic), method: :delete %> </div> <% end %>","title":"Adding relations"},{"location":"Elixir/34_adding_relations/#adding-relations","text":"We will add an user_id to the topics table. def change do alter table(:topics) do add :user_id, references(:users) end end Then call mix ecto.migrate and modify the web/models/topic.ex model: schema \"topics\" do field :title, :string belongs_to :user, Discuss.User end As well as the user model: schema \"users\" do field :email, :string field :provider, :string field :token, :string has_many :topics, Discuss.Topic timestamps() end Now to test it iex(11)> Discuss.Repo.get(Discuss.Topic, 1) [debug] QUERY OK source=\"topics\" db=3.9ms queue=0.1ms SELECT t0.\"id\", t0.\"title\", t0.\"user_id\" FROM \"topics\" AS t0 WHERE (t0.\"id\" = $1) [1] %Discuss.Topic{ __meta__: #Ecto.Schema.Metadata<:loaded, \"topics\">, id: 1, title: \"New topic\", user: #Ecto.Association.NotLoaded<association :user is not loaded>, user_id: nil } iex(10)> Discuss.Repo.get(Discuss.User, 1) [debug] QUERY OK source=\"users\" db=1.6ms SELECT u0.\"id\", u0.\"email\", u0.\"provider\", u0.\"token\", u0.\"inserted_at\", u0.\"updated_at\" FROM \"users\" AS u0 WHERE (u0.\"id\" = $1) [1] %Discuss.User{ __meta__: #Ecto.Schema.Metadata<:loaded, \"users\">, id: 1, ... topics: #Ecto.Association.NotLoaded<association :topics is not loaded>, } Now we can modify the TopicController at the create method to include the user_id: # changeset = Topic.changeset(%Topic{}, topic) changeset = conn.assigns.user |> build_assoc(:topics) |> Topic.changeset(topic) We also might need to check whether the topic belogs to the user before editing, updating and deleting by using a function plug: plug :check_post_owner when action in [:update, :edit, :delete] def check_topic_owner(conn, _params) do %{params: %{\"id\" => topic_id}} = conn if Repo.get(Topic, topic_id).user_id == conn.assigns.user.id do conn else conn |> put_flash(:error, \"You cannot edit that\") |> redirect(to: topic_path(conn, :index)) |> halt() end end Also, edit the web/templates/topic/index.html.eex <%= if @conn.assigns.user.id == topic.user_id do %> <div class=\"right\"> <%= link \"Edit\", to: topic_path(@conn, :edit, topic) %> <%= link \"Delete\", to: topic_path(@conn, :delete, topic), method: :delete %> </div> <% end %>","title":"Adding relations"},{"location":"Elixir/35_websockets/","text":"WebSockets in Phoenix \u00b6 Preparation \u00b6 We will create comment section that will work in real time using websockets. In order to do that, first we create comments: mix ecto.gen.migration add_comments create table(:comments) do add :content, :string add :user_id, references(:users) add :topic_id, references(:topics) end Make the comment model in web/models/comment.ex : defmodule Discuss.Comment do use Discuss.Web, :model schema \"comments\" do field :content, :string belongs_to :user, Discuss.User belongs_to :topic, Discuss.Topic end def changeset(struct, params \\\\ %{}) do struct |> cast(params, [:content, :user_id, :topic_id]) |> validate_required([:content, :user_id, :topic_id]) end end And modify the User and Topic models. has_many :comments, Discuss.Comment Actual WebSockets \u00b6 Phoenix has built in WebSockets support to exchange information in real-time. It also supports alternatives like HTTP Long Polling. WebSockets have implementation both on server-side and client-side. On the server side, the web socket configuration can be managed in the web/channels directory. The client side can be managed using the web/static/js/socket.js file. The WebSockets interface is split into a separate channels which is similar to a controller. It has multiple methods like join , handle_in . So, we can start with creating a connection on the client side by modifying the web/static/js/socket.js import {Socket} from \"phoenix\" let socket = new Socket(\"/socket\", {params: {token: window.userToken}}) socket.connect() const createSocket = (topicId) => { let channel = socket.channel('comments:' + topicId, {}) channel.join() .receive(\"ok\", resp => { renderComments(resp.comments) }) .receive(\"error\", resp => { console.log(\"Unable to join\", resp) }) document.querySelector('button').addEventListener('click', function(e) { e.preventDefault() const textarea = document.querySelector('textarea') const content = textarea.value channel.push('comments:add', {content: content}) textarea.value = \"\"; }) channel.on(`comments:${topicId}:new`, renderComment) } function renderComments(comments) { console.log('connected', comments) const renderedComments = comments.map(commentTemplate) document.querySelector('.collection').innerHTML = renderedComments.join('') } function renderComment(event) { const renderedComment = commentTemplate(event.comment); document.querySelector('.collection').innerHTML += renderedComment; } function commentTemplate(comment) { const author = (comment.user == null) ? \"Anonymous\" : comment.user.email; return ` <li class=\"collection-item\"> ${comment.content} <div class=\"right\"> ${author} </div> </li> `; } window.createSocket = createSocket Then we import this file in web/static/js/app.js import \"./socket\" After that, we modify the web/templates/topic/show.html.eex template: <h5><%= @topic.title %></h5> <ul class=\"collection\"> </ul> <div class=\"input-field\"> <textarea class=\"materialize-textarea\"></textarea> <button class=\"btn\">Add comment</button> </div> <script> document.addEventListener(\"DOMContentLoaded\", function() { window.createSocket(<%= @topic.id %>) }) </script> Add the comments channel to the web/channels/user_socket.ex which is a routes.ex -like file for WebSockets. defmodule Discuss.UserSocket do use Phoenix.Socket channel \"comments:*\", Discuss.CommentsChannel transport :websocket, Phoenix.Transports.WebSocket def connect(%{\"token\" => token}, socket) do case Phoenix.Token.verify(socket, \"key\", token) do {:ok, user_id} -> {:ok, assign(socket, :user_id, user_id)} {:error, _reason} -> {:ok, assign(socket, :user_id, nil)} end end def id(_socket), do: nil end Notice, that it handles authentication too, it is handled by using a token that is set in web/templates/layout/app.html.eex <%= if @conn.assigns.user do %> <script> window.userToken = \"<%= Phoenix.Token.sign(Discuss.Endpoint, \"key\", @conn.assigns.user.id) %>\"; </script> <% end %> Then finally, we add the web/channels/comments_channel.ex defmodule Discuss.CommentsChannel do use Discuss.Web, :channel alias Discuss.Topic alias Discuss.Comment alias Discuss.User def join(\"comments:\" <> topic_id, _params, socket) do topic_id = String.to_integer(topic_id) topic = Topic |> Repo.get(topic_id) |> Repo.preload(comments: [:user]) {:ok, %{comments: topic.comments}, assign(socket, :topic, topic)} end def handle_in(\"comments:add\", %{\"content\" => content}, socket) do topic = socket.assigns.topic user_id = socket.assigns.user_id changeset = topic |> build_assoc(:comments, user_id: user_id) |> Repo.preload(:user) |> Comment.changeset(%{content: content, user_id: user_id}) case Repo.insert(changeset) do {:ok, comment} -> broadcast!(socket, \"comments:#{socket.assigns.topic.id}:new\", %{comment: comment}) {:reply, :ok, socket} {:error, _reason} -> {:reply, {:error, %{errors: changeset}}, socket} end end end The last things to do, since the the websockets uses JSON encode, we need to modify the Comment and User model in order to tell them on how to encode them: defmodule Discuss.Comment do use Discuss.Web, :model @derive {Poison.Encoder, only: [:content, :user]} schema \"comments\" do field :content, :string belongs_to :user, Discuss.User belongs_to :topic, Discuss.Topic end def changeset(struct, params \\\\ %{}) do struct |> cast(params, [:content]) |> validate_required([:content]) end end defmodule Discuss.User do use Discuss.Web, :model @derive {Poison.Encoder, only: [:email]} schema \"users\" do field :email, :string field :provider, :string field :token, :string has_many :topics, Discuss.Topic has_many :comments, Discuss.Comment timestamps() end @doc \"\"\" Builds a changeset based on the `struct` and `params`. \"\"\" def changeset(struct, params \\\\ %{}) do struct |> cast(params, [:email, :provider, :token]) |> validate_required([:email, :provider, :token]) end end","title":"WebSockets in Phoenix"},{"location":"Elixir/35_websockets/#websockets-in-phoenix","text":"","title":"WebSockets in Phoenix"},{"location":"Elixir/35_websockets/#preparation","text":"We will create comment section that will work in real time using websockets. In order to do that, first we create comments: mix ecto.gen.migration add_comments create table(:comments) do add :content, :string add :user_id, references(:users) add :topic_id, references(:topics) end Make the comment model in web/models/comment.ex : defmodule Discuss.Comment do use Discuss.Web, :model schema \"comments\" do field :content, :string belongs_to :user, Discuss.User belongs_to :topic, Discuss.Topic end def changeset(struct, params \\\\ %{}) do struct |> cast(params, [:content, :user_id, :topic_id]) |> validate_required([:content, :user_id, :topic_id]) end end And modify the User and Topic models. has_many :comments, Discuss.Comment","title":"Preparation"},{"location":"Elixir/35_websockets/#actual-websockets","text":"Phoenix has built in WebSockets support to exchange information in real-time. It also supports alternatives like HTTP Long Polling. WebSockets have implementation both on server-side and client-side. On the server side, the web socket configuration can be managed in the web/channels directory. The client side can be managed using the web/static/js/socket.js file. The WebSockets interface is split into a separate channels which is similar to a controller. It has multiple methods like join , handle_in . So, we can start with creating a connection on the client side by modifying the web/static/js/socket.js import {Socket} from \"phoenix\" let socket = new Socket(\"/socket\", {params: {token: window.userToken}}) socket.connect() const createSocket = (topicId) => { let channel = socket.channel('comments:' + topicId, {}) channel.join() .receive(\"ok\", resp => { renderComments(resp.comments) }) .receive(\"error\", resp => { console.log(\"Unable to join\", resp) }) document.querySelector('button').addEventListener('click', function(e) { e.preventDefault() const textarea = document.querySelector('textarea') const content = textarea.value channel.push('comments:add', {content: content}) textarea.value = \"\"; }) channel.on(`comments:${topicId}:new`, renderComment) } function renderComments(comments) { console.log('connected', comments) const renderedComments = comments.map(commentTemplate) document.querySelector('.collection').innerHTML = renderedComments.join('') } function renderComment(event) { const renderedComment = commentTemplate(event.comment); document.querySelector('.collection').innerHTML += renderedComment; } function commentTemplate(comment) { const author = (comment.user == null) ? \"Anonymous\" : comment.user.email; return ` <li class=\"collection-item\"> ${comment.content} <div class=\"right\"> ${author} </div> </li> `; } window.createSocket = createSocket Then we import this file in web/static/js/app.js import \"./socket\" After that, we modify the web/templates/topic/show.html.eex template: <h5><%= @topic.title %></h5> <ul class=\"collection\"> </ul> <div class=\"input-field\"> <textarea class=\"materialize-textarea\"></textarea> <button class=\"btn\">Add comment</button> </div> <script> document.addEventListener(\"DOMContentLoaded\", function() { window.createSocket(<%= @topic.id %>) }) </script> Add the comments channel to the web/channels/user_socket.ex which is a routes.ex -like file for WebSockets. defmodule Discuss.UserSocket do use Phoenix.Socket channel \"comments:*\", Discuss.CommentsChannel transport :websocket, Phoenix.Transports.WebSocket def connect(%{\"token\" => token}, socket) do case Phoenix.Token.verify(socket, \"key\", token) do {:ok, user_id} -> {:ok, assign(socket, :user_id, user_id)} {:error, _reason} -> {:ok, assign(socket, :user_id, nil)} end end def id(_socket), do: nil end Notice, that it handles authentication too, it is handled by using a token that is set in web/templates/layout/app.html.eex <%= if @conn.assigns.user do %> <script> window.userToken = \"<%= Phoenix.Token.sign(Discuss.Endpoint, \"key\", @conn.assigns.user.id) %>\"; </script> <% end %> Then finally, we add the web/channels/comments_channel.ex defmodule Discuss.CommentsChannel do use Discuss.Web, :channel alias Discuss.Topic alias Discuss.Comment alias Discuss.User def join(\"comments:\" <> topic_id, _params, socket) do topic_id = String.to_integer(topic_id) topic = Topic |> Repo.get(topic_id) |> Repo.preload(comments: [:user]) {:ok, %{comments: topic.comments}, assign(socket, :topic, topic)} end def handle_in(\"comments:add\", %{\"content\" => content}, socket) do topic = socket.assigns.topic user_id = socket.assigns.user_id changeset = topic |> build_assoc(:comments, user_id: user_id) |> Repo.preload(:user) |> Comment.changeset(%{content: content, user_id: user_id}) case Repo.insert(changeset) do {:ok, comment} -> broadcast!(socket, \"comments:#{socket.assigns.topic.id}:new\", %{comment: comment}) {:reply, :ok, socket} {:error, _reason} -> {:reply, {:error, %{errors: changeset}}, socket} end end end The last things to do, since the the websockets uses JSON encode, we need to modify the Comment and User model in order to tell them on how to encode them: defmodule Discuss.Comment do use Discuss.Web, :model @derive {Poison.Encoder, only: [:content, :user]} schema \"comments\" do field :content, :string belongs_to :user, Discuss.User belongs_to :topic, Discuss.Topic end def changeset(struct, params \\\\ %{}) do struct |> cast(params, [:content]) |> validate_required([:content]) end end defmodule Discuss.User do use Discuss.Web, :model @derive {Poison.Encoder, only: [:email]} schema \"users\" do field :email, :string field :provider, :string field :token, :string has_many :topics, Discuss.Topic has_many :comments, Discuss.Comment timestamps() end @doc \"\"\" Builds a changeset based on the `struct` and `params`. \"\"\" def changeset(struct, params \\\\ %{}) do struct |> cast(params, [:email, :provider, :token]) |> validate_required([:email, :provider, :token]) end end","title":"Actual WebSockets"},{"location":"Elixir/36_dockerized_phoenix_interactive/","text":"Run dockerized phoenix in interactive mode \u00b6 docker-compose run --service-ports app-phoenix bash iex -S mix phx.server recompile","title":"Run dockerized phoenix in interactive mode"},{"location":"Elixir/36_dockerized_phoenix_interactive/#run-dockerized-phoenix-in-interactive-mode","text":"docker-compose run --service-ports app-phoenix bash iex -S mix phx.server recompile","title":"Run dockerized phoenix in interactive mode"},{"location":"Elixir/Phoenix/Phoenix%20enable%20GZIP%20compression/","text":"In lib/myapp_web/endpoint.ex change [[GZIP]] from false to true : plug(Plug.Static, at: \"/\", from: :tilex, gzip: true, only: ~w(assets ...)) But why stop there? We can [[compress]] our dynamic document bodies just as easily. In config/prod.exs , add compress: true to the [[HTTP]] config of the [[endpoint]]. config :my_app, MyAppWeb.Endpoint, http: [port: {:system, \"PORT\"}, compress: true] Once done, you can validate that there is a content-encoding: gzip [[HTTP headers]] in the [[HTTP Response]].","title":"Phoenix enable GZIP compression"},{"location":"Frontend/React/Setup%20React/","text":"Setting up for React development \u00b6 1. Install node.js \u00b6 curl -sL https://deb.nodesource.com/setup_11.x | sudo -E bash - sudo apt-get install -y nodejs davis@davis-mint \ue0b0 /tmp \ue0b0 node -v v11.3.0 davis@davis-mint \ue0b0 /tmp \ue0b0 npm -v 6.4.1 2. Create new React app \u00b6 npx create-react-app react-app \u2718 davis@davis-mint \ue0b0 ~/Sites \ue0b0 npx create-react-app react-app npx: installed 63 in 1.511s Creating a new React app in /home/davis/Sites/react-app. Installing packages. This might take a couple of minutes. Installing react, react-dom, and react-scripts... + react-scripts@2.1.1 + react@16.6.3 + react-dom@16.6.3 added 1701 packages from 661 contributors and audited 35639 packages in 95.539s found 0 vulnerabilities Initialized a git repository. Success! Created react-app at /home/davis/Sites/react-app Inside that directory, you can run several commands: npm start Starts the development server. npm run build Bundles the app into static files for production. npm test Starts the test runner. npm run eject Removes this tool and copies build dependencies, configuration files and scripts into the app directory. If you do this, you can\u2019t go back! We suggest that you begin by typing: cd react-app npm start Happy hacking! cd react-app npm start","title":"Setting up for React development"},{"location":"Frontend/React/Setup%20React/#setting-up-for-react-development","text":"","title":"Setting up for React development"},{"location":"Frontend/React/Setup%20React/#1-install-nodejs","text":"curl -sL https://deb.nodesource.com/setup_11.x | sudo -E bash - sudo apt-get install -y nodejs davis@davis-mint \ue0b0 /tmp \ue0b0 node -v v11.3.0 davis@davis-mint \ue0b0 /tmp \ue0b0 npm -v 6.4.1","title":"1. Install node.js"},{"location":"Frontend/React/Setup%20React/#2-create-new-react-app","text":"npx create-react-app react-app \u2718 davis@davis-mint \ue0b0 ~/Sites \ue0b0 npx create-react-app react-app npx: installed 63 in 1.511s Creating a new React app in /home/davis/Sites/react-app. Installing packages. This might take a couple of minutes. Installing react, react-dom, and react-scripts... + react-scripts@2.1.1 + react@16.6.3 + react-dom@16.6.3 added 1701 packages from 661 contributors and audited 35639 packages in 95.539s found 0 vulnerabilities Initialized a git repository. Success! Created react-app at /home/davis/Sites/react-app Inside that directory, you can run several commands: npm start Starts the development server. npm run build Bundles the app into static files for production. npm test Starts the test runner. npm run eject Removes this tool and copies build dependencies, configuration files and scripts into the app directory. If you do this, you can\u2019t go back! We suggest that you begin by typing: cd react-app npm start Happy hacking! cd react-app npm start","title":"2. Create new React app"},{"location":"Git/","text":"GIT \u00b6 Learning the CLI side of git. Sources: - Git Complete: The definitive, step-by-step guide to Git","title":"GIT"},{"location":"Git/#git","text":"Learning the CLI side of git. Sources: - Git Complete: The definitive, step-by-step guide to Git","title":"GIT"},{"location":"Git/01-quickstart/","text":"Git Quickstart \u00b6 You can confirm that git is installed by using: git version Git requires two bits of information before we can do things - name and email address. If it's not provided, git will try to automatically figure it out, but this process is unreliable. git config --global user.name \"Your name\" git config --global user.email \"your@email.com\" The changes can be verified by using command: git config --global --list A project can be downloaded from a repository to local machine by using clone command: git clone https://github.com/daviskregers/git.git davis@davis-arch \ue0b0 ~/projects \ue0b0 git clone https://github.com/daviskregers/git.git Cloning into 'git'... remote: Enumerating objects: 3, done. remote: Counting objects: 100% (3/3), done. remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0 Unpacking objects: 100% (3/3), done. davis@davis-arch \ue0b0 ~/projects \ue0b0 cd git davis@davis-arch \ue0b0 ~/projects/git \ue0b0 \ue0a0 master \ue0b0 ls README.md davis@davis-arch \ue0b0 ~/projects/git \ue0b0 \ue0a0 master \ue0b0 You can check the status of the repository by using status command. davis@davis-arch \ue0b0 ~/projects/git \ue0b0 \ue0a0 master \ue0b0 git status On branch master Your branch is up to date with 'origin/master'. nothing to commit, working tree clean You can see that you're on the master branch and it is up-to-date with the remote's master branch. There are no working directory changes. If we create a new file, we'll see it in the status davis@davis-arch \ue0b0 ~/projects/git \ue0b0 \ue0a0 master \ue0b0 echo \"Test Git Quick Start demo\" >> start.txt davis@davis-arch \ue0b0 ~/projects/git \ue0b0 \ue0a0 master \ue0b0 ls README.md start.txt davis@davis-arch \ue0b0 ~/projects/git \ue0b0 \ue0a0 master \ue0b0 cat start.txt Test Git Quick Start demo davis@davis-arch \ue0b0 ~/projects/git \ue0b0 \ue0a0 master \ue0b0 git status On branch master Your branch is up to date with 'origin/master'. Untracked files: (use \"git add <file>...\" to include in what will be committed) start.txt nothing added to commit but untracked files present (use \"git add\" to track) davis@davis-arch \ue0b0 ~/projects/git \ue0b0 \ue0a0 master \ue0b0 If we want to add the file to the staging area, we use davis@davis-arch \ue0b0 ~/projects/git \ue0b0 \ue0a0 master \ue0b0 git add start.txt davis@davis-arch \ue0b0 ~/projects/git \ue0b0 \ue0a0 master \u271a \ue0b0 git status On branch master Your branch is up to date with 'origin/master'. Changes to be committed: (use \"git reset HEAD <file>...\" to unstage) new file: start.txt Now we can make a commit from the staging area by using: davis@davis-arch \ue0b0 ~/projects/git \ue0b0 \ue0a0 master \u271a \ue0b0 git commit -m \"Adding start text file\" [master 9d8a074] Adding start text file 1 file changed, 1 insertion(+) create mode 100644 start.txt Now, when using status davis@davis-arch \ue0b0 ~/projects/git \ue0b0 \ue0a0 master \ue0b0 git status On branch master Your branch is ahead of 'origin/master' by 1 commit. (use \"git push\" to publish your local commits) nothing to commit, working tree clean We can see that there is nothing to commit, our master branch is ahead of origin/master by 1 commit. Now we can push to upload it to the repository. davis@davis-arch \ue0b0 ~/projects/git \ue0b0 \ue0a0 master \ue0b0 git push origin master Username for 'https://github.com': ... Password for '...@github.com': Enumerating objects: 4, done. Counting objects: 100% (4/4), done. Delta compression using up to 8 threads Compressing objects: 100% (2/2), done. Writing objects: 100% (3/3), 326 bytes | 326.00 KiB/s, done. Total 3 (delta 0), reused 0 (delta 0) To https://github.com/daviskregers/git.git 2c533a8..9d8a074 master -> master The origin is a remote name, which was created automatically when we cloned, the master is a branch on the remote. If everything is correct, we can see the file in the remote repository.","title":"Git Quickstart"},{"location":"Git/01-quickstart/#git-quickstart","text":"You can confirm that git is installed by using: git version Git requires two bits of information before we can do things - name and email address. If it's not provided, git will try to automatically figure it out, but this process is unreliable. git config --global user.name \"Your name\" git config --global user.email \"your@email.com\" The changes can be verified by using command: git config --global --list A project can be downloaded from a repository to local machine by using clone command: git clone https://github.com/daviskregers/git.git davis@davis-arch \ue0b0 ~/projects \ue0b0 git clone https://github.com/daviskregers/git.git Cloning into 'git'... remote: Enumerating objects: 3, done. remote: Counting objects: 100% (3/3), done. remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0 Unpacking objects: 100% (3/3), done. davis@davis-arch \ue0b0 ~/projects \ue0b0 cd git davis@davis-arch \ue0b0 ~/projects/git \ue0b0 \ue0a0 master \ue0b0 ls README.md davis@davis-arch \ue0b0 ~/projects/git \ue0b0 \ue0a0 master \ue0b0 You can check the status of the repository by using status command. davis@davis-arch \ue0b0 ~/projects/git \ue0b0 \ue0a0 master \ue0b0 git status On branch master Your branch is up to date with 'origin/master'. nothing to commit, working tree clean You can see that you're on the master branch and it is up-to-date with the remote's master branch. There are no working directory changes. If we create a new file, we'll see it in the status davis@davis-arch \ue0b0 ~/projects/git \ue0b0 \ue0a0 master \ue0b0 echo \"Test Git Quick Start demo\" >> start.txt davis@davis-arch \ue0b0 ~/projects/git \ue0b0 \ue0a0 master \ue0b0 ls README.md start.txt davis@davis-arch \ue0b0 ~/projects/git \ue0b0 \ue0a0 master \ue0b0 cat start.txt Test Git Quick Start demo davis@davis-arch \ue0b0 ~/projects/git \ue0b0 \ue0a0 master \ue0b0 git status On branch master Your branch is up to date with 'origin/master'. Untracked files: (use \"git add <file>...\" to include in what will be committed) start.txt nothing added to commit but untracked files present (use \"git add\" to track) davis@davis-arch \ue0b0 ~/projects/git \ue0b0 \ue0a0 master \ue0b0 If we want to add the file to the staging area, we use davis@davis-arch \ue0b0 ~/projects/git \ue0b0 \ue0a0 master \ue0b0 git add start.txt davis@davis-arch \ue0b0 ~/projects/git \ue0b0 \ue0a0 master \u271a \ue0b0 git status On branch master Your branch is up to date with 'origin/master'. Changes to be committed: (use \"git reset HEAD <file>...\" to unstage) new file: start.txt Now we can make a commit from the staging area by using: davis@davis-arch \ue0b0 ~/projects/git \ue0b0 \ue0a0 master \u271a \ue0b0 git commit -m \"Adding start text file\" [master 9d8a074] Adding start text file 1 file changed, 1 insertion(+) create mode 100644 start.txt Now, when using status davis@davis-arch \ue0b0 ~/projects/git \ue0b0 \ue0a0 master \ue0b0 git status On branch master Your branch is ahead of 'origin/master' by 1 commit. (use \"git push\" to publish your local commits) nothing to commit, working tree clean We can see that there is nothing to commit, our master branch is ahead of origin/master by 1 commit. Now we can push to upload it to the repository. davis@davis-arch \ue0b0 ~/projects/git \ue0b0 \ue0a0 master \ue0b0 git push origin master Username for 'https://github.com': ... Password for '...@github.com': Enumerating objects: 4, done. Counting objects: 100% (4/4), done. Delta compression using up to 8 threads Compressing objects: 100% (2/2), done. Writing objects: 100% (3/3), 326 bytes | 326.00 KiB/s, done. Total 3 (delta 0), reused 0 (delta 0) To https://github.com/daviskregers/git.git 2c533a8..9d8a074 master -> master The origin is a remote name, which was created automatically when we cloned, the master is a branch on the remote. If everything is correct, we can see the file in the remote repository.","title":"Git Quickstart"},{"location":"Git/02-configure-editor-with-git/","text":"Configure your editor to work with git \u00b6 You can use config, to use a specific command as the git's core editor, I'll use Visual Studio Code. git config --global core.editor \"code\" Now we run a command: git config --global -e The git global configuration file will open up in Visual Studio Code editor. Also, if we now run a git commit command without any parameters, the commit message will be opened in Visual Studio Code for editing.","title":"Configure your editor to work with git"},{"location":"Git/02-configure-editor-with-git/#configure-your-editor-to-work-with-git","text":"You can use config, to use a specific command as the git's core editor, I'll use Visual Studio Code. git config --global core.editor \"code\" Now we run a command: git config --global -e The git global configuration file will open up in Visual Studio Code editor. Also, if we now run a git commit command without any parameters, the commit message will be opened in Visual Studio Code for editing.","title":"Configure your editor to work with git"},{"location":"Git/03-starting-with-a-fresh-project/","text":"Starting with a fresh project (git init) \u00b6 In order to start a fresh git repository, you can use the init command git init project-name It will create a folder project-name . davis@davis-arch \ue0b0 ~/projects/learning-git \ue0b0 git init project Initialized empty Git repository in /home/davis/projects/learning-git/project/.git/ davis@davis-arch \ue0b0 ~/projects/learning-git \ue0b0 ls project davis@davis-arch \ue0b0 ~/projects/learning-git \ue0b0 cd project master \ue0b0 ls master \ue0b0 git status On branch master No commits yet nothing to commit (create/copy files and use \"git add\" to track) master \ue0b0 ls -al total 12 drwxr-xr-x 3 davis www-data 4096 Jan 21 20:04 . drwxr-xr-x 3 davis www-data 4096 Jan 21 20:04 .. drwxr-xr-x 7 davis www-data 4096 Jan 21 20:04 .git master \ue0b0 You can see that a diretory called project is created, it contains the .git hidden directory.","title":"03 starting with a fresh project"},{"location":"Git/03-starting-with-a-fresh-project/#starting-with-a-fresh-project-git-init","text":"In order to start a fresh git repository, you can use the init command git init project-name It will create a folder project-name . davis@davis-arch \ue0b0 ~/projects/learning-git \ue0b0 git init project Initialized empty Git repository in /home/davis/projects/learning-git/project/.git/ davis@davis-arch \ue0b0 ~/projects/learning-git \ue0b0 ls project davis@davis-arch \ue0b0 ~/projects/learning-git \ue0b0 cd project master \ue0b0 ls master \ue0b0 git status On branch master No commits yet nothing to commit (create/copy files and use \"git add\" to track) master \ue0b0 ls -al total 12 drwxr-xr-x 3 davis www-data 4096 Jan 21 20:04 . drwxr-xr-x 3 davis www-data 4096 Jan 21 20:04 .. drwxr-xr-x 7 davis www-data 4096 Jan 21 20:04 .git master \ue0b0 You can see that a diretory called project is created, it contains the .git hidden directory.","title":"Starting  with a fresh project (git init)"},{"location":"Git/04-init-existing-project/","text":"Adding git to an existing project \u00b6 If we have a directory that already has files in it and we want to version it using git, we can: davis@davis-arch \ue0b0 ~/projects \ue0b0 mkdir existing-project davis@davis-arch \ue0b0 ~/projects \ue0b0 cd existing-project davis@davis-arch \ue0b0 ~/projects/existing-project \ue0b0 touch file1 davis@davis-arch \ue0b0 ~/projects/existing-project \ue0b0 touch file2 davis@davis-arch \ue0b0 ~/projects/existing-project \ue0b0 ls -al total 8 drwxr-xr-x 2 davis www-data 4096 Jan 21 20:12 . drwxr-xr-x 20 davis www-data 4096 Jan 21 20:12 .. -rw-r--r-- 1 davis www-data 0 Jan 21 20:12 file1 -rw-r--r-- 1 davis www-data 0 Jan 21 20:12 file2 davis@davis-arch \ue0b0 ~/projects/existing-project \ue0b0 git init Initialized empty Git repository in /home/davis/projects/existing-project/.git/ davis@davis-arch \ue0b0 ~/projects/existing-project \ue0b0 \ue0a0 master \ue0b0 ls -al total 12 drwxr-xr-x 3 davis www-data 4096 Jan 21 20:12 . drwxr-xr-x 20 davis www-data 4096 Jan 21 20:12 .. -rw-r--r-- 1 davis www-data 0 Jan 21 20:12 file1 -rw-r--r-- 1 davis www-data 0 Jan 21 20:12 file2 drwxr-xr-x 7 davis www-data 4096 Jan 21 20:12 .git davis@davis-arch \ue0b0 ~/projects/existing-project \ue0b0 \ue0a0 master \ue0b0 git status On branch master No commits yet Untracked files: (use \"git add <file>...\" to include in what will be committed) file1 file2 nothing added to commit but untracked files present (use \"git add\" to track) Now we can commit the files recursively: davis@davis-arch \ue0b0 ~/projects/existing-project \ue0b0 \ue0a0 master \ue0b0 git add . davis@davis-arch \ue0b0 ~/projects/existing-project \ue0b0 \ue0a0 master \u271a \ue0b0 git commit -m \"Initial commit\" [master (root-commit) 4bd5a61] Initial commit 2 files changed, 0 insertions(+), 0 deletions(-) create mode 100644 file1 create mode 100644 file2 davis@davis-arch \ue0b0 ~/projects/existing-project \ue0b0 \ue0a0 master \ue0b0 git status On branch master nothing to commit, working tree clean To remove the git versioning: davis@davis-arch \ue0b0 ~/projects/existing-project \ue0b0 \ue0a0 master \ue0b0 rm -rf .git davis@davis-arch \ue0b0 ~/projects/existing-project \ue0b0 ls -al total 8 drwxr-xr-x 2 davis www-data 4096 Jan 21 20:15 . drwxr-xr-x 20 davis www-data 4096 Jan 21 20:12 .. -rw-r--r-- 1 davis www-data 0 Jan 21 20:12 file1 -rw-r--r-- 1 davis www-data 0 Jan 21 20:12 file2","title":"04 init existing project"},{"location":"Git/04-init-existing-project/#adding-git-to-an-existing-project","text":"If we have a directory that already has files in it and we want to version it using git, we can: davis@davis-arch \ue0b0 ~/projects \ue0b0 mkdir existing-project davis@davis-arch \ue0b0 ~/projects \ue0b0 cd existing-project davis@davis-arch \ue0b0 ~/projects/existing-project \ue0b0 touch file1 davis@davis-arch \ue0b0 ~/projects/existing-project \ue0b0 touch file2 davis@davis-arch \ue0b0 ~/projects/existing-project \ue0b0 ls -al total 8 drwxr-xr-x 2 davis www-data 4096 Jan 21 20:12 . drwxr-xr-x 20 davis www-data 4096 Jan 21 20:12 .. -rw-r--r-- 1 davis www-data 0 Jan 21 20:12 file1 -rw-r--r-- 1 davis www-data 0 Jan 21 20:12 file2 davis@davis-arch \ue0b0 ~/projects/existing-project \ue0b0 git init Initialized empty Git repository in /home/davis/projects/existing-project/.git/ davis@davis-arch \ue0b0 ~/projects/existing-project \ue0b0 \ue0a0 master \ue0b0 ls -al total 12 drwxr-xr-x 3 davis www-data 4096 Jan 21 20:12 . drwxr-xr-x 20 davis www-data 4096 Jan 21 20:12 .. -rw-r--r-- 1 davis www-data 0 Jan 21 20:12 file1 -rw-r--r-- 1 davis www-data 0 Jan 21 20:12 file2 drwxr-xr-x 7 davis www-data 4096 Jan 21 20:12 .git davis@davis-arch \ue0b0 ~/projects/existing-project \ue0b0 \ue0a0 master \ue0b0 git status On branch master No commits yet Untracked files: (use \"git add <file>...\" to include in what will be committed) file1 file2 nothing added to commit but untracked files present (use \"git add\" to track) Now we can commit the files recursively: davis@davis-arch \ue0b0 ~/projects/existing-project \ue0b0 \ue0a0 master \ue0b0 git add . davis@davis-arch \ue0b0 ~/projects/existing-project \ue0b0 \ue0a0 master \u271a \ue0b0 git commit -m \"Initial commit\" [master (root-commit) 4bd5a61] Initial commit 2 files changed, 0 insertions(+), 0 deletions(-) create mode 100644 file1 create mode 100644 file2 davis@davis-arch \ue0b0 ~/projects/existing-project \ue0b0 \ue0a0 master \ue0b0 git status On branch master nothing to commit, working tree clean To remove the git versioning: davis@davis-arch \ue0b0 ~/projects/existing-project \ue0b0 \ue0a0 master \ue0b0 rm -rf .git davis@davis-arch \ue0b0 ~/projects/existing-project \ue0b0 ls -al total 8 drwxr-xr-x 2 davis www-data 4096 Jan 21 20:15 . drwxr-xr-x 20 davis www-data 4096 Jan 21 20:12 .. -rw-r--r-- 1 davis www-data 0 Jan 21 20:12 file1 -rw-r--r-- 1 davis www-data 0 Jan 21 20:12 file2","title":"Adding git to an existing project"},{"location":"Git/05-joining-an-existing-project/","text":"Starting by joining an existing project \u00b6 You can locate a repository you want to work on, for example, on GitHub, and click on the fork option, this will create an own personal copy of the repository in our account. Then you can get the clone url to download it to your local machine. davis@davis-arch \ue0b0 ~/projects/learning-git \ue0b0 git clone https://github.com/daviskregers/notes.git Cloning into 'notes'... remote: Enumerating objects: 303, done. remote: Counting objects: 100% (303/303), done. remote: Compressing objects: 100% (276/276), done. remote: Total 303 (delta 84), reused 218 (delta 10), pack-reused 0 Receiving objects: 100% (303/303), 4.81 MiB | 1.99 MiB/s, done. Resolving deltas: 100% (84/84), done. davis@davis-arch \ue0b0 ~/projects/learning-git \ue0b0 ls -al total 12 drwxr-xr-x 3 davis www-data 4096 Jan 21 20:17 . drwxr-xr-x 20 davis www-data 4096 Jan 21 20:17 .. drwxr-xr-x 14 davis www-data 4096 Jan 21 20:17 notes davis@davis-arch \ue0b0 ~/projects/learning-git \ue0b0 cd notes davis@davis-arch \ue0b0 ~/projects/learning-git/notes \ue0b0 \ue0a0 master \ue0b0 ls -al total 60 drwxr-xr-x 14 davis www-data 4096 Jan 21 20:17 . drwxr-xr-x 3 davis www-data 4096 Jan 21 20:17 .. drwxr-xr-x 2 davis www-data 4096 Jan 21 20:17 aws drwxr-xr-x 3 davis www-data 4096 Jan 21 20:17 databases drwxr-xr-x 2 davis www-data 4096 Jan 21 20:17 docker drwxr-xr-x 2 davis www-data 4096 Jan 21 20:17 elixir drwxr-xr-x 3 davis www-data 4096 Jan 21 20:17 frontend drwxr-xr-x 8 davis www-data 4096 Jan 21 20:17 .git drwxr-xr-x 2 davis www-data 4096 Jan 21 20:17 images drwxr-xr-x 2 davis www-data 4096 Jan 21 20:17 javascript drwxr-xr-x 6 davis www-data 4096 Jan 21 20:17 linux drwxr-xr-x 2 davis www-data 4096 Jan 21 20:17 osx drwxr-xr-x 3 davis www-data 4096 Jan 21 20:17 php -rw-r--r-- 1 davis www-data 34 Jan 21 20:17 README.md drwxr-xr-x 2 davis www-data 4096 Jan 21 20:17 rust davis@davis-arch \ue0b0 ~/projects/learning-git/notes \ue0b0 \ue0a0 master \ue0b0 git status On branch master Your branch is up to date with 'origin/master'. nothing to commit, working tree clean","title":"Starting by joining an existing project"},{"location":"Git/05-joining-an-existing-project/#starting-by-joining-an-existing-project","text":"You can locate a repository you want to work on, for example, on GitHub, and click on the fork option, this will create an own personal copy of the repository in our account. Then you can get the clone url to download it to your local machine. davis@davis-arch \ue0b0 ~/projects/learning-git \ue0b0 git clone https://github.com/daviskregers/notes.git Cloning into 'notes'... remote: Enumerating objects: 303, done. remote: Counting objects: 100% (303/303), done. remote: Compressing objects: 100% (276/276), done. remote: Total 303 (delta 84), reused 218 (delta 10), pack-reused 0 Receiving objects: 100% (303/303), 4.81 MiB | 1.99 MiB/s, done. Resolving deltas: 100% (84/84), done. davis@davis-arch \ue0b0 ~/projects/learning-git \ue0b0 ls -al total 12 drwxr-xr-x 3 davis www-data 4096 Jan 21 20:17 . drwxr-xr-x 20 davis www-data 4096 Jan 21 20:17 .. drwxr-xr-x 14 davis www-data 4096 Jan 21 20:17 notes davis@davis-arch \ue0b0 ~/projects/learning-git \ue0b0 cd notes davis@davis-arch \ue0b0 ~/projects/learning-git/notes \ue0b0 \ue0a0 master \ue0b0 ls -al total 60 drwxr-xr-x 14 davis www-data 4096 Jan 21 20:17 . drwxr-xr-x 3 davis www-data 4096 Jan 21 20:17 .. drwxr-xr-x 2 davis www-data 4096 Jan 21 20:17 aws drwxr-xr-x 3 davis www-data 4096 Jan 21 20:17 databases drwxr-xr-x 2 davis www-data 4096 Jan 21 20:17 docker drwxr-xr-x 2 davis www-data 4096 Jan 21 20:17 elixir drwxr-xr-x 3 davis www-data 4096 Jan 21 20:17 frontend drwxr-xr-x 8 davis www-data 4096 Jan 21 20:17 .git drwxr-xr-x 2 davis www-data 4096 Jan 21 20:17 images drwxr-xr-x 2 davis www-data 4096 Jan 21 20:17 javascript drwxr-xr-x 6 davis www-data 4096 Jan 21 20:17 linux drwxr-xr-x 2 davis www-data 4096 Jan 21 20:17 osx drwxr-xr-x 3 davis www-data 4096 Jan 21 20:17 php -rw-r--r-- 1 davis www-data 34 Jan 21 20:17 README.md drwxr-xr-x 2 davis www-data 4096 Jan 21 20:17 rust davis@davis-arch \ue0b0 ~/projects/learning-git/notes \ue0b0 \ue0a0 master \ue0b0 git status On branch master Your branch is up to date with 'origin/master'. nothing to commit, working tree clean","title":"Starting by joining an existing project"},{"location":"Git/06-basic-workflow/","text":"Basic Git Workflow \u00b6 The basic Git workflow will look like this: git add filename git commit -m \"The description of the commit\" git pull origin master git push origin master So, we add a filename to the staging area, commit it, pull the latest changes from the origin 's master branch, and push our commit to the origin 's master branch.","title":"06 basic workflow"},{"location":"Git/06-basic-workflow/#basic-git-workflow","text":"The basic Git workflow will look like this: git add filename git commit -m \"The description of the commit\" git pull origin master git push origin master So, we add a filename to the staging area, commit it, pull the latest changes from the origin 's master branch, and push our commit to the origin 's master branch.","title":"Basic Git Workflow"},{"location":"Git/07-tracked-files/","text":"Tracked files \u00b6 If we have a file that is already commited and we change it, git will track it and let us know that it has been changed: davis@davis-arch \ue0b0 ~/projects/learning-git \ue0b0 git init project Initialized empty Git repository in /home/davis/projects/learning-git/project/.git/ davis@davis-arch \ue0b0 ~/projects/learning-git \ue0b0 cd project master \ue0b0 touch file master \ue0b0 git add file master \u271a \ue0b0 git commit -m \"initial commit\" [master (root-commit) ac6cbef] initial commit 1 file changed, 0 insertions(+), 0 deletions(-) create mode 100644 file master \ue0b0 git status On branch master nothing to commit, working tree clean master \ue0b0 echo \"changed\" >> file master \u25cf \ue0b0 git status On branch master Changes not staged for commit: (use \"git add <file>...\" to update what will be committed) (use \"git checkout -- <file>...\" to discard changes in working directory) modified: file no changes added to commit (use \"git add\" and/or \"git commit -a\") We can shortcut to both add file to staging area and commit by using a following command: master \u25cf \ue0b0 git commit -am \"commit message\" [master e93c1a1] commit message 1 file changed, 1 insertion(+) master \ue0b0 git status On branch master nothing to commit, working tree clean The -a stands for add to staging area, -m for message. A way to find out if our file is being tracked by git we can use a command: master \ue0b0 git ls-files file We can see that git tracks the file . If we add a new file, it wont be tracked unless added to staging area or commited: master \ue0b0 touch file2 master \ue0b0 git ls-files file","title":"07 tracked files"},{"location":"Git/07-tracked-files/#tracked-files","text":"If we have a file that is already commited and we change it, git will track it and let us know that it has been changed: davis@davis-arch \ue0b0 ~/projects/learning-git \ue0b0 git init project Initialized empty Git repository in /home/davis/projects/learning-git/project/.git/ davis@davis-arch \ue0b0 ~/projects/learning-git \ue0b0 cd project master \ue0b0 touch file master \ue0b0 git add file master \u271a \ue0b0 git commit -m \"initial commit\" [master (root-commit) ac6cbef] initial commit 1 file changed, 0 insertions(+), 0 deletions(-) create mode 100644 file master \ue0b0 git status On branch master nothing to commit, working tree clean master \ue0b0 echo \"changed\" >> file master \u25cf \ue0b0 git status On branch master Changes not staged for commit: (use \"git add <file>...\" to update what will be committed) (use \"git checkout -- <file>...\" to discard changes in working directory) modified: file no changes added to commit (use \"git add\" and/or \"git commit -a\") We can shortcut to both add file to staging area and commit by using a following command: master \u25cf \ue0b0 git commit -am \"commit message\" [master e93c1a1] commit message 1 file changed, 1 insertion(+) master \ue0b0 git status On branch master nothing to commit, working tree clean The -a stands for add to staging area, -m for message. A way to find out if our file is being tracked by git we can use a command: master \ue0b0 git ls-files file We can see that git tracks the file . If we add a new file, it wont be tracked unless added to staging area or commited: master \ue0b0 touch file2 master \ue0b0 git ls-files file","title":"Tracked files"},{"location":"Git/08-recursive-add/","text":"Recursive add \u00b6 master \u25cf \ue0b0 mkdir -p l1/l2/l3/l4 master \u25cf \ue0b0 touch l1/file1 master \u25cf \ue0b0 touch l1/l2/file2 master \u25cf \ue0b0 touch l1/l2/l3/file3 master \u25cf \ue0b0 touch l1/l2/l3/l4/file4 Git will not show the files changed recursively: master \ue0b0 git status On branch master No commits yet Untracked files: (use \"git add <file>...\" to include in what will be committed) l1/ nothing added to commit but untracked files present (use \"git add\" to track) But we can add the changes recursively to the staging area: master \ue0b0 git add . master \u271a \ue0b0 git status On branch master No commits yet Changes to be committed: (use \"git rm --cached <file>...\" to unstage) new file: l1/file1 new file: l1/l2/file2 new file: l1/l2/l3/file3 new file: l1/l2/l3/l4/file4","title":"08 recursive add"},{"location":"Git/08-recursive-add/#recursive-add","text":"master \u25cf \ue0b0 mkdir -p l1/l2/l3/l4 master \u25cf \ue0b0 touch l1/file1 master \u25cf \ue0b0 touch l1/l2/file2 master \u25cf \ue0b0 touch l1/l2/l3/file3 master \u25cf \ue0b0 touch l1/l2/l3/l4/file4 Git will not show the files changed recursively: master \ue0b0 git status On branch master No commits yet Untracked files: (use \"git add <file>...\" to include in what will be committed) l1/ nothing added to commit but untracked files present (use \"git add\" to track) But we can add the changes recursively to the staging area: master \ue0b0 git add . master \u271a \ue0b0 git status On branch master No commits yet Changes to be committed: (use \"git rm --cached <file>...\" to unstage) new file: l1/file1 new file: l1/l2/file2 new file: l1/l2/l3/file3 new file: l1/l2/l3/l4/file4","title":"Recursive add"},{"location":"Git/09-discarding/","text":"Backing out changes \u00b6 If we need to back-out the changes from the staging area (unstage), we can use a following command: master \ue0b0 touch l1/file1-2 master \ue0b0 touch l1/l2/file-2-2 master \ue0b0 touch l1/l2/l3/file-3-2 master \ue0b0 touch l1/l2/l3/l4/file-4-2 master \ue0b0 git add . master \u271a \ue0b0 git status On branch master Changes to be committed: (use \"git reset HEAD <file>...\" to unstage) new file: l1/file1-2 new file: l1/l2/file-2-2 new file: l1/l2/l3/file-3-2 new file: l1/l2/l3/l4/file-4-2 master \u271a \ue0b0 git reset HEAD l1/l2/file-2-2 master \u271a \ue0b0 git status On branch master Changes to be committed: (use \"git reset HEAD <file>...\" to unstage) new file: l1/file1-2 new file: l1/l2/l3/file-3-2 new file: l1/l2/l3/l4/file-4-2 Untracked files: (use \"git add <file>...\" to include in what will be committed) l1/l2/file-2-2 Now, the file l1/l2/file2-2 has moved back into the working directory state. If we want to revert ( discard ) the changes to the file as it was last commited: master \u271a \ue0b0 echo \"changes\" >> l1/l2/l3/file-3-2 master \u25cf\u271a \ue0b0 git status On branch master Changes to be committed: (use \"git reset HEAD <file>...\" to unstage) new file: l1/file1-2 new file: l1/l2/l3/file-3-2 new file: l1/l2/l3/l4/file-4-2 Changes not staged for commit: (use \"git add <file>...\" to update what will be committed) (use \"git checkout -- <file>...\" to discard changes in working directory) modified: l1/l2/l3/file-3-2 Untracked files: (use \"git add <file>...\" to include in what will be committed) l1/l2/file-2-2 master \u25cf\u271a \ue0b0 git checkout l1/l2/l3/file-3-2 master \u271a \ue0b0 git status On branch master Changes to be committed: (use \"git reset HEAD <file>...\" to unstage) new file: l1/file1-2 new file: l1/l2/l3/file-3-2 new file: l1/l2/l3/l4/file-4-2 Untracked files: (use \"git add <file>...\" to include in what will be committed) l1/l2/file-2-2","title":"09 discarding"},{"location":"Git/09-discarding/#backing-out-changes","text":"If we need to back-out the changes from the staging area (unstage), we can use a following command: master \ue0b0 touch l1/file1-2 master \ue0b0 touch l1/l2/file-2-2 master \ue0b0 touch l1/l2/l3/file-3-2 master \ue0b0 touch l1/l2/l3/l4/file-4-2 master \ue0b0 git add . master \u271a \ue0b0 git status On branch master Changes to be committed: (use \"git reset HEAD <file>...\" to unstage) new file: l1/file1-2 new file: l1/l2/file-2-2 new file: l1/l2/l3/file-3-2 new file: l1/l2/l3/l4/file-4-2 master \u271a \ue0b0 git reset HEAD l1/l2/file-2-2 master \u271a \ue0b0 git status On branch master Changes to be committed: (use \"git reset HEAD <file>...\" to unstage) new file: l1/file1-2 new file: l1/l2/l3/file-3-2 new file: l1/l2/l3/l4/file-4-2 Untracked files: (use \"git add <file>...\" to include in what will be committed) l1/l2/file-2-2 Now, the file l1/l2/file2-2 has moved back into the working directory state. If we want to revert ( discard ) the changes to the file as it was last commited: master \u271a \ue0b0 echo \"changes\" >> l1/l2/l3/file-3-2 master \u25cf\u271a \ue0b0 git status On branch master Changes to be committed: (use \"git reset HEAD <file>...\" to unstage) new file: l1/file1-2 new file: l1/l2/l3/file-3-2 new file: l1/l2/l3/l4/file-4-2 Changes not staged for commit: (use \"git add <file>...\" to update what will be committed) (use \"git checkout -- <file>...\" to discard changes in working directory) modified: l1/l2/l3/file-3-2 Untracked files: (use \"git add <file>...\" to include in what will be committed) l1/l2/file-2-2 master \u25cf\u271a \ue0b0 git checkout l1/l2/l3/file-3-2 master \u271a \ue0b0 git status On branch master Changes to be committed: (use \"git reset HEAD <file>...\" to unstage) new file: l1/file1-2 new file: l1/l2/l3/file-3-2 new file: l1/l2/l3/l4/file-4-2 Untracked files: (use \"git add <file>...\" to include in what will be committed) l1/l2/file-2-2","title":"Backing out changes"},{"location":"Git/10-renaming-and-moving-files/","text":"Renaming and moving files \u00b6 We can move and rename files by using the following command: master \ue0b0 git status On branch master nothing to commit, working tree clean master \ue0b0 git mv l1/l2/l3/file-3-2 l3-file master \u271a \ue0b0 git status On branch master Changes to be committed: (use \"git reset HEAD <file>...\" to unstage) renamed: l1/l2/l3/file-3-2 -> l3-file master \u271a \ue0b0 git commit -m \"moved file\" [master aa6daef] moved file 1 file changed, 0 insertions(+), 0 deletions(-) rename l1/l2/l3/file-3-2 => l3-file (100%) master \ue0b0 ls l1 l3-file When using operating system, it will not track the renamed files: master \ue0b0 mv l1/l2/l3/l4/file-4-2 l1/l2/l3/l4/file-4.2 master \u25cf \ue0b0 git status On branch master Changes not staged for commit: (use \"git add/rm <file>...\" to update what will be committed) (use \"git checkout -- <file>...\" to discard changes in working directory) deleted: l1/l2/l3/l4/file-4-2 Untracked files: (use \"git add <file>...\" to include in what will be committed) l1/l2/l3/l4/file-4.2 no changes added to commit (use \"git add\" and/or \"git commit -a\") We can use a command to see the rename: master \u25cf\u271a \ue0b0 git add -u master \u271a \ue0b0 git status On branch master Changes to be committed: (use \"git reset HEAD <file>...\" to unstage) renamed: l1/l2/l3/l4/file-4-2 -> l1/l2/l3/l4/file-4.2","title":"10 renaming and moving files"},{"location":"Git/10-renaming-and-moving-files/#renaming-and-moving-files","text":"We can move and rename files by using the following command: master \ue0b0 git status On branch master nothing to commit, working tree clean master \ue0b0 git mv l1/l2/l3/file-3-2 l3-file master \u271a \ue0b0 git status On branch master Changes to be committed: (use \"git reset HEAD <file>...\" to unstage) renamed: l1/l2/l3/file-3-2 -> l3-file master \u271a \ue0b0 git commit -m \"moved file\" [master aa6daef] moved file 1 file changed, 0 insertions(+), 0 deletions(-) rename l1/l2/l3/file-3-2 => l3-file (100%) master \ue0b0 ls l1 l3-file When using operating system, it will not track the renamed files: master \ue0b0 mv l1/l2/l3/l4/file-4-2 l1/l2/l3/l4/file-4.2 master \u25cf \ue0b0 git status On branch master Changes not staged for commit: (use \"git add/rm <file>...\" to update what will be committed) (use \"git checkout -- <file>...\" to discard changes in working directory) deleted: l1/l2/l3/l4/file-4-2 Untracked files: (use \"git add <file>...\" to include in what will be committed) l1/l2/l3/l4/file-4.2 no changes added to commit (use \"git add\" and/or \"git commit -a\") We can use a command to see the rename: master \u25cf\u271a \ue0b0 git add -u master \u271a \ue0b0 git status On branch master Changes to be committed: (use \"git reset HEAD <file>...\" to unstage) renamed: l1/l2/l3/l4/file-4-2 -> l1/l2/l3/l4/file-4.2","title":"Renaming and moving files"},{"location":"Git/11-deleting-files/","text":"Deleting files \u00b6 If we haven't tracked a file, we will need to use operating systems rm command: master \ue0b0 git status On branch master nothing to commit, working tree clean master \ue0b0 touch another-file master \ue0b0 git status On branch master Untracked files: (use \"git add <file>...\" to include in what will be committed) another-file nothing added to commit but untracked files present (use \"git add\" to track) master \ue0b0 rm another-file master \ue0b0 git status On branch master nothing to commit, working tree clean If the file is tracked, we can use the git rm command: master \ue0b0 git status On branch master nothing to commit, working tree clean master \ue0b0 git ls-files l1/file1 l1/file1-2 l1/l2/file-2-2 l1/l2/file2 l1/l2/l3/file3 l1/l2/l3/l4/file-4-2 l1/l2/l3/l4/file4 l3-file master \ue0b0 git rm l3-file rm 'l3-file' master \u271a \ue0b0 git status On branch master Changes to be committed: (use \"git reset HEAD <file>...\" to unstage) deleted: l3-file master \u271a \ue0b0 ls l1 We can revert the deleted file: master \u25cf \ue0b0 git checkout l3-file master \ue0b0 git status On branch master nothing to commit, working tree clean master \ue0b0 ls l1 l3-file To stage the deleted files: master \ue0b0 rm -r l1/l2 master \u25cf \ue0b0 git status On branch master Changes not staged for commit: (use \"git add/rm <file>...\" to update what will be committed) (use \"git checkout -- <file>...\" to discard changes in working directory) deleted: l1/l2/file-2-2 deleted: l1/l2/file2 deleted: l1/l2/l3/file3 deleted: l1/l2/l3/l4/file-4-2 deleted: l1/l2/l3/l4/file4 no changes added to commit (use \"git add\" and/or \"git commit -a\") master \u25cf \ue0b0 git add -A master \u271a \ue0b0 git status On branch master Changes to be committed: (use \"git reset HEAD <file>...\" to unstage) deleted: l1/l2/file-2-2 deleted: l1/l2/file2 deleted: l1/l2/l3/file3 deleted: l1/l2/l3/l4/file-4-2 deleted: l1/l2/l3/l4/file4 Revert it: master \u271a \ue0b0 git reset HEAD . Unstaged changes after reset: D l1/l2/file-2-2 D l1/l2/file2 D l1/l2/l3/file3 D l1/l2/l3/l4/file-4-2 D l1/l2/l3/l4/file4 master \u25cf \ue0b0 git status On branch master Changes not staged for commit: (use \"git add/rm <file>...\" to update what will be committed) (use \"git checkout -- <file>...\" to discard changes in working directory) deleted: l1/l2/file-2-2 deleted: l1/l2/file2 deleted: l1/l2/l3/file3 deleted: l1/l2/l3/l4/file-4-2 deleted: l1/l2/l3/l4/file4 no changes added to commit (use \"git add\" and/or \"git commit -a\") master \u25cf \ue0b0 git checkout . master \ue0b0 git status On branch master nothing to commit, working tree clean","title":"11 deleting files"},{"location":"Git/11-deleting-files/#deleting-files","text":"If we haven't tracked a file, we will need to use operating systems rm command: master \ue0b0 git status On branch master nothing to commit, working tree clean master \ue0b0 touch another-file master \ue0b0 git status On branch master Untracked files: (use \"git add <file>...\" to include in what will be committed) another-file nothing added to commit but untracked files present (use \"git add\" to track) master \ue0b0 rm another-file master \ue0b0 git status On branch master nothing to commit, working tree clean If the file is tracked, we can use the git rm command: master \ue0b0 git status On branch master nothing to commit, working tree clean master \ue0b0 git ls-files l1/file1 l1/file1-2 l1/l2/file-2-2 l1/l2/file2 l1/l2/l3/file3 l1/l2/l3/l4/file-4-2 l1/l2/l3/l4/file4 l3-file master \ue0b0 git rm l3-file rm 'l3-file' master \u271a \ue0b0 git status On branch master Changes to be committed: (use \"git reset HEAD <file>...\" to unstage) deleted: l3-file master \u271a \ue0b0 ls l1 We can revert the deleted file: master \u25cf \ue0b0 git checkout l3-file master \ue0b0 git status On branch master nothing to commit, working tree clean master \ue0b0 ls l1 l3-file To stage the deleted files: master \ue0b0 rm -r l1/l2 master \u25cf \ue0b0 git status On branch master Changes not staged for commit: (use \"git add/rm <file>...\" to update what will be committed) (use \"git checkout -- <file>...\" to discard changes in working directory) deleted: l1/l2/file-2-2 deleted: l1/l2/file2 deleted: l1/l2/l3/file3 deleted: l1/l2/l3/l4/file-4-2 deleted: l1/l2/l3/l4/file4 no changes added to commit (use \"git add\" and/or \"git commit -a\") master \u25cf \ue0b0 git add -A master \u271a \ue0b0 git status On branch master Changes to be committed: (use \"git reset HEAD <file>...\" to unstage) deleted: l1/l2/file-2-2 deleted: l1/l2/file2 deleted: l1/l2/l3/file3 deleted: l1/l2/l3/l4/file-4-2 deleted: l1/l2/l3/l4/file4 Revert it: master \u271a \ue0b0 git reset HEAD . Unstaged changes after reset: D l1/l2/file-2-2 D l1/l2/file2 D l1/l2/l3/file3 D l1/l2/l3/l4/file-4-2 D l1/l2/l3/l4/file4 master \u25cf \ue0b0 git status On branch master Changes not staged for commit: (use \"git add/rm <file>...\" to update what will be committed) (use \"git checkout -- <file>...\" to discard changes in working directory) deleted: l1/l2/file-2-2 deleted: l1/l2/file2 deleted: l1/l2/l3/file3 deleted: l1/l2/l3/l4/file-4-2 deleted: l1/l2/l3/l4/file4 no changes added to commit (use \"git add\" and/or \"git commit -a\") master \u25cf \ue0b0 git checkout . master \ue0b0 git status On branch master nothing to commit, working tree clean","title":"Deleting files"},{"location":"Git/12-history/","text":"History \u00b6 In order to see the git commit history, you can use command: git log commit aa6daef8b82dddc6fd98f253b0ead79d9980421d (HEAD -> master) Author: D\u0101vis Kr\u0113gers <example@gmail.com> Date: Mon Jan 21 20:53:09 2019 +0200 moved file commit a6a2066dc8ce8206f995f89c5cc5a36febf230dc Author: D\u0101vis Kr\u0113gers <example@gmail.com> Date: Mon Jan 21 20:51:07 2019 +0200 another commit commit a635aaf7f592db3696d3b98bb386c4590ac8e06f Author: D\u0101vis Kr\u0113gers <example@gmail.com> Date: Mon Jan 21 20:42:29 2019 +0200 initial Note that the last commit is at the top. You can use also git log abbrev-commit commit aa6daef (HEAD -> master) Author: D\u0101vis Kr\u0113gers <example@gmail.com> Date: Mon Jan 21 20:53:09 2019 +0200 moved file commit a6a2066 Author: D\u0101vis Kr\u0113gers <example@gmail.com> Date: Mon Jan 21 20:51:07 2019 +0200 another commit commit a635aaf Author: D\u0101vis Kr\u0113gers <example@gmail.com> Date: Mon Jan 21 20:42:29 2019 +0200 initial Now the commit ID will be shortened. git log --oneline --graph --decorate This will display the log command with entries compressed in one line, there will be an ASCII graph that will graph the branching, decorate will add any labels or tags that annotates the commits. * aa6daef (HEAD -> master) moved file * a6a2066 another commit * a635aaf initial You can specify a range, git log a635aaf...a6a2066 commit a6a2066dc8ce8206f995f89c5cc5a36febf230dc Author: D\u0101vis Kr\u0113gers <example@gmail.com> Date: Mon Jan 21 20:51:07 2019 +0200 another commit You can also date-base search: git log --since=\"3 days ago\" commit aa6daef (HEAD -> master) Author: D\u0101vis Kr\u0113gers <example@gmail.com> Date: Mon Jan 21 20:53:09 2019 +0200 moved file commit a6a2066 Author: D\u0101vis Kr\u0113gers <example@gmail.com> Date: Mon Jan 21 20:51:07 2019 +0200 another commit commit a635aaf Author: D\u0101vis Kr\u0113gers <example@gmail.com> Date: Mon Jan 21 20:42:29 2019 +0200 initial To follow the renames of a file: git log --follow -- l3-file To show information about the commit: git show aa6daef8b82dddc6fd98f253b0ead79d9980421d commit aa6daef8b82dddc6fd98f253b0ead79d9980421d (HEAD -> master) Author: D\u0101vis Kr\u0113gers <example@gmail.com> Date: Mon Jan 21 20:53:09 2019 +0200 moved file diff --git a/l1/l2/l3/file-3-2 b/l3-file similarity index 100% rename from l1/l2/l3/file-3-2 rename to l3-file","title":"12 history"},{"location":"Git/12-history/#history","text":"In order to see the git commit history, you can use command: git log commit aa6daef8b82dddc6fd98f253b0ead79d9980421d (HEAD -> master) Author: D\u0101vis Kr\u0113gers <example@gmail.com> Date: Mon Jan 21 20:53:09 2019 +0200 moved file commit a6a2066dc8ce8206f995f89c5cc5a36febf230dc Author: D\u0101vis Kr\u0113gers <example@gmail.com> Date: Mon Jan 21 20:51:07 2019 +0200 another commit commit a635aaf7f592db3696d3b98bb386c4590ac8e06f Author: D\u0101vis Kr\u0113gers <example@gmail.com> Date: Mon Jan 21 20:42:29 2019 +0200 initial Note that the last commit is at the top. You can use also git log abbrev-commit commit aa6daef (HEAD -> master) Author: D\u0101vis Kr\u0113gers <example@gmail.com> Date: Mon Jan 21 20:53:09 2019 +0200 moved file commit a6a2066 Author: D\u0101vis Kr\u0113gers <example@gmail.com> Date: Mon Jan 21 20:51:07 2019 +0200 another commit commit a635aaf Author: D\u0101vis Kr\u0113gers <example@gmail.com> Date: Mon Jan 21 20:42:29 2019 +0200 initial Now the commit ID will be shortened. git log --oneline --graph --decorate This will display the log command with entries compressed in one line, there will be an ASCII graph that will graph the branching, decorate will add any labels or tags that annotates the commits. * aa6daef (HEAD -> master) moved file * a6a2066 another commit * a635aaf initial You can specify a range, git log a635aaf...a6a2066 commit a6a2066dc8ce8206f995f89c5cc5a36febf230dc Author: D\u0101vis Kr\u0113gers <example@gmail.com> Date: Mon Jan 21 20:51:07 2019 +0200 another commit You can also date-base search: git log --since=\"3 days ago\" commit aa6daef (HEAD -> master) Author: D\u0101vis Kr\u0113gers <example@gmail.com> Date: Mon Jan 21 20:53:09 2019 +0200 moved file commit a6a2066 Author: D\u0101vis Kr\u0113gers <example@gmail.com> Date: Mon Jan 21 20:51:07 2019 +0200 another commit commit a635aaf Author: D\u0101vis Kr\u0113gers <example@gmail.com> Date: Mon Jan 21 20:42:29 2019 +0200 initial To follow the renames of a file: git log --follow -- l3-file To show information about the commit: git show aa6daef8b82dddc6fd98f253b0ead79d9980421d commit aa6daef8b82dddc6fd98f253b0ead79d9980421d (HEAD -> master) Author: D\u0101vis Kr\u0113gers <example@gmail.com> Date: Mon Jan 21 20:53:09 2019 +0200 moved file diff --git a/l1/l2/l3/file-3-2 b/l3-file similarity index 100% rename from l1/l2/l3/file-3-2 rename to l3-file","title":"History"},{"location":"Git/13-aliases/","text":"Git aliases \u00b6 We can create our own commands that will compress long commands like git log --all --graph --decorate --oneline master \ue0b0 git hist git: 'hist' is not a git command. See 'git --help'. master \ue0b0 git config --global alias.hist \"log --all --graph --decorate --oneline\" master \ue0b0 git hist Not that commands inside the alias does not contain the git prefix. * aa6daef (HEAD -> master) moved file * a6a2066 another commit * a635aaf initial","title":"Git aliases"},{"location":"Git/13-aliases/#git-aliases","text":"We can create our own commands that will compress long commands like git log --all --graph --decorate --oneline master \ue0b0 git hist git: 'hist' is not a git command. See 'git --help'. master \ue0b0 git config --global alias.hist \"log --all --graph --decorate --oneline\" master \ue0b0 git hist Not that commands inside the alias does not contain the git prefix. * aa6daef (HEAD -> master) moved file * a6a2066 another commit * a635aaf initial","title":"Git aliases"},{"location":"Git/14-ignoring-unwanted-files/","text":"Ignoring unwanted files \u00b6 Usually there are files in the repository like .DS_Store on mac, ar source cache like .pyc in python, that we don't want in our repository. We can tell git to ignore them. master \ue0b0 touch .DS_Store master \ue0b0 touch l1/.DS_Store master \ue0b0 git status On branch master Untracked files: (use \"git add <file>...\" to include in what will be committed) .DS_Store l1/.DS_Store nothing added to commit but untracked files present (use \"git add\" to track) master \ue0b0 echo \".DS_Store\" >> .gitignore master \ue0b0 git status On branch master Untracked files: (use \"git add <file>...\" to include in what will be committed) .gitignore nothing added to commit but untracked files present (use \"git add\" to track) We can also use pattern matching it: master \ue0b0 touch l1/l2/1.ignored master \ue0b0 touch l1/l2/2.ignored master \ue0b0 touch l1/l2/3.ignored master \ue0b0 git status On branch master Untracked files: (use \"git add <file>...\" to include in what will be committed) .gitignore l1/l2/1.ignored l1/l2/2.ignored l1/l2/3.ignored nothing added to commit but untracked files present (use \"git add\" to track) master \ue0b0 echo \"*.ignored\" >> .gitignore master \ue0b0 git status On branch master Untracked files: (use \"git add <file>...\" to include in what will be committed) .gitignore nothing added to commit but untracked files present (use \"git add\" to track) And negate it master \ue0b0 echo '!2.ignored' >> .gitignore master \ue0b0 cat .gitignore .DS_Store *.ignored !2.ignored master \ue0b0 git status On branch master Untracked files: (use \"git add <file>...\" to include in what will be committed) .gitignore l1/l2/2.ignored nothing added to commit but untracked files present (use \"git add\" to track)","title":"Ignoring unwanted files"},{"location":"Git/14-ignoring-unwanted-files/#ignoring-unwanted-files","text":"Usually there are files in the repository like .DS_Store on mac, ar source cache like .pyc in python, that we don't want in our repository. We can tell git to ignore them. master \ue0b0 touch .DS_Store master \ue0b0 touch l1/.DS_Store master \ue0b0 git status On branch master Untracked files: (use \"git add <file>...\" to include in what will be committed) .DS_Store l1/.DS_Store nothing added to commit but untracked files present (use \"git add\" to track) master \ue0b0 echo \".DS_Store\" >> .gitignore master \ue0b0 git status On branch master Untracked files: (use \"git add <file>...\" to include in what will be committed) .gitignore nothing added to commit but untracked files present (use \"git add\" to track) We can also use pattern matching it: master \ue0b0 touch l1/l2/1.ignored master \ue0b0 touch l1/l2/2.ignored master \ue0b0 touch l1/l2/3.ignored master \ue0b0 git status On branch master Untracked files: (use \"git add <file>...\" to include in what will be committed) .gitignore l1/l2/1.ignored l1/l2/2.ignored l1/l2/3.ignored nothing added to commit but untracked files present (use \"git add\" to track) master \ue0b0 echo \"*.ignored\" >> .gitignore master \ue0b0 git status On branch master Untracked files: (use \"git add <file>...\" to include in what will be committed) .gitignore nothing added to commit but untracked files present (use \"git add\" to track) And negate it master \ue0b0 echo '!2.ignored' >> .gitignore master \ue0b0 cat .gitignore .DS_Store *.ignored !2.ignored master \ue0b0 git status On branch master Untracked files: (use \"git add <file>...\" to include in what will be committed) .gitignore l1/l2/2.ignored nothing added to commit but untracked files present (use \"git add\" to track)","title":"Ignoring unwanted files"},{"location":"Git/15-visual-diff-merge/","text":"Visual Diff / Merge Tool \u00b6 In order to make things easier when working with diffs and branch merges, we can use P4Merge tool that helps with comparing and conflict merging. You can install it on arch: yaourt -S p4v On ubuntu-based machine: cd /tmp wget http://www.perforce.com/downloads/perforce/r18.1/bin.linux26x86_64/p4v.tgz tar zxvf p4v.tgz sudo cp -r p4v-* /usr/local/p4v/ sudo ln -s /usr/local/p4v/bin/p4merge /usr/local/bin/p4merge Now we can configure git to use it for comparing and conflict resolution (notice that in ubuntu case it's /usr/local/bin not /usr/bin ): git config --global merge.tool p4merge git config --global mergetool.p4merge.path \"/usr/bin/p4merge\" git config --global mergetool.prompt false git config --global diff.tool p4merge git config --global difftool.p4merge.path \"/usr/bin/p4merge\" git config --global diftool.prompt false Verify changes: git config --list --global","title":"Visual Diff / Merge Tool"},{"location":"Git/15-visual-diff-merge/#visual-diff-merge-tool","text":"In order to make things easier when working with diffs and branch merges, we can use P4Merge tool that helps with comparing and conflict merging. You can install it on arch: yaourt -S p4v On ubuntu-based machine: cd /tmp wget http://www.perforce.com/downloads/perforce/r18.1/bin.linux26x86_64/p4v.tgz tar zxvf p4v.tgz sudo cp -r p4v-* /usr/local/p4v/ sudo ln -s /usr/local/p4v/bin/p4merge /usr/local/bin/p4merge Now we can configure git to use it for comparing and conflict resolution (notice that in ubuntu case it's /usr/local/bin not /usr/bin ): git config --global merge.tool p4merge git config --global mergetool.p4merge.path \"/usr/bin/p4merge\" git config --global mergetool.prompt false git config --global diff.tool p4merge git config --global difftool.p4merge.path \"/usr/bin/p4merge\" git config --global diftool.prompt false Verify changes: git config --list --global","title":"Visual Diff / Merge Tool"},{"location":"Git/16-comparing/","text":"Comparing working directory and staging area \u00b6 master \ue0b0 git status On branch master nothing to commit, working tree clean master \ue0b0 echo \"initial\" >> file master \ue0b0 git add file master \u271a \ue0b0 git status On branch master Changes to be committed: (use \"git reset HEAD <file>...\" to unstage) new file: file master \u271a \ue0b0 echo \"change\" >> file master \u25cf\u271a \ue0b0 git status On branch master Changes to be committed: (use \"git reset HEAD <file>...\" to unstage) new file: file Changes not staged for commit: (use \"git add <file>...\" to update what will be committed) (use \"git checkout -- <file>...\" to discard changes in working directory) modified: file git diff diff --git a/file b/file index e79c5e8..8ea0713 100644 --- a/file +++ b/file @@ -1 +1,2 @@ initial +change git difftool Comparing working directory and Repository (last commit) \u00b6 master \ue0b0 git status On branch master nothing to commit, working tree clean master \ue0b0 echo \"initial\" >> file master \ue0b0 git add file master \u271a \ue0b0 git commit -m \"add file\" [master 84f00a8] add file 1 file changed, 1 insertion(+) create mode 100644 file master \ue0b0 echo \"something\" >> file master \u25cf \ue0b0 git status On branch master Changes not staged for commit: (use \"git add <file>...\" to update what will be committed) (use \"git checkout -- <file>...\" to discard changes in working directory) modified: file no changes added to commit (use \"git add\" and/or \"git commit -a\") git diff HEAD diff --git a/file b/file index e79c5e8..0339ca8 100644 --- a/file +++ b/file @@ -1 +1,2 @@ initial +something git difftool HEAD Compare between staging and repository (last commit) \u00b6 master \u25cf \ue0b0 git status On branch master Changes not staged for commit: (use \"git add <file>...\" to update what will be committed) (use \"git checkout -- <file>...\" to discard changes in working directory) modified: file no changes added to commit (use \"git add\" and/or \"git commit -a\") master \u25cf \ue0b0 git add file master \u271a \ue0b0 git status On branch master Changes to be committed: (use \"git reset HEAD <file>...\" to unstage) modified: file git diff --head HEAD diff --git a/file b/file index e79c5e8..0339ca8 100644 --- a/file +++ b/file @@ -1 +1,2 @@ initial +something git diftool --staged HEAD Limiting comparison to one file \u00b6 master \u25cf\u271a \ue0b0 git status On branch master Changes to be committed: (use \"git reset HEAD <file>...\" to unstage) modified: file new file: something else Changes not staged for commit: (use \"git add/rm <file>...\" to update what will be committed) (use \"git checkout -- <file>...\" to discard changes in working directory) modified: file deleted: something else Untracked files: (use \"git add <file>...\" to include in what will be committed) file2 file3 git diff -- file diff --git a/file b/file index 0339ca8..762bbcd 100644 --- a/file +++ b/file @@ -1,2 +1,3 @@ initial something +asdf git diftool -- file Comparing between commits \u00b6 git log --oneline 84f00a8 (HEAD -> master) add file 4e4ddba remove file e980c5c Add file aa6daef moved file a6a2066 Yello a635aaf initial git diff 4e4ddba HEAD diff --git a/file b/file new file mode 100644 index 0000000..e79c5e8 --- /dev/null +++ b/file @@ -0,0 +1 @@ +initial git diff aa6daef e980c5c diff --git a/file b/file new file mode 100644 index 0000000..8ea0713 --- /dev/null +++ b/file @@ -0,0 +1,2 @@ +initial +change Compare between local and remote master branch \u00b6 git diff master origin/master","title":"Comparing working directory and staging area"},{"location":"Git/16-comparing/#comparing-working-directory-and-staging-area","text":"master \ue0b0 git status On branch master nothing to commit, working tree clean master \ue0b0 echo \"initial\" >> file master \ue0b0 git add file master \u271a \ue0b0 git status On branch master Changes to be committed: (use \"git reset HEAD <file>...\" to unstage) new file: file master \u271a \ue0b0 echo \"change\" >> file master \u25cf\u271a \ue0b0 git status On branch master Changes to be committed: (use \"git reset HEAD <file>...\" to unstage) new file: file Changes not staged for commit: (use \"git add <file>...\" to update what will be committed) (use \"git checkout -- <file>...\" to discard changes in working directory) modified: file git diff diff --git a/file b/file index e79c5e8..8ea0713 100644 --- a/file +++ b/file @@ -1 +1,2 @@ initial +change git difftool","title":"Comparing working directory and staging area"},{"location":"Git/16-comparing/#comparing-working-directory-and-repository-last-commit","text":"master \ue0b0 git status On branch master nothing to commit, working tree clean master \ue0b0 echo \"initial\" >> file master \ue0b0 git add file master \u271a \ue0b0 git commit -m \"add file\" [master 84f00a8] add file 1 file changed, 1 insertion(+) create mode 100644 file master \ue0b0 echo \"something\" >> file master \u25cf \ue0b0 git status On branch master Changes not staged for commit: (use \"git add <file>...\" to update what will be committed) (use \"git checkout -- <file>...\" to discard changes in working directory) modified: file no changes added to commit (use \"git add\" and/or \"git commit -a\") git diff HEAD diff --git a/file b/file index e79c5e8..0339ca8 100644 --- a/file +++ b/file @@ -1 +1,2 @@ initial +something git difftool HEAD","title":"Comparing working directory and Repository (last commit)"},{"location":"Git/16-comparing/#compare-between-staging-and-repository-last-commit","text":"master \u25cf \ue0b0 git status On branch master Changes not staged for commit: (use \"git add <file>...\" to update what will be committed) (use \"git checkout -- <file>...\" to discard changes in working directory) modified: file no changes added to commit (use \"git add\" and/or \"git commit -a\") master \u25cf \ue0b0 git add file master \u271a \ue0b0 git status On branch master Changes to be committed: (use \"git reset HEAD <file>...\" to unstage) modified: file git diff --head HEAD diff --git a/file b/file index e79c5e8..0339ca8 100644 --- a/file +++ b/file @@ -1 +1,2 @@ initial +something git diftool --staged HEAD","title":"Compare between staging and repository (last commit)"},{"location":"Git/16-comparing/#limiting-comparison-to-one-file","text":"master \u25cf\u271a \ue0b0 git status On branch master Changes to be committed: (use \"git reset HEAD <file>...\" to unstage) modified: file new file: something else Changes not staged for commit: (use \"git add/rm <file>...\" to update what will be committed) (use \"git checkout -- <file>...\" to discard changes in working directory) modified: file deleted: something else Untracked files: (use \"git add <file>...\" to include in what will be committed) file2 file3 git diff -- file diff --git a/file b/file index 0339ca8..762bbcd 100644 --- a/file +++ b/file @@ -1,2 +1,3 @@ initial something +asdf git diftool -- file","title":"Limiting comparison to one file"},{"location":"Git/16-comparing/#comparing-between-commits","text":"git log --oneline 84f00a8 (HEAD -> master) add file 4e4ddba remove file e980c5c Add file aa6daef moved file a6a2066 Yello a635aaf initial git diff 4e4ddba HEAD diff --git a/file b/file new file mode 100644 index 0000000..e79c5e8 --- /dev/null +++ b/file @@ -0,0 +1 @@ +initial git diff aa6daef e980c5c diff --git a/file b/file new file mode 100644 index 0000000..8ea0713 --- /dev/null +++ b/file @@ -0,0 +1,2 @@ +initial +change","title":"Comparing between commits"},{"location":"Git/16-comparing/#compare-between-local-and-remote-master-branch","text":"git diff master origin/master","title":"Compare between local and remote master branch"},{"location":"Git/17-comparing-working-and-last-commit/","text":"Comparing working directory and repository (last commit) \u00b6 master \ue0b0 git status On branch master nothing to commit, working tree clean master \ue0b0 echo \"initial\" >> file master \ue0b0 git add file master \u271a \ue0b0 git commit -m \"add file\" [master 84f00a8] add file 1 file changed, 1 insertion(+) create mode 100644 file master \ue0b0 echo \"something\" >> file ``` git diff HEAD","title":"Comparing working directory and repository (last commit)"},{"location":"Git/17-comparing-working-and-last-commit/#comparing-working-directory-and-repository-last-commit","text":"master \ue0b0 git status On branch master nothing to commit, working tree clean master \ue0b0 echo \"initial\" >> file master \ue0b0 git add file master \u271a \ue0b0 git commit -m \"add file\" [master 84f00a8] add file 1 file changed, 1 insertion(+) create mode 100644 file master \ue0b0 echo \"something\" >> file ``` git diff HEAD","title":"Comparing working directory and repository (last commit)"},{"location":"Git/18-branches/","text":"Branching \u00b6 So far all the changes has been made on the master branch, which is not the best practice. We should make our changes into feature branches, then merge them into the master branch, once the features has been stabilized. To get a list of all local and remote branches: git branch -a To create a new branch: git branch newbranch git branch -a * master newbranch Switch branches git checkout newbranch git branch -a master * newbranch git log --oneline --decorate 84f00a8 (HEAD -> newbranch, master) add file 4e4ddba remove file e980c5c Add file aa6daef moved file a6a2066 Yello a635aaf initial rename branch git branch -m newbranch oldbranch delete branch git branch -d oldbranch","title":"Branching"},{"location":"Git/18-branches/#branching","text":"So far all the changes has been made on the master branch, which is not the best practice. We should make our changes into feature branches, then merge them into the master branch, once the features has been stabilized. To get a list of all local and remote branches: git branch -a To create a new branch: git branch newbranch git branch -a * master newbranch Switch branches git checkout newbranch git branch -a master * newbranch git log --oneline --decorate 84f00a8 (HEAD -> newbranch, master) add file 4e4ddba remove file e980c5c Add file aa6daef moved file a6a2066 Yello a635aaf initial rename branch git branch -m newbranch oldbranch delete branch git branch -d oldbranch","title":"Branching"},{"location":"Git/19-fast-forward-merges/","text":"Fast Forward merges \u00b6 master \ue0b0 git status On branch master nothing to commit, working tree clean master \ue0b0 git checkout -b some-change Switched to a new branch 'some-change' some-change \ue0b0 git status On branch some-change nothing to commit, working tree clean some-change \ue0b0 echo \"change\" >> file some-change \u25cf \ue0b0 git status On branch some-change Changes not staged for commit: (use \"git add <file>...\" to update what will be committed) (use \"git checkout -- <file>...\" to discard changes in working directory) modified: file no changes added to commit (use \"git add\" and/or \"git commit -a\") some-change \u25cf \ue0b0 git commit -am \"Change file\" [some-change 5040750] Change file 1 file changed, 1 insertion(+) some-change \ue0b0 git log --oneline 5040750 (HEAD -> some-change) Change file 84f00a8 (master) add file 4e4ddba remove file e980c5c Add file aa6daef moved file a6a2066 Yello a635aaf initial some-change \ue0b0 git checkout master Switched to branch 'master' master \ue0b0 git diff master some-change diff --git a/file b/file index e79c5e8..8ea0713 100644 --- a/file +++ b/file @@ -1 +1,2 @@ initial +change master \ue0b0 git merge some-change master Updating 84f00a8..5040750 Fast-forward file | 1 + 1 file changed, 1 insertion(+) master \ue0b0 git status On branch master nothing to commit, working tree clean git log --oneline --graph --decorate * 5040750 (HEAD -> master, some-change) Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial master \ue0b0 git branch -d some-change Deleted branch some-change (was 5040750).","title":"Fast Forward merges"},{"location":"Git/19-fast-forward-merges/#fast-forward-merges","text":"master \ue0b0 git status On branch master nothing to commit, working tree clean master \ue0b0 git checkout -b some-change Switched to a new branch 'some-change' some-change \ue0b0 git status On branch some-change nothing to commit, working tree clean some-change \ue0b0 echo \"change\" >> file some-change \u25cf \ue0b0 git status On branch some-change Changes not staged for commit: (use \"git add <file>...\" to update what will be committed) (use \"git checkout -- <file>...\" to discard changes in working directory) modified: file no changes added to commit (use \"git add\" and/or \"git commit -a\") some-change \u25cf \ue0b0 git commit -am \"Change file\" [some-change 5040750] Change file 1 file changed, 1 insertion(+) some-change \ue0b0 git log --oneline 5040750 (HEAD -> some-change) Change file 84f00a8 (master) add file 4e4ddba remove file e980c5c Add file aa6daef moved file a6a2066 Yello a635aaf initial some-change \ue0b0 git checkout master Switched to branch 'master' master \ue0b0 git diff master some-change diff --git a/file b/file index e79c5e8..8ea0713 100644 --- a/file +++ b/file @@ -1 +1,2 @@ initial +change master \ue0b0 git merge some-change master Updating 84f00a8..5040750 Fast-forward file | 1 + 1 file changed, 1 insertion(+) master \ue0b0 git status On branch master nothing to commit, working tree clean git log --oneline --graph --decorate * 5040750 (HEAD -> master, some-change) Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial master \ue0b0 git branch -d some-change Deleted branch some-change (was 5040750).","title":"Fast Forward merges"},{"location":"Git/20-disable-fast-forward-merges/","text":"Disable fast forward merges \u00b6 master \ue0b0 git status On branch master nothing to commit, working tree clean master \ue0b0 git checkout -b other-change Switched to a new branch 'other-change' other-change \ue0b0 echo \"other change\" >> file other-change \u25cf \ue0b0 git status On branch other-change Changes not staged for commit: (use \"git add <file>...\" to update what will be committed) (use \"git checkout -- <file>...\" to discard changes in working directory) modified: file no changes added to commit (use \"git add\" and/or \"git commit -a\") other-change \u25cf \ue0b0 git commit -am \"Other change\" [other-change a4d0bc9] Other change 1 file changed, 1 insertion(+) other-change \ue0b0 git status On branch other-change nothing to commit, working tree clean other-change \ue0b0 git checkout master Switched to branch 'master' master \ue0b0 git merge other-change --no-ff Merge made by the 'recursive' strategy. file | 1 + 1 file changed, 1 insertion(+) master \ue0b0 git status On branch master master \ue0b0 git log --oneline --decorate --graph * 9521a8d (HEAD -> master) Merge branch 'other-change' |\\ | * a4d0bc9 (other-change) Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial master \ue0b0 git branch -d other-change Deleted branch other-change (was a4d0bc9).","title":"Disable fast forward merges"},{"location":"Git/20-disable-fast-forward-merges/#disable-fast-forward-merges","text":"master \ue0b0 git status On branch master nothing to commit, working tree clean master \ue0b0 git checkout -b other-change Switched to a new branch 'other-change' other-change \ue0b0 echo \"other change\" >> file other-change \u25cf \ue0b0 git status On branch other-change Changes not staged for commit: (use \"git add <file>...\" to update what will be committed) (use \"git checkout -- <file>...\" to discard changes in working directory) modified: file no changes added to commit (use \"git add\" and/or \"git commit -a\") other-change \u25cf \ue0b0 git commit -am \"Other change\" [other-change a4d0bc9] Other change 1 file changed, 1 insertion(+) other-change \ue0b0 git status On branch other-change nothing to commit, working tree clean other-change \ue0b0 git checkout master Switched to branch 'master' master \ue0b0 git merge other-change --no-ff Merge made by the 'recursive' strategy. file | 1 + 1 file changed, 1 insertion(+) master \ue0b0 git status On branch master master \ue0b0 git log --oneline --decorate --graph * 9521a8d (HEAD -> master) Merge branch 'other-change' |\\ | * a4d0bc9 (other-change) Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial master \ue0b0 git branch -d other-change Deleted branch other-change (was a4d0bc9).","title":"Disable fast forward merges"},{"location":"Git/21-automatic-merges/","text":"Automatic merges \u00b6 master \ue0b0 git status On branch master nothing to commit, working tree clean master \ue0b0 git checkout -b simple-change Switched to a new branch 'simple-change' simple-change \ue0b0 echo \"simple change\" >> file \u2718 simple-change \u25cf \ue0b0 git commit -am \"simple change\" [simple-change 83f3159] simple change 1 file changed, 1 insertion(+) simple-change \ue0b0 git status On branch simple-change nothing to commit, working tree clean simple-change \ue0b0 git checkout branch error: pathspec 'branch' did not match any file(s) known to git \u2718 simple-change \ue0b0 git checkout master Switched to branch 'master' master \ue0b0 echo \"file2\" >> file2 \u2718 master \ue0b0 git add file2 master \u271a \ue0b0 git commit -m \"file2\" [master 19ab207] file2 1 file changed, 1 insertion(+) create mode 100644 file2 git log --oneline --decorate --graph --all * 19ab207 (HEAD -> master) file2 | * 83f3159 (simple-change) simple change |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial master \ue0b0 git merge simple-change -m \"Mergin changes from simple-change Branch\" Merge made by the 'recursive' strategy. file | 1 + 1 file changed, 1 insertion(+) * 620ff72 (HEAD -> master) Mergin changes from simple-change Branch |\\ | * 83f3159 (simple-change) simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial master \ue0b0 git branch -d simple-change Deleted branch simple-change (was 83f3159). master \ue0b0 cat file initial change other change simple change","title":"Automatic merges"},{"location":"Git/21-automatic-merges/#automatic-merges","text":"master \ue0b0 git status On branch master nothing to commit, working tree clean master \ue0b0 git checkout -b simple-change Switched to a new branch 'simple-change' simple-change \ue0b0 echo \"simple change\" >> file \u2718 simple-change \u25cf \ue0b0 git commit -am \"simple change\" [simple-change 83f3159] simple change 1 file changed, 1 insertion(+) simple-change \ue0b0 git status On branch simple-change nothing to commit, working tree clean simple-change \ue0b0 git checkout branch error: pathspec 'branch' did not match any file(s) known to git \u2718 simple-change \ue0b0 git checkout master Switched to branch 'master' master \ue0b0 echo \"file2\" >> file2 \u2718 master \ue0b0 git add file2 master \u271a \ue0b0 git commit -m \"file2\" [master 19ab207] file2 1 file changed, 1 insertion(+) create mode 100644 file2 git log --oneline --decorate --graph --all * 19ab207 (HEAD -> master) file2 | * 83f3159 (simple-change) simple change |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial master \ue0b0 git merge simple-change -m \"Mergin changes from simple-change Branch\" Merge made by the 'recursive' strategy. file | 1 + 1 file changed, 1 insertion(+) * 620ff72 (HEAD -> master) Mergin changes from simple-change Branch |\\ | * 83f3159 (simple-change) simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial master \ue0b0 git branch -d simple-change Deleted branch simple-change (was 83f3159). master \ue0b0 cat file initial change other change simple change","title":"Automatic merges"},{"location":"Git/22-conficting-merge-and-resolution/","text":"Conflicting merge and resolution \u00b6 master \ue0b0 git status On branch master nothing to commit, working tree clean master \ue0b0 git checkout -b conflicting-change Switched to a new branch 'conflicting-change' conflicting-change \ue0b0 echo \"conflicting\">> file conflicting-change \u25cf \ue0b0 git commit -am \"Conflicting change\" [conflicting-change 5d495a8] Conflicting change 1 file changed, 1 insertion(+) conflicting-change \ue0b0 git checkout master Switched to branch 'master' master \ue0b0 echo \"yet another change\" >> file master \u25cf \ue0b0 git commit -am \"yet another change\" [master 14ae637] yet another change 1 file changed, 1 insertion(+) master \ue0b0 git status On branch master nothing to commit, working tree clean git log --oneline --decorate --graph --all * 14ae637 (HEAD -> master) yet another change | * 5d495a8 (conflicting-change) Conflicting change |/ * 620ff72 Mergin changes from simple-change Branch |\\ | * 83f3159 simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial git diff master conflicting-change diff --git a/file b/file index 811f265..0f9aba8 100644 --- a/file +++ b/file @@ -2,4 +2,4 @@ initial change other change simple change -yet another change +conflicting master \ue0b0 git merge conflicting-change Auto-merging file CONFLICT (content): Merge conflict in file Automatic merge failed; fix conflicts and then commit the result. Now we are in a merging state, we have to fix the merge manually. \u2718 master \u25cf\u271a >M< \ue0b0 cat file initial change other change simple change <<<<<<< HEAD yet another change ======= conflicting >>>>>>> conflicting-change You can fix them by hand, or use a graphical tool: git mergetool You can decide which side you want to use. I'm going to select the right side, and click \"save\". master \u271a >M< \ue0b0 git mergetool No files need merging master \u271a >M< \ue0b0 git status On branch master All conflicts fixed but you are still merging. (use \"git commit\" to conclude merge) Changes to be committed: modified: file Untracked files: (use \"git add <file>...\" to include in what will be committed) file.orig master \u271a >M< \ue0b0 git commit -am \"merge conflict\" [master fa79281] merge conflict master \ue0b0 git status On branch master Untracked files: (use \"git add <file>...\" to include in what will be committed) file.orig nothing added to commit but untracked files present (use \"git add\" to track) The *.orig file will be automatically created by the git so the changes hasn't been lost, we can add the *.orig to .gitignore to ignore these files. git log --oneline --decorate --graph --all * fa79281 (HEAD -> master) merge conflict |\\ | * 5d495a8 (conflicting-change) Conflicting change * | 14ae637 yet another change |/ * 620ff72 Mergin changes from simple-change Branch |\\ | * 83f3159 simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial","title":"Conflicting merge and resolution"},{"location":"Git/22-conficting-merge-and-resolution/#conflicting-merge-and-resolution","text":"master \ue0b0 git status On branch master nothing to commit, working tree clean master \ue0b0 git checkout -b conflicting-change Switched to a new branch 'conflicting-change' conflicting-change \ue0b0 echo \"conflicting\">> file conflicting-change \u25cf \ue0b0 git commit -am \"Conflicting change\" [conflicting-change 5d495a8] Conflicting change 1 file changed, 1 insertion(+) conflicting-change \ue0b0 git checkout master Switched to branch 'master' master \ue0b0 echo \"yet another change\" >> file master \u25cf \ue0b0 git commit -am \"yet another change\" [master 14ae637] yet another change 1 file changed, 1 insertion(+) master \ue0b0 git status On branch master nothing to commit, working tree clean git log --oneline --decorate --graph --all * 14ae637 (HEAD -> master) yet another change | * 5d495a8 (conflicting-change) Conflicting change |/ * 620ff72 Mergin changes from simple-change Branch |\\ | * 83f3159 simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial git diff master conflicting-change diff --git a/file b/file index 811f265..0f9aba8 100644 --- a/file +++ b/file @@ -2,4 +2,4 @@ initial change other change simple change -yet another change +conflicting master \ue0b0 git merge conflicting-change Auto-merging file CONFLICT (content): Merge conflict in file Automatic merge failed; fix conflicts and then commit the result. Now we are in a merging state, we have to fix the merge manually. \u2718 master \u25cf\u271a >M< \ue0b0 cat file initial change other change simple change <<<<<<< HEAD yet another change ======= conflicting >>>>>>> conflicting-change You can fix them by hand, or use a graphical tool: git mergetool You can decide which side you want to use. I'm going to select the right side, and click \"save\". master \u271a >M< \ue0b0 git mergetool No files need merging master \u271a >M< \ue0b0 git status On branch master All conflicts fixed but you are still merging. (use \"git commit\" to conclude merge) Changes to be committed: modified: file Untracked files: (use \"git add <file>...\" to include in what will be committed) file.orig master \u271a >M< \ue0b0 git commit -am \"merge conflict\" [master fa79281] merge conflict master \ue0b0 git status On branch master Untracked files: (use \"git add <file>...\" to include in what will be committed) file.orig nothing added to commit but untracked files present (use \"git add\" to track) The *.orig file will be automatically created by the git so the changes hasn't been lost, we can add the *.orig to .gitignore to ignore these files. git log --oneline --decorate --graph --all * fa79281 (HEAD -> master) merge conflict |\\ | * 5d495a8 (conflicting-change) Conflicting change * | 14ae637 yet another change |/ * 620ff72 Mergin changes from simple-change Branch |\\ | * 83f3159 simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial","title":"Conflicting merge and resolution"},{"location":"Git/23-simple-rebase/","text":"Simple rebase \u00b6 master \ue0b0 git checkout -b myfeature Switched to a new branch 'myfeature' myfeature \ue0b0 echo \"feature\" >> file2 myfeature \u25cf \ue0b0 git commit -am \"feature\" [myfeature 156f433] feature 1 file changed, 1 insertion(+) myfeature \ue0b0 git checkout master Switched to branch 'master' master \ue0b0 echo \"edit\" >> file \u2718 master \u25cf \ue0b0 git commit -am \"edit\" [master 02064fc] edit 1 file changed, 1 insertion(+) git log --oneline --decorate --all --graph * 02064fc (HEAD -> master) edit | * 156f433 (myfeature) feature |/ * fa79281 merge conflict |\\ | * 5d495a8 (conflicting-change) Conflicting change * | 14ae637 yet another change |/ * 620ff72 Mergin changes from simple-change Branch |\\ | * 83f3159 simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial \u2718 master \ue0b0 git checkout myfeature Switched to branch 'myfeature' myfeature \ue0b0 git rebase master First, rewinding head to replay your work on top of it... Applying: feature The rebase added the master changes into the feature branch, now it appears as the changes in the feature branch were made after the commit on the master branch. git log --oneline --decorate --all --graph * c671488 (HEAD -> myfeature) feature * 02064fc (master) edit * fa79281 merge conflict |\\ | * 5d495a8 (conflicting-change) Conflicting change * | 14ae637 yet another change |/ * 620ff72 Mergin changes from simple-change Branch |\\ | * 83f3159 simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial \u2718 myfeature \ue0b0 echo \"another edit\" >> file myfeature \u25cf \ue0b0 git commit -am \"Another change\" [myfeature 42f5865] Another change 1 file changed, 1 insertion(+) myfeature \ue0b0 git checkout master Switched to branch 'master' master \ue0b0 git merge myfeature Updating 02064fc..42f5865 Fast-forward file | 1 + file2 | 1 + 2 files changed, 2 insertions(+) Since we rebased, we ended up with a fast-forward merge instead of a merge conflict.","title":"Simple rebase"},{"location":"Git/23-simple-rebase/#simple-rebase","text":"master \ue0b0 git checkout -b myfeature Switched to a new branch 'myfeature' myfeature \ue0b0 echo \"feature\" >> file2 myfeature \u25cf \ue0b0 git commit -am \"feature\" [myfeature 156f433] feature 1 file changed, 1 insertion(+) myfeature \ue0b0 git checkout master Switched to branch 'master' master \ue0b0 echo \"edit\" >> file \u2718 master \u25cf \ue0b0 git commit -am \"edit\" [master 02064fc] edit 1 file changed, 1 insertion(+) git log --oneline --decorate --all --graph * 02064fc (HEAD -> master) edit | * 156f433 (myfeature) feature |/ * fa79281 merge conflict |\\ | * 5d495a8 (conflicting-change) Conflicting change * | 14ae637 yet another change |/ * 620ff72 Mergin changes from simple-change Branch |\\ | * 83f3159 simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial \u2718 master \ue0b0 git checkout myfeature Switched to branch 'myfeature' myfeature \ue0b0 git rebase master First, rewinding head to replay your work on top of it... Applying: feature The rebase added the master changes into the feature branch, now it appears as the changes in the feature branch were made after the commit on the master branch. git log --oneline --decorate --all --graph * c671488 (HEAD -> myfeature) feature * 02064fc (master) edit * fa79281 merge conflict |\\ | * 5d495a8 (conflicting-change) Conflicting change * | 14ae637 yet another change |/ * 620ff72 Mergin changes from simple-change Branch |\\ | * 83f3159 simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial \u2718 myfeature \ue0b0 echo \"another edit\" >> file myfeature \u25cf \ue0b0 git commit -am \"Another change\" [myfeature 42f5865] Another change 1 file changed, 1 insertion(+) myfeature \ue0b0 git checkout master Switched to branch 'master' master \ue0b0 git merge myfeature Updating 02064fc..42f5865 Fast-forward file | 1 + file2 | 1 + 2 files changed, 2 insertions(+) Since we rebased, we ended up with a fast-forward merge instead of a merge conflict.","title":"Simple rebase"},{"location":"Git/24-rebasing-conflict/","text":"Rebasing conflict \u00b6 master \ue0b0 git status On branch master nothing to commit, working tree clean master \ue0b0 echo \"change\" >> file master \u25cf \ue0b0 git commit -am \"mb before rebase conflicts\" [master 0c5efff] mb before rebase conflicts 1 file changed, 1 insertion(+) master \ue0b0 git checkout -b bigtrouble Switched to a new branch 'bigtrouble' bigtrouble \ue0b0 echo \"other change\" >> file \u2718 bigtrouble \u25cf \ue0b0 git commit -am \"fb adding trouble to file\" [bigtrouble 8b32aa4] fb adding trouble to file 1 file changed, 1 insertion(+) bigtrouble \ue0b0 git checkout master Switched to branch 'master' master \ue0b0 echo \"something else\" >> file master \u25cf \ue0b0 git commit -am \"mb conflicting changes brewing\" [master 0861e5d] mb conflicting changes brewing 1 file changed, 1 insertion(+) git log --oneline --decorate --all --graph * 0861e5d (HEAD -> master) mb conflicting changes brewing | * 8b32aa4 (bigtrouble) fb adding trouble to file |/ * 0c5efff mb before rebase conflicts * 42f5865 Another change * c671488 feature * 02064fc edit * fa79281 merge conflict |\\ | * 5d495a8 (conflicting-change) Conflicting change * | 14ae637 yet another change |/ * 620ff72 Mergin changes from simple-change Branch |\\ | * 83f3159 simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial bigtrouble \ue0b0 git rebase master First, rewinding head to replay your work on top of it... Applying: fb adding trouble to file Using index info to reconstruct a base tree... M file Falling back to patching base and 3-way merge... Auto-merging file CONFLICT (content): Merge conflict in file error: Failed to merge in the changes. Patch failed at 0001 fb adding trouble to file hint: Use 'git am --show-current-patch' to see the failed patch Resolve all conflicts manually, mark them as resolved with \"git add/rm <conflicted_files>\", then run \"git rebase --continue\". You can instead skip this commit: run \"git rebase --skip\". To abort and get back to the state before \"git rebase\", run \"git rebase --abort\". We can abort the rebase and no changes will be made. davis@davis-arch \ue0b0 ~/projects/learning-git/project \ue0b0 \u27a6 0861e5d \u25cf\u271a >R> \ue0b0 git rebase --abort Or we can resolve the conflict by using the mergetool: git mergetool Select the changes and save. davis@davis-arch \ue0b0 ~/projects/learning-git/project \ue0b0 \u27a6 0861e5d \u271a >R> \ue0b0 git status rebase in progress; onto 0861e5d You are currently rebasing branch 'bigtrouble' on '0861e5d'. (all conflicts fixed: run \"git rebase --continue\") Changes to be committed: (use \"git reset HEAD <file>...\" to unstage) modified: file Untracked files: (use \"git add <file>...\" to include in what will be committed) file.orig davis@davis-arch \ue0b0 ~/projects/learning-git/project \ue0b0 \u27a6 0861e5d \u271a >R> \ue0b0 git rebase --continue Applying: fb adding trouble to file git log --oneline --decorate --all --graph * 9066d92 (HEAD -> bigtrouble) fb adding trouble to file * 0861e5d (master) mb conflicting changes brewing * 0c5efff mb before rebase conflicts * 42f5865 Another change * c671488 feature * 02064fc edit * fa79281 merge conflict |\\ | * 5d495a8 (conflicting-change) Conflicting change * | 14ae637 yet another change |/ * 620ff72 Mergin changes from simple-change Branch |\\ | * 83f3159 simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial Do some additional changes: bigtrouble \ue0b0 echo \"Yet another change\" >> file bigtrouble \u25cf \ue0b0 git commit -am \"change after rebase\" [bigtrouble ddef7ba] change after rebase 1 file changed, 1 insertion(+) bigtrouble \ue0b0 git checkout master Switched to branch 'master' master \ue0b0 git merge bigtrouble Updating 0861e5d..ddef7ba Fast-forward file | 2 ++ 1 file changed, 2 insertions(+) master \ue0b0 git status On branch master Untracked files: (use \"git add <file>...\" to include in what will be committed) file.orig nothing added to commit but untracked files present (use \"git add\" to track) And again, the merge resulted in fast forward merge, no conflict found.","title":"Rebasing conflict"},{"location":"Git/24-rebasing-conflict/#rebasing-conflict","text":"master \ue0b0 git status On branch master nothing to commit, working tree clean master \ue0b0 echo \"change\" >> file master \u25cf \ue0b0 git commit -am \"mb before rebase conflicts\" [master 0c5efff] mb before rebase conflicts 1 file changed, 1 insertion(+) master \ue0b0 git checkout -b bigtrouble Switched to a new branch 'bigtrouble' bigtrouble \ue0b0 echo \"other change\" >> file \u2718 bigtrouble \u25cf \ue0b0 git commit -am \"fb adding trouble to file\" [bigtrouble 8b32aa4] fb adding trouble to file 1 file changed, 1 insertion(+) bigtrouble \ue0b0 git checkout master Switched to branch 'master' master \ue0b0 echo \"something else\" >> file master \u25cf \ue0b0 git commit -am \"mb conflicting changes brewing\" [master 0861e5d] mb conflicting changes brewing 1 file changed, 1 insertion(+) git log --oneline --decorate --all --graph * 0861e5d (HEAD -> master) mb conflicting changes brewing | * 8b32aa4 (bigtrouble) fb adding trouble to file |/ * 0c5efff mb before rebase conflicts * 42f5865 Another change * c671488 feature * 02064fc edit * fa79281 merge conflict |\\ | * 5d495a8 (conflicting-change) Conflicting change * | 14ae637 yet another change |/ * 620ff72 Mergin changes from simple-change Branch |\\ | * 83f3159 simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial bigtrouble \ue0b0 git rebase master First, rewinding head to replay your work on top of it... Applying: fb adding trouble to file Using index info to reconstruct a base tree... M file Falling back to patching base and 3-way merge... Auto-merging file CONFLICT (content): Merge conflict in file error: Failed to merge in the changes. Patch failed at 0001 fb adding trouble to file hint: Use 'git am --show-current-patch' to see the failed patch Resolve all conflicts manually, mark them as resolved with \"git add/rm <conflicted_files>\", then run \"git rebase --continue\". You can instead skip this commit: run \"git rebase --skip\". To abort and get back to the state before \"git rebase\", run \"git rebase --abort\". We can abort the rebase and no changes will be made. davis@davis-arch \ue0b0 ~/projects/learning-git/project \ue0b0 \u27a6 0861e5d \u25cf\u271a >R> \ue0b0 git rebase --abort Or we can resolve the conflict by using the mergetool: git mergetool Select the changes and save. davis@davis-arch \ue0b0 ~/projects/learning-git/project \ue0b0 \u27a6 0861e5d \u271a >R> \ue0b0 git status rebase in progress; onto 0861e5d You are currently rebasing branch 'bigtrouble' on '0861e5d'. (all conflicts fixed: run \"git rebase --continue\") Changes to be committed: (use \"git reset HEAD <file>...\" to unstage) modified: file Untracked files: (use \"git add <file>...\" to include in what will be committed) file.orig davis@davis-arch \ue0b0 ~/projects/learning-git/project \ue0b0 \u27a6 0861e5d \u271a >R> \ue0b0 git rebase --continue Applying: fb adding trouble to file git log --oneline --decorate --all --graph * 9066d92 (HEAD -> bigtrouble) fb adding trouble to file * 0861e5d (master) mb conflicting changes brewing * 0c5efff mb before rebase conflicts * 42f5865 Another change * c671488 feature * 02064fc edit * fa79281 merge conflict |\\ | * 5d495a8 (conflicting-change) Conflicting change * | 14ae637 yet another change |/ * 620ff72 Mergin changes from simple-change Branch |\\ | * 83f3159 simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial Do some additional changes: bigtrouble \ue0b0 echo \"Yet another change\" >> file bigtrouble \u25cf \ue0b0 git commit -am \"change after rebase\" [bigtrouble ddef7ba] change after rebase 1 file changed, 1 insertion(+) bigtrouble \ue0b0 git checkout master Switched to branch 'master' master \ue0b0 git merge bigtrouble Updating 0861e5d..ddef7ba Fast-forward file | 2 ++ 1 file changed, 2 insertions(+) master \ue0b0 git status On branch master Untracked files: (use \"git add <file>...\" to include in what will be committed) file.orig nothing added to commit but untracked files present (use \"git add\" to track) And again, the merge resulted in fast forward merge, no conflict found.","title":"Rebasing conflict"},{"location":"Git/25-pull-with-rebase/","text":"Pull with rebase \u00b6 When there are changes on the remote and we have made changes, we can merge them using fetch: git fetch origin master But, if we want to make so our changes come after the changes in remote, we can use pull with rebase: git pull --rebase origin master","title":"Pull with rebase"},{"location":"Git/25-pull-with-rebase/#pull-with-rebase","text":"When there are changes on the remote and we have made changes, we can merge them using fetch: git fetch origin master But, if we want to make so our changes come after the changes in remote, we can use pull with rebase: git pull --rebase origin master","title":"Pull with rebase"},{"location":"Git/26-simple-stash/","text":"Simple stash \u00b6 A stash is used whenever you have modified something and you need to change something else now. The changes made previously are WIP and aren't done. You can use stash to save them and work on something else. ``` \ue0a0 master \ue0b0 git status On branch master nothing to commit, working tree clean \ue0a0 master \ue0b0 ls file file2 l1 l3-file \ue0a0 master \ue0b0 echo \"Stash this\" >> file \ue0a0 master \u25cf \ue0b0 git status On branch master Changes not staged for commit: (use \"git add ...\" to update what will be committed) (use \"git checkout -- ...\" to discard changes in working directory) modified: file no changes added to commit (use \"git add\" and/or \"git commit -a\") \ue0a0 master \u25cf \ue0b0 git stash Saved working directory and index state WIP on master: ddef7ba change after rebase \ue0a0 master \ue0b0 git status On branch master nothing to commit, working tree clean \ue0a0 master \ue0b0 cat file initial change other change simple change conflicting yet another change edit another edit change other change something else Yet another change \ue0a0 master \ue0b0 echo \"Important change\" >> file \ue0a0 master \u25cf \ue0b0 git status On branch master Changes not staged for commit: (use \"git add ...\" to update what will be committed) (use \"git checkout -- ...\" to discard changes in working directory) modified: file no changes added to commit (use \"git add\" and/or \"git commit -a\") \ue0a0 master \u25cf \ue0b0 git commit -am \"Quick fix\" [master b8e1096] Quick fix 1 file changed, 1 insertion(+) \ue0a0 master \ue0b0 git status On branch master nothing to commit, working tree clean \ue0a0 master \ue0b0 git stash apply Auto-merging file CONFLICT (content): Merge conflict in file \ue0a0 master \u25cf\u271a \ue0b0 git mergetool Merging: file Normal merge conflict for 'file': {local}: modified file {remote}: modified file QString::arg: 1 argument(s) missing in %1 - %2 \ue0a0 master \u271a \ue0b0 git status On branch master Changes to be committed: (use \"git reset HEAD ...\" to unstage) modified: file Untracked files: (use \"git add ...\" to include in what will be committed) file.orig \ue0a0 master \u271a \ue0b0 rm file.orig \ue0a0 master \u271a \ue0b0 git commit -am \"Done with the WIP\" [master c8d946a] Done with the WIP 1 file changed, 1 insertion(+) \ue0a0 master \ue0b0 git status On branch master nothing to commit, working tree clean \ue0a0 master \ue0b0 git stash list stash@{0}: WIP on master: ddef7ba change after rebase \ue0a0 master \ue0b0 git stash drop Dropped refs/stash@{0} (1b28e504f2725e160d03c9174e9b6a26dae4771e) ```","title":"Simple stash"},{"location":"Git/26-simple-stash/#simple-stash","text":"A stash is used whenever you have modified something and you need to change something else now. The changes made previously are WIP and aren't done. You can use stash to save them and work on something else. ``` \ue0a0 master \ue0b0 git status On branch master nothing to commit, working tree clean \ue0a0 master \ue0b0 ls file file2 l1 l3-file \ue0a0 master \ue0b0 echo \"Stash this\" >> file \ue0a0 master \u25cf \ue0b0 git status On branch master Changes not staged for commit: (use \"git add ...\" to update what will be committed) (use \"git checkout -- ...\" to discard changes in working directory) modified: file no changes added to commit (use \"git add\" and/or \"git commit -a\") \ue0a0 master \u25cf \ue0b0 git stash Saved working directory and index state WIP on master: ddef7ba change after rebase \ue0a0 master \ue0b0 git status On branch master nothing to commit, working tree clean \ue0a0 master \ue0b0 cat file initial change other change simple change conflicting yet another change edit another edit change other change something else Yet another change \ue0a0 master \ue0b0 echo \"Important change\" >> file \ue0a0 master \u25cf \ue0b0 git status On branch master Changes not staged for commit: (use \"git add ...\" to update what will be committed) (use \"git checkout -- ...\" to discard changes in working directory) modified: file no changes added to commit (use \"git add\" and/or \"git commit -a\") \ue0a0 master \u25cf \ue0b0 git commit -am \"Quick fix\" [master b8e1096] Quick fix 1 file changed, 1 insertion(+) \ue0a0 master \ue0b0 git status On branch master nothing to commit, working tree clean \ue0a0 master \ue0b0 git stash apply Auto-merging file CONFLICT (content): Merge conflict in file \ue0a0 master \u25cf\u271a \ue0b0 git mergetool Merging: file Normal merge conflict for 'file': {local}: modified file {remote}: modified file QString::arg: 1 argument(s) missing in %1 - %2 \ue0a0 master \u271a \ue0b0 git status On branch master Changes to be committed: (use \"git reset HEAD ...\" to unstage) modified: file Untracked files: (use \"git add ...\" to include in what will be committed) file.orig \ue0a0 master \u271a \ue0b0 rm file.orig \ue0a0 master \u271a \ue0b0 git commit -am \"Done with the WIP\" [master c8d946a] Done with the WIP 1 file changed, 1 insertion(+) \ue0a0 master \ue0b0 git status On branch master nothing to commit, working tree clean \ue0a0 master \ue0b0 git stash list stash@{0}: WIP on master: ddef7ba change after rebase \ue0a0 master \ue0b0 git stash drop Dropped refs/stash@{0} (1b28e504f2725e160d03c9174e9b6a26dae4771e) ```","title":"Simple stash"},{"location":"Git/27-stash-untracked-files-pop/","text":"Stashing untracked files and using pop \u00b6 Normally stashing works with tracked files: ```\ue0a0 master \ue0b0 git ls-files file file2 l1/file1 l1/file1-2 l1/l2/file-2-2 l1/l2/file2 l1/l2/l3/file3 l1/l2/l3/l4/file-4-2 l1/l2/l3/l4/file4 l3-file \ue0a0 master \ue0b0 echo \"Some change\" >> file2 \ue0a0 master \u25cf \ue0b0 git status On branch master Changes not staged for commit: (use \"git add ...\" to update what will be committed) (use \"git checkout -- ...\" to discard changes in working directory) modified: file2 no changes added to commit (use \"git add\" and/or \"git commit -a\") \ue0a0 master \u25cf \ue0b0 echo \"new file\" >> file3 \ue0a0 master \u25cf \ue0b0 git status On branch master Changes not staged for commit: (use \"git add ...\" to update what will be committed) (use \"git checkout -- ...\" to discard changes in working directory) modified: file2 Untracked files: (use \"git add ...\" to include in what will be committed) file3 no changes added to commit (use \"git add\" and/or \"git commit -a\") \ue0a0 master \u25cf \ue0b0 git stash Saved working directory and index state WIP on master: c8d946a Done with the WIP \ue0a0 master \ue0b0 git status On branch master Untracked files: (use \"git add ...\" to include in what will be committed) file3 nothing added to commit but untracked files present (use \"git add\" to track) \ue0a0 master \ue0b0 git stash apply On branch master Changes not staged for commit: (use \"git add ...\" to update what will be committed) (use \"git checkout -- ...\" to discard changes in working directory) modified: file2 Untracked files: (use \"git add ...\" to include in what will be committed) file3 no changes added to commit (use \"git add\" and/or \"git commit -a\") \ue0a0 master \u25cf \ue0b0 git stash drop Dropped refs/stash@{0} (fa963e048c53b322f8715e497aea64030dea9261) We can do a workaround for this: \ue0a0 master \u25cf \ue0b0 git stash -u Saved working directory and index state WIP on master: c8d946a Done with the WIP \ue0a0 master \ue0b0 git status On branch master nothing to commit, working tree clean \ue0a0 master \ue0b0 echo \"Yet another change\" >> file2 \ue0a0 master \u25cf \ue0b0 git commit -am \"Emergency fix\" [master b419d7e] Emergency fix 1 file changed, 1 insertion(+) \ue0a0 master \ue0b0 git status On branch master nothing to commit, working tree clean \ue0a0 master \ue0b0 git stash pop Auto-merging file2 CONFLICT (content): Merge conflict in file2 \ue0a0 master \u25cf\u271a \ue0b0 git mergetool Merging: file2 Normal merge conflict for 'file2': {local}: modified file {remote}: modified file QString::arg: 1 argument(s) missing in %1 - %2 \ue0a0 master \u271a \ue0b0 cat file2 file2 feature Some change Yet another change \ue0a0 master \u271a \ue0b0 git status On branch master Changes to be committed: (use \"git reset HEAD ...\" to unstage) modified: file2 Untracked files: (use \"git add ...\" to include in what will be committed) file2.orig file3 ```","title":"Stashing untracked files and using pop"},{"location":"Git/27-stash-untracked-files-pop/#stashing-untracked-files-and-using-pop","text":"Normally stashing works with tracked files: ```\ue0a0 master \ue0b0 git ls-files file file2 l1/file1 l1/file1-2 l1/l2/file-2-2 l1/l2/file2 l1/l2/l3/file3 l1/l2/l3/l4/file-4-2 l1/l2/l3/l4/file4 l3-file \ue0a0 master \ue0b0 echo \"Some change\" >> file2 \ue0a0 master \u25cf \ue0b0 git status On branch master Changes not staged for commit: (use \"git add ...\" to update what will be committed) (use \"git checkout -- ...\" to discard changes in working directory) modified: file2 no changes added to commit (use \"git add\" and/or \"git commit -a\") \ue0a0 master \u25cf \ue0b0 echo \"new file\" >> file3 \ue0a0 master \u25cf \ue0b0 git status On branch master Changes not staged for commit: (use \"git add ...\" to update what will be committed) (use \"git checkout -- ...\" to discard changes in working directory) modified: file2 Untracked files: (use \"git add ...\" to include in what will be committed) file3 no changes added to commit (use \"git add\" and/or \"git commit -a\") \ue0a0 master \u25cf \ue0b0 git stash Saved working directory and index state WIP on master: c8d946a Done with the WIP \ue0a0 master \ue0b0 git status On branch master Untracked files: (use \"git add ...\" to include in what will be committed) file3 nothing added to commit but untracked files present (use \"git add\" to track) \ue0a0 master \ue0b0 git stash apply On branch master Changes not staged for commit: (use \"git add ...\" to update what will be committed) (use \"git checkout -- ...\" to discard changes in working directory) modified: file2 Untracked files: (use \"git add ...\" to include in what will be committed) file3 no changes added to commit (use \"git add\" and/or \"git commit -a\") \ue0a0 master \u25cf \ue0b0 git stash drop Dropped refs/stash@{0} (fa963e048c53b322f8715e497aea64030dea9261) We can do a workaround for this: \ue0a0 master \u25cf \ue0b0 git stash -u Saved working directory and index state WIP on master: c8d946a Done with the WIP \ue0a0 master \ue0b0 git status On branch master nothing to commit, working tree clean \ue0a0 master \ue0b0 echo \"Yet another change\" >> file2 \ue0a0 master \u25cf \ue0b0 git commit -am \"Emergency fix\" [master b419d7e] Emergency fix 1 file changed, 1 insertion(+) \ue0a0 master \ue0b0 git status On branch master nothing to commit, working tree clean \ue0a0 master \ue0b0 git stash pop Auto-merging file2 CONFLICT (content): Merge conflict in file2 \ue0a0 master \u25cf\u271a \ue0b0 git mergetool Merging: file2 Normal merge conflict for 'file2': {local}: modified file {remote}: modified file QString::arg: 1 argument(s) missing in %1 - %2 \ue0a0 master \u271a \ue0b0 cat file2 file2 feature Some change Yet another change \ue0a0 master \u271a \ue0b0 git status On branch master Changes to be committed: (use \"git reset HEAD ...\" to unstage) modified: file2 Untracked files: (use \"git add ...\" to include in what will be committed) file2.orig file3 ```","title":"Stashing untracked files and using pop"},{"location":"Git/28-managing-multiple-stashes/","text":"Managing multiple stashes \u00b6 \ue0a0 master \ue0b0 git status On branch master nothing to commit, working tree clean \ue0a0 master \ue0b0 ls file file2 file3 l1 l3-file \ue0a0 master \ue0b0 echo \"Change\" >> file \ue0a0 master \u25cf \ue0b0 git stash save \"simple change\" Saved working directory and index state On master: simple change \ue0a0 master \ue0b0 echo \"Other change\" >> file2 \ue0a0 master \u25cf \ue0b0 git stash save \"other change\" Saved working directory and index state On master: other change \ue0a0 master \ue0b0 echo \"Yet another change\" >> file3 \ue0a0 master \u25cf \ue0b0 git stash save \"yet another change\" Saved working directory and index state On master: yet another change git stash list stash@{0}: On master: yet another change stash@{1}: On master: other change stash@{2}: On master: simple change stash@{3}: WIP on master: c8d946a Done with the WIP Note that the list is reversed, index 0 is the last stash. git stash show stash@{0} file3 | 1 + 1 file changed, 1 insertion(+) git stash show stash@{1} file2 | 1 + 1 file changed, 1 insertion(+) Reapply stashes \ue0a0 master \ue0b0 git status On branch master nothing to commit, working tree clean \ue0a0 master \ue0b0 git stash apply stash@{1} On branch master Changes not staged for commit: (use \"git add <file>...\" to update what will be committed) (use \"git checkout -- <file>...\" to discard changes in working directory) modified: file2 no changes added to commit (use \"git add\" and/or \"git commit -a\") \ue0a0 master \u25cf \ue0b0 git stash list stash@{0}: On master: yet another change stash@{1}: On master: other change stash@{2}: On master: simple change stash@{3}: WIP on master: c8d946a Done with the WIP git stash drop stash@{1} git stash clear","title":"Managing multiple stashes"},{"location":"Git/28-managing-multiple-stashes/#managing-multiple-stashes","text":"\ue0a0 master \ue0b0 git status On branch master nothing to commit, working tree clean \ue0a0 master \ue0b0 ls file file2 file3 l1 l3-file \ue0a0 master \ue0b0 echo \"Change\" >> file \ue0a0 master \u25cf \ue0b0 git stash save \"simple change\" Saved working directory and index state On master: simple change \ue0a0 master \ue0b0 echo \"Other change\" >> file2 \ue0a0 master \u25cf \ue0b0 git stash save \"other change\" Saved working directory and index state On master: other change \ue0a0 master \ue0b0 echo \"Yet another change\" >> file3 \ue0a0 master \u25cf \ue0b0 git stash save \"yet another change\" Saved working directory and index state On master: yet another change git stash list stash@{0}: On master: yet another change stash@{1}: On master: other change stash@{2}: On master: simple change stash@{3}: WIP on master: c8d946a Done with the WIP Note that the list is reversed, index 0 is the last stash. git stash show stash@{0} file3 | 1 + 1 file changed, 1 insertion(+) git stash show stash@{1} file2 | 1 + 1 file changed, 1 insertion(+) Reapply stashes \ue0a0 master \ue0b0 git status On branch master nothing to commit, working tree clean \ue0a0 master \ue0b0 git stash apply stash@{1} On branch master Changes not staged for commit: (use \"git add <file>...\" to update what will be committed) (use \"git checkout -- <file>...\" to discard changes in working directory) modified: file2 no changes added to commit (use \"git add\" and/or \"git commit -a\") \ue0a0 master \u25cf \ue0b0 git stash list stash@{0}: On master: yet another change stash@{1}: On master: other change stash@{2}: On master: simple change stash@{3}: WIP on master: c8d946a Done with the WIP git stash drop stash@{1} git stash clear","title":"Managing multiple stashes"},{"location":"Git/29-stashing-into-a-branch/","text":"Stashing into a branch \u00b6","title":"Stashing into a branch"},{"location":"Git/29-stashing-into-a-branch/#stashing-into-a-branch","text":"","title":"Stashing into a branch"},{"location":"Git/30-simple-tag/","text":"Simple tag \u00b6 davis@davis-arch \ue0b0 ~/projects/learning-git/project \ue0b0 \ue0a0 master \ue0b0 git status On branch master nothing to commit, working tree clean davis@davis-arch \ue0b0 ~/projects/learning-git/project \ue0b0 \ue0a0 master \ue0b0 git log --oneline --decorate --graph * b4ff9f3 (HEAD -> master) Done with the WIP2 * 0ce9769 Done with the WIP2 * b419d7e Emergency fix * c8d946a Done with the WIP * b8e1096 Quick fix * ddef7ba (bigtrouble) change after rebase * 9066d92 fb adding trouble to file * 0861e5d mb conflicting changes brewing * 0c5efff mb before rebase conflicts * 42f5865 Another change * c671488 feature * 02064fc edit * fa79281 merge conflict |\\ | * 5d495a8 (conflicting-change) Conflicting change * | 14ae637 yet another change |/ * 620ff72 Mergin changes from simple-change Branch |\\ | * 83f3159 simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial git tag myTag git log --oneline --decorate --graph * b4ff9f3 (HEAD -> master, tag: myTag) Done with the WIP2 * 0ce9769 Done with the WIP2 * b419d7e Emergency fix * c8d946a Done with the WIP * b8e1096 Quick fix * ddef7ba (bigtrouble) change after rebase * 9066d92 fb adding trouble to file * 0861e5d mb conflicting changes brewing * 0c5efff mb before rebase conflicts * 42f5865 Another change * c671488 feature * 02064fc edit * fa79281 merge conflict |\\ | * 5d495a8 (conflicting-change) Conflicting change * | 14ae637 yet another change |/ * 620ff72 Mergin changes from simple-change Branch |\\ | * 83f3159 simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial git tag --list myTag git show myTag commit b4ff9f375867cd2ebdfee4d7ea7e1bed1485578d (HEAD -> master, tag: myTag) Author: D\u0101vis Kr\u0113gers <kregers.davis@gmail.com> Date: Tue Jan 22 19:05:38 2019 +0200 Done with the WIP2 diff --git a/file3 b/file3 new file mode 100644 index 0000000..fa49b07 --- /dev/null +++ b/file3 @@ -0,0 +1 @@ +new file git tag --delete myTag Deleted tag 'myTag' (was b4ff9f3)","title":"Simple tag"},{"location":"Git/30-simple-tag/#simple-tag","text":"davis@davis-arch \ue0b0 ~/projects/learning-git/project \ue0b0 \ue0a0 master \ue0b0 git status On branch master nothing to commit, working tree clean davis@davis-arch \ue0b0 ~/projects/learning-git/project \ue0b0 \ue0a0 master \ue0b0 git log --oneline --decorate --graph * b4ff9f3 (HEAD -> master) Done with the WIP2 * 0ce9769 Done with the WIP2 * b419d7e Emergency fix * c8d946a Done with the WIP * b8e1096 Quick fix * ddef7ba (bigtrouble) change after rebase * 9066d92 fb adding trouble to file * 0861e5d mb conflicting changes brewing * 0c5efff mb before rebase conflicts * 42f5865 Another change * c671488 feature * 02064fc edit * fa79281 merge conflict |\\ | * 5d495a8 (conflicting-change) Conflicting change * | 14ae637 yet another change |/ * 620ff72 Mergin changes from simple-change Branch |\\ | * 83f3159 simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial git tag myTag git log --oneline --decorate --graph * b4ff9f3 (HEAD -> master, tag: myTag) Done with the WIP2 * 0ce9769 Done with the WIP2 * b419d7e Emergency fix * c8d946a Done with the WIP * b8e1096 Quick fix * ddef7ba (bigtrouble) change after rebase * 9066d92 fb adding trouble to file * 0861e5d mb conflicting changes brewing * 0c5efff mb before rebase conflicts * 42f5865 Another change * c671488 feature * 02064fc edit * fa79281 merge conflict |\\ | * 5d495a8 (conflicting-change) Conflicting change * | 14ae637 yet another change |/ * 620ff72 Mergin changes from simple-change Branch |\\ | * 83f3159 simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial git tag --list myTag git show myTag commit b4ff9f375867cd2ebdfee4d7ea7e1bed1485578d (HEAD -> master, tag: myTag) Author: D\u0101vis Kr\u0113gers <kregers.davis@gmail.com> Date: Tue Jan 22 19:05:38 2019 +0200 Done with the WIP2 diff --git a/file3 b/file3 new file mode 100644 index 0000000..fa49b07 --- /dev/null +++ b/file3 @@ -0,0 +1 @@ +new file git tag --delete myTag Deleted tag 'myTag' (was b4ff9f3)","title":"Simple tag"},{"location":"Git/31-annotated-tags/","text":"Annotated tags \u00b6 An annotated tag is similar to a simple tag, but it has extra annotated information. It usually has an equivalent information to a commit message. \ue0a0 master \ue0b0 git status On branch master nothing to commit, working tree clean \ue0a0 master \ue0b0 git tag -a v-1.0 \ue0a0 master \ue0b0 git tag --list v-1.0 git log --oneline --decorate --graph * b4ff9f3 (HEAD -> master, tag: v-1.0) Done with the WIP2 * 0ce9769 Done with the WIP2 * b419d7e Emergency fix * c8d946a Done with the WIP * b8e1096 Quick fix * ddef7ba (bigtrouble) change after rebase * 9066d92 fb adding trouble to file * 0861e5d mb conflicting changes brewing * 0c5efff mb before rebase conflicts * 42f5865 Another change * c671488 feature * 02064fc edit * fa79281 merge conflict |\\ | * 5d495a8 (conflicting-change) Conflicting change * | 14ae637 yet another change |/ * 620ff72 Mergin changes from simple-change Branch |\\ | * 83f3159 simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial git show v-1.0 tag v-1.0 Tagger: D\u0101vis Kr\u0113gers <kregers.davis@gmail.com> Date: Tue Jan 22 19:28:46 2019 +0200 Release v1.0 ... Here are some release notes... commit b4ff9f375867cd2ebdfee4d7ea7e1bed1485578d (HEAD -> master, tag: v-1.0) Author: D\u0101vis Kr\u0113gers <kregers.davis@gmail.com> Date: Tue Jan 22 19:05:38 2019 +0200 Done with the WIP2 diff --git a/file3 b/file3 new file mode 100644 index 0000000..fa49b07 --- /dev/null +++ b/file3 @@ -0,0 +1 @@ +new file (END)","title":"Annotated tags"},{"location":"Git/31-annotated-tags/#annotated-tags","text":"An annotated tag is similar to a simple tag, but it has extra annotated information. It usually has an equivalent information to a commit message. \ue0a0 master \ue0b0 git status On branch master nothing to commit, working tree clean \ue0a0 master \ue0b0 git tag -a v-1.0 \ue0a0 master \ue0b0 git tag --list v-1.0 git log --oneline --decorate --graph * b4ff9f3 (HEAD -> master, tag: v-1.0) Done with the WIP2 * 0ce9769 Done with the WIP2 * b419d7e Emergency fix * c8d946a Done with the WIP * b8e1096 Quick fix * ddef7ba (bigtrouble) change after rebase * 9066d92 fb adding trouble to file * 0861e5d mb conflicting changes brewing * 0c5efff mb before rebase conflicts * 42f5865 Another change * c671488 feature * 02064fc edit * fa79281 merge conflict |\\ | * 5d495a8 (conflicting-change) Conflicting change * | 14ae637 yet another change |/ * 620ff72 Mergin changes from simple-change Branch |\\ | * 83f3159 simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial git show v-1.0 tag v-1.0 Tagger: D\u0101vis Kr\u0113gers <kregers.davis@gmail.com> Date: Tue Jan 22 19:28:46 2019 +0200 Release v1.0 ... Here are some release notes... commit b4ff9f375867cd2ebdfee4d7ea7e1bed1485578d (HEAD -> master, tag: v-1.0) Author: D\u0101vis Kr\u0113gers <kregers.davis@gmail.com> Date: Tue Jan 22 19:05:38 2019 +0200 Done with the WIP2 diff --git a/file3 b/file3 new file mode 100644 index 0000000..fa49b07 --- /dev/null +++ b/file3 @@ -0,0 +1 @@ +new file (END)","title":"Annotated tags"},{"location":"Git/32-comparing-tags/","text":"Comparing tags \u00b6 \ue0a0 master \ue0b0 git status On branch master nothing to commit, working tree clean \ue0a0 master \ue0b0 echo \"file\" >> file \ue0a0 master \u25cf \ue0b0 git commit -am \"Tweaking a file\" [master 6a09e40] Tweaking a file 1 file changed, 1 insertion(+) git log --oneline --decorate --graph * 6a09e40 (HEAD -> master) Tweaking a file * b4ff9f3 (tag: v-1.0) Done with the WIP2 * 0ce9769 Done with the WIP2 * b419d7e Emergency fix * c8d946a Done with the WIP * b8e1096 Quick fix * ddef7ba (bigtrouble) change after rebase * 9066d92 fb adding trouble to file * 0861e5d mb conflicting changes brewing * 0c5efff mb before rebase conflicts * 42f5865 Another change * c671488 feature * 02064fc edit * fa79281 merge conflict |\\ | * 5d495a8 (conflicting-change) Conflicting change * | 14ae637 yet another change |/ * 620ff72 Mergin changes from simple-change Branch |\\ | * 83f3159 simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial git tag -a v-1.1 \ue0a0 master \ue0b0 echo \"file change\" >> file2 \ue0a0 master \u25cf \ue0b0 git commit -am \"Updating for v-1.2\" [master af6b837] Updating for v-1.2 1 file changed, 1 insertion(+) \ue0a0 master \ue0b0 git tag -a v-1.2 git tag --list v-1.0 v-1.1 v-1.2 git log --oneline --decorate --graph * af6b837 (HEAD -> master, tag: v-1.2) Updating for v-1.2 * 6a09e40 (tag: v-1.1) Tweaking a file * b4ff9f3 (tag: v-1.0) Done with the WIP2 * 0ce9769 Done with the WIP2 * b419d7e Emergency fix * c8d946a Done with the WIP * b8e1096 Quick fix * ddef7ba (bigtrouble) change after rebase * 9066d92 fb adding trouble to file * 0861e5d mb conflicting changes brewing * 0c5efff mb before rebase conflicts * 42f5865 Another change * c671488 feature * 02064fc edit * fa79281 merge conflict |\\ | * 5d495a8 (conflicting-change) Conflicting change * | 14ae637 yet another change |/ * 620ff72 Mergin changes from simple-change Branch |\\ | * 83f3159 simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial git diff v-1.1 v-1.2 diff --git a/file2 b/file2 index 0844032..461fcdc 100644 --- a/file2 +++ b/file2 @@ -2,3 +2,4 @@ file2 feature Some change Yet another change +file change git difftool v-1.0 v-1.2","title":"Comparing tags"},{"location":"Git/32-comparing-tags/#comparing-tags","text":"\ue0a0 master \ue0b0 git status On branch master nothing to commit, working tree clean \ue0a0 master \ue0b0 echo \"file\" >> file \ue0a0 master \u25cf \ue0b0 git commit -am \"Tweaking a file\" [master 6a09e40] Tweaking a file 1 file changed, 1 insertion(+) git log --oneline --decorate --graph * 6a09e40 (HEAD -> master) Tweaking a file * b4ff9f3 (tag: v-1.0) Done with the WIP2 * 0ce9769 Done with the WIP2 * b419d7e Emergency fix * c8d946a Done with the WIP * b8e1096 Quick fix * ddef7ba (bigtrouble) change after rebase * 9066d92 fb adding trouble to file * 0861e5d mb conflicting changes brewing * 0c5efff mb before rebase conflicts * 42f5865 Another change * c671488 feature * 02064fc edit * fa79281 merge conflict |\\ | * 5d495a8 (conflicting-change) Conflicting change * | 14ae637 yet another change |/ * 620ff72 Mergin changes from simple-change Branch |\\ | * 83f3159 simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial git tag -a v-1.1 \ue0a0 master \ue0b0 echo \"file change\" >> file2 \ue0a0 master \u25cf \ue0b0 git commit -am \"Updating for v-1.2\" [master af6b837] Updating for v-1.2 1 file changed, 1 insertion(+) \ue0a0 master \ue0b0 git tag -a v-1.2 git tag --list v-1.0 v-1.1 v-1.2 git log --oneline --decorate --graph * af6b837 (HEAD -> master, tag: v-1.2) Updating for v-1.2 * 6a09e40 (tag: v-1.1) Tweaking a file * b4ff9f3 (tag: v-1.0) Done with the WIP2 * 0ce9769 Done with the WIP2 * b419d7e Emergency fix * c8d946a Done with the WIP * b8e1096 Quick fix * ddef7ba (bigtrouble) change after rebase * 9066d92 fb adding trouble to file * 0861e5d mb conflicting changes brewing * 0c5efff mb before rebase conflicts * 42f5865 Another change * c671488 feature * 02064fc edit * fa79281 merge conflict |\\ | * 5d495a8 (conflicting-change) Conflicting change * | 14ae637 yet another change |/ * 620ff72 Mergin changes from simple-change Branch |\\ | * 83f3159 simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial git diff v-1.1 v-1.2 diff --git a/file2 b/file2 index 0844032..461fcdc 100644 --- a/file2 +++ b/file2 @@ -2,3 +2,4 @@ file2 feature Some change Yet another change +file change git difftool v-1.0 v-1.2","title":"Comparing tags"},{"location":"Git/33-tagging-a-specific-commit/","text":"Tagging a specific commit \u00b6 git log --oneline --decorate --graph * af6b837 (HEAD -> master, tag: v-1.2) Updating for v-1.2 * 6a09e40 (tag: v-1.1) Tweaking a file * b4ff9f3 (tag: v-1.0) Done with the WIP2 * 0ce9769 Done with the WIP2 * b419d7e Emergency fix * c8d946a Done with the WIP * b8e1096 Quick fix * ddef7ba (bigtrouble) change after rebase * 9066d92 fb adding trouble to file * 0861e5d mb conflicting changes brewing * 0c5efff mb before rebase conflicts * 42f5865 Another change * c671488 feature * 02064fc edit * fa79281 merge conflict |\\ | * 5d495a8 (conflicting-change) Conflicting change * | 14ae637 yet another change |/ * 620ff72 Mergin changes from simple-change Branch |\\ | * 83f3159 simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial git tag -a v-0.9-beta c671488 -m \"Beta Release 0.9\" git log --oneline --decorate --graph * af6b837 (HEAD -> master, tag: v-1.2) Updating for v-1.2 * 6a09e40 (tag: v-1.1) Tweaking a file * b4ff9f3 (tag: v-1.0) Done with the WIP2 * 0ce9769 Done with the WIP2 * b419d7e Emergency fix * c8d946a Done with the WIP * b8e1096 Quick fix * ddef7ba (bigtrouble) change after rebase * 9066d92 fb adding trouble to file * 0861e5d mb conflicting changes brewing * 0c5efff mb before rebase conflicts * 42f5865 Another change * c671488 (tag: v-0.9-beta) feature * 02064fc edit * fa79281 merge conflict |\\ | * 5d495a8 (conflicting-change) Conflicting change * | 14ae637 yet another change |/ * 620ff72 Mergin changes from simple-change Branch |\\ | * 83f3159 simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial","title":"Tagging a specific commit"},{"location":"Git/33-tagging-a-specific-commit/#tagging-a-specific-commit","text":"git log --oneline --decorate --graph * af6b837 (HEAD -> master, tag: v-1.2) Updating for v-1.2 * 6a09e40 (tag: v-1.1) Tweaking a file * b4ff9f3 (tag: v-1.0) Done with the WIP2 * 0ce9769 Done with the WIP2 * b419d7e Emergency fix * c8d946a Done with the WIP * b8e1096 Quick fix * ddef7ba (bigtrouble) change after rebase * 9066d92 fb adding trouble to file * 0861e5d mb conflicting changes brewing * 0c5efff mb before rebase conflicts * 42f5865 Another change * c671488 feature * 02064fc edit * fa79281 merge conflict |\\ | * 5d495a8 (conflicting-change) Conflicting change * | 14ae637 yet another change |/ * 620ff72 Mergin changes from simple-change Branch |\\ | * 83f3159 simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial git tag -a v-0.9-beta c671488 -m \"Beta Release 0.9\" git log --oneline --decorate --graph * af6b837 (HEAD -> master, tag: v-1.2) Updating for v-1.2 * 6a09e40 (tag: v-1.1) Tweaking a file * b4ff9f3 (tag: v-1.0) Done with the WIP2 * 0ce9769 Done with the WIP2 * b419d7e Emergency fix * c8d946a Done with the WIP * b8e1096 Quick fix * ddef7ba (bigtrouble) change after rebase * 9066d92 fb adding trouble to file * 0861e5d mb conflicting changes brewing * 0c5efff mb before rebase conflicts * 42f5865 Another change * c671488 (tag: v-0.9-beta) feature * 02064fc edit * fa79281 merge conflict |\\ | * 5d495a8 (conflicting-change) Conflicting change * | 14ae637 yet another change |/ * 620ff72 Mergin changes from simple-change Branch |\\ | * 83f3159 simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial","title":"Tagging a specific commit"},{"location":"Git/34-updating-tags/","text":"Updating tags \u00b6 git log --oneline --decorate --graph * af6b837 (HEAD -> master, tag: v-1.2) Updating for v-1.2 * 6a09e40 (tag: v-1.1) Tweaking a file * b4ff9f3 (tag: v-1.0) Done with the WIP2 * 0ce9769 Done with the WIP2 * b419d7e Emergency fix * c8d946a Done with the WIP * b8e1096 Quick fix * ddef7ba (bigtrouble) change after rebase * 9066d92 fb adding trouble to file * 0861e5d mb conflicting changes brewing * 0c5efff mb before rebase conflicts * 42f5865 Another change * c671488 (tag: v-0.9-beta) feature * 02064fc edit * fa79281 merge conflict |\\ | * 5d495a8 (conflicting-change) Conflicting change * | 14ae637 yet another change |/ * 620ff72 Mergin changes from simple-change Branch |\\ | * 83f3159 simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial git tag -a v-0.9-beta -f 02064fc -m \"Corrected tag to commit 02064fc\" Updated tag 'v-0.9-beta' (was 2c0cc94)","title":"Updating tags"},{"location":"Git/34-updating-tags/#updating-tags","text":"git log --oneline --decorate --graph * af6b837 (HEAD -> master, tag: v-1.2) Updating for v-1.2 * 6a09e40 (tag: v-1.1) Tweaking a file * b4ff9f3 (tag: v-1.0) Done with the WIP2 * 0ce9769 Done with the WIP2 * b419d7e Emergency fix * c8d946a Done with the WIP * b8e1096 Quick fix * ddef7ba (bigtrouble) change after rebase * 9066d92 fb adding trouble to file * 0861e5d mb conflicting changes brewing * 0c5efff mb before rebase conflicts * 42f5865 Another change * c671488 (tag: v-0.9-beta) feature * 02064fc edit * fa79281 merge conflict |\\ | * 5d495a8 (conflicting-change) Conflicting change * | 14ae637 yet another change |/ * 620ff72 Mergin changes from simple-change Branch |\\ | * 83f3159 simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial git tag -a v-0.9-beta -f 02064fc -m \"Corrected tag to commit 02064fc\" Updated tag 'v-0.9-beta' (was 2c0cc94)","title":"Updating tags"},{"location":"Git/35-using-tags-with-github/","text":"Using tags with GithHub \u00b6 When using github, you can use tags to version your code, when done, push it to Github and it will show up on the releases section. git push origin v-0.9-beta To push all tags: git push origin master --tags To delete tag: git push origin :v-0.9-beta","title":"Using tags with GithHub"},{"location":"Git/35-using-tags-with-github/#using-tags-with-githhub","text":"When using github, you can use tags to version your code, when done, push it to Github and it will show up on the releases section. git push origin v-0.9-beta To push all tags: git push origin master --tags To delete tag: git push origin :v-0.9-beta","title":"Using tags with GithHub"},{"location":"Git/36-rebase-and-reflog/","text":"Rebase and Reflog \u00b6 Remove the last commit. git status On branch master nothing to commit, working tree clean git log --oneline --decorate --graph * af6b837 (HEAD -> master, tag: v-1.2) Updating for v-1.2 * 6a09e40 (tag: v-1.1) Tweaking a file * b4ff9f3 (tag: v-1.0) Done with the WIP2 * 0ce9769 Done with the WIP2 * b419d7e Emergency fix * c8d946a Done with the WIP * b8e1096 Quick fix * ddef7ba (bigtrouble) change after rebase * 9066d92 fb adding trouble to file * 0861e5d mb conflicting changes brewing * 0c5efff mb before rebase conflicts * 42f5865 Another change * c671488 feature * 02064fc (tag: v-0.9-beta) edit * fa79281 merge conflict |\\ | * 5d495a8 (conflicting-change) Conflicting change * | 14ae637 yet another change |/ * 620ff72 Mergin changes from simple-change Branch |\\ | * 83f3159 simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial git reset HEAD^1 Unstaged changes after reset: M file2 git status On branch master Changes not staged for commit: (use \"git add <file>...\" to update what will be committed) (use \"git checkout -- <file>...\" to discard changes in working directory) modified: file2 no changes added to commit (use \"git add\" and/or \"git commit -a\") git log --oneline --decorate --graph * 6a09e40 (HEAD -> master, tag: v-1.1) Tweaking a file * b4ff9f3 (tag: v-1.0) Done with the WIP2 * 0ce9769 Done with the WIP2 * b419d7e Emergency fix * c8d946a Done with the WIP * b8e1096 Quick fix * ddef7ba (bigtrouble) change after rebase * 9066d92 fb adding trouble to file * 0861e5d mb conflicting changes brewing * 0c5efff mb before rebase conflicts * 42f5865 Another change * c671488 feature * 02064fc (tag: v-0.9-beta) edit * fa79281 merge conflict |\\ | * 5d495a8 (conflicting-change) Conflicting change * | 14ae637 yet another change |/ * 620ff72 Mergin changes from simple-change Branch |\\ | * 83f3159 simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial Reflog is a log of all the changes made git reflog 6a09e40 (HEAD -> master, tag: v-1.1) HEAD@{0}: reset: moving to HEAD^1 af6b837 (tag: v-1.2) HEAD@{1}: commit: Updating for v-1.2 6a09e40 (HEAD -> master, tag: v-1.1) HEAD@{2}: commit: Tweaking a file b4ff9f3 (tag: v-1.0) HEAD@{3}: reset: moving to HEAD b4ff9f3 (tag: v-1.0) HEAD@{4}: checkout: moving from newchanges to master b4ff9f3 (tag: v-1.0) HEAD@{5}: reset: moving to HEAD b4ff9f3 (tag: v-1.0) HEAD@{6}: checkout: moving from master to newchanges b4ff9f3 (tag: v-1.0) HEAD@{7}: reset: moving to HEAD b4ff9f3 (tag: v-1.0) HEAD@{8}: reset: moving to HEAD b4ff9f3 (tag: v-1.0) HEAD@{9}: reset: moving to HEAD b4ff9f3 (tag: v-1.0) HEAD@{10}: reset: moving to HEAD b4ff9f3 (tag: v-1.0) HEAD@{11}: commit: Done with the WIP2 0ce9769 HEAD@{12}: commit: Done with the WIP2 b419d7e HEAD@{13}: commit: Emergency fix c8d946a HEAD@{14}: reset: moving to HEAD c8d946a HEAD@{15}: reset: moving to HEAD c8d946a HEAD@{16}: commit: Done with the WIP b8e1096 HEAD@{17}: commit: Quick fix ddef7ba (bigtrouble) HEAD@{18}: reset: moving to HEAD ddef7ba (bigtrouble) HEAD@{19}: merge bigtrouble: Fast-forward 0861e5d HEAD@{20}: checkout: moving from bigtrouble to master ddef7ba (bigtrouble) HEAD@{21}: commit: change after rebase 9066d92 HEAD@{22}: rebase finished: returning to refs/heads/bigtrouble 9066d92 HEAD@{23}: rebase: fb adding trouble to file 0861e5d HEAD@{24}: rebase: checkout master 8b32aa4 HEAD@{25}: rebase: updating HEAD 8b32aa4 HEAD@{26}: rebase: updating HEAD 0861e5d HEAD@{27}: rebase: checkout master 8b32aa4 HEAD@{28}: checkout: moving from master to bigtrouble 0861e5d HEAD@{29}: commit: mb conflicting changes brewing 0c5efff HEAD@{30}: checkout: moving from bigtrouble to master 8b32aa4 HEAD@{31}: commit: fb adding trouble to file 0c5efff HEAD@{32}: checkout: moving from master to bigtrouble 0c5efff HEAD@{33}: commit: mb before rebase conflicts 42f5865 HEAD@{34}: merge myfeature: Fast-forward 02064fc (tag: v-0.9-beta) HEAD@{35}: checkout: moving from myfeature to master 42f5865 HEAD@{36}: commit: Another change c671488 HEAD@{37}: rebase finished: returning to refs/heads/myfeature c671488 HEAD@{38}: rebase: feature 02064fc (tag: v-0.9-beta) HEAD@{39}: rebase: checkout master 156f433 HEAD@{40}: checkout: moving from master to myfeature 02064fc (tag: v-0.9-beta) HEAD@{41}: commit: edit fa79281 HEAD@{42}: checkout: moving from myfeature to master 156f433 HEAD@{43}: commit: feature fa79281 HEAD@{44}: checkout: moving from master to myfeature fa79281 HEAD@{45}: commit (merge): merge conflict 14ae637 HEAD@{46}: commit: yet another change 620ff72 HEAD@{47}: checkout: moving from conflicting-change to master 5d495a8 (conflicting-change) HEAD@{48}: commit: Conflicting change 620ff72 HEAD@{49}: checkout: moving from master to conflicting-change 620ff72 HEAD@{50}: merge simple-change: Merge made by the 'recursive' strategy. 19ab207 HEAD@{51}: commit: file2 9521a8d HEAD@{52}: checkout: moving from simple-change to master 83f3159 HEAD@{53}: commit: simple change 9521a8d HEAD@{54}: checkout: moving from master to simple-change 9521a8d HEAD@{55}: merge other-change: Merge made by the 'recursive' strategy. 5040750 HEAD@{56}: checkout: moving from other-change to master a4d0bc9 HEAD@{57}: commit: Other change 5040750 HEAD@{58}: checkout: moving from master to other-change 5040750 HEAD@{59}: checkout: moving from other-change to master 5040750 HEAD@{60}: checkout: moving from master to other-change 5040750 HEAD@{61}: merge some-change: Fast-forward 84f00a8 HEAD@{62}: checkout: moving from some-change to master 5040750 HEAD@{63}: commit: Change file 84f00a8 HEAD@{64}: checkout: moving from master to some-change 84f00a8 HEAD@{65}: checkout: moving from newbranch to master 84f00a8 HEAD@{66}: checkout: moving from master to newbranch 84f00a8 HEAD@{67}: commit: add file 4e4ddba HEAD@{68}: commit: remove file e980c5c HEAD@{69}: commit: Add file aa6daef HEAD@{70}: commit: moved file a6a2066 HEAD@{71}: commit: Yello a635aaf HEAD@{72}: commit (initial): initial Reset to state before removing the last commit. git reset af6b837 git log --oneline --decorate --graph * af6b837 (HEAD -> master, tag: v-1.2) Updating for v-1.2 * 6a09e40 (tag: v-1.1) Tweaking a file * b4ff9f3 (tag: v-1.0) Done with the WIP2 * 0ce9769 Done with the WIP2 * b419d7e Emergency fix * c8d946a Done with the WIP * b8e1096 Quick fix * ddef7ba (bigtrouble) change after rebase * 9066d92 fb adding trouble to file * 0861e5d mb conflicting changes brewing * 0c5efff mb before rebase conflicts * 42f5865 Another change * c671488 feature * 02064fc (tag: v-0.9-beta) edit * fa79281 merge conflict |\\ | * 5d495a8 (conflicting-change) Conflicting change * | 14ae637 yet another change |/ * 620ff72 Mergin changes from simple-change Branch |\\ | * 83f3159 simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial Different resets: Mixed reset Reset staging area and point the head to new location Hard reset Reset local working directory, staging area and point the head to new location Soft reset Point the head to new location","title":"Rebase and Reflog"},{"location":"Git/36-rebase-and-reflog/#rebase-and-reflog","text":"Remove the last commit. git status On branch master nothing to commit, working tree clean git log --oneline --decorate --graph * af6b837 (HEAD -> master, tag: v-1.2) Updating for v-1.2 * 6a09e40 (tag: v-1.1) Tweaking a file * b4ff9f3 (tag: v-1.0) Done with the WIP2 * 0ce9769 Done with the WIP2 * b419d7e Emergency fix * c8d946a Done with the WIP * b8e1096 Quick fix * ddef7ba (bigtrouble) change after rebase * 9066d92 fb adding trouble to file * 0861e5d mb conflicting changes brewing * 0c5efff mb before rebase conflicts * 42f5865 Another change * c671488 feature * 02064fc (tag: v-0.9-beta) edit * fa79281 merge conflict |\\ | * 5d495a8 (conflicting-change) Conflicting change * | 14ae637 yet another change |/ * 620ff72 Mergin changes from simple-change Branch |\\ | * 83f3159 simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial git reset HEAD^1 Unstaged changes after reset: M file2 git status On branch master Changes not staged for commit: (use \"git add <file>...\" to update what will be committed) (use \"git checkout -- <file>...\" to discard changes in working directory) modified: file2 no changes added to commit (use \"git add\" and/or \"git commit -a\") git log --oneline --decorate --graph * 6a09e40 (HEAD -> master, tag: v-1.1) Tweaking a file * b4ff9f3 (tag: v-1.0) Done with the WIP2 * 0ce9769 Done with the WIP2 * b419d7e Emergency fix * c8d946a Done with the WIP * b8e1096 Quick fix * ddef7ba (bigtrouble) change after rebase * 9066d92 fb adding trouble to file * 0861e5d mb conflicting changes brewing * 0c5efff mb before rebase conflicts * 42f5865 Another change * c671488 feature * 02064fc (tag: v-0.9-beta) edit * fa79281 merge conflict |\\ | * 5d495a8 (conflicting-change) Conflicting change * | 14ae637 yet another change |/ * 620ff72 Mergin changes from simple-change Branch |\\ | * 83f3159 simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial Reflog is a log of all the changes made git reflog 6a09e40 (HEAD -> master, tag: v-1.1) HEAD@{0}: reset: moving to HEAD^1 af6b837 (tag: v-1.2) HEAD@{1}: commit: Updating for v-1.2 6a09e40 (HEAD -> master, tag: v-1.1) HEAD@{2}: commit: Tweaking a file b4ff9f3 (tag: v-1.0) HEAD@{3}: reset: moving to HEAD b4ff9f3 (tag: v-1.0) HEAD@{4}: checkout: moving from newchanges to master b4ff9f3 (tag: v-1.0) HEAD@{5}: reset: moving to HEAD b4ff9f3 (tag: v-1.0) HEAD@{6}: checkout: moving from master to newchanges b4ff9f3 (tag: v-1.0) HEAD@{7}: reset: moving to HEAD b4ff9f3 (tag: v-1.0) HEAD@{8}: reset: moving to HEAD b4ff9f3 (tag: v-1.0) HEAD@{9}: reset: moving to HEAD b4ff9f3 (tag: v-1.0) HEAD@{10}: reset: moving to HEAD b4ff9f3 (tag: v-1.0) HEAD@{11}: commit: Done with the WIP2 0ce9769 HEAD@{12}: commit: Done with the WIP2 b419d7e HEAD@{13}: commit: Emergency fix c8d946a HEAD@{14}: reset: moving to HEAD c8d946a HEAD@{15}: reset: moving to HEAD c8d946a HEAD@{16}: commit: Done with the WIP b8e1096 HEAD@{17}: commit: Quick fix ddef7ba (bigtrouble) HEAD@{18}: reset: moving to HEAD ddef7ba (bigtrouble) HEAD@{19}: merge bigtrouble: Fast-forward 0861e5d HEAD@{20}: checkout: moving from bigtrouble to master ddef7ba (bigtrouble) HEAD@{21}: commit: change after rebase 9066d92 HEAD@{22}: rebase finished: returning to refs/heads/bigtrouble 9066d92 HEAD@{23}: rebase: fb adding trouble to file 0861e5d HEAD@{24}: rebase: checkout master 8b32aa4 HEAD@{25}: rebase: updating HEAD 8b32aa4 HEAD@{26}: rebase: updating HEAD 0861e5d HEAD@{27}: rebase: checkout master 8b32aa4 HEAD@{28}: checkout: moving from master to bigtrouble 0861e5d HEAD@{29}: commit: mb conflicting changes brewing 0c5efff HEAD@{30}: checkout: moving from bigtrouble to master 8b32aa4 HEAD@{31}: commit: fb adding trouble to file 0c5efff HEAD@{32}: checkout: moving from master to bigtrouble 0c5efff HEAD@{33}: commit: mb before rebase conflicts 42f5865 HEAD@{34}: merge myfeature: Fast-forward 02064fc (tag: v-0.9-beta) HEAD@{35}: checkout: moving from myfeature to master 42f5865 HEAD@{36}: commit: Another change c671488 HEAD@{37}: rebase finished: returning to refs/heads/myfeature c671488 HEAD@{38}: rebase: feature 02064fc (tag: v-0.9-beta) HEAD@{39}: rebase: checkout master 156f433 HEAD@{40}: checkout: moving from master to myfeature 02064fc (tag: v-0.9-beta) HEAD@{41}: commit: edit fa79281 HEAD@{42}: checkout: moving from myfeature to master 156f433 HEAD@{43}: commit: feature fa79281 HEAD@{44}: checkout: moving from master to myfeature fa79281 HEAD@{45}: commit (merge): merge conflict 14ae637 HEAD@{46}: commit: yet another change 620ff72 HEAD@{47}: checkout: moving from conflicting-change to master 5d495a8 (conflicting-change) HEAD@{48}: commit: Conflicting change 620ff72 HEAD@{49}: checkout: moving from master to conflicting-change 620ff72 HEAD@{50}: merge simple-change: Merge made by the 'recursive' strategy. 19ab207 HEAD@{51}: commit: file2 9521a8d HEAD@{52}: checkout: moving from simple-change to master 83f3159 HEAD@{53}: commit: simple change 9521a8d HEAD@{54}: checkout: moving from master to simple-change 9521a8d HEAD@{55}: merge other-change: Merge made by the 'recursive' strategy. 5040750 HEAD@{56}: checkout: moving from other-change to master a4d0bc9 HEAD@{57}: commit: Other change 5040750 HEAD@{58}: checkout: moving from master to other-change 5040750 HEAD@{59}: checkout: moving from other-change to master 5040750 HEAD@{60}: checkout: moving from master to other-change 5040750 HEAD@{61}: merge some-change: Fast-forward 84f00a8 HEAD@{62}: checkout: moving from some-change to master 5040750 HEAD@{63}: commit: Change file 84f00a8 HEAD@{64}: checkout: moving from master to some-change 84f00a8 HEAD@{65}: checkout: moving from newbranch to master 84f00a8 HEAD@{66}: checkout: moving from master to newbranch 84f00a8 HEAD@{67}: commit: add file 4e4ddba HEAD@{68}: commit: remove file e980c5c HEAD@{69}: commit: Add file aa6daef HEAD@{70}: commit: moved file a6a2066 HEAD@{71}: commit: Yello a635aaf HEAD@{72}: commit (initial): initial Reset to state before removing the last commit. git reset af6b837 git log --oneline --decorate --graph * af6b837 (HEAD -> master, tag: v-1.2) Updating for v-1.2 * 6a09e40 (tag: v-1.1) Tweaking a file * b4ff9f3 (tag: v-1.0) Done with the WIP2 * 0ce9769 Done with the WIP2 * b419d7e Emergency fix * c8d946a Done with the WIP * b8e1096 Quick fix * ddef7ba (bigtrouble) change after rebase * 9066d92 fb adding trouble to file * 0861e5d mb conflicting changes brewing * 0c5efff mb before rebase conflicts * 42f5865 Another change * c671488 feature * 02064fc (tag: v-0.9-beta) edit * fa79281 merge conflict |\\ | * 5d495a8 (conflicting-change) Conflicting change * | 14ae637 yet another change |/ * 620ff72 Mergin changes from simple-change Branch |\\ | * 83f3159 simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial Different resets: Mixed reset Reset staging area and point the head to new location Hard reset Reset local working directory, staging area and point the head to new location Soft reset Point the head to new location","title":"Rebase and Reflog"},{"location":"Git/37-getting-help/","text":"Getting help \u00b6 You can use help command to get information about commands: git help stash This will open up a man page about the stash command.","title":"Getting help"},{"location":"Git/37-getting-help/#getting-help","text":"You can use help command to get information about commands: git help stash This will open up a man page about the stash command.","title":"Getting help"},{"location":"Git/38-cherrypicking/","text":"Cherrypicking \u00b6 If you have 2 branches and you have a a change that needs to be applied from one to another, but only the one specific change, you can use cherrypicking. \ue0a0 master \ue0b0 git status On branch master nothing to commit, working tree clean \ue0a0 master \ue0b0 git checkout -b develop Switched to a new branch 'develop' \ue0a0 develop \ue0b0 echo \"fix 1\" >> file \ue0a0 develop \u25cf \ue0b0 git commit -am \"Fix 1\" [develop 7a837cc] Fix 1 1 file changed, 1 insertion(+) \ue0a0 develop \ue0b0 echo \"fix 2\" >> file \ue0a0 develop \u25cf \ue0b0 git commit -am \"Fix 2\" [develop c5bd510] Fix 2 1 file changed, 1 insertion(+) \ue0a0 develop \ue0b0 git log --oneline --decorate --graph * c5bd510 (HEAD -> develop) Fix 2 * 7a837cc Fix 1 * af6b837 (tag: v-1.2, master) Updating for v-1.2 * 6a09e40 (tag: v-1.1) Tweaking a file * b4ff9f3 (tag: v-1.0) Done with the WIP2 * 0ce9769 Done with the WIP2 * b419d7e Emergency fix * c8d946a Done with the WIP * b8e1096 Quick fix * ddef7ba (bigtrouble) change after rebase * 9066d92 fb adding trouble to file * 0861e5d mb conflicting changes brewing * 0c5efff mb before rebase conflicts * 42f5865 Another change * c671488 feature * 02064fc (tag: v-0.9-beta) edit * fa79281 merge conflict |\\ | * 5d495a8 (conflicting-change) Conflicting change * | 14ae637 yet another change |/ * 620ff72 Mergin changes from simple-change Branch |\\ | * 83f3159 simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial \ue0a0 develop \ue0b0 git checkout master Switched to branch 'master' \ue0a0 master \ue0b0 echo \"hello\" >> file2 \ue0a0 master \u25cf \ue0b0 git commit -am \"Updates away from develop\" [master 0c27eb4] Updates away from develop 1 file changed, 1 insertion(+) git cherry-pick 7a837cc [master 4dde5df] Fix 1 Date: Tue Jan 22 20:21:37 2019 +0200 1 file changed, 1 insertion(+) git status On branch master nothing to commit, working tree clean git log --oneline --decorate --graph --all * 4dde5df (HEAD -> master) Fix 1 * 0c27eb4 Updates away from develop | * c5bd510 (develop) Fix 2 | * 7a837cc Fix 1 |/ * af6b837 (tag: v-1.2) Updating for v-1.2 * 6a09e40 (tag: v-1.1) Tweaking a file * b4ff9f3 (tag: v-1.0) Done with the WIP2 * 0ce9769 Done with the WIP2 ...","title":"Cherrypicking"},{"location":"Git/38-cherrypicking/#cherrypicking","text":"If you have 2 branches and you have a a change that needs to be applied from one to another, but only the one specific change, you can use cherrypicking. \ue0a0 master \ue0b0 git status On branch master nothing to commit, working tree clean \ue0a0 master \ue0b0 git checkout -b develop Switched to a new branch 'develop' \ue0a0 develop \ue0b0 echo \"fix 1\" >> file \ue0a0 develop \u25cf \ue0b0 git commit -am \"Fix 1\" [develop 7a837cc] Fix 1 1 file changed, 1 insertion(+) \ue0a0 develop \ue0b0 echo \"fix 2\" >> file \ue0a0 develop \u25cf \ue0b0 git commit -am \"Fix 2\" [develop c5bd510] Fix 2 1 file changed, 1 insertion(+) \ue0a0 develop \ue0b0 git log --oneline --decorate --graph * c5bd510 (HEAD -> develop) Fix 2 * 7a837cc Fix 1 * af6b837 (tag: v-1.2, master) Updating for v-1.2 * 6a09e40 (tag: v-1.1) Tweaking a file * b4ff9f3 (tag: v-1.0) Done with the WIP2 * 0ce9769 Done with the WIP2 * b419d7e Emergency fix * c8d946a Done with the WIP * b8e1096 Quick fix * ddef7ba (bigtrouble) change after rebase * 9066d92 fb adding trouble to file * 0861e5d mb conflicting changes brewing * 0c5efff mb before rebase conflicts * 42f5865 Another change * c671488 feature * 02064fc (tag: v-0.9-beta) edit * fa79281 merge conflict |\\ | * 5d495a8 (conflicting-change) Conflicting change * | 14ae637 yet another change |/ * 620ff72 Mergin changes from simple-change Branch |\\ | * 83f3159 simple change * | 19ab207 file2 |/ * 9521a8d Merge branch 'other-change' |\\ | * a4d0bc9 Other change |/ * 5040750 Change file * 84f00a8 add file * 4e4ddba remove file * e980c5c Add file * aa6daef moved file * a6a2066 Yello * a635aaf initial \ue0a0 develop \ue0b0 git checkout master Switched to branch 'master' \ue0a0 master \ue0b0 echo \"hello\" >> file2 \ue0a0 master \u25cf \ue0b0 git commit -am \"Updates away from develop\" [master 0c27eb4] Updates away from develop 1 file changed, 1 insertion(+) git cherry-pick 7a837cc [master 4dde5df] Fix 1 Date: Tue Jan 22 20:21:37 2019 +0200 1 file changed, 1 insertion(+) git status On branch master nothing to commit, working tree clean git log --oneline --decorate --graph --all * 4dde5df (HEAD -> master) Fix 1 * 0c27eb4 Updates away from develop | * c5bd510 (develop) Fix 2 | * 7a837cc Fix 1 |/ * af6b837 (tag: v-1.2) Updating for v-1.2 * 6a09e40 (tag: v-1.1) Tweaking a file * b4ff9f3 (tag: v-1.0) Done with the WIP2 * 0ce9769 Done with the WIP2 ...","title":"Cherrypicking"},{"location":"Git/39-submodules/","text":"Submodules \u00b6 git submodule add [remote url] [local path] git submodule init git submodule update git submodule rm [local path]","title":"Submodules"},{"location":"Git/39-submodules/#submodules","text":"git submodule add [remote url] [local path] git submodule init git submodule update git submodule rm [local path]","title":"Submodules"},{"location":"Git/install-windows/","text":"Installing on windows \u00b6 You can go to https://git-scm.com/download/win and download the git installer. It will take you through the steps on installing it. Once that's done, you can open command prompt (cmd) and use git commands. If using vscode, the git commands should start working after restarting the application.","title":"Installing on windows"},{"location":"Git/install-windows/#installing-on-windows","text":"You can go to https://git-scm.com/download/win and download the git installer. It will take you through the steps on installing it. Once that's done, you can open command prompt (cmd) and use git commands. If using vscode, the git commands should start working after restarting the application.","title":"Installing on windows"},{"location":"Golang/","text":"Golang \u00b6 Learning golang Code available at https://github.com/daviskregers/golang . Sources: - Learn Go Programming - Golang Tutorial for Beginners","title":"Golang"},{"location":"Golang/#golang","text":"Learning golang Code available at https://github.com/daviskregers/golang . Sources: - Learn Go Programming - Golang Tutorial for Beginners","title":"Golang"},{"location":"Golang/00-introduction/","text":"Introduction \u00b6 Go was created by a small team in Google, which consisted of - Rober Griesemar - Rob Pike - Ken Thompson Why a new language? Python - Easy to use, but slow Java - Increasingly complex type system C/C++ - Complex type system, slow compile times Golang has - strong and statically typed language - excellent community - key features - simplicity - fast compile times - garbage collected - built-in concurrency - compile to standalone binaries Useful resources - https://golang.org/ - https://golangbridge.org/ - https://play.golang.org","title":"Introduction"},{"location":"Golang/00-introduction/#introduction","text":"Go was created by a small team in Google, which consisted of - Rober Griesemar - Rob Pike - Ken Thompson Why a new language? Python - Easy to use, but slow Java - Increasingly complex type system C/C++ - Complex type system, slow compile times Golang has - strong and statically typed language - excellent community - key features - simplicity - fast compile times - garbage collected - built-in concurrency - compile to standalone binaries Useful resources - https://golang.org/ - https://golangbridge.org/ - https://play.golang.org","title":"Introduction"},{"location":"Golang/01-setting-dev-environment/","text":"Setting golang dev environment \u00b6 You can download go language and it's tools at https://golang.org/dl . The installation instructions can be found at https://golang.org/doc/install . Since I'm on arch, I can run the following command. \u279c learning git:(master) \u2717 sudo pacman -S go go-tools resolving dependencies... looking for conflicting packages... Packages (2) go-2:1.13.5-1 go-tools-2:1.13+3523+65e3620a7-3 Total Download Size: 140.22 MiB Total Installed Size: 624.15 MiB :: Proceed with installation? [Y/n] :: Retrieving packages... go-2:1.13.5-1-x86_64 113.0 MiB 1082 KiB/s 01:47 [#######################################################################################] 100% go-tools-2:1.13+3523+65e3620a7-3-x86_64 27.2 MiB 1186 KiB/s 00:23 [#######################################################################################] 100% (2/2) checking keys in keyring [#######################################################################################] 100% (2/2) checking package integrity [#######################################################################################] 100% (2/2) loading package files [#######################################################################################] 100% (2/2) checking for file conflicts [#######################################################################################] 100% (2/2) checking available disk space [#######################################################################################] 100% :: Processing package changes... (1/2) installing go [#######################################################################################] 100% (2/2) installing go-tools [#######################################################################################] 100% :: Running post-transaction hooks... (1/1) Arming ConditionNeedsUpdate... Test that it works \u00b6 \u279c learning git:(master) \u2717 go version go version go1.13.5 linux/amd64 Setting up paths \u00b6 There are 2 environment variables that can be set up for golang. $GOROOT - indicates on where the golang is installed. $GOPATH - indicates on where all the packages will go - You can specify multiple directories, it will search all of them. - When using `go get` that downloads packages, it will use the first directory. If needed, you can modify them in your .basrc / .zshrc files. I'm going to set up 2 paths - one for libraries, one for this learning project. export GOPATH=/home/davis/.golib:/home/davis/projects/learning/golang Structure \u00b6 Within these paths you will see 3 main directories: bin - compiled binaries pkg - when compiling, creates intermediate binaries so you don't have to recompile them every time src - source code We are going to create these 3 directories in our GOPATH . \u279c golang git:(master) \u2717 pwd /home/davis/projects/learning/golang \u279c golang git:(master) \u2717 mkdir bin pkg src \u279c golang git:(master) \u2717 ls bin pkg src Hello world application \u00b6 When creating a new application, we will create a new project in the src directory. The standard structure is to mirror the structure it would look like in source control. So it should look like this: \u279c golang git:(master) \u2717 mkdir -p src/github.com/daviskregers/helloworld \u279c golang git:(master) \u2717 tree . |-- bin |-- pkg `-- src `-- github.com `-- daviskregers `-- helloworld In this helloworld directory we can add a new file Main.go with following contents. package main import \"fmt\" func main() { fmt.Println(\"Hello Go!\") } Now there are multiple steps on how we can run the program. By running it: \u279c golang git:(master) \u2717 go run ./src/github.com/daviskregers/helloworld/Main.go Hello Go! Building and running it \u279c golang git:(master) \u2717 go build ./src/github.com/daviskregers/helloworld \u279c golang git:(master) \u2717 ./helloworld Hello Go! By installing it \u279c golang git:(master) \u2717 pwd /home/davis/projects/learning/golang \u279c golang git:(master) \u2717 ls bin \u279c golang git:(master) \u2717 go install github.com/daviskregers/helloworld \u279c golang git:(master) \u2717 ls bin helloworld \u279c golang git:(master) \u2717 bin/helloworld Hello Go!","title":"Setting golang dev environment"},{"location":"Golang/01-setting-dev-environment/#setting-golang-dev-environment","text":"You can download go language and it's tools at https://golang.org/dl . The installation instructions can be found at https://golang.org/doc/install . Since I'm on arch, I can run the following command. \u279c learning git:(master) \u2717 sudo pacman -S go go-tools resolving dependencies... looking for conflicting packages... Packages (2) go-2:1.13.5-1 go-tools-2:1.13+3523+65e3620a7-3 Total Download Size: 140.22 MiB Total Installed Size: 624.15 MiB :: Proceed with installation? [Y/n] :: Retrieving packages... go-2:1.13.5-1-x86_64 113.0 MiB 1082 KiB/s 01:47 [#######################################################################################] 100% go-tools-2:1.13+3523+65e3620a7-3-x86_64 27.2 MiB 1186 KiB/s 00:23 [#######################################################################################] 100% (2/2) checking keys in keyring [#######################################################################################] 100% (2/2) checking package integrity [#######################################################################################] 100% (2/2) loading package files [#######################################################################################] 100% (2/2) checking for file conflicts [#######################################################################################] 100% (2/2) checking available disk space [#######################################################################################] 100% :: Processing package changes... (1/2) installing go [#######################################################################################] 100% (2/2) installing go-tools [#######################################################################################] 100% :: Running post-transaction hooks... (1/1) Arming ConditionNeedsUpdate...","title":"Setting golang dev environment"},{"location":"Golang/01-setting-dev-environment/#test-that-it-works","text":"\u279c learning git:(master) \u2717 go version go version go1.13.5 linux/amd64","title":"Test that it works"},{"location":"Golang/01-setting-dev-environment/#setting-up-paths","text":"There are 2 environment variables that can be set up for golang. $GOROOT - indicates on where the golang is installed. $GOPATH - indicates on where all the packages will go - You can specify multiple directories, it will search all of them. - When using `go get` that downloads packages, it will use the first directory. If needed, you can modify them in your .basrc / .zshrc files. I'm going to set up 2 paths - one for libraries, one for this learning project. export GOPATH=/home/davis/.golib:/home/davis/projects/learning/golang","title":"Setting up paths"},{"location":"Golang/01-setting-dev-environment/#structure","text":"Within these paths you will see 3 main directories: bin - compiled binaries pkg - when compiling, creates intermediate binaries so you don't have to recompile them every time src - source code We are going to create these 3 directories in our GOPATH . \u279c golang git:(master) \u2717 pwd /home/davis/projects/learning/golang \u279c golang git:(master) \u2717 mkdir bin pkg src \u279c golang git:(master) \u2717 ls bin pkg src","title":"Structure"},{"location":"Golang/01-setting-dev-environment/#hello-world-application","text":"When creating a new application, we will create a new project in the src directory. The standard structure is to mirror the structure it would look like in source control. So it should look like this: \u279c golang git:(master) \u2717 mkdir -p src/github.com/daviskregers/helloworld \u279c golang git:(master) \u2717 tree . |-- bin |-- pkg `-- src `-- github.com `-- daviskregers `-- helloworld In this helloworld directory we can add a new file Main.go with following contents. package main import \"fmt\" func main() { fmt.Println(\"Hello Go!\") } Now there are multiple steps on how we can run the program. By running it: \u279c golang git:(master) \u2717 go run ./src/github.com/daviskregers/helloworld/Main.go Hello Go! Building and running it \u279c golang git:(master) \u2717 go build ./src/github.com/daviskregers/helloworld \u279c golang git:(master) \u2717 ./helloworld Hello Go! By installing it \u279c golang git:(master) \u2717 pwd /home/davis/projects/learning/golang \u279c golang git:(master) \u2717 ls bin \u279c golang git:(master) \u2717 go install github.com/daviskregers/helloworld \u279c golang git:(master) \u2717 ls bin helloworld \u279c golang git:(master) \u2717 bin/helloworld Hello Go!","title":"Hello world application"},{"location":"Golang/02-variables/","text":"Variables \u00b6 There are 3 ways to declare variables in go. package main import ( \"fmt\" ) func main() { var i int i = 42 i = 27 fmt.Println(i) var j int = 42 fmt.Println(j) k := 12 fmt.Println(k) } Another method is to declare variables at package level. var l float32 = 56 func main() { fmt.Println(l) } We can also define a block of variables. var ( actorName string = \"Elisabeth Sladen\" companion string = \"Sarah Jane Smith\" doctorNumber int = 3 season int = 11 ) Redeclare variables \u00b6 In go we can't really redeclare variables, but we can shadow them. Which means, if the variable is declared in a higher level scope, we can redeclare it in a lower level scope. var l float32 = 56 func main() { fmt.Println(l) var l int = 42 fmt.Println(l) var m int = 26 // var m float32 = 12 -- will throw an error } The output will be 56 42 Other interesting things about variables \u00b6 They always have to be used. When you are declaring and variable that is not used, the compiler will throw an error: $variable declared and not used Visibility lower case first letter variables are used for package scope any file at that package can access the variable upper case first letter to export exported from the package, globally visible no private scope you can't scope variable to the scope itself, but you can declare it in a block and scope to it. Naming conventions Pascal or camelCase Capitalize acronyms (HTTP, URL) As short as reasonable longer names for longer lives Conversion destinationType(variable) use strconv package for strings j = float32(l) fmt.Printf(\"%v, %T\\n\", j, j) l = int(j) fmt.Printf(\"%v, %T\\n\", l, l) var m int = 42 fmt.Printf(\"%v, %T\\n\", m, m) var n string n = string(m) fmt.Printf(\"%v, %T\\n\", n, n) // will result in `*` n = strconv.Itoa(m) fmt.Printf(\"%v, %T\\n\", n, n) // will result to `42` 42, float32 42, int 42, int *, string 42, string","title":"Variables"},{"location":"Golang/02-variables/#variables","text":"There are 3 ways to declare variables in go. package main import ( \"fmt\" ) func main() { var i int i = 42 i = 27 fmt.Println(i) var j int = 42 fmt.Println(j) k := 12 fmt.Println(k) } Another method is to declare variables at package level. var l float32 = 56 func main() { fmt.Println(l) } We can also define a block of variables. var ( actorName string = \"Elisabeth Sladen\" companion string = \"Sarah Jane Smith\" doctorNumber int = 3 season int = 11 )","title":"Variables"},{"location":"Golang/02-variables/#redeclare-variables","text":"In go we can't really redeclare variables, but we can shadow them. Which means, if the variable is declared in a higher level scope, we can redeclare it in a lower level scope. var l float32 = 56 func main() { fmt.Println(l) var l int = 42 fmt.Println(l) var m int = 26 // var m float32 = 12 -- will throw an error } The output will be 56 42","title":"Redeclare variables"},{"location":"Golang/02-variables/#other-interesting-things-about-variables","text":"They always have to be used. When you are declaring and variable that is not used, the compiler will throw an error: $variable declared and not used Visibility lower case first letter variables are used for package scope any file at that package can access the variable upper case first letter to export exported from the package, globally visible no private scope you can't scope variable to the scope itself, but you can declare it in a block and scope to it. Naming conventions Pascal or camelCase Capitalize acronyms (HTTP, URL) As short as reasonable longer names for longer lives Conversion destinationType(variable) use strconv package for strings j = float32(l) fmt.Printf(\"%v, %T\\n\", j, j) l = int(j) fmt.Printf(\"%v, %T\\n\", l, l) var m int = 42 fmt.Printf(\"%v, %T\\n\", m, m) var n string n = string(m) fmt.Printf(\"%v, %T\\n\", n, n) // will result in `*` n = strconv.Itoa(m) fmt.Printf(\"%v, %T\\n\", n, n) // will result to `42` 42, float32 42, int 42, int *, string 42, string","title":"Other interesting things about variables"},{"location":"Golang/03-primitives/","text":"Primitives \u00b6 At this section we are going to look at primitive data types that are available at the go language. There are 3 categories of types we can store data in go: Boolean type \u00b6 Values are true or false Not an alias for other types (e.g. int) Zero value if false var n bool = true fmt.Printf(\"%v, %T\\n\", n, n) n = false fmt.Printf(\"%v, %T\\n\", n, n) m := 1 == 1 fmt.Printf(\"%v, %T\\n\", m, m) var o bool // default value is `0` fmt.Printf(\"%v, %T\\n\", o, o) true, bool false, bool true, bool false, bool Numberic types \u00b6 Integers \u00b6 Signed Integers int type has varying size, but min 32 bits 8 bit (int8) through 64 bit (int64) Unsigned integers 8 bit (byte and uint8) through 32 bit (uint32) Arithmetic operations Addition, subtraction, multiplication, division, remainder Bitwise operations and, or, xor, and not Zero value is 0 Can't mix types in same family (uint16 + uint32 = error) // Signed Integers i := 42 // int var i1 int8 = 42 // 8 bit integer (-128 to -127) var i2 int16 = 42 // 16 bit integer (-32 768 to 32 767) var i3 int32 = 42 // 32 bit integer (-2147483648 to 2147483647) var i4 int64 = 42 // 64 bit integer (-9223372036854775808 to 9223372036854775807) fmt.Printf(\"%v, %T\\n\", i, i) fmt.Printf(\"%v, %T\\n\", i1, i1) fmt.Printf(\"%v, %T\\n\", i2, i2) fmt.Printf(\"%v, %T\\n\", i3, i3) fmt.Printf(\"%v, %T\\n\", i4, i4) // Unsigned Integers var ui1 uint8 = 42 // 8 bit unsigned integer (0 to 255) var ui2 uint16 = 42 // 16 bit unsigned integer (0 to 65535) var ui3 uint32 = 42 // 32 bit unsigned integer (0 to 4294967295) fmt.Printf(\"%v, %T\\n\", ui1, ui1) fmt.Printf(\"%v, %T\\n\", ui2, ui2) fmt.Printf(\"%v, %T\\n\", ui3, ui3) // we don't have a 64 bit uint, but we have byte. // basic operations // variable types must match or do type conversion a := 10 // 1010 b := 3 // 0011 fmt.Println(a + b) fmt.Println(a - b) fmt.Println(a * b) fmt.Println(a / b) // note that it drops reminder, since we preserve integer type fmt.Println(a % b) // reminder fmt.Println(a ^ b) // AND -- 1010 ^ 0011 = 0010 fmt.Println(a | b) // OR -- 1010 | 0011 = 1011 fmt.Println(a ^ b) // XOR -- 1010 ^ 0011 = 1001 fmt.Println(a &^ b) // AND NOT -- 1010 &^ 0011 = 0100 a = 8 // 2^3 fmt.Println(a << 3) // bit shift 3 places left, 2^3 * 2^3 = 2^6 fmt.Println(a >> 3) // bit shift 3 places right, 2^3 / 2^3 = 2^0 42, int 42, int8 42, int16 42, int32 42, int64 42, uint8 42, uint16 42, uint32 13 7 30 3 1 9 11 9 8 64 1 Floating point \u00b6 Follow IEEE-754 standard Zero value is 0 32 and 64 bit versions Literal types Decimal (3.14) Exponential (13e18 or 2E10) Mixed (13.7e12) Arithmetic operations addition, subtraction, multiplication, division // Floating numbers // 32 bit float (precision from +/- 1.18E38 to +/-3.4E38) // 64 bit float (precision from +/- 2.23E-308 to +/-1.8E308) var f0 float32 = 3.14E12 f1 := 3.14 f2 := 13.7e72 f3 := 2.1E14 fmt.Printf(\"%v, %T\\n\", f0, f0) fmt.Printf(\"%v, %T\\n\", f1, f1) fmt.Printf(\"%v, %T\\n\", f2, f2) fmt.Printf(\"%v, %T\\n\", f3, f3) a1 := 10.2 b1 := 3.7 fmt.Println(a1 + b1) fmt.Println(a1 + b1) fmt.Println(a1 * b1) fmt.Println(a1 / b1) 3.14e+12, float32 3.14, float64 1.37e+73, float64 2.1e+14, float64 13.899999999999999 13.899999999999999 37.74 2.7567567567567566 Complex numbers \u00b6 Zero value is (0+0i) 64 and 128 bit versions Built-in functions complex - make complex number from 2 floats real - get real part as float imag - get imaginary part as float Arithmetic operations Addition, subtraction, multiplication, division // complex numbers // there are 2 types - // - complex64 (float32 real + float32 imaginary parts) // - complex128 (float64 real + float64 imaginary parts) var c1 complex64 = 1 + 2i var c2 complex128 = 1 + 3i fmt.Printf(\"%v, %T\\n\", c1, c1) fmt.Printf(\"%v, %T\\n\", c2, c2) fmt.Printf(\"%v, %T\\n\", real(c1), real(c1)) fmt.Printf(\"%v, %T\\n\", imag(c1), imag(c1)) fmt.Printf(\"%v, %T\\n\", real(c2), real(c2)) fmt.Printf(\"%v, %T\\n\", imag(c2), imag(c2)) a2 := 1 + 2i b2 := 2 + 5.2i fmt.Println(a2 + b2) fmt.Println(a2 - b2) fmt.Println(a2 * b2) fmt.Println(a2 / b2) (1+2i), complex64 (1+3i), complex128 1, float32 2, float32 1, float64 3, float64 (3+7.2i) (-1-3.2i) (-8.4+9.2i) (0.3994845360824742-0.038659793814433i) Text types \u00b6 Strings UTF-8 Immutable Can be concatenated with plus (+) operator Can be coverted to []byte Rune UTF-32 Alias for int32 Special methods normally required to process e.g. strings.Reader#ReadRune s := \"this is a string\" s2 := \"this is also a string\" fmt.Printf(\"%v, %T\\n\", s, s) fmt.Printf(\"%v, %T\\n\", s[2], s[2]) // 105, uint8 fmt.Printf(\"%v, %T\\n\", string(s[2]), string(s[2])) // i, string fmt.Printf(\"%v, %T\\n\", s + s2, s + s2) // convert to bytes sb := []byte(s) fmt.Printf(\"%v, %T\\n\", sb, sb) // rune r := 'a' var r1 rune = 'a' fmt.Printf(\"%v, %T\\n\", r, r) // 97, int32 fmt.Printf(\"%v, %T\\n\", r1, r1) // 97, int32 this is a string, string 105, uint8 i, string this is a stringthis is also a string, string [116 104 105 115 32 105 115 32 97 32 115 116 114 105 110 103], []uint8 97, int32 97, int32","title":"Primitives"},{"location":"Golang/03-primitives/#primitives","text":"At this section we are going to look at primitive data types that are available at the go language. There are 3 categories of types we can store data in go:","title":"Primitives"},{"location":"Golang/03-primitives/#boolean-type","text":"Values are true or false Not an alias for other types (e.g. int) Zero value if false var n bool = true fmt.Printf(\"%v, %T\\n\", n, n) n = false fmt.Printf(\"%v, %T\\n\", n, n) m := 1 == 1 fmt.Printf(\"%v, %T\\n\", m, m) var o bool // default value is `0` fmt.Printf(\"%v, %T\\n\", o, o) true, bool false, bool true, bool false, bool","title":"Boolean type"},{"location":"Golang/03-primitives/#numberic-types","text":"","title":"Numberic types"},{"location":"Golang/03-primitives/#integers","text":"Signed Integers int type has varying size, but min 32 bits 8 bit (int8) through 64 bit (int64) Unsigned integers 8 bit (byte and uint8) through 32 bit (uint32) Arithmetic operations Addition, subtraction, multiplication, division, remainder Bitwise operations and, or, xor, and not Zero value is 0 Can't mix types in same family (uint16 + uint32 = error) // Signed Integers i := 42 // int var i1 int8 = 42 // 8 bit integer (-128 to -127) var i2 int16 = 42 // 16 bit integer (-32 768 to 32 767) var i3 int32 = 42 // 32 bit integer (-2147483648 to 2147483647) var i4 int64 = 42 // 64 bit integer (-9223372036854775808 to 9223372036854775807) fmt.Printf(\"%v, %T\\n\", i, i) fmt.Printf(\"%v, %T\\n\", i1, i1) fmt.Printf(\"%v, %T\\n\", i2, i2) fmt.Printf(\"%v, %T\\n\", i3, i3) fmt.Printf(\"%v, %T\\n\", i4, i4) // Unsigned Integers var ui1 uint8 = 42 // 8 bit unsigned integer (0 to 255) var ui2 uint16 = 42 // 16 bit unsigned integer (0 to 65535) var ui3 uint32 = 42 // 32 bit unsigned integer (0 to 4294967295) fmt.Printf(\"%v, %T\\n\", ui1, ui1) fmt.Printf(\"%v, %T\\n\", ui2, ui2) fmt.Printf(\"%v, %T\\n\", ui3, ui3) // we don't have a 64 bit uint, but we have byte. // basic operations // variable types must match or do type conversion a := 10 // 1010 b := 3 // 0011 fmt.Println(a + b) fmt.Println(a - b) fmt.Println(a * b) fmt.Println(a / b) // note that it drops reminder, since we preserve integer type fmt.Println(a % b) // reminder fmt.Println(a ^ b) // AND -- 1010 ^ 0011 = 0010 fmt.Println(a | b) // OR -- 1010 | 0011 = 1011 fmt.Println(a ^ b) // XOR -- 1010 ^ 0011 = 1001 fmt.Println(a &^ b) // AND NOT -- 1010 &^ 0011 = 0100 a = 8 // 2^3 fmt.Println(a << 3) // bit shift 3 places left, 2^3 * 2^3 = 2^6 fmt.Println(a >> 3) // bit shift 3 places right, 2^3 / 2^3 = 2^0 42, int 42, int8 42, int16 42, int32 42, int64 42, uint8 42, uint16 42, uint32 13 7 30 3 1 9 11 9 8 64 1","title":"Integers"},{"location":"Golang/03-primitives/#floating-point","text":"Follow IEEE-754 standard Zero value is 0 32 and 64 bit versions Literal types Decimal (3.14) Exponential (13e18 or 2E10) Mixed (13.7e12) Arithmetic operations addition, subtraction, multiplication, division // Floating numbers // 32 bit float (precision from +/- 1.18E38 to +/-3.4E38) // 64 bit float (precision from +/- 2.23E-308 to +/-1.8E308) var f0 float32 = 3.14E12 f1 := 3.14 f2 := 13.7e72 f3 := 2.1E14 fmt.Printf(\"%v, %T\\n\", f0, f0) fmt.Printf(\"%v, %T\\n\", f1, f1) fmt.Printf(\"%v, %T\\n\", f2, f2) fmt.Printf(\"%v, %T\\n\", f3, f3) a1 := 10.2 b1 := 3.7 fmt.Println(a1 + b1) fmt.Println(a1 + b1) fmt.Println(a1 * b1) fmt.Println(a1 / b1) 3.14e+12, float32 3.14, float64 1.37e+73, float64 2.1e+14, float64 13.899999999999999 13.899999999999999 37.74 2.7567567567567566","title":"Floating point"},{"location":"Golang/03-primitives/#complex-numbers","text":"Zero value is (0+0i) 64 and 128 bit versions Built-in functions complex - make complex number from 2 floats real - get real part as float imag - get imaginary part as float Arithmetic operations Addition, subtraction, multiplication, division // complex numbers // there are 2 types - // - complex64 (float32 real + float32 imaginary parts) // - complex128 (float64 real + float64 imaginary parts) var c1 complex64 = 1 + 2i var c2 complex128 = 1 + 3i fmt.Printf(\"%v, %T\\n\", c1, c1) fmt.Printf(\"%v, %T\\n\", c2, c2) fmt.Printf(\"%v, %T\\n\", real(c1), real(c1)) fmt.Printf(\"%v, %T\\n\", imag(c1), imag(c1)) fmt.Printf(\"%v, %T\\n\", real(c2), real(c2)) fmt.Printf(\"%v, %T\\n\", imag(c2), imag(c2)) a2 := 1 + 2i b2 := 2 + 5.2i fmt.Println(a2 + b2) fmt.Println(a2 - b2) fmt.Println(a2 * b2) fmt.Println(a2 / b2) (1+2i), complex64 (1+3i), complex128 1, float32 2, float32 1, float64 3, float64 (3+7.2i) (-1-3.2i) (-8.4+9.2i) (0.3994845360824742-0.038659793814433i)","title":"Complex numbers"},{"location":"Golang/03-primitives/#text-types","text":"Strings UTF-8 Immutable Can be concatenated with plus (+) operator Can be coverted to []byte Rune UTF-32 Alias for int32 Special methods normally required to process e.g. strings.Reader#ReadRune s := \"this is a string\" s2 := \"this is also a string\" fmt.Printf(\"%v, %T\\n\", s, s) fmt.Printf(\"%v, %T\\n\", s[2], s[2]) // 105, uint8 fmt.Printf(\"%v, %T\\n\", string(s[2]), string(s[2])) // i, string fmt.Printf(\"%v, %T\\n\", s + s2, s + s2) // convert to bytes sb := []byte(s) fmt.Printf(\"%v, %T\\n\", sb, sb) // rune r := 'a' var r1 rune = 'a' fmt.Printf(\"%v, %T\\n\", r, r) // 97, int32 fmt.Printf(\"%v, %T\\n\", r1, r1) // 97, int32 this is a string, string 105, uint8 i, string this is a stringthis is also a string, string [116 104 105 115 32 105 115 32 97 32 115 116 114 105 110 103], []uint8 97, int32 97, int32","title":"Text types"},{"location":"Golang/04-constants/","text":"Constants \u00b6 Immutable, but can be shadowed Replaced by the compiler at compile time Value bust be calculable at compile time Named like variables PascalCase for exported constants camelCase for internal constants Typed constants work like immutable variables can interoperate only with same type Untyped constants work like literals Can interoperate with similar types Enumerated Constants Special symbol iota allows related constants to be created easily Iota starts at 0 in each const block and increments by one Watch out of constant values that m,atch zero values for variables Enumverated expressions Operations that can be determined at compile time are allowed Arithmetic Bitwise operations Bitshifting import ( \"fmt\" ) const e int16 = 27 const h = iota // Enumerated constant, counter const ( i = iota j = iota k = iota ) const ( _ = iota // default, error / zero value catSpecialist dogSpecialist snakeSpecialist ) const ( // enumerated expressions _ = iota // ignore first value by assigning to blank identifier KB = 1 << (10 * iota) MB GB TB PB EB ZB YB ) const ( isAdmin = 1 << iota isHeadquarters canSeeFinancials canSeeAfrica canSeeAsia canSeeEurope canSeeNorthAmerica canSeeSouthAmerica ) func main() { // typed constants const a int = 42 const b string = \"foo\" const c float32 = 3.14 const d bool = true const e int = 34 fmt.Printf(\"%v, %T\\n\", a, a) fmt.Printf(\"%v, %T\\n\", b, b) fmt.Printf(\"%v, %T\\n\", c, c) fmt.Printf(\"%v, %T\\n\", d, d) fmt.Printf(\"%v, %T\\n\", e, e) // 34, int // untyped constants const f = 42 const g int16 = 27 fmt.Printf(\"%v, %T\\n\", f, f) // int fmt.Printf(\"%v, %T\\n\", g, g) // int16 fmt.Printf(\"%v, %T\\n\", f + g, f + g) // int16 // enumerated constants fmt.Printf(\"%v, %T\\n\", h, h) // 0, int fmt.Printf(\"%v, %T\\n\", i, i) // 0, int fmt.Printf(\"%v, %T\\n\", j, j) // 1, int fmt.Printf(\"%v, %T\\n\", k, k) // 2, int fmt.Printf(\"%v, %T\\n\", catSpecialist, catSpecialist) // 1, int fmt.Printf(\"%v, %T\\n\", dogSpecialist, dogSpecialist) // 2, int fmt.Printf(\"%v, %T\\n\", snakeSpecialist, snakeSpecialist) // 3, int var spacialistType int fmt.Printf(\"%v\\n\", spacialistType == catSpecialist) // false // enumerated expressions fmt.Printf(\"%v, %T\\n\", KB, KB) // 1024, int fmt.Printf(\"%v, %T\\n\", MB, MB) // 1048576, int fmt.Printf(\"%v, %T\\n\", GB, GB) // 1073741824, int var roles byte = isAdmin | canSeeFinancials | canSeeEurope fmt.Printf(\"%b\\n\", roles) // 100101 fmt.Printf(\"Is Admin? %v\\n\", isAdmin & roles == isAdmin) // true fmt.Printf(\"Is HQ? %v\\n\", isHeadquarters & roles == isHeadquarters) // false } 42, int foo, string 3.14, float32 true, bool 34, int 42, int 27, int16 69, int16 0, int 0, int 1, int 2, int 1, int 2, int 3, int false 1024, int 1048576, int 1073741824, int 100101 Is Admin? true Is HQ? false","title":"Constants"},{"location":"Golang/04-constants/#constants","text":"Immutable, but can be shadowed Replaced by the compiler at compile time Value bust be calculable at compile time Named like variables PascalCase for exported constants camelCase for internal constants Typed constants work like immutable variables can interoperate only with same type Untyped constants work like literals Can interoperate with similar types Enumerated Constants Special symbol iota allows related constants to be created easily Iota starts at 0 in each const block and increments by one Watch out of constant values that m,atch zero values for variables Enumverated expressions Operations that can be determined at compile time are allowed Arithmetic Bitwise operations Bitshifting import ( \"fmt\" ) const e int16 = 27 const h = iota // Enumerated constant, counter const ( i = iota j = iota k = iota ) const ( _ = iota // default, error / zero value catSpecialist dogSpecialist snakeSpecialist ) const ( // enumerated expressions _ = iota // ignore first value by assigning to blank identifier KB = 1 << (10 * iota) MB GB TB PB EB ZB YB ) const ( isAdmin = 1 << iota isHeadquarters canSeeFinancials canSeeAfrica canSeeAsia canSeeEurope canSeeNorthAmerica canSeeSouthAmerica ) func main() { // typed constants const a int = 42 const b string = \"foo\" const c float32 = 3.14 const d bool = true const e int = 34 fmt.Printf(\"%v, %T\\n\", a, a) fmt.Printf(\"%v, %T\\n\", b, b) fmt.Printf(\"%v, %T\\n\", c, c) fmt.Printf(\"%v, %T\\n\", d, d) fmt.Printf(\"%v, %T\\n\", e, e) // 34, int // untyped constants const f = 42 const g int16 = 27 fmt.Printf(\"%v, %T\\n\", f, f) // int fmt.Printf(\"%v, %T\\n\", g, g) // int16 fmt.Printf(\"%v, %T\\n\", f + g, f + g) // int16 // enumerated constants fmt.Printf(\"%v, %T\\n\", h, h) // 0, int fmt.Printf(\"%v, %T\\n\", i, i) // 0, int fmt.Printf(\"%v, %T\\n\", j, j) // 1, int fmt.Printf(\"%v, %T\\n\", k, k) // 2, int fmt.Printf(\"%v, %T\\n\", catSpecialist, catSpecialist) // 1, int fmt.Printf(\"%v, %T\\n\", dogSpecialist, dogSpecialist) // 2, int fmt.Printf(\"%v, %T\\n\", snakeSpecialist, snakeSpecialist) // 3, int var spacialistType int fmt.Printf(\"%v\\n\", spacialistType == catSpecialist) // false // enumerated expressions fmt.Printf(\"%v, %T\\n\", KB, KB) // 1024, int fmt.Printf(\"%v, %T\\n\", MB, MB) // 1048576, int fmt.Printf(\"%v, %T\\n\", GB, GB) // 1073741824, int var roles byte = isAdmin | canSeeFinancials | canSeeEurope fmt.Printf(\"%b\\n\", roles) // 100101 fmt.Printf(\"Is Admin? %v\\n\", isAdmin & roles == isAdmin) // true fmt.Printf(\"Is HQ? %v\\n\", isHeadquarters & roles == isHeadquarters) // false } 42, int foo, string 3.14, float32 true, bool 34, int 42, int 27, int16 69, int16 0, int 0, int 1, int 2, int 1, int 2, int 3, int false 1024, int 1048576, int 1073741824, int 100101 Is Admin? true Is HQ? false","title":"Constants"},{"location":"Golang/05-arrays-and-slices/","text":"Arrays and slices \u00b6 Arrays Collection of items with same type Fixed size Declaration styles a := [3]int{1,2,3} a := [...]int{1,2,3} var a [3]int Access via zero based index a := [3]int {1, 3, 5} // a[1] == 3 len function returns size of array Copies refer to different underlying data Slices Backed by array Creation styles Slice an existing array or slice Literal style Via function make a := make([]int, 10) // create slice with capacity and length == 10 a := make([], 10, 100) // slice with length == 10 and capacity == 100 len function returns length of slice cap function returns length of underlying array append function to add elements to slice may cause expensive copy operations if underlying array is too small Copies refer to same underlying array package main import ( \"fmt\" ) func main() { // arrays grades := [3]int{97, 85, 93} fmt.Printf(\"Grades: %v\\n\", grades) // create array that is just large enough to hold the data grades2 := [...]int{97, 85, 93} fmt.Printf(\"Grades2: %v\\n\", grades2) var students [3]string fmt.Printf(\"Students %v\\n\", students) students[0] = \"Lisa\" fmt.Printf(\"Students %v\\n\", students) students[1] = \"Ahmed\" fmt.Printf(\"Students %v\\n\", students) students[2] = \"Arnold\" fmt.Printf(\"Students %v\\n\", students) fmt.Printf(\"Student #1: %v\\n\", students[0]) fmt.Printf(\"Student #2: %v\\n\", students[1]) fmt.Printf(\"Student #3: %v\\n\", students[2]) fmt.Printf(\"Number of Students: %v\\n\", len(students)) var identityMatrix [3][3]int = [3][3]int{ [3]int{1,0,0}, [3]int{0,1,0}, [3]int{0,0,1}, } fmt.Println(identityMatrix) var identityMatrix2 [3][3]int identityMatrix2[0] = [3]int{1,0,0} identityMatrix2[1] = [3]int{0,1,0} identityMatrix2[2] = [3]int{0,0,1} fmt.Println(identityMatrix2) // you create copies not point to the same data a := [...]int{1,2,3} b := a b[1] = 5 fmt.Println(a) // [1 2 3] fmt.Println(b) // [1 5 3] // slices s := []int{1,2,3} fmt.Println(s) fmt.Printf(\"Length: %v\\n\", len(s)) fmt.Printf(\"Capacity: %v\\n\", cap(s)) // slices do point to the same underlying data s1 := s s1[1] = 5 fmt.Println(s) // [1 5 3] fmt.Println(s1) // [1 5 3] s2 := [...]int{1,2,3,4,5,6,7,8,9,10} s3 := s2[:] // slice of all elements, [1 2 3 4 5 6 7 8 9 10] s4 := s2[3:] // slice from 4th element to end, [4 5 6 7 8 9 10] s5 := s2[:6] // slice first 6 elements, [1 2 3 4 5 6] s6 := s2[3:6] // slice the 4th, 5th, 6th elements, [4 5 6] s2[5] = 42 // now s2, s5, s6 has 6th element as 42 fmt.Println(s2) fmt.Println(s3) fmt.Println(s4) fmt.Println(s5) fmt.Println(s6) // make function ss := make([]int, 3, 100) // type, size fmt.Println(ss) // [0 0 0] fmt.Printf(\"Length: %v\\n\", len(ss)) // 3 fmt.Printf(\"Capacity: %v\\n\", cap(ss)) // 100 // sa := []int{} fmt.Println(sa) // [] fmt.Printf(\"Length: %v\\n\", len(sa)) // 0 fmt.Printf(\"Capacity: %v\\n\", cap(sa)) // 0 sa = append(sa, 1) fmt.Println(sa) // [1] fmt.Printf(\"Length: %v\\n\", len(sa)) // 1 fmt.Printf(\"Capacity: %v\\n\", cap(sa)) // 1 sa = append(sa, 2,3,4,5,6) fmt.Println(sa) // [1 2 3 4 5 6] fmt.Printf(\"Length: %v\\n\", len(sa)) // 6 fmt.Printf(\"Capacity: %v\\n\", cap(sa)) // 6 // concatenate slices sa = append(sa, []int{2,3,4,5,6}...) fmt.Println(sa) // [1 2 3 4 5 6 2 3 4 5 6] fmt.Printf(\"Length: %v\\n\", len(sa)) // 11 fmt.Printf(\"Capacity: %v\\n\", cap(sa)) // 12 // stack operations st := []int{1, 2, 3, 4, 5} fmt.Println(st) st1 := st[:len(st)-1] fmt.Println(st1) // remove elements from middle sm := append(st[:2], st[3:]...) fmt.Println(sm) // be careful when dealing with these situations // may cause unexpected results fmt.Println(st) // [1 2 3 4 5 5] } Grades: [97 85 93] Grades2: [97 85 93] Students [ ] Students [Lisa ] Students [Lisa Ahmed ] Students [Lisa Ahmed Arnold] Student #1: Lisa Student #2: Ahmed Student #3: Arnold Number of Students: 3 [[1 0 0] [0 1 0] [0 0 1]] [[1 0 0] [0 1 0] [0 0 1]] [1 2 3] [1 5 3] [1 2 3] Length: 3 Capacity: 3 [1 5 3] [1 5 3] [1 2 3 4 5 42 7 8 9 10] [1 2 3 4 5 42 7 8 9 10] [4 5 42 7 8 9 10] [1 2 3 4 5 42] [4 5 42] [0 0 0] Length: 3 Capacity: 100 [] Length: 0 Capacity: 0 [1] Length: 1 Capacity: 1 [1 2 3 4 5 6] Length: 6 Capacity: 6 [1 2 3 4 5 6 2 3 4 5 6] Length: 11 Capacity: 12 [1 2 3 4 5] [1 2 3 4] [1 2 4 5] [1 2 4 5 5]","title":"Arrays and slices"},{"location":"Golang/05-arrays-and-slices/#arrays-and-slices","text":"Arrays Collection of items with same type Fixed size Declaration styles a := [3]int{1,2,3} a := [...]int{1,2,3} var a [3]int Access via zero based index a := [3]int {1, 3, 5} // a[1] == 3 len function returns size of array Copies refer to different underlying data Slices Backed by array Creation styles Slice an existing array or slice Literal style Via function make a := make([]int, 10) // create slice with capacity and length == 10 a := make([], 10, 100) // slice with length == 10 and capacity == 100 len function returns length of slice cap function returns length of underlying array append function to add elements to slice may cause expensive copy operations if underlying array is too small Copies refer to same underlying array package main import ( \"fmt\" ) func main() { // arrays grades := [3]int{97, 85, 93} fmt.Printf(\"Grades: %v\\n\", grades) // create array that is just large enough to hold the data grades2 := [...]int{97, 85, 93} fmt.Printf(\"Grades2: %v\\n\", grades2) var students [3]string fmt.Printf(\"Students %v\\n\", students) students[0] = \"Lisa\" fmt.Printf(\"Students %v\\n\", students) students[1] = \"Ahmed\" fmt.Printf(\"Students %v\\n\", students) students[2] = \"Arnold\" fmt.Printf(\"Students %v\\n\", students) fmt.Printf(\"Student #1: %v\\n\", students[0]) fmt.Printf(\"Student #2: %v\\n\", students[1]) fmt.Printf(\"Student #3: %v\\n\", students[2]) fmt.Printf(\"Number of Students: %v\\n\", len(students)) var identityMatrix [3][3]int = [3][3]int{ [3]int{1,0,0}, [3]int{0,1,0}, [3]int{0,0,1}, } fmt.Println(identityMatrix) var identityMatrix2 [3][3]int identityMatrix2[0] = [3]int{1,0,0} identityMatrix2[1] = [3]int{0,1,0} identityMatrix2[2] = [3]int{0,0,1} fmt.Println(identityMatrix2) // you create copies not point to the same data a := [...]int{1,2,3} b := a b[1] = 5 fmt.Println(a) // [1 2 3] fmt.Println(b) // [1 5 3] // slices s := []int{1,2,3} fmt.Println(s) fmt.Printf(\"Length: %v\\n\", len(s)) fmt.Printf(\"Capacity: %v\\n\", cap(s)) // slices do point to the same underlying data s1 := s s1[1] = 5 fmt.Println(s) // [1 5 3] fmt.Println(s1) // [1 5 3] s2 := [...]int{1,2,3,4,5,6,7,8,9,10} s3 := s2[:] // slice of all elements, [1 2 3 4 5 6 7 8 9 10] s4 := s2[3:] // slice from 4th element to end, [4 5 6 7 8 9 10] s5 := s2[:6] // slice first 6 elements, [1 2 3 4 5 6] s6 := s2[3:6] // slice the 4th, 5th, 6th elements, [4 5 6] s2[5] = 42 // now s2, s5, s6 has 6th element as 42 fmt.Println(s2) fmt.Println(s3) fmt.Println(s4) fmt.Println(s5) fmt.Println(s6) // make function ss := make([]int, 3, 100) // type, size fmt.Println(ss) // [0 0 0] fmt.Printf(\"Length: %v\\n\", len(ss)) // 3 fmt.Printf(\"Capacity: %v\\n\", cap(ss)) // 100 // sa := []int{} fmt.Println(sa) // [] fmt.Printf(\"Length: %v\\n\", len(sa)) // 0 fmt.Printf(\"Capacity: %v\\n\", cap(sa)) // 0 sa = append(sa, 1) fmt.Println(sa) // [1] fmt.Printf(\"Length: %v\\n\", len(sa)) // 1 fmt.Printf(\"Capacity: %v\\n\", cap(sa)) // 1 sa = append(sa, 2,3,4,5,6) fmt.Println(sa) // [1 2 3 4 5 6] fmt.Printf(\"Length: %v\\n\", len(sa)) // 6 fmt.Printf(\"Capacity: %v\\n\", cap(sa)) // 6 // concatenate slices sa = append(sa, []int{2,3,4,5,6}...) fmt.Println(sa) // [1 2 3 4 5 6 2 3 4 5 6] fmt.Printf(\"Length: %v\\n\", len(sa)) // 11 fmt.Printf(\"Capacity: %v\\n\", cap(sa)) // 12 // stack operations st := []int{1, 2, 3, 4, 5} fmt.Println(st) st1 := st[:len(st)-1] fmt.Println(st1) // remove elements from middle sm := append(st[:2], st[3:]...) fmt.Println(sm) // be careful when dealing with these situations // may cause unexpected results fmt.Println(st) // [1 2 3 4 5 5] } Grades: [97 85 93] Grades2: [97 85 93] Students [ ] Students [Lisa ] Students [Lisa Ahmed ] Students [Lisa Ahmed Arnold] Student #1: Lisa Student #2: Ahmed Student #3: Arnold Number of Students: 3 [[1 0 0] [0 1 0] [0 0 1]] [[1 0 0] [0 1 0] [0 0 1]] [1 2 3] [1 5 3] [1 2 3] Length: 3 Capacity: 3 [1 5 3] [1 5 3] [1 2 3 4 5 42 7 8 9 10] [1 2 3 4 5 42 7 8 9 10] [4 5 42 7 8 9 10] [1 2 3 4 5 42] [4 5 42] [0 0 0] Length: 3 Capacity: 100 [] Length: 0 Capacity: 0 [1] Length: 1 Capacity: 1 [1 2 3 4 5 6] Length: 6 Capacity: 6 [1 2 3 4 5 6 2 3 4 5 6] Length: 11 Capacity: 12 [1 2 3 4 5] [1 2 3 4] [1 2 4 5] [1 2 4 5 5]","title":"Arrays and slices"},{"location":"Golang/06-maps-and-structs/","text":"Maps and structs \u00b6 Maps Collections of value types that are acessed via keys Created via literals or via make function Members accessed via [key] syntax myMap[\"key\"] = \"value\" Check for presence with \"value, ok\" form of result Multiple assignments refer to same underlying data Structs Collections of disparate data types that describe a single concept Keyed by named fields Normally created as types, but anonymous structs are allowed Structs are value types No inheritance, but can use composition via embedding Tags can be added to struct fields to describe field package main import ( \"fmt\" \"reflect\" ) type Doctor struct { number int actorName string companions []string } func main() { // Maps statePopulations := map[string]int{ \"California\": 39250017, \"Texas\": 27862596, \"Florida\": 20612439, \"New York\": 19745289, \"Pennsylvania\": 12802503, \"Illinois\": 12801539, \"Ohio\": 11614373, } fmt.Println(statePopulations) // You can use arrays as keys m := map[[3]int]string{} fmt.Println(m) // make function stpop := make(map[string]int) stpop = map[string]int{\"California\": 39250017} fmt.Println(stpop) // fmt.Println(statePopulations[\"California\"]) statePopulations[\"Georgia\"] = 10310371 fmt.Println(statePopulations) delete(statePopulations, \"Georgia\") fmt.Println(statePopulations) fmt.Println(statePopulations[\"Georgia\"]) // 0 pop, ok := statePopulations[\"Georgia\"] fmt.Println(pop, ok) // 0, false // underlying data is passed by references sp := statePopulations delete(sp, \"Ohio\") fmt.Println(sp) fmt.Println(statePopulations) // structs aDoctor := Doctor { number: 3, actorName: \"Jon Pertwee\", companions: []string { \"Liz Shaw\", \"Jo Grant\", \"Sarah Jane Smith\", }, } fmt.Println(aDoctor) fmt.Println(aDoctor.companions) fmt.Println(aDoctor.companions[1]) // you can also initialize the struct without // providing field names (positional structs), but later on // if there are any changes in the code, this might break. // not recommended. bDoctor := Doctor{3, \"Jon Pertwee\", []string{\"1\", \"2\"}} fmt.Println(bDoctor) // anonymous structs doc := struct{name string}{name: \"John Pertwee\"} fmt.Println(doc) // unlike maps, we create copies anotherDoctor := doc anotherDoctor.name = \"Tom Baker\" fmt.Println(doc) fmt.Println(anotherDoctor) // If we like to to reference, we use a pointer yetAnotherDoc := &doc yetAnotherDoc.name = \"Thomas Baker\" fmt.Println(doc) fmt.Println(yetAnotherDoc) // Composition, similar concept to inheritance type Animal struct { Name string Origin string } type Bird struct { Animal SpeedKPH float32 CanFly bool } b := Bird{} b.Name = \"Emu\" b.Origin = \"Australia\" b.SpeedKPH = 48 b.CanFly = false fmt.Println(b) // {{Emu Australia} 48 false} fmt.Println(b.Name) // Emu c := Bird { Animal: Animal{Name: \"Emu\", Origin: \"Australia\"}, SpeedKPH: 48, CanFly: false, } fmt.Println(c) // tags, using reflect package type AnimalWithTags struct { Name string `required max:\"100\"` Origin string } t := reflect.TypeOf(AnimalWithTags{}) field, _ := t.FieldByName(\"Name\") fmt.Println(field.Tag) // required max:\"100\" } map[California:39250017 Florida:20612439 Illinois:12801539 New York:19745289 Ohio:11614373 Pennsylvania:12802503 Texas:27862596] map[] map[California:39250017] 39250017 map[California:39250017 Florida:20612439 Georgia:10310371 Illinois:12801539 New York:19745289 Ohio:11614373 Pennsylvania:12802503 Texas:27862596] map[California:39250017 Florida:20612439 Illinois:12801539 New York:19745289 Ohio:11614373 Pennsylvania:12802503 Texas:27862596] 0 0 false map[California:39250017 Florida:20612439 Illinois:12801539 New York:19745289 Pennsylvania:12802503 Texas:27862596] map[California:39250017 Florida:20612439 Illinois:12801539 New York:19745289 Pennsylvania:12802503 Texas:27862596] {3 Jon Pertwee [Liz Shaw Jo Grant Sarah Jane Smith]} [Liz Shaw Jo Grant Sarah Jane Smith] Jo Grant {3 Jon Pertwee [1 2]} {John Pertwee} {John Pertwee} {Tom Baker} {Thomas Baker} &{Thomas Baker} {{Emu Australia} 48 false} Emu {{Emu Australia} 48 false} required max:\"100\"","title":"Maps and structs"},{"location":"Golang/06-maps-and-structs/#maps-and-structs","text":"Maps Collections of value types that are acessed via keys Created via literals or via make function Members accessed via [key] syntax myMap[\"key\"] = \"value\" Check for presence with \"value, ok\" form of result Multiple assignments refer to same underlying data Structs Collections of disparate data types that describe a single concept Keyed by named fields Normally created as types, but anonymous structs are allowed Structs are value types No inheritance, but can use composition via embedding Tags can be added to struct fields to describe field package main import ( \"fmt\" \"reflect\" ) type Doctor struct { number int actorName string companions []string } func main() { // Maps statePopulations := map[string]int{ \"California\": 39250017, \"Texas\": 27862596, \"Florida\": 20612439, \"New York\": 19745289, \"Pennsylvania\": 12802503, \"Illinois\": 12801539, \"Ohio\": 11614373, } fmt.Println(statePopulations) // You can use arrays as keys m := map[[3]int]string{} fmt.Println(m) // make function stpop := make(map[string]int) stpop = map[string]int{\"California\": 39250017} fmt.Println(stpop) // fmt.Println(statePopulations[\"California\"]) statePopulations[\"Georgia\"] = 10310371 fmt.Println(statePopulations) delete(statePopulations, \"Georgia\") fmt.Println(statePopulations) fmt.Println(statePopulations[\"Georgia\"]) // 0 pop, ok := statePopulations[\"Georgia\"] fmt.Println(pop, ok) // 0, false // underlying data is passed by references sp := statePopulations delete(sp, \"Ohio\") fmt.Println(sp) fmt.Println(statePopulations) // structs aDoctor := Doctor { number: 3, actorName: \"Jon Pertwee\", companions: []string { \"Liz Shaw\", \"Jo Grant\", \"Sarah Jane Smith\", }, } fmt.Println(aDoctor) fmt.Println(aDoctor.companions) fmt.Println(aDoctor.companions[1]) // you can also initialize the struct without // providing field names (positional structs), but later on // if there are any changes in the code, this might break. // not recommended. bDoctor := Doctor{3, \"Jon Pertwee\", []string{\"1\", \"2\"}} fmt.Println(bDoctor) // anonymous structs doc := struct{name string}{name: \"John Pertwee\"} fmt.Println(doc) // unlike maps, we create copies anotherDoctor := doc anotherDoctor.name = \"Tom Baker\" fmt.Println(doc) fmt.Println(anotherDoctor) // If we like to to reference, we use a pointer yetAnotherDoc := &doc yetAnotherDoc.name = \"Thomas Baker\" fmt.Println(doc) fmt.Println(yetAnotherDoc) // Composition, similar concept to inheritance type Animal struct { Name string Origin string } type Bird struct { Animal SpeedKPH float32 CanFly bool } b := Bird{} b.Name = \"Emu\" b.Origin = \"Australia\" b.SpeedKPH = 48 b.CanFly = false fmt.Println(b) // {{Emu Australia} 48 false} fmt.Println(b.Name) // Emu c := Bird { Animal: Animal{Name: \"Emu\", Origin: \"Australia\"}, SpeedKPH: 48, CanFly: false, } fmt.Println(c) // tags, using reflect package type AnimalWithTags struct { Name string `required max:\"100\"` Origin string } t := reflect.TypeOf(AnimalWithTags{}) field, _ := t.FieldByName(\"Name\") fmt.Println(field.Tag) // required max:\"100\" } map[California:39250017 Florida:20612439 Illinois:12801539 New York:19745289 Ohio:11614373 Pennsylvania:12802503 Texas:27862596] map[] map[California:39250017] 39250017 map[California:39250017 Florida:20612439 Georgia:10310371 Illinois:12801539 New York:19745289 Ohio:11614373 Pennsylvania:12802503 Texas:27862596] map[California:39250017 Florida:20612439 Illinois:12801539 New York:19745289 Ohio:11614373 Pennsylvania:12802503 Texas:27862596] 0 0 false map[California:39250017 Florida:20612439 Illinois:12801539 New York:19745289 Pennsylvania:12802503 Texas:27862596] map[California:39250017 Florida:20612439 Illinois:12801539 New York:19745289 Pennsylvania:12802503 Texas:27862596] {3 Jon Pertwee [Liz Shaw Jo Grant Sarah Jane Smith]} [Liz Shaw Jo Grant Sarah Jane Smith] Jo Grant {3 Jon Pertwee [1 2]} {John Pertwee} {John Pertwee} {Tom Baker} {Thomas Baker} &{Thomas Baker} {{Emu Australia} 48 false} Emu {{Emu Australia} 48 false} required max:\"100\"","title":"Maps and structs"},{"location":"Golang/07-if-and-switch-statements/","text":"If and Switch statements \u00b6 If statements initializer comparison operators logical operators short circuiting if - else statements if - else if statements equality and floats switch statements switching on a tag cases with multiple tests initializers switches with no tags Fallthrough Type switches Breaking out early package main import ( \"fmt\" \"math\" ) func main() { // if statements if true { fmt.Println(\"The test is true\") } if false { fmt.Println(\"The test is false\") } statePopulations := map[string]int{\"California\": 39250017} if pop, ok := statePopulations[\"California\"]; ok { fmt.Println(pop) } if _, ok := statePopulations[\"Ohio\"]; ok { fmt.Println(\"...\") } if true || false { fmt.Println(\"...\") } if true && true { fmt.Println(\"...\") } myNum := 0.123456789 if math.Abs(myNum / math.Pow(math.Sqrt(myNum), 2) - 1) < 0.001 { fmt.Println(\"These are the same\") } else { fmt.Println(\"These are different\") } // switch statements switch i := 2+3; i { case 1, 5, 10: fmt.Println(\"one, five or ten\") case 2, 4, 6: fmt.Println(\"two, four or six\") default: fmt.Println(\"another number\") } i := 10 switch { case i <= 10: fmt.Println(\"less than or equal to ten\") fallthrough // execute the statements in the next case as well case i >= 20: fmt.Println(\"less than or equal to twenty\") default: fmt.Println(\"greater than twenty\") } // type switch var j interface{} = 1 // can take any type of data switch j.(type) { case int: fmt.Println(\"j is an int\") break fmt.Println(\"This this wont print\") case float64: fmt.Println(\"j is a float64\") case string: fmt.Println(\"j is a string\") default: fmt.Println(\"j is another type\") } } The test is true 39250017 ... ... These are the same one, five or ten less than or equal to ten less than or equal to twenty j is an int","title":"If and Switch statements"},{"location":"Golang/07-if-and-switch-statements/#if-and-switch-statements","text":"If statements initializer comparison operators logical operators short circuiting if - else statements if - else if statements equality and floats switch statements switching on a tag cases with multiple tests initializers switches with no tags Fallthrough Type switches Breaking out early package main import ( \"fmt\" \"math\" ) func main() { // if statements if true { fmt.Println(\"The test is true\") } if false { fmt.Println(\"The test is false\") } statePopulations := map[string]int{\"California\": 39250017} if pop, ok := statePopulations[\"California\"]; ok { fmt.Println(pop) } if _, ok := statePopulations[\"Ohio\"]; ok { fmt.Println(\"...\") } if true || false { fmt.Println(\"...\") } if true && true { fmt.Println(\"...\") } myNum := 0.123456789 if math.Abs(myNum / math.Pow(math.Sqrt(myNum), 2) - 1) < 0.001 { fmt.Println(\"These are the same\") } else { fmt.Println(\"These are different\") } // switch statements switch i := 2+3; i { case 1, 5, 10: fmt.Println(\"one, five or ten\") case 2, 4, 6: fmt.Println(\"two, four or six\") default: fmt.Println(\"another number\") } i := 10 switch { case i <= 10: fmt.Println(\"less than or equal to ten\") fallthrough // execute the statements in the next case as well case i >= 20: fmt.Println(\"less than or equal to twenty\") default: fmt.Println(\"greater than twenty\") } // type switch var j interface{} = 1 // can take any type of data switch j.(type) { case int: fmt.Println(\"j is an int\") break fmt.Println(\"This this wont print\") case float64: fmt.Println(\"j is a float64\") case string: fmt.Println(\"j is a string\") default: fmt.Println(\"j is another type\") } } The test is true 39250017 ... ... These are the same one, five or ten less than or equal to ten less than or equal to twenty j is an int","title":"If and Switch statements"},{"location":"Golang/08-looping/","text":"Looping \u00b6 For statements Simple loops for initializer; test; incrementer {} for test {} for {} Exiting early break continue labels Looping over collections arrays, slices, maps, strings, channels for k, v := range collection {} package main import \"fmt\" func main() { // for loops for i := 0; i < 5; i++ { fmt.Println(i) } for i,j := 0, 0; i < 5; i, j = i+1, j+2 { fmt.Println(i, j) } i := 0 for ; i < 5; i++ { // ... } fmt.Println(i) // break for { fmt.Println(i) i++ if i == 10 { break } } // continue for i := 0; i < 10; i++ { if i % 2 == 0 { continue } fmt.Println(i) } // Loop: for i := 1; i <= 3; i++ { for j := 1; j <= 3; j ++ { fmt.Println(i * j) if i * j >= 3 { break Loop // break out of both loops at once } } } // collections with loops s := []int{1,2,3} for key, value := range s { fmt.Println(key, value) } statePopulations := map[string]int{ \"California\": 39250017, \"Texas\": 27862596, \"Florida\": 20612439, \"New York\": 19745289, \"Pennsylvania\": 12802503, \"Illinois\": 12801539, \"Ohio\": 11614373, } for k, v := range statePopulations { fmt.Println(k, v) } for k := range statePopulations { fmt.Println(k) } for _, v := range statePopulations { fmt.Println(v) } st := \"Hello Go!\" for k, v := range st { fmt.Println(k, v, string(v)) } } 0 1 2 3 4 0 0 1 2 2 4 3 6 4 8 5 5 6 7 8 9 1 3 5 7 9 1 2 3 0 1 1 2 2 3 New York 19745289 Pennsylvania 12802503 Illinois 12801539 Ohio 11614373 California 39250017 Texas 27862596 Florida 20612439 California Texas Florida New York Pennsylvania Illinois Ohio 39250017 27862596 20612439 19745289 12802503 12801539 11614373 0 72 H 1 101 e 2 108 l 3 108 l 4 111 o 5 32 6 71 G 7 111 o 8 33 !","title":"Looping"},{"location":"Golang/08-looping/#looping","text":"For statements Simple loops for initializer; test; incrementer {} for test {} for {} Exiting early break continue labels Looping over collections arrays, slices, maps, strings, channels for k, v := range collection {} package main import \"fmt\" func main() { // for loops for i := 0; i < 5; i++ { fmt.Println(i) } for i,j := 0, 0; i < 5; i, j = i+1, j+2 { fmt.Println(i, j) } i := 0 for ; i < 5; i++ { // ... } fmt.Println(i) // break for { fmt.Println(i) i++ if i == 10 { break } } // continue for i := 0; i < 10; i++ { if i % 2 == 0 { continue } fmt.Println(i) } // Loop: for i := 1; i <= 3; i++ { for j := 1; j <= 3; j ++ { fmt.Println(i * j) if i * j >= 3 { break Loop // break out of both loops at once } } } // collections with loops s := []int{1,2,3} for key, value := range s { fmt.Println(key, value) } statePopulations := map[string]int{ \"California\": 39250017, \"Texas\": 27862596, \"Florida\": 20612439, \"New York\": 19745289, \"Pennsylvania\": 12802503, \"Illinois\": 12801539, \"Ohio\": 11614373, } for k, v := range statePopulations { fmt.Println(k, v) } for k := range statePopulations { fmt.Println(k) } for _, v := range statePopulations { fmt.Println(v) } st := \"Hello Go!\" for k, v := range st { fmt.Println(k, v, string(v)) } } 0 1 2 3 4 0 0 1 2 2 4 3 6 4 8 5 5 6 7 8 9 1 3 5 7 9 1 2 3 0 1 1 2 2 3 New York 19745289 Pennsylvania 12802503 Illinois 12801539 Ohio 11614373 California 39250017 Texas 27862596 Florida 20612439 California Texas Florida New York Pennsylvania Illinois Ohio 39250017 27862596 20612439 19745289 12802503 12801539 11614373 0 72 H 1 101 e 2 108 l 3 108 l 4 111 o 5 32 6 71 G 7 111 o 8 33 !","title":"Looping"},{"location":"Golang/09-defer-panic-recover/","text":"Defer, Panic and Recover \u00b6 Defer Used to delay execution of an assignment until function exits Useful to group \"open\" and \"close\" functions together Be careful in loops Run in LIFO (last in, first out) order Arguments evaluated at time defer is executed, not at time of called function execution Panic Occur when programm cannot continue at all Don't use when file can't be opened, unless it's critical Use for unrecoverable events - cannot obtain TCP port for web server Function will stop executing Deffered functions will still fire If nothging handles panic, program will exit Recover Used to recover from panics Only useful in deffered functions The current function will not attemt to continue, but higher functions in call stack will package main import ( \"fmt\" \"io/ioutil\" \"log\" \"net/http\" ) func main() { fmt.Println(\"start\") defer fmt.Println(\"middle\") // execute at the end of the function, before return fmt.Println(\"end\") // start // end // middle defer fmt.Println(\"start\") defer fmt.Println(\"middle\") defer fmt.Println(\"end\") // Executes in LIFO: // end // middle // start res, err := http.Get(\"http://www.google.com/robots.txt\") defer res.Body.Close() if err != nil { log.Fatal(err) } robots, err := ioutil.ReadAll(res.Body) if err != nil { log.Fatal(err) } fmt.Printf(\"%s\\n\", robots[0:100]) // defer func() { if err := recover(); err != nil { log.Println(\"Error: \", err) } }() a, b := 1, 0 ans := a / b fmt.Println(ans) // will not execute this because 1 / 0 threw a panic, stopped execution panic(\"Something bad happened!\") } start end User-agent: * Disallow: /search Allow: /search/about Allow: /search/static Allow: /search/howsearchw 2019/12/25 13:44:16 Error: runtime error: integer divide by zero end middle start middle","title":"Defer, Panic and Recover"},{"location":"Golang/09-defer-panic-recover/#defer-panic-and-recover","text":"Defer Used to delay execution of an assignment until function exits Useful to group \"open\" and \"close\" functions together Be careful in loops Run in LIFO (last in, first out) order Arguments evaluated at time defer is executed, not at time of called function execution Panic Occur when programm cannot continue at all Don't use when file can't be opened, unless it's critical Use for unrecoverable events - cannot obtain TCP port for web server Function will stop executing Deffered functions will still fire If nothging handles panic, program will exit Recover Used to recover from panics Only useful in deffered functions The current function will not attemt to continue, but higher functions in call stack will package main import ( \"fmt\" \"io/ioutil\" \"log\" \"net/http\" ) func main() { fmt.Println(\"start\") defer fmt.Println(\"middle\") // execute at the end of the function, before return fmt.Println(\"end\") // start // end // middle defer fmt.Println(\"start\") defer fmt.Println(\"middle\") defer fmt.Println(\"end\") // Executes in LIFO: // end // middle // start res, err := http.Get(\"http://www.google.com/robots.txt\") defer res.Body.Close() if err != nil { log.Fatal(err) } robots, err := ioutil.ReadAll(res.Body) if err != nil { log.Fatal(err) } fmt.Printf(\"%s\\n\", robots[0:100]) // defer func() { if err := recover(); err != nil { log.Println(\"Error: \", err) } }() a, b := 1, 0 ans := a / b fmt.Println(ans) // will not execute this because 1 / 0 threw a panic, stopped execution panic(\"Something bad happened!\") } start end User-agent: * Disallow: /search Allow: /search/about Allow: /search/static Allow: /search/howsearchw 2019/12/25 13:44:16 Error: runtime error: integer divide by zero end middle start middle","title":"Defer, Panic and Recover"},{"location":"Golang/10-pointers/","text":"Pointers \u00b6 Creating [[Pointers]] Pointer types use an asterisk (*) as a prefix to type pointed to *int a pointer to an integer Use the addressof operator (&) to get an address of variable Dereferencing pointers dereference a pointer by preceding with an asterisk (*) Complex types (e.g.) structs are automatically dereferenced Create pointers to objects Can use the addressof operator (&) if value type already exists ms := myStruct{foo: 42} p := &ms Use addressof operator before initializer &myStruct{foo: 42} Use the new keyword Can't initialize fields at the same time Types with internal pointers All assignment operations in Go are copy operations Slices and maps contain internal pointers, so copies point to same underlying data package main import ( \"fmt\" ) func main() { var a int = 42 var b *int = &a a = 27 fmt.Println(a) // 27 fmt.Println(&a) // 0xc0000180d8 (reference) fmt.Println(b) // 0xc0000180d8 fmt.Println(a) // 27 fmt.Println(*b) // 27 (dereference) *b = 14 fmt.Println(a, *b) // 14 14 c := [3]int{1,2,3} d := &c[0] e := &c[1] fmt.Println(\"%v %p %p\\n\", c, d, e) // for pointer arithmetic, we can lookup `unsafe` package var ms *myStruct ms = &myStruct{foo: 42} fmt.Println(ms) var mz *myStruct fmt.Println(mz) // nil mz = new(myStruct) fmt.Println(mz) // 0 var mx *myStruct mx = new(myStruct) (*mx).foo = 42 fmt.Println((*mx).foo) // 42 fmt.Println(mx.foo) // 42 } type myStruct struct { foo int } 27 0xc0000180d8 0xc0000180d8 27 27 14 14 %v %p %p [1 2 3] 0xc000014180 0xc000014188 &{42} <nil> &{0} 42 42","title":"Pointers"},{"location":"Golang/10-pointers/#pointers","text":"Creating [[Pointers]] Pointer types use an asterisk (*) as a prefix to type pointed to *int a pointer to an integer Use the addressof operator (&) to get an address of variable Dereferencing pointers dereference a pointer by preceding with an asterisk (*) Complex types (e.g.) structs are automatically dereferenced Create pointers to objects Can use the addressof operator (&) if value type already exists ms := myStruct{foo: 42} p := &ms Use addressof operator before initializer &myStruct{foo: 42} Use the new keyword Can't initialize fields at the same time Types with internal pointers All assignment operations in Go are copy operations Slices and maps contain internal pointers, so copies point to same underlying data package main import ( \"fmt\" ) func main() { var a int = 42 var b *int = &a a = 27 fmt.Println(a) // 27 fmt.Println(&a) // 0xc0000180d8 (reference) fmt.Println(b) // 0xc0000180d8 fmt.Println(a) // 27 fmt.Println(*b) // 27 (dereference) *b = 14 fmt.Println(a, *b) // 14 14 c := [3]int{1,2,3} d := &c[0] e := &c[1] fmt.Println(\"%v %p %p\\n\", c, d, e) // for pointer arithmetic, we can lookup `unsafe` package var ms *myStruct ms = &myStruct{foo: 42} fmt.Println(ms) var mz *myStruct fmt.Println(mz) // nil mz = new(myStruct) fmt.Println(mz) // 0 var mx *myStruct mx = new(myStruct) (*mx).foo = 42 fmt.Println((*mx).foo) // 42 fmt.Println(mx.foo) // 42 } type myStruct struct { foo int } 27 0xc0000180d8 0xc0000180d8 27 27 14 14 %v %p %p [1 2 3] 0xc000014180 0xc000014188 &{42} <nil> &{0} 42 42","title":"Pointers"},{"location":"Golang/11-functions/","text":"Functions \u00b6 Basic syntax func foo() { ... } Parameters Comma delimited list of variables and types func foo(bar string, baz int) Parameters of same type list type once func foo(bar, baz int) When pointers are passed in, the function can change the value in the caller This is always true for data of slices and maps Use variadic parameters to send list of same types in Must be last parameter Received as a slice func foo (bar string, baz ...int) Return values Single return values just list type func foo() int multiple return value list types surrounded by parenttheses func foo() (int, error) The (result type, error) paradigm is a very common idiom Can use named return values Initializes returned variable Return using return keyword on it's own Can return addresses of local variables Automatically promoted from local memory (stack) to shared memory (heap) Anonymous functions Functions don't have names if they are: Immediately invoked func() {...}() Assigned to a variable or passed as an argument to function a := func() {...} a() Functions as types Can assign functions to variables or use as arguments and return values in functions Type signature is like function signature, with no paramter names var f func(string, string, int) (int, error) Methods Function that executes in context of a type Format func (g greeter) greet() { ... } Receiver can be value or pointer Value receiver gets copy of type Pointer receiver gets pointer to type package main import ( \"fmt\" ) func sayMessage(msg string, idx int) { fmt.Println(msg, idx) msg = \"123\" } func sayGreeting(greeting *string, name *string) { fmt.Println(*greeting, *name) // Hello Stacey *name = \"Ted\" fmt.Println(*name) // Ted } func sum(values ...int) (result int) { fmt.Println(values) for _, v := range values { result += v } return } func divide(a, b float64) (float64, error) { if b == 0.0 { return 0.0, fmt.Errorf(\"Cannot divide by zero\") } return a / b, nil } func main() { var message string = \"Hello Go!\" for i := 0; i < 5; i++ { sayMessage(message, i) } fmt.Println(message) // Hello Go!, not 123 // with pointers greeting := \"Hello\" name := \"Stacey\" sayGreeting(&greeting, &name) fmt.Println(name) // Ted // variadic parameters s := sum(1, 2, 3, 4, 5) fmt.Println(\"The sum is \", s) // d, err := divide(5.0, 0.0) if err != nil { fmt.Println(err) } fmt.Println(d) // anonymous functions func() { fmt.Println(\"Hello Anonymous Go!\") }() // these () invoke the function var f func() = func() { fmt.Println(\"Hello Go!\") } f() var divide func(float64, float64) (float64, error) divide = func(a, b float64) (float64, error) { // ... return 0, nil } divide(0.0, 0.0) // methods g := greeter { greeting: \"Hello\", name: \"Go\", } g.greet() fmt.Println(\"The new name is\", g.name) g.greets() fmt.Println(\"The new name is\", g.name) } type greeter struct { greeting string name string } func (g greeter) greet() { fmt.Println(g.greeting, g.name) g.name = \"not Go\" } func (g *greeter) greets() { fmt.Println(g.greeting, g.name) g.name = \"not Go\" } Hello Go! 0 Hello Go! 1 Hello Go! 2 Hello Go! 3 Hello Go! 4 Hello Go! Hello Stacey Ted Ted [1 2 3 4 5] The sum is 15 Cannot divide by zero 0 Hello Anonymous Go! Hello Go! Hello Go The new name is Go Hello Go The new name is not Go","title":"Functions"},{"location":"Golang/11-functions/#functions","text":"Basic syntax func foo() { ... } Parameters Comma delimited list of variables and types func foo(bar string, baz int) Parameters of same type list type once func foo(bar, baz int) When pointers are passed in, the function can change the value in the caller This is always true for data of slices and maps Use variadic parameters to send list of same types in Must be last parameter Received as a slice func foo (bar string, baz ...int) Return values Single return values just list type func foo() int multiple return value list types surrounded by parenttheses func foo() (int, error) The (result type, error) paradigm is a very common idiom Can use named return values Initializes returned variable Return using return keyword on it's own Can return addresses of local variables Automatically promoted from local memory (stack) to shared memory (heap) Anonymous functions Functions don't have names if they are: Immediately invoked func() {...}() Assigned to a variable or passed as an argument to function a := func() {...} a() Functions as types Can assign functions to variables or use as arguments and return values in functions Type signature is like function signature, with no paramter names var f func(string, string, int) (int, error) Methods Function that executes in context of a type Format func (g greeter) greet() { ... } Receiver can be value or pointer Value receiver gets copy of type Pointer receiver gets pointer to type package main import ( \"fmt\" ) func sayMessage(msg string, idx int) { fmt.Println(msg, idx) msg = \"123\" } func sayGreeting(greeting *string, name *string) { fmt.Println(*greeting, *name) // Hello Stacey *name = \"Ted\" fmt.Println(*name) // Ted } func sum(values ...int) (result int) { fmt.Println(values) for _, v := range values { result += v } return } func divide(a, b float64) (float64, error) { if b == 0.0 { return 0.0, fmt.Errorf(\"Cannot divide by zero\") } return a / b, nil } func main() { var message string = \"Hello Go!\" for i := 0; i < 5; i++ { sayMessage(message, i) } fmt.Println(message) // Hello Go!, not 123 // with pointers greeting := \"Hello\" name := \"Stacey\" sayGreeting(&greeting, &name) fmt.Println(name) // Ted // variadic parameters s := sum(1, 2, 3, 4, 5) fmt.Println(\"The sum is \", s) // d, err := divide(5.0, 0.0) if err != nil { fmt.Println(err) } fmt.Println(d) // anonymous functions func() { fmt.Println(\"Hello Anonymous Go!\") }() // these () invoke the function var f func() = func() { fmt.Println(\"Hello Go!\") } f() var divide func(float64, float64) (float64, error) divide = func(a, b float64) (float64, error) { // ... return 0, nil } divide(0.0, 0.0) // methods g := greeter { greeting: \"Hello\", name: \"Go\", } g.greet() fmt.Println(\"The new name is\", g.name) g.greets() fmt.Println(\"The new name is\", g.name) } type greeter struct { greeting string name string } func (g greeter) greet() { fmt.Println(g.greeting, g.name) g.name = \"not Go\" } func (g *greeter) greets() { fmt.Println(g.greeting, g.name) g.name = \"not Go\" } Hello Go! 0 Hello Go! 1 Hello Go! 2 Hello Go! 3 Hello Go! 4 Hello Go! Hello Stacey Ted Ted [1 2 3 4 5] The sum is 15 Cannot divide by zero 0 Hello Anonymous Go! Hello Go! Hello Go The new name is Go Hello Go The new name is not Go","title":"Functions"},{"location":"Golang/12-interfaces/","text":"Interfaces \u00b6 Basics ```go type Writer interface { Write([]byte) (int, error) } type ConsoleWriter struct {} func (cw ConsoleWriter) Write(data []byte) (int, error) { n, err := fmt.Println(string(data)) return n, err } - Composing interfaces go type Writer interface { Write([]byte) (int, error) } type Closer interface { Close() error } type WriterCloser interface { Writer Closer } - Type conversion go var wc WriterCloser = NewBufferedWriterCloser() bwc := wc.(*BufferedWriterCloser) - The empty interface and type switches go var i interface{} = 0 switch i.(type) { case int: ... case string: ... default: ... } `` - Implementing with values vs pointers - Method set of a value is all methods with value receivers - Method set of a pointer` is all methods, regardless of receiver type - Best Practices - Use many, small interfaces vs large monolythic ones - io.Writer, io.Reader, interface{} - Don't export interfaces for types that will be consumed - Do export interfaces for types that will be used by package - Design functions and methods to receive interfaces whenever possible package main import ( \"bytes\" \"fmt\" ) func main() { var w Writer = ConsoleWriter{} w.Write([]byte(\"Hello Go!\")) myInt := IntCounter(0) var inc Incrementer = &myInt for i := 0; i < 10; i++ { fmt.Println(inc.Inrement()) } var wc WriterCloser = NewBufferedWriterCloser() wc.Write([]byte(\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer efficitur consequat facilisis. Sed iaculis metus.\")) wc.Close() } type Writer interface { Write([]byte) (int, error) } type ConsoleWriter struct {} func (cw ConsoleWriter) Write(data []byte) (int, error) { n, err := fmt.Println(string(data)) return n, err } // type Incrementer interface { Inrement() int } type IntCounter int func (ic *IntCounter) Inrement() int { *ic++ return int(*ic) } // type Closer interface { Close() error } type WriterCloser interface { Writer Closer } type BufferedWriterCloser struct { buffer *bytes.Buffer } func (bwc *BufferedWriterCloser) Write(data[]byte) (int, error) { n, err := bwc.buffer.Write(data) if err != nil { return 0, err } v := make([]byte, 8) for bwc.buffer.Len() > 8 { _, err := bwc.buffer.Read(v) if err != nil { return 0, err } _, err = fmt.Println(string(v)) if err != nil { return 0, err } } return n, nil } func (bwc *BufferedWriterCloser) Close() error { for bwc.buffer.Len() > 0 { data := bwc.buffer.Next(8) _, err := fmt.Println(string(data)) if err != nil { return err } } return nil } func NewBufferedWriterCloser() *BufferedWriterCloser { return &BufferedWriterCloser{ buffer: bytes.NewBuffer([]byte{}), } } Hello Go! 1 2 3 4 5 6 7 8 9 10 Lorem ip sum dolo r sit am et, cons ectetur adipisci ng elit. Integer efficit ur conse quat fac ilisis. Sed iacu lis metu s.","title":"Interfaces"},{"location":"Golang/12-interfaces/#interfaces","text":"Basics ```go type Writer interface { Write([]byte) (int, error) } type ConsoleWriter struct {} func (cw ConsoleWriter) Write(data []byte) (int, error) { n, err := fmt.Println(string(data)) return n, err } - Composing interfaces go type Writer interface { Write([]byte) (int, error) } type Closer interface { Close() error } type WriterCloser interface { Writer Closer } - Type conversion go var wc WriterCloser = NewBufferedWriterCloser() bwc := wc.(*BufferedWriterCloser) - The empty interface and type switches go var i interface{} = 0 switch i.(type) { case int: ... case string: ... default: ... } `` - Implementing with values vs pointers - Method set of a value is all methods with value receivers - Method set of a pointer` is all methods, regardless of receiver type - Best Practices - Use many, small interfaces vs large monolythic ones - io.Writer, io.Reader, interface{} - Don't export interfaces for types that will be consumed - Do export interfaces for types that will be used by package - Design functions and methods to receive interfaces whenever possible package main import ( \"bytes\" \"fmt\" ) func main() { var w Writer = ConsoleWriter{} w.Write([]byte(\"Hello Go!\")) myInt := IntCounter(0) var inc Incrementer = &myInt for i := 0; i < 10; i++ { fmt.Println(inc.Inrement()) } var wc WriterCloser = NewBufferedWriterCloser() wc.Write([]byte(\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer efficitur consequat facilisis. Sed iaculis metus.\")) wc.Close() } type Writer interface { Write([]byte) (int, error) } type ConsoleWriter struct {} func (cw ConsoleWriter) Write(data []byte) (int, error) { n, err := fmt.Println(string(data)) return n, err } // type Incrementer interface { Inrement() int } type IntCounter int func (ic *IntCounter) Inrement() int { *ic++ return int(*ic) } // type Closer interface { Close() error } type WriterCloser interface { Writer Closer } type BufferedWriterCloser struct { buffer *bytes.Buffer } func (bwc *BufferedWriterCloser) Write(data[]byte) (int, error) { n, err := bwc.buffer.Write(data) if err != nil { return 0, err } v := make([]byte, 8) for bwc.buffer.Len() > 8 { _, err := bwc.buffer.Read(v) if err != nil { return 0, err } _, err = fmt.Println(string(v)) if err != nil { return 0, err } } return n, nil } func (bwc *BufferedWriterCloser) Close() error { for bwc.buffer.Len() > 0 { data := bwc.buffer.Next(8) _, err := fmt.Println(string(data)) if err != nil { return err } } return nil } func NewBufferedWriterCloser() *BufferedWriterCloser { return &BufferedWriterCloser{ buffer: bytes.NewBuffer([]byte{}), } } Hello Go! 1 2 3 4 5 6 7 8 9 10 Lorem ip sum dolo r sit am et, cons ectetur adipisci ng elit. Integer efficit ur conse quat fac ilisis. Sed iacu lis metu s.","title":"Interfaces"},{"location":"Golang/13-goroutines/","text":"Goroutines \u00b6 Creating goroutines Use go keyword in front of function call When using anonymous functions, pass data as local variables Synchronization Use sync.WaitGroup to wait for groups of goroutines to complete Use sync.Mutex and sync.RWMutes to protect data access Parallelism By default, Go will use CPU threads equal to available cores Change with runtime.GOMAXPROCS More threads can increase performance, but too many can slow it down Best Practices Don't create goroutines in libraries Let consumer control concurrency When creating a goroutine, know how it will end Avoids subtle memory leaks Check for race conditions at compile time ``` \u279c daviskregers git:(master) \u2717 go run -race ./13_goroutines/Main.go Hello #0 ================== WARNING: DATA RACE Write at 0x0000005fe178 by goroutine 8: main.increment() /home/davis/projects/learning/golang/src/github.com/daviskregers/13_goroutines/Main.go:46 +0x5a Previous read at 0x0000005fe178 by goroutine 7: main.sayHello() /home/davis/projects/learning/golang/src/github.com/daviskregers/13_goroutines/Main.go:35 +0x3e Goroutine 8 (running) created at: main.main() /home/davis/projects/learning/golang/src/github.com/daviskregers/13_goroutines/Main.go:17 +0x80 Goroutine 7 (finished) created at: main.main() /home/davis/projects/learning/golang/src/github.com/daviskregers/13_goroutines/Main.go:16 +0x68 ================== Hello #1 Hello #2 Hello #3 Hello #4 Hello #5 Hello #6 Hello #7 Hello #8 Hello #9 Threads: 8 Hello #10 Hello #11 Hello #12 Hello #13 Hello #14 Hello #15 Hello #16 Hello #17 Hello #18 Hello #19 Found 1 data race(s) exit status 66 ``` package main import ( \"fmt\" \"runtime\" \"sync\" ) var wg = sync.WaitGroup{} var counter = 0 var m = sync.RWMutex{} func main() { for i := 0; i < 10; i++ { wg.Add(2) go sayHello() go increment() } wg.Wait() // fmt.Printf(\"Threads: %v\\n\", runtime.GOMAXPROCS(-1)) for i := 0; i < 10; i++ { wg.Add(2) m.RLock() go sayHelloM() m.Lock() go incrementM() } wg.Wait() } func sayHello() { fmt.Printf(\"Hello #%v\\n\", counter) wg.Done() } func sayHelloM() { fmt.Printf(\"Hello #%v\\n\", counter) m.RUnlock() wg.Done() } func increment() { counter++ wg.Done() } func incrementM() { counter++ m.Unlock() wg.Done() } Hello #2 Hello #4 Hello #0 Hello #2 Hello #5 Hello #5 Hello #7 Hello #8 Hello #8 Hello #9 Threads: 8 Hello #10 Hello #11 Hello #12 Hello #13 Hello #14 Hello #15 Hello #16 Hello #17 Hello #18 Hello #19","title":"Goroutines"},{"location":"Golang/13-goroutines/#goroutines","text":"Creating goroutines Use go keyword in front of function call When using anonymous functions, pass data as local variables Synchronization Use sync.WaitGroup to wait for groups of goroutines to complete Use sync.Mutex and sync.RWMutes to protect data access Parallelism By default, Go will use CPU threads equal to available cores Change with runtime.GOMAXPROCS More threads can increase performance, but too many can slow it down Best Practices Don't create goroutines in libraries Let consumer control concurrency When creating a goroutine, know how it will end Avoids subtle memory leaks Check for race conditions at compile time ``` \u279c daviskregers git:(master) \u2717 go run -race ./13_goroutines/Main.go Hello #0 ================== WARNING: DATA RACE Write at 0x0000005fe178 by goroutine 8: main.increment() /home/davis/projects/learning/golang/src/github.com/daviskregers/13_goroutines/Main.go:46 +0x5a Previous read at 0x0000005fe178 by goroutine 7: main.sayHello() /home/davis/projects/learning/golang/src/github.com/daviskregers/13_goroutines/Main.go:35 +0x3e Goroutine 8 (running) created at: main.main() /home/davis/projects/learning/golang/src/github.com/daviskregers/13_goroutines/Main.go:17 +0x80 Goroutine 7 (finished) created at: main.main() /home/davis/projects/learning/golang/src/github.com/daviskregers/13_goroutines/Main.go:16 +0x68 ================== Hello #1 Hello #2 Hello #3 Hello #4 Hello #5 Hello #6 Hello #7 Hello #8 Hello #9 Threads: 8 Hello #10 Hello #11 Hello #12 Hello #13 Hello #14 Hello #15 Hello #16 Hello #17 Hello #18 Hello #19 Found 1 data race(s) exit status 66 ``` package main import ( \"fmt\" \"runtime\" \"sync\" ) var wg = sync.WaitGroup{} var counter = 0 var m = sync.RWMutex{} func main() { for i := 0; i < 10; i++ { wg.Add(2) go sayHello() go increment() } wg.Wait() // fmt.Printf(\"Threads: %v\\n\", runtime.GOMAXPROCS(-1)) for i := 0; i < 10; i++ { wg.Add(2) m.RLock() go sayHelloM() m.Lock() go incrementM() } wg.Wait() } func sayHello() { fmt.Printf(\"Hello #%v\\n\", counter) wg.Done() } func sayHelloM() { fmt.Printf(\"Hello #%v\\n\", counter) m.RUnlock() wg.Done() } func increment() { counter++ wg.Done() } func incrementM() { counter++ m.Unlock() wg.Done() } Hello #2 Hello #4 Hello #0 Hello #2 Hello #5 Hello #5 Hello #7 Hello #8 Hello #8 Hello #9 Threads: 8 Hello #10 Hello #11 Hello #12 Hello #13 Hello #14 Hello #15 Hello #16 Hello #17 Hello #18 Hello #19","title":"Goroutines"},{"location":"Golang/14-channels/","text":"Channels \u00b6 Channel basics create a channel with make command make(chan int) send a message into channel ch <- val receive from channel val := <-ch Can have multiple senders and receivers Restricting data flow Channel can be cast into send-only or receive-only versions send-only: chan <- int receive only: <-chan int Buffered channels Channels block sender side till receiver is available Block receiver side till message is available Can decouple sender and receiver with buffered channels make(chan int, 50) Use buffered channels when sender and receiver have assymmetric loads For..range loops with channels Use to monitor channel and process messages as they arrive Loop exits when channel is closed Select statements Allows goroutine to monitor several channels at once Blocks if all channels block If multiple channels receive value simultaneously, behaviour is undefined package main import ( \"fmt\" \"sync\" \"time\" ) var wg = sync.WaitGroup{} var logCh = make(chan logEntry, 50) var doneCh = make(chan struct{}) // signal only channel func main() { ch := make(chan int) // Bidirectional comms wg.Add(2) go func() { i := <- ch fmt.Println(i) // 42 ch <- 27 wg.Done() }() go func() { i := 42 ch <- i fmt.Println(<-ch) wg.Done() }() wg.Wait() // 1 send, 1 read thread wg.Add(2) go func(ch <-chan int) { i := <- ch fmt.Println(i) // ch <- 27 -- cannot send, receive only wg.Done() }(ch) go func(ch chan<- int) { ch <- 42 // fmt.Println(<-ch) -- cannot receive, send only wg.Done() }(ch) wg.Wait() // multiple messages ch2 := make(chan int, 50) wg.Add(2) go func(ch <- chan int) { for { if i, ok := <- ch; ok { fmt.Println(i) } else { break } } wg.Done() }(ch2) go func(ch chan<- int) { ch <- 42 ch <- 27 ch <- 13 close(ch) wg.Done() }(ch2) wg.Wait() // logger go logger() logCh <- logEntry{time.Now(), logInfo, \"App is starting\"} logCh <- logEntry{time.Now(), logInfo, \"App is shutting down\"} time.Sleep(100 * time.Millisecond) doneCh <- struct{}{} } const ( logInfo = \"INFO\" logWarning = \"WARNING\" logError = \"ERROR\" ) type logEntry struct { time time.Time severity string message string } func logger() { for { select { case entry := <- logCh: fmt.Printf(\"%v - [%v] %v\\n\", entry.time.Format(\"2006-01-02T15:04:05\"), entry.severity, entry.message) case <-doneCh: break } } } 42 27 42 42 27 13 2019-12-25T16:20:09 - [INFO] App is starting 2019-12-25T16:20:09 - [INFO] App is shutting down","title":"Channels"},{"location":"Golang/14-channels/#channels","text":"Channel basics create a channel with make command make(chan int) send a message into channel ch <- val receive from channel val := <-ch Can have multiple senders and receivers Restricting data flow Channel can be cast into send-only or receive-only versions send-only: chan <- int receive only: <-chan int Buffered channels Channels block sender side till receiver is available Block receiver side till message is available Can decouple sender and receiver with buffered channels make(chan int, 50) Use buffered channels when sender and receiver have assymmetric loads For..range loops with channels Use to monitor channel and process messages as they arrive Loop exits when channel is closed Select statements Allows goroutine to monitor several channels at once Blocks if all channels block If multiple channels receive value simultaneously, behaviour is undefined package main import ( \"fmt\" \"sync\" \"time\" ) var wg = sync.WaitGroup{} var logCh = make(chan logEntry, 50) var doneCh = make(chan struct{}) // signal only channel func main() { ch := make(chan int) // Bidirectional comms wg.Add(2) go func() { i := <- ch fmt.Println(i) // 42 ch <- 27 wg.Done() }() go func() { i := 42 ch <- i fmt.Println(<-ch) wg.Done() }() wg.Wait() // 1 send, 1 read thread wg.Add(2) go func(ch <-chan int) { i := <- ch fmt.Println(i) // ch <- 27 -- cannot send, receive only wg.Done() }(ch) go func(ch chan<- int) { ch <- 42 // fmt.Println(<-ch) -- cannot receive, send only wg.Done() }(ch) wg.Wait() // multiple messages ch2 := make(chan int, 50) wg.Add(2) go func(ch <- chan int) { for { if i, ok := <- ch; ok { fmt.Println(i) } else { break } } wg.Done() }(ch2) go func(ch chan<- int) { ch <- 42 ch <- 27 ch <- 13 close(ch) wg.Done() }(ch2) wg.Wait() // logger go logger() logCh <- logEntry{time.Now(), logInfo, \"App is starting\"} logCh <- logEntry{time.Now(), logInfo, \"App is shutting down\"} time.Sleep(100 * time.Millisecond) doneCh <- struct{}{} } const ( logInfo = \"INFO\" logWarning = \"WARNING\" logError = \"ERROR\" ) type logEntry struct { time time.Time severity string message string } func logger() { for { select { case entry := <- logCh: fmt.Printf(\"%v - [%v] %v\\n\", entry.time.Format(\"2006-01-02T15:04:05\"), entry.severity, entry.message) case <-doneCh: break } } } 42 27 42 42 27 13 2019-12-25T16:20:09 - [INFO] App is starting 2019-12-25T16:20:09 - [INFO] App is shutting down","title":"Channels"},{"location":"GraphQL/","text":"GraphQL \u00b6 Learning GraphQL for backend (node.js) and frontend (react2). Code available at https://github.com/daviskregers/graphql . Sources: - GraphQL Full Course - Novice to Expert","title":"GraphQL"},{"location":"GraphQL/#graphql","text":"Learning GraphQL for backend (node.js) and frontend (react2). Code available at https://github.com/daviskregers/graphql . Sources: - GraphQL Full Course - Novice to Expert","title":"GraphQL"},{"location":"GraphQL/01-introduction/","text":"Introduction \u00b6 GraphQL is basically a powerfull query language that is used to communicate data between client and a server. It allows us to communicate data in a more flexible and efficient approach than RESTful approach would. RESTful approach \u00b6 A RESTful approach would look something like this: - Endpoint for getting a particular book: domain.com/books/:id title, genre, reviews, authorid - Endpoint for getting the author info of that book: domain.com/authors/:id name, age, biography, booksids Typically what we would do if we needed to get the author of the book - we would grab the id from the first request and then make a second request to get the author. Then, we might also want to to get all the books of that particular author. With that we might need to call the books endpoint multiple times. This starts to show the inefficiencies with RESTful approach. GraphQL approach \u00b6 The GraphQL approach to this problem will look like this: - Query to get book data and it's author data (AND other books): { book(id: 123) { title genre reviews author { name bio books { name } } } } The great thing about this approach is that we can get all the previously needed data with a single request and we can decide what fields do we need in order to not bloat the response.","title":"Introduction"},{"location":"GraphQL/01-introduction/#introduction","text":"GraphQL is basically a powerfull query language that is used to communicate data between client and a server. It allows us to communicate data in a more flexible and efficient approach than RESTful approach would.","title":"Introduction"},{"location":"GraphQL/01-introduction/#restful-approach","text":"A RESTful approach would look something like this: - Endpoint for getting a particular book: domain.com/books/:id title, genre, reviews, authorid - Endpoint for getting the author info of that book: domain.com/authors/:id name, age, biography, booksids Typically what we would do if we needed to get the author of the book - we would grab the id from the first request and then make a second request to get the author. Then, we might also want to to get all the books of that particular author. With that we might need to call the books endpoint multiple times. This starts to show the inefficiencies with RESTful approach.","title":"RESTful approach"},{"location":"GraphQL/01-introduction/#graphql-approach","text":"The GraphQL approach to this problem will look like this: - Query to get book data and it's author data (AND other books): { book(id: 123) { title genre reviews author { name bio books { name } } } } The great thing about this approach is that we can get all the previously needed data with a single request and we can decide what fields do we need in order to not bloat the response.","title":"GraphQL approach"},{"location":"GraphQL/02-making-frontend-queries/","text":"Making frontend queries \u00b6 In order to make a frontend query we can use a query in a structure like this: { books { name genre id } } It starts with a curly braces and in them we recursively define what fields to we want to get. The response will be like this: { \"data\": { \"books\": [ { \"name\": \"The Final Empire\", \"genre\": \"Fantasy\", \"id\": \"5aa2906a9da47848141d622b\" }, ... ] } } Then, we can remove the ID field if not needed: { books { name genre } } And get a less bloated response: { \"data\": { \"books\": [ { \"name\": \"The Final Empire\", \"genre\": \"Fantasy\", }, ... ] } }","title":"Making frontend queries"},{"location":"GraphQL/02-making-frontend-queries/#making-frontend-queries","text":"In order to make a frontend query we can use a query in a structure like this: { books { name genre id } } It starts with a curly braces and in them we recursively define what fields to we want to get. The response will be like this: { \"data\": { \"books\": [ { \"name\": \"The Final Empire\", \"genre\": \"Fantasy\", \"id\": \"5aa2906a9da47848141d622b\" }, ... ] } } Then, we can remove the ID field if not needed: { books { name genre } } And get a less bloated response: { \"data\": { \"books\": [ { \"name\": \"The Final Empire\", \"genre\": \"Fantasy\", }, ... ] } }","title":"Making frontend queries"},{"location":"GraphQL/03-setting-up-express/","text":"Setting up express \u00b6 We are going to create a new project directory. \u279c projects cd learning \u279c learning mkdir graphql \u279c learning cd graphql \u279c graphql git init In it, we are going to make an express.js server app: \u279c graphql git:(master) mkdir server \u279c graphql git:(master) cd server \u279c server git:(master) npm init This utility will walk you through creating a package.json file. It only covers the most common items, and tries to guess sensible defaults. See `npm help json` for definitive documentation on these fields and exactly what they do. Use `npm install <pkg>` afterwards to install a package and save it as a dependency in the package.json file. Press ^C at any time to quit. package name: (server) version: (1.0.0) description: entry point: (index.js) test command: git repository: keywords: author: license: (ISC) About to write to /home/davis/projects/learning/graphql/server/package.json: { \"name\": \"server\", \"version\": \"1.0.0\", \"description\": \"\", \"main\": \"index.js\", \"scripts\": { \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\" }, \"author\": \"\", \"license\": \"ISC\" } Is this OK? (yes) \u279c server git:(master) \u2717 npm i express --save npm notice created a lockfile as package-lock.json. You should commit this file. npm WARN server@1.0.0 No description npm WARN server@1.0.0 No repository field. + express@4.17.1 added 50 packages from 37 contributors and audited 126 packages in 2.306s found 0 vulnerabilities Then we are going to create an app.js file in the server directory: // IMPORTS const express = require('express'); // CONSTANTS const app = express(); const APP_PORT= 4000; // LOGIC app.listen(APP_PORT, () => console.log(`Listening for requests on port ${APP_PORT}`)); An if we run it: \u279c server git:(master) \u2717 node app.js Listening for requests on port 4000","title":"Setting up express"},{"location":"GraphQL/03-setting-up-express/#setting-up-express","text":"We are going to create a new project directory. \u279c projects cd learning \u279c learning mkdir graphql \u279c learning cd graphql \u279c graphql git init In it, we are going to make an express.js server app: \u279c graphql git:(master) mkdir server \u279c graphql git:(master) cd server \u279c server git:(master) npm init This utility will walk you through creating a package.json file. It only covers the most common items, and tries to guess sensible defaults. See `npm help json` for definitive documentation on these fields and exactly what they do. Use `npm install <pkg>` afterwards to install a package and save it as a dependency in the package.json file. Press ^C at any time to quit. package name: (server) version: (1.0.0) description: entry point: (index.js) test command: git repository: keywords: author: license: (ISC) About to write to /home/davis/projects/learning/graphql/server/package.json: { \"name\": \"server\", \"version\": \"1.0.0\", \"description\": \"\", \"main\": \"index.js\", \"scripts\": { \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\" }, \"author\": \"\", \"license\": \"ISC\" } Is this OK? (yes) \u279c server git:(master) \u2717 npm i express --save npm notice created a lockfile as package-lock.json. You should commit this file. npm WARN server@1.0.0 No description npm WARN server@1.0.0 No repository field. + express@4.17.1 added 50 packages from 37 contributors and audited 126 packages in 2.306s found 0 vulnerabilities Then we are going to create an app.js file in the server directory: // IMPORTS const express = require('express'); // CONSTANTS const app = express(); const APP_PORT= 4000; // LOGIC app.listen(APP_PORT, () => console.log(`Listening for requests on port ${APP_PORT}`)); An if we run it: \u279c server git:(master) \u2717 node app.js Listening for requests on port 4000","title":"Setting up express"},{"location":"GraphQL/04-setting-up-graphql/","text":"Setting up GraphQL \u00b6 In the previously made backend express.js project we are going to install the following packages: \u279c server git:(master) \u2717 npm i graphql express-graphql --save npm WARN server@1.0.0 No description npm WARN server@1.0.0 No repository field. + express-graphql@0.9.0 + graphql@14.5.8 added 6 packages from 5 contributors and audited 151 packages in 1.92s found 0 vulnerabilities Then we are going to modify the previously made app.js file: // IMPORTS const express = require('express'); const graphqlHTTP = require('express-graphql'); // CONSTANTS const app = express(); const APP_PORT= 4000; // LOGIC app.use('/graphql', graphqlHTTP({ })); app.listen(APP_PORT, () => console.log(`Listening for requests on port ${APP_PORT}`)); Currently, if we visit the /graphql endpoint that will be used to query all the data, it will return an error like this: {\"errors\":[{\"message\":\"GraphQL middleware options must contain a schema.\"}]} This is because we haven't provided any schema to it. This will be done in the next steps.","title":"Setting up GraphQL"},{"location":"GraphQL/04-setting-up-graphql/#setting-up-graphql","text":"In the previously made backend express.js project we are going to install the following packages: \u279c server git:(master) \u2717 npm i graphql express-graphql --save npm WARN server@1.0.0 No description npm WARN server@1.0.0 No repository field. + express-graphql@0.9.0 + graphql@14.5.8 added 6 packages from 5 contributors and audited 151 packages in 1.92s found 0 vulnerabilities Then we are going to modify the previously made app.js file: // IMPORTS const express = require('express'); const graphqlHTTP = require('express-graphql'); // CONSTANTS const app = express(); const APP_PORT= 4000; // LOGIC app.use('/graphql', graphqlHTTP({ })); app.listen(APP_PORT, () => console.log(`Listening for requests on port ${APP_PORT}`)); Currently, if we visit the /graphql endpoint that will be used to query all the data, it will return an error like this: {\"errors\":[{\"message\":\"GraphQL middleware options must contain a schema.\"}]} This is because we haven't provided any schema to it. This will be done in the next steps.","title":"Setting up GraphQL"},{"location":"GraphQL/05-graphql-schema/","text":"Defining GraphQL Schema \u00b6 Now that we have imported the GraphQL in our project, we need to start making a schema for it. We are going to make a directory schema in the server and add a schema.js file in it: // IMPORTS const graphql = require('graphql'); // CONSTANTS const { GraphQLObjectType, GraphQLString } = graphql // DEFINE OBJECT TYPES const BookType = new GraphQLObjectType({ name: 'Book', fields: () => ({ id: { type: GraphQLString }, name: { type: GraphQLString }, genre: { type: GraphQLString } }) }); This file contains all the object types we are going to be using with defined fields and their types as well as the root queries that are going to be looked at in the next section.","title":"Defining GraphQL Schema"},{"location":"GraphQL/05-graphql-schema/#defining-graphql-schema","text":"Now that we have imported the GraphQL in our project, we need to start making a schema for it. We are going to make a directory schema in the server and add a schema.js file in it: // IMPORTS const graphql = require('graphql'); // CONSTANTS const { GraphQLObjectType, GraphQLString } = graphql // DEFINE OBJECT TYPES const BookType = new GraphQLObjectType({ name: 'Book', fields: () => ({ id: { type: GraphQLString }, name: { type: GraphQLString }, genre: { type: GraphQLString } }) }); This file contains all the object types we are going to be using with defined fields and their types as well as the root queries that are going to be looked at in the next section.","title":"Defining GraphQL Schema"},{"location":"GraphQL/06-root-query/","text":"Root Queries \u00b6 By defining root queries we are going to define what queries we will have on our graphql that are not being relations - what are the root nodes of our objects that we are going to query. // IMPORTS const graphql = require('graphql'); // CONSTANTS const { GraphQLObjectType, GraphQLString, GraphQLSchema } = graphql // DEFINE OBJECT TYPES const BookType = new GraphQLObjectType({ name: 'Book', fields: () => ({ id: { type: GraphQLString }, name: { type: GraphQLString }, genre: { type: GraphQLString } }) }); // DEFINE ROOT QUERIES const RootQuery = new GraphQLObjectType({ name: 'RootQueryType', fields: { book: { type: BookType, args: { id: { type: GraphQLString } }, resolve(parent, args) { // code to get data from db or other source } } } }) module.exports = new GraphQLSchema({ query: RootQuery }) The args are provided to know what arguments are going to be passed to the root query. The resolve function is a function that will describe on how we are going to get the data from database or other source. Since we now have a valid schema built, we can also modify the app.js : // IMPORTS const express = require('express'); const graphqlHTTP = require('express-graphql'); const schema = require('./schema/schema'); // CONSTANTS const app = express(); const APP_PORT= 4000; // LOGIC app.use('/graphql', graphqlHTTP({ schema })); app.listen(APP_PORT, () => console.log(`Listening for requests on port ${APP_PORT}`)); Now, when visiting the /graphql endpoint, we are going to get a message like this: {\"errors\":[{\"message\":\"Must provide query string.\"}]} The next step is to make a resolve function for the BookType so we can test it.","title":"Root Queries"},{"location":"GraphQL/06-root-query/#root-queries","text":"By defining root queries we are going to define what queries we will have on our graphql that are not being relations - what are the root nodes of our objects that we are going to query. // IMPORTS const graphql = require('graphql'); // CONSTANTS const { GraphQLObjectType, GraphQLString, GraphQLSchema } = graphql // DEFINE OBJECT TYPES const BookType = new GraphQLObjectType({ name: 'Book', fields: () => ({ id: { type: GraphQLString }, name: { type: GraphQLString }, genre: { type: GraphQLString } }) }); // DEFINE ROOT QUERIES const RootQuery = new GraphQLObjectType({ name: 'RootQueryType', fields: { book: { type: BookType, args: { id: { type: GraphQLString } }, resolve(parent, args) { // code to get data from db or other source } } } }) module.exports = new GraphQLSchema({ query: RootQuery }) The args are provided to know what arguments are going to be passed to the root query. The resolve function is a function that will describe on how we are going to get the data from database or other source. Since we now have a valid schema built, we can also modify the app.js : // IMPORTS const express = require('express'); const graphqlHTTP = require('express-graphql'); const schema = require('./schema/schema'); // CONSTANTS const app = express(); const APP_PORT= 4000; // LOGIC app.use('/graphql', graphqlHTTP({ schema })); app.listen(APP_PORT, () => console.log(`Listening for requests on port ${APP_PORT}`)); Now, when visiting the /graphql endpoint, we are going to get a message like this: {\"errors\":[{\"message\":\"Must provide query string.\"}]} The next step is to make a resolve function for the BookType so we can test it.","title":"Root Queries"},{"location":"GraphQL/07-resolve-function/","text":"Resolve Function \u00b6 The resolve function describes the steps to take in order to retrieve the data we are querying for. The data can be fetched from a database or other source. We are going to modify the schema file to resolve the data from a static object for now, add a second root query - authors. // IMPORTS const graphql = require('graphql'); const _ = require('lodash') // CONSTANTS const { GraphQLObjectType, GraphQLString, GraphQLSchema, GraphQLID, GraphQLInt } = graphql var books = [ {name: \"Name of the Wind\", genre: \"Fantasy\", id: '1'}, {name: \"The Final Empire\", genre: \"Fantasy\", id: '2'}, {name: \"The Long Earth\", genre: \"Sci-Fi\", id: '3'} ]; var authors = [ {name: \"Patric Rothfuss\", age: 44, id: '1'}, {name: \"Brandon Sanderson\", age: 42, id: '2'}, {name: \"Terry Pratchett\", age: 66, id: '3'} ]; // DEFINE OBJECT TYPES const BookType = new GraphQLObjectType({ name: 'Book', fields: () => ({ id: { type: GraphQLID }, name: { type: GraphQLString }, genre: { type: GraphQLString } }) }); const AuthorType = new GraphQLObjectType({ name: 'Author', fields: () => ({ id: { type: GraphQLID }, name: { type: GraphQLString }, age: { type: GraphQLInt } }) }); // DEFINE ROOT QUERIES const RootQuery = new GraphQLObjectType({ name: 'RootQueryType', fields: { book: { type: BookType, args: { id: { type: GraphQLID } }, resolve(parent, args) { return _.find(books, { id: args.id }); } }, author: { type: AuthorType, args: { id: { type: GraphQLID } }, resolve(paremt, args) { return _.find(authors, { id: args.id }); } } } }) module.exports = new GraphQLSchema({ query: RootQuery }) Now, if we query the /graphql endpoint: { book(id: 1){ id name genre } } --- {\"data\":{\"book\":{\"id\":\"1\",\"name\":\"Name of the Wind\",\"genre\":\"Fantasy\"}}} { author(id: 1){ id name age } } --- {\"data\":{\"author\":{\"id\":\"1\",\"name\":\"Patric Rothfuss\",\"age\":44}}}","title":"Resolve Function"},{"location":"GraphQL/07-resolve-function/#resolve-function","text":"The resolve function describes the steps to take in order to retrieve the data we are querying for. The data can be fetched from a database or other source. We are going to modify the schema file to resolve the data from a static object for now, add a second root query - authors. // IMPORTS const graphql = require('graphql'); const _ = require('lodash') // CONSTANTS const { GraphQLObjectType, GraphQLString, GraphQLSchema, GraphQLID, GraphQLInt } = graphql var books = [ {name: \"Name of the Wind\", genre: \"Fantasy\", id: '1'}, {name: \"The Final Empire\", genre: \"Fantasy\", id: '2'}, {name: \"The Long Earth\", genre: \"Sci-Fi\", id: '3'} ]; var authors = [ {name: \"Patric Rothfuss\", age: 44, id: '1'}, {name: \"Brandon Sanderson\", age: 42, id: '2'}, {name: \"Terry Pratchett\", age: 66, id: '3'} ]; // DEFINE OBJECT TYPES const BookType = new GraphQLObjectType({ name: 'Book', fields: () => ({ id: { type: GraphQLID }, name: { type: GraphQLString }, genre: { type: GraphQLString } }) }); const AuthorType = new GraphQLObjectType({ name: 'Author', fields: () => ({ id: { type: GraphQLID }, name: { type: GraphQLString }, age: { type: GraphQLInt } }) }); // DEFINE ROOT QUERIES const RootQuery = new GraphQLObjectType({ name: 'RootQueryType', fields: { book: { type: BookType, args: { id: { type: GraphQLID } }, resolve(parent, args) { return _.find(books, { id: args.id }); } }, author: { type: AuthorType, args: { id: { type: GraphQLID } }, resolve(paremt, args) { return _.find(authors, { id: args.id }); } } } }) module.exports = new GraphQLSchema({ query: RootQuery }) Now, if we query the /graphql endpoint: { book(id: 1){ id name genre } } --- {\"data\":{\"book\":{\"id\":\"1\",\"name\":\"Name of the Wind\",\"genre\":\"Fantasy\"}}} { author(id: 1){ id name age } } --- {\"data\":{\"author\":{\"id\":\"1\",\"name\":\"Patric Rothfuss\",\"age\":44}}}","title":"Resolve Function"},{"location":"GraphQL/08-type-relations/","text":"Type Relations \u00b6 Now we have 2 different types - book and author. We can introduce relations that will between those objects. We can define a relation in the data. var books = [ {name: \"Name of the Wind\", genre: \"Fantasy\", id: '1', authorId: '1'}, {name: \"The Final Empire\", genre: \"Fantasy\", id: '2', authorId: '2'}, {name: \"The Long Earth\", genre: \"Sci-Fi\", id: '3', authorId: '3'} ]; And then add a field to the BookType . const BookType = new GraphQLObjectType({ name: 'Book', fields: () => ({ id: { type: GraphQLID }, name: { type: GraphQLString }, genre: { type: GraphQLString }, author: { type: AuthorType, resolve(parent, args) { return _.find(authors, {id: parent.authorId }); } } }) }); Now we test a query { book(id: 1) { id name genre author { id name age } } } --- { \"data\": { \"book\": { \"id\": \"1\", \"name\": \"Name of the Wind\", \"genre\": \"Fantasy\", \"author\": { \"id\": \"1\", \"name\": \"Patric Rothfuss\", \"age\": 44 } } } }","title":"Type Relations"},{"location":"GraphQL/08-type-relations/#type-relations","text":"Now we have 2 different types - book and author. We can introduce relations that will between those objects. We can define a relation in the data. var books = [ {name: \"Name of the Wind\", genre: \"Fantasy\", id: '1', authorId: '1'}, {name: \"The Final Empire\", genre: \"Fantasy\", id: '2', authorId: '2'}, {name: \"The Long Earth\", genre: \"Sci-Fi\", id: '3', authorId: '3'} ]; And then add a field to the BookType . const BookType = new GraphQLObjectType({ name: 'Book', fields: () => ({ id: { type: GraphQLID }, name: { type: GraphQLString }, genre: { type: GraphQLString }, author: { type: AuthorType, resolve(parent, args) { return _.find(authors, {id: parent.authorId }); } } }) }); Now we test a query { book(id: 1) { id name genre author { id name age } } } --- { \"data\": { \"book\": { \"id\": \"1\", \"name\": \"Name of the Wind\", \"genre\": \"Fantasy\", \"author\": { \"id\": \"1\", \"name\": \"Patric Rothfuss\", \"age\": 44 } } } }","title":"Type Relations"},{"location":"GraphQL/09-graphql-lists/","text":"GraphQL lists \u00b6 Now that we have defined relation between book and it's author. We might want to define a relation the other way around. But it is a one to many relation, which will need to return a list of all the books of that author. First, we are going to add more books: var books = [ {name: \"Name of the Wind\", genre: \"Fantasy\", id: '1', authorId: '1'}, {name: \"The Final Empire\", genre: \"Fantasy\", id: '2', authorId: '2'}, {name: \"The Long Earth\", genre: \"Sci-Fi\", id: '3', authorId: '3'}, {name: \"The Hero of Ages\", genre: \"Fantasy\", id: '4', authorId: '3'}, {name: \"The Colour of Magic\", genre: \"Fantasy\", id: '5', authorId: '3'}, {name: \"The Light Fantastic\", genre: \"Fantasy\", id: '6', authorId: '3'} ]; Then, we are going to define a relation with a type of a list of BookType . const AuthorType = new GraphQLObjectType({ name: 'Author', fields: () => ({ id: { type: GraphQLID }, name: { type: GraphQLString }, age: { type: GraphQLInt }, books: { type: new GraphQLList(BookType), resolve(parent, args) { return _.filter(books, { authorId: parent.id }); } } }) }); Now we can query it: { author(id: 3) { id name age books { id name genre } } } --- { \"data\": { \"author\": { \"id\": \"3\", \"name\": \"Terry Pratchett\", \"age\": 66, \"books\": [ { \"id\": \"3\", \"name\": \"The Long Earth\", \"genre\": \"Sci-Fi\" }, { \"id\": \"4\", \"name\": \"The Hero of Ages\", \"genre\": \"Fantasy\" }, { \"id\": \"5\", \"name\": \"The Colour of Magic\", \"genre\": \"Fantasy\" }, { \"id\": \"6\", \"name\": \"The Light Fantastic\", \"genre\": \"Fantasy\" } ] } } }","title":"GraphQL lists"},{"location":"GraphQL/09-graphql-lists/#graphql-lists","text":"Now that we have defined relation between book and it's author. We might want to define a relation the other way around. But it is a one to many relation, which will need to return a list of all the books of that author. First, we are going to add more books: var books = [ {name: \"Name of the Wind\", genre: \"Fantasy\", id: '1', authorId: '1'}, {name: \"The Final Empire\", genre: \"Fantasy\", id: '2', authorId: '2'}, {name: \"The Long Earth\", genre: \"Sci-Fi\", id: '3', authorId: '3'}, {name: \"The Hero of Ages\", genre: \"Fantasy\", id: '4', authorId: '3'}, {name: \"The Colour of Magic\", genre: \"Fantasy\", id: '5', authorId: '3'}, {name: \"The Light Fantastic\", genre: \"Fantasy\", id: '6', authorId: '3'} ]; Then, we are going to define a relation with a type of a list of BookType . const AuthorType = new GraphQLObjectType({ name: 'Author', fields: () => ({ id: { type: GraphQLID }, name: { type: GraphQLString }, age: { type: GraphQLInt }, books: { type: new GraphQLList(BookType), resolve(parent, args) { return _.filter(books, { authorId: parent.id }); } } }) }); Now we can query it: { author(id: 3) { id name age books { id name genre } } } --- { \"data\": { \"author\": { \"id\": \"3\", \"name\": \"Terry Pratchett\", \"age\": 66, \"books\": [ { \"id\": \"3\", \"name\": \"The Long Earth\", \"genre\": \"Sci-Fi\" }, { \"id\": \"4\", \"name\": \"The Hero of Ages\", \"genre\": \"Fantasy\" }, { \"id\": \"5\", \"name\": \"The Colour of Magic\", \"genre\": \"Fantasy\" }, { \"id\": \"6\", \"name\": \"The Light Fantastic\", \"genre\": \"Fantasy\" } ] } } }","title":"GraphQL lists"},{"location":"GraphQL/10-more-on-root-queries/","text":"More on Root Queries \u00b6 Previously we created root queries to fetch books and authors from given IDs. Now we might want to add root queries to fetch a list of all of the books and authors. // DEFINE ROOT QUERIES const RootQuery = new GraphQLObjectType({ name: 'RootQueryType', fields: { book: { type: BookType, args: { id: { type: GraphQLID } }, resolve(parent, args) { return _.find(books, { id: args.id }); } }, author: { type: AuthorType, args: { id: { type: GraphQLID } }, resolve(parent, args) { return _.find(authors, { id: args.id }); } }, books: { type: new GraphQLList(BookType), resolve(parent, args) { return books; } }, authors: { type: new GraphQLList(AuthorType), resolve(parent, args) { return authors; } } } }) Now if we query it: { books { id name genre } } --- { \"data\": { \"books\": [ { \"id\": \"1\", \"name\": \"Name of the Wind\", \"genre\": \"Fantasy\" }, { \"id\": \"2\", \"name\": \"The Final Empire\", \"genre\": \"Fantasy\" }, { \"id\": \"3\", \"name\": \"The Long Earth\", \"genre\": \"Sci-Fi\" }, { \"id\": \"4\", \"name\": \"The Hero of Ages\", \"genre\": \"Fantasy\" }, { \"id\": \"5\", \"name\": \"The Colour of Magic\", \"genre\": \"Fantasy\" }, { \"id\": \"6\", \"name\": \"The Light Fantastic\", \"genre\": \"Fantasy\" } ] } } { authors { id name age } } --- { \"data\": { \"authors\": [ { \"id\": \"1\", \"name\": \"Patric Rothfuss\", \"age\": 44 }, { \"id\": \"2\", \"name\": \"Brandon Sanderson\", \"age\": 42 }, { \"id\": \"3\", \"name\": \"Terry Pratchett\", \"age\": 66 } ] } }","title":"More on Root Queries"},{"location":"GraphQL/10-more-on-root-queries/#more-on-root-queries","text":"Previously we created root queries to fetch books and authors from given IDs. Now we might want to add root queries to fetch a list of all of the books and authors. // DEFINE ROOT QUERIES const RootQuery = new GraphQLObjectType({ name: 'RootQueryType', fields: { book: { type: BookType, args: { id: { type: GraphQLID } }, resolve(parent, args) { return _.find(books, { id: args.id }); } }, author: { type: AuthorType, args: { id: { type: GraphQLID } }, resolve(parent, args) { return _.find(authors, { id: args.id }); } }, books: { type: new GraphQLList(BookType), resolve(parent, args) { return books; } }, authors: { type: new GraphQLList(AuthorType), resolve(parent, args) { return authors; } } } }) Now if we query it: { books { id name genre } } --- { \"data\": { \"books\": [ { \"id\": \"1\", \"name\": \"Name of the Wind\", \"genre\": \"Fantasy\" }, { \"id\": \"2\", \"name\": \"The Final Empire\", \"genre\": \"Fantasy\" }, { \"id\": \"3\", \"name\": \"The Long Earth\", \"genre\": \"Sci-Fi\" }, { \"id\": \"4\", \"name\": \"The Hero of Ages\", \"genre\": \"Fantasy\" }, { \"id\": \"5\", \"name\": \"The Colour of Magic\", \"genre\": \"Fantasy\" }, { \"id\": \"6\", \"name\": \"The Light Fantastic\", \"genre\": \"Fantasy\" } ] } } { authors { id name age } } --- { \"data\": { \"authors\": [ { \"id\": \"1\", \"name\": \"Patric Rothfuss\", \"age\": 44 }, { \"id\": \"2\", \"name\": \"Brandon Sanderson\", \"age\": 42 }, { \"id\": \"3\", \"name\": \"Terry Pratchett\", \"age\": 66 } ] } }","title":"More on Root Queries"},{"location":"GraphQL/11-connecting-to-mlab/","text":"Connecting to mLab \u00b6 We can use mLab to use as a mongodb service, which is a free service for sandbox mongodb. We are going to sign up for the service and create a free tier cluster. Once it's done deploying, we are going to click on Connect and create a new user: Now we are going to install mongoose. \u279c server git:(master) \u2717 npm i mongoose --save npm WARN server@1.0.0 No description npm WARN server@1.0.0 No repository field. + mongoose@5.8.2 added 19 packages from 13 contributors and audited 178 packages in 8.253s 1 package is looking for funding run `npm fund` for details found 0 vulnerabilities We're going to install dotenv as well, so we don't have to hardcode the credentials. \u279c server git:(master) \u2717 npm i dotenv --save npm WARN server@1.0.0 No description npm WARN server@1.0.0 No repository field. + dotenv@8.2.0 added 1 package and audited 179 packages in 0.959s 1 package is looking for funding run `npm fund` for details found 0 vulnerabilities And then we are going to modif the app: // IMPORTS const express = require('express'); const graphqlHTTP = require('express-graphql'); const schema = require('./schema/schema'); const mongoose = require('mongoose'); const dotenv = require('dotenv'); // CONSTANTS dotenv.config(); const app = express(); const APP_PORT= process.env.APP_PORT; const MONGOOSE_USER = process.env.MONGOOSE_USER; const MONGOOSE_PASS = process.env.MONGOOSE_PASS; const MONGOOSE_HOST = process.env.MONGOOSE_HOST; // LOGIC mongoose.connect(`mongodb+srv://${MONGOOSE_USER}:${MONGOOSE_PASS}@${MONGOOSE_HOST}?retryWrites=true&w=majority`, { useNewUrlParser: true, useUnifiedTopology: true }); mongoose.connection.once('open', () => { console.log('connected to database'); }); app.use('/graphql', graphqlHTTP({ schema })); app.listen(APP_PORT, () => console.log(`Listening for requests on port ${APP_PORT}`)); If we run the app, we should see that we are connected to the database. \u279c server git:(master) \u2717 nodemon app.js [nodemon] 2.0.2 [nodemon] to restart at any time, enter `rs` [nodemon] watching dir(s): *.* [nodemon] watching extensions: js,mjs,json [nodemon] starting `node app.js` Listening for requests on port 4000 connected to database","title":"Connecting to mLab"},{"location":"GraphQL/11-connecting-to-mlab/#connecting-to-mlab","text":"We can use mLab to use as a mongodb service, which is a free service for sandbox mongodb. We are going to sign up for the service and create a free tier cluster. Once it's done deploying, we are going to click on Connect and create a new user: Now we are going to install mongoose. \u279c server git:(master) \u2717 npm i mongoose --save npm WARN server@1.0.0 No description npm WARN server@1.0.0 No repository field. + mongoose@5.8.2 added 19 packages from 13 contributors and audited 178 packages in 8.253s 1 package is looking for funding run `npm fund` for details found 0 vulnerabilities We're going to install dotenv as well, so we don't have to hardcode the credentials. \u279c server git:(master) \u2717 npm i dotenv --save npm WARN server@1.0.0 No description npm WARN server@1.0.0 No repository field. + dotenv@8.2.0 added 1 package and audited 179 packages in 0.959s 1 package is looking for funding run `npm fund` for details found 0 vulnerabilities And then we are going to modif the app: // IMPORTS const express = require('express'); const graphqlHTTP = require('express-graphql'); const schema = require('./schema/schema'); const mongoose = require('mongoose'); const dotenv = require('dotenv'); // CONSTANTS dotenv.config(); const app = express(); const APP_PORT= process.env.APP_PORT; const MONGOOSE_USER = process.env.MONGOOSE_USER; const MONGOOSE_PASS = process.env.MONGOOSE_PASS; const MONGOOSE_HOST = process.env.MONGOOSE_HOST; // LOGIC mongoose.connect(`mongodb+srv://${MONGOOSE_USER}:${MONGOOSE_PASS}@${MONGOOSE_HOST}?retryWrites=true&w=majority`, { useNewUrlParser: true, useUnifiedTopology: true }); mongoose.connection.once('open', () => { console.log('connected to database'); }); app.use('/graphql', graphqlHTTP({ schema })); app.listen(APP_PORT, () => console.log(`Listening for requests on port ${APP_PORT}`)); If we run the app, we should see that we are connected to the database. \u279c server git:(master) \u2717 nodemon app.js [nodemon] 2.0.2 [nodemon] to restart at any time, enter `rs` [nodemon] watching dir(s): *.* [nodemon] watching extensions: js,mjs,json [nodemon] starting `node app.js` Listening for requests on port 4000 connected to database","title":"Connecting to mLab"},{"location":"GraphQL/12-mongoose-models/","text":"Mongoose Models \u00b6 Before we start putting data in the database, we will need to create Mongoose Schema that defines what data format we are going to use. We are going to create a new directory models in the server directory and create two files: book.js and author.js . const mongoose = require('mongoose'); const Schema = mongoose.Schema; const bookSchema = new Schema({ name: String, genre: String, authorId: String, // id is not needed, mongodb creates that on it's own }); module.exports = mongoose.model('Book', bookSchema); const mongoose = require('mongoose'); const Schema = mongoose.Schema; const authorSchema = new Schema({ name: String, age: Number, }); module.exports = mongoose.model('Author', authorSchema); Now we can remove the dummy hardcoded data from the schema/schema.js file as well as modify the resolve functions. // IMPORTS const graphql = require('graphql'); const Book = require('../models/book'); const Author = require('../models/author'); // CONSTANTS const { GraphQLObjectType, GraphQLString, GraphQLSchema, GraphQLID, GraphQLInt, GraphQLList } = graphql // DEFINE OBJECT TYPES // DEFINE OBJECT TYPES const BookType = new GraphQLObjectType({ name: 'Book', fields: () => ({ id: { type: GraphQLID }, name: { type: GraphQLString }, genre: { type: GraphQLString }, author: { type: AuthorType, resolve(parent, args) { return Author.findById(parent.authorId); } } }) }); const AuthorType = new GraphQLObjectType({ name: 'Author', fields: () => ({ id: { type: GraphQLID }, name: { type: GraphQLString }, age: { type: GraphQLInt }, books: { type: new GraphQLList(BookType), resolve(parent, args) { return Book.find({ authorId: parent.id }); } } }) }); // DEFINE ROOT QUERIES const RootQuery = new GraphQLObjectType({ name: 'RootQueryType', fields: { book: { type: BookType, args: { id: { type: GraphQLID } }, resolve(parent, args) { return Book.findById(args.id); } }, author: { type: AuthorType, args: { id: { type: GraphQLID } }, resolve(parent, args) { return Author.findById(args.id); } }, books: { type: new GraphQLList(BookType), resolve(parent, args) { return Book.find({}); } }, authors: { type: new GraphQLList(AuthorType), resolve(parent, args) { return Author.find({}); } } } }); module.exports = new GraphQLSchema({ query: RootQuery });","title":"Mongoose Models"},{"location":"GraphQL/12-mongoose-models/#mongoose-models","text":"Before we start putting data in the database, we will need to create Mongoose Schema that defines what data format we are going to use. We are going to create a new directory models in the server directory and create two files: book.js and author.js . const mongoose = require('mongoose'); const Schema = mongoose.Schema; const bookSchema = new Schema({ name: String, genre: String, authorId: String, // id is not needed, mongodb creates that on it's own }); module.exports = mongoose.model('Book', bookSchema); const mongoose = require('mongoose'); const Schema = mongoose.Schema; const authorSchema = new Schema({ name: String, age: Number, }); module.exports = mongoose.model('Author', authorSchema); Now we can remove the dummy hardcoded data from the schema/schema.js file as well as modify the resolve functions. // IMPORTS const graphql = require('graphql'); const Book = require('../models/book'); const Author = require('../models/author'); // CONSTANTS const { GraphQLObjectType, GraphQLString, GraphQLSchema, GraphQLID, GraphQLInt, GraphQLList } = graphql // DEFINE OBJECT TYPES // DEFINE OBJECT TYPES const BookType = new GraphQLObjectType({ name: 'Book', fields: () => ({ id: { type: GraphQLID }, name: { type: GraphQLString }, genre: { type: GraphQLString }, author: { type: AuthorType, resolve(parent, args) { return Author.findById(parent.authorId); } } }) }); const AuthorType = new GraphQLObjectType({ name: 'Author', fields: () => ({ id: { type: GraphQLID }, name: { type: GraphQLString }, age: { type: GraphQLInt }, books: { type: new GraphQLList(BookType), resolve(parent, args) { return Book.find({ authorId: parent.id }); } } }) }); // DEFINE ROOT QUERIES const RootQuery = new GraphQLObjectType({ name: 'RootQueryType', fields: { book: { type: BookType, args: { id: { type: GraphQLID } }, resolve(parent, args) { return Book.findById(args.id); } }, author: { type: AuthorType, args: { id: { type: GraphQLID } }, resolve(parent, args) { return Author.findById(args.id); } }, books: { type: new GraphQLList(BookType), resolve(parent, args) { return Book.find({}); } }, authors: { type: new GraphQLList(AuthorType), resolve(parent, args) { return Author.find({}); } } } }); module.exports = new GraphQLSchema({ query: RootQuery });","title":"Mongoose Models"},{"location":"GraphQL/13-mutations/","text":"Mutations \u00b6 Since we don't have any data in our database, it is a perfect time to setup mutations, to insert that data. const Mutation = new GraphQLObjectType({ name: 'Mutation', fields: { addAuthor: { type: AuthorType, args: { name: { type: GraphQLString }, age: { type: GraphQLInt }, }, resolve(parent, args) { const author = new Author({ name: args.name, age: args.age }); author.save(); } }, addBook: { type: BookType, args: { name: { type: GraphQLString }, genre: { type: GraphQLString }, authorId: { type: GraphQLID } }, resolve(parent, args) { const book = new Book({ name: args.name, genre: args.genre, authorId: args.authorId }); book.save(); } } } }); module.exports = new GraphQLSchema({ query: RootQuery, mutation: Mutation }); Now if we post the data: mutation { addAuthor(name: \"Shaun\", age: 30) { id name age } } We are going to get a response of: { \"data\": { \"addAuthor\": null } } But, in the mongodb, it has added it. The reason why it does not return the data, is because we haven't returned anything from the resolve function. const Mutation = new GraphQLObjectType({ name: 'Mutation', fields: { addAuthor: { type: AuthorType, args: { name: { type: GraphQLString }, age: { type: GraphQLInt }, }, resolve(parent, args) { const author = new Author({ name: args.name, age: args.age }); return author.save(); } }, addBook: { type: BookType, args: { name: { type: GraphQLString }, genre: { type: GraphQLString }, authorId: { type: GraphQLID } }, resolve(parent, args) { const book = new Book({ name: args.name, genre: args.genre, authorId: args.authorId }); return book.save(); } } } }); module.exports = new GraphQLSchema({ query: RootQuery, mutation: Mutation }); Now we can query this:","title":"Mutations"},{"location":"GraphQL/13-mutations/#mutations","text":"Since we don't have any data in our database, it is a perfect time to setup mutations, to insert that data. const Mutation = new GraphQLObjectType({ name: 'Mutation', fields: { addAuthor: { type: AuthorType, args: { name: { type: GraphQLString }, age: { type: GraphQLInt }, }, resolve(parent, args) { const author = new Author({ name: args.name, age: args.age }); author.save(); } }, addBook: { type: BookType, args: { name: { type: GraphQLString }, genre: { type: GraphQLString }, authorId: { type: GraphQLID } }, resolve(parent, args) { const book = new Book({ name: args.name, genre: args.genre, authorId: args.authorId }); book.save(); } } } }); module.exports = new GraphQLSchema({ query: RootQuery, mutation: Mutation }); Now if we post the data: mutation { addAuthor(name: \"Shaun\", age: 30) { id name age } } We are going to get a response of: { \"data\": { \"addAuthor\": null } } But, in the mongodb, it has added it. The reason why it does not return the data, is because we haven't returned anything from the resolve function. const Mutation = new GraphQLObjectType({ name: 'Mutation', fields: { addAuthor: { type: AuthorType, args: { name: { type: GraphQLString }, age: { type: GraphQLInt }, }, resolve(parent, args) { const author = new Author({ name: args.name, age: args.age }); return author.save(); } }, addBook: { type: BookType, args: { name: { type: GraphQLString }, genre: { type: GraphQLString }, authorId: { type: GraphQLID } }, resolve(parent, args) { const book = new Book({ name: args.name, genre: args.genre, authorId: args.authorId }); return book.save(); } } } }); module.exports = new GraphQLSchema({ query: RootQuery, mutation: Mutation }); Now we can query this:","title":"Mutations"},{"location":"GraphQL/14-graphql-nonnull/","text":"GrapGL NonNull \u00b6 We can use GrapQLNonNull function to validate that mutations require given fields, they cannot be null. const Mutation = new GraphQLObjectType({ name: 'Mutation', fields: { addAuthor: { type: AuthorType, args: { name: { type: new GraphQLNonNull(GraphQLString) }, age: { type: new GraphQLNonNull(GraphQLInt) }, }, resolve(parent, args) { const author = new Author({ name: args.name, age: args.age }); return author.save(); } }, addBook: { type: BookType, args: { name: { type: new GraphQLNonNull(GraphQLString) }, genre: { type: new GraphQLNonNull(GraphQLString) }, authorId: { type: new GraphQLNonNull(GraphQLID) } }, resolve(parent, args) { const book = new Book({ name: args.name, genre: args.genre, authorId: args.authorId }); return book.save(); } } } });","title":"GrapGL NonNull"},{"location":"GraphQL/14-graphql-nonnull/#grapgl-nonnull","text":"We can use GrapQLNonNull function to validate that mutations require given fields, they cannot be null. const Mutation = new GraphQLObjectType({ name: 'Mutation', fields: { addAuthor: { type: AuthorType, args: { name: { type: new GraphQLNonNull(GraphQLString) }, age: { type: new GraphQLNonNull(GraphQLInt) }, }, resolve(parent, args) { const author = new Author({ name: args.name, age: args.age }); return author.save(); } }, addBook: { type: BookType, args: { name: { type: new GraphQLNonNull(GraphQLString) }, genre: { type: new GraphQLNonNull(GraphQLString) }, authorId: { type: new GraphQLNonNull(GraphQLID) } }, resolve(parent, args) { const book = new Book({ name: args.name, genre: args.genre, authorId: args.authorId }); return book.save(); } } } });","title":"GrapGL NonNull"},{"location":"GraphQL/15-adding-frontend/","text":"Adding frontend \u00b6 Currently we have made a server with a GraphQL endpoint. We are going to make a new React app that will use Apollo to query the graphql data. \u279c ~ cd projects/learning/graphql \u279c graphql git:(master) sudo npm i create-react-app -g [sudo] password for davis: /usr/bin/create-react-app -> /usr/lib/node_modules/create-react-app/index.js + create-react-app@3.3.0 added 91 packages from 45 contributors in 9.11s \u279c graphql git:(master) create-react-app client Creating a new React app in /home/davis/projects/learning/graphql/client. Installing packages. This might take a couple of minutes. Installing react, react-dom, and react-scripts with cra-template... > core-js@2.6.11 postinstall /home/davis/projects/learning/graphql/client/node_modules/babel-runtime/node_modules/core-js > node -e \"try{require('./postinstall')}catch(e){}\" > core-js@3.6.0 postinstall /home/davis/projects/learning/graphql/client/node_modules/core-js > node -e \"try{require('./postinstall')}catch(e){}\" > core-js-pure@3.6.0 postinstall /home/davis/projects/learning/graphql/client/node_modules/core-js-pure > node -e \"try{require('./postinstall')}catch(e){}\" + cra-template@1.0.0 + react-dom@16.12.0 + react@16.12.0 + react-scripts@3.3.0 added 1535 packages from 745 contributors and audited 906206 packages in 68.619s 32 packages are looking for funding run `npm fund` for details found 0 vulnerabilities Installing template dependencies using npm... npm WARN tsutils@3.17.1 requires a peer of typescript@>=2.8.0 || >= 3.2.0-dev || >= 3.3.0-dev || >= 3.4.0-dev || >= 3.5.0-dev || >= 3.6.0-dev || >= 3.6.0-beta || >= 3.7.0-dev || >= 3.7.0-beta but none is installed. You must install peer dependencies yourself. npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.11 (node_modules/jest-haste-map/node_modules/fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.11: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"}) npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.11 (node_modules/chokidar/node_modules/fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.11: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"}) npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@2.1.2 (node_modules/fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@2.1.2: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"}) + @testing-library/react@9.4.0 + @testing-library/jest-dom@4.2.4 + @testing-library/user-event@7.2.1 added 17 packages from 40 contributors and audited 906378 packages in 16.231s 32 packages are looking for funding run `npm fund` for details found 0 vulnerabilities Removing template package using npm... npm WARN tsutils@3.17.1 requires a peer of typescript@>=2.8.0 || >= 3.2.0-dev || >= 3.3.0-dev || >= 3.4.0-dev || >= 3.5.0-dev || >= 3.6.0-dev || >= 3.6.0-beta || >= 3.7.0-dev || >= 3.7.0-beta but none is installed. You must install peer dependencies yourself. npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.11 (node_modules/jest-haste-map/node_modules/fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.11: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"}) npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.11 (node_modules/chokidar/node_modules/fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.11: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"}) npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@2.1.2 (node_modules/fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@2.1.2: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"}) removed 1 package and audited 906377 packages in 8.443s 32 packages are looking for funding run `npm fund` for details found 0 vulnerabilities Success! Created client at /home/davis/projects/learning/graphql/client Inside that directory, you can run several commands: npm start Starts the development server. npm run build Bundles the app into static files for production. npm test Starts the test runner. npm run eject Removes this tool and copies build dependencies, configuration files and scripts into the app directory. If you do this, you can\u2019t go back! We suggest that you begin by typing: cd client npm start Happy hacking! \u279c graphql git:(master) \u2717 Now we can start the app \u279c graphql git:(master) \u2717 cd client \u279c client git:(master) \u2717 npm start Compiled successfully! You can now view client in the browser. Local: http://localhost:3000/ On Your Network: http://192.168.1.171:3000/ Note that the development build is not optimized. To create a production build, use npm run build. Now we are going to clean up the project - delete the logo, test related files, app.css and modify the index.js: import React from 'react'; import ReactDOM from 'react-dom'; import './index.css'; import App from './App'; ReactDOM.render(<App />, document.getElementById('root')); As well as app.js import React from 'react'; import BookList from './components/BookList' function App() { return ( <div id=\"main\"> <h1>Reading List</h1> <BookList /> </div> ); } export default App; Now we are going to create a new file src/components/BookList.js . import React, { Component } from 'react'; class BookList extends Component { render() { return ( <div> <ul id=\"book-list\"> <li>Item</li> </ul> </div> ) } } export default BookList;","title":"Adding frontend"},{"location":"GraphQL/15-adding-frontend/#adding-frontend","text":"Currently we have made a server with a GraphQL endpoint. We are going to make a new React app that will use Apollo to query the graphql data. \u279c ~ cd projects/learning/graphql \u279c graphql git:(master) sudo npm i create-react-app -g [sudo] password for davis: /usr/bin/create-react-app -> /usr/lib/node_modules/create-react-app/index.js + create-react-app@3.3.0 added 91 packages from 45 contributors in 9.11s \u279c graphql git:(master) create-react-app client Creating a new React app in /home/davis/projects/learning/graphql/client. Installing packages. This might take a couple of minutes. Installing react, react-dom, and react-scripts with cra-template... > core-js@2.6.11 postinstall /home/davis/projects/learning/graphql/client/node_modules/babel-runtime/node_modules/core-js > node -e \"try{require('./postinstall')}catch(e){}\" > core-js@3.6.0 postinstall /home/davis/projects/learning/graphql/client/node_modules/core-js > node -e \"try{require('./postinstall')}catch(e){}\" > core-js-pure@3.6.0 postinstall /home/davis/projects/learning/graphql/client/node_modules/core-js-pure > node -e \"try{require('./postinstall')}catch(e){}\" + cra-template@1.0.0 + react-dom@16.12.0 + react@16.12.0 + react-scripts@3.3.0 added 1535 packages from 745 contributors and audited 906206 packages in 68.619s 32 packages are looking for funding run `npm fund` for details found 0 vulnerabilities Installing template dependencies using npm... npm WARN tsutils@3.17.1 requires a peer of typescript@>=2.8.0 || >= 3.2.0-dev || >= 3.3.0-dev || >= 3.4.0-dev || >= 3.5.0-dev || >= 3.6.0-dev || >= 3.6.0-beta || >= 3.7.0-dev || >= 3.7.0-beta but none is installed. You must install peer dependencies yourself. npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.11 (node_modules/jest-haste-map/node_modules/fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.11: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"}) npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.11 (node_modules/chokidar/node_modules/fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.11: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"}) npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@2.1.2 (node_modules/fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@2.1.2: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"}) + @testing-library/react@9.4.0 + @testing-library/jest-dom@4.2.4 + @testing-library/user-event@7.2.1 added 17 packages from 40 contributors and audited 906378 packages in 16.231s 32 packages are looking for funding run `npm fund` for details found 0 vulnerabilities Removing template package using npm... npm WARN tsutils@3.17.1 requires a peer of typescript@>=2.8.0 || >= 3.2.0-dev || >= 3.3.0-dev || >= 3.4.0-dev || >= 3.5.0-dev || >= 3.6.0-dev || >= 3.6.0-beta || >= 3.7.0-dev || >= 3.7.0-beta but none is installed. You must install peer dependencies yourself. npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.11 (node_modules/jest-haste-map/node_modules/fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.11: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"}) npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.11 (node_modules/chokidar/node_modules/fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.11: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"}) npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@2.1.2 (node_modules/fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@2.1.2: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"}) removed 1 package and audited 906377 packages in 8.443s 32 packages are looking for funding run `npm fund` for details found 0 vulnerabilities Success! Created client at /home/davis/projects/learning/graphql/client Inside that directory, you can run several commands: npm start Starts the development server. npm run build Bundles the app into static files for production. npm test Starts the test runner. npm run eject Removes this tool and copies build dependencies, configuration files and scripts into the app directory. If you do this, you can\u2019t go back! We suggest that you begin by typing: cd client npm start Happy hacking! \u279c graphql git:(master) \u2717 Now we can start the app \u279c graphql git:(master) \u2717 cd client \u279c client git:(master) \u2717 npm start Compiled successfully! You can now view client in the browser. Local: http://localhost:3000/ On Your Network: http://192.168.1.171:3000/ Note that the development build is not optimized. To create a production build, use npm run build. Now we are going to clean up the project - delete the logo, test related files, app.css and modify the index.js: import React from 'react'; import ReactDOM from 'react-dom'; import './index.css'; import App from './App'; ReactDOM.render(<App />, document.getElementById('root')); As well as app.js import React from 'react'; import BookList from './components/BookList' function App() { return ( <div id=\"main\"> <h1>Reading List</h1> <BookList /> </div> ); } export default App; Now we are going to create a new file src/components/BookList.js . import React, { Component } from 'react'; class BookList extends Component { render() { return ( <div> <ul id=\"book-list\"> <li>Item</li> </ul> </div> ) } } export default BookList;","title":"Adding frontend"},{"location":"GraphQL/16-apollo-client-setup/","text":"Apollo Client Setup \u00b6 We are going to use Apollo as a client to the GraphQL endpoint. It will query our node.js server from react. \u279c client git:(master) \u2717 npm i apollo-boost react-apollo graphql --save npm WARN tsutils@3.17.1 requires a peer of typescript@>=2.8.0 || >= 3.2.0-dev || >= 3.3.0-dev || >= 3.4.0-dev || >= 3.5.0-dev || >= 3.6.0-dev || >= 3.6.0-beta || >= 3.7.0-dev || >= 3.7.0-beta but none is installed. You must install peer dependencies yourself. npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.11 (node_modules/jest-haste-map/node_modules/fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.11: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"}) npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.11 (node_modules/chokidar/node_modules/fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.11: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"}) npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@2.1.2 (node_modules/fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@2.1.2: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"}) + graphql@14.5.8 + apollo-boost@0.4.7 + react-apollo@3.1.3 added 27 packages from 22 contributors and audited 906623 packages in 27.466s 32 packages are looking for funding run `npm fund` for details found 0 vulnerabilities Now we are going to modify the App.js to use the apollo and inject it into our application. import React from 'react'; import BookList from './components/BookList'; import { ApolloProvider } from 'react-apollo'; // components import ApolloClient from 'apollo-boost'; // apollo client setup const client = new ApolloClient({ uri: 'http://localhost:4000/graphql', }); function App() { return ( <ApolloProvider client={client}> <div id=\"main\"> <h1>Reading List</h1> <BookList /> </div> </ApolloProvider> ); } export default App;","title":"Apollo Client Setup"},{"location":"GraphQL/16-apollo-client-setup/#apollo-client-setup","text":"We are going to use Apollo as a client to the GraphQL endpoint. It will query our node.js server from react. \u279c client git:(master) \u2717 npm i apollo-boost react-apollo graphql --save npm WARN tsutils@3.17.1 requires a peer of typescript@>=2.8.0 || >= 3.2.0-dev || >= 3.3.0-dev || >= 3.4.0-dev || >= 3.5.0-dev || >= 3.6.0-dev || >= 3.6.0-beta || >= 3.7.0-dev || >= 3.7.0-beta but none is installed. You must install peer dependencies yourself. npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.11 (node_modules/jest-haste-map/node_modules/fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.11: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"}) npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.11 (node_modules/chokidar/node_modules/fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.11: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"}) npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@2.1.2 (node_modules/fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@2.1.2: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"}) + graphql@14.5.8 + apollo-boost@0.4.7 + react-apollo@3.1.3 added 27 packages from 22 contributors and audited 906623 packages in 27.466s 32 packages are looking for funding run `npm fund` for details found 0 vulnerabilities Now we are going to modify the App.js to use the apollo and inject it into our application. import React from 'react'; import BookList from './components/BookList'; import { ApolloProvider } from 'react-apollo'; // components import ApolloClient from 'apollo-boost'; // apollo client setup const client = new ApolloClient({ uri: 'http://localhost:4000/graphql', }); function App() { return ( <ApolloProvider client={client}> <div id=\"main\"> <h1>Reading List</h1> <BookList /> </div> </ApolloProvider> ); } export default App;","title":"Apollo Client Setup"},{"location":"GraphQL/17-making-queries-from-react/","text":"Making queries from React \u00b6 We can now modify the BookList.js component to query graphql and get a list of books. Backend Cross-origin \u00b6 First, we will construct the query, for that we will need another package in the server. \u279c server git:(master) \u2717 npm i cors --save npm WARN server@1.0.0 No description npm WARN server@1.0.0 No repository field. + cors@2.8.5 added 2 packages from 2 contributors and audited 182 packages in 0.999s 1 package is looking for funding run `npm fund` for details found 0 vulnerabilities And then add it to our app.js const cors = require('cors'); app.use(cors()); This will allow our server to handle cross-origin requests. Handling requests from React \u00b6 We are going to modify the BookList.js file to use graphql queries, which will store the result in the props. import React, { Component } from 'react'; import { gql } from 'apollo-boost'; import { graphql } from 'react-apollo'; const getBooksQuery = gql` { books { name id } } ` class BookList extends Component { displayBooks() { if(this.props.data.loading) { return (<div>Loading books...</div>); } return this.props.data.books.map(book => { return (<li key={book.id}>{book.name}</li>) }); } render() { return ( <div> <ul id=\"book-list\"> { this.displayBooks() } </ul> </div> ) } } export default graphql(getBooksQuery)(BookList);","title":"Making queries from React"},{"location":"GraphQL/17-making-queries-from-react/#making-queries-from-react","text":"We can now modify the BookList.js component to query graphql and get a list of books.","title":"Making queries from React"},{"location":"GraphQL/17-making-queries-from-react/#backend-cross-origin","text":"First, we will construct the query, for that we will need another package in the server. \u279c server git:(master) \u2717 npm i cors --save npm WARN server@1.0.0 No description npm WARN server@1.0.0 No repository field. + cors@2.8.5 added 2 packages from 2 contributors and audited 182 packages in 0.999s 1 package is looking for funding run `npm fund` for details found 0 vulnerabilities And then add it to our app.js const cors = require('cors'); app.use(cors()); This will allow our server to handle cross-origin requests.","title":"Backend Cross-origin"},{"location":"GraphQL/17-making-queries-from-react/#handling-requests-from-react","text":"We are going to modify the BookList.js file to use graphql queries, which will store the result in the props. import React, { Component } from 'react'; import { gql } from 'apollo-boost'; import { graphql } from 'react-apollo'; const getBooksQuery = gql` { books { name id } } ` class BookList extends Component { displayBooks() { if(this.props.data.loading) { return (<div>Loading books...</div>); } return this.props.data.books.map(book => { return (<li key={book.id}>{book.name}</li>) }); } render() { return ( <div> <ul id=\"book-list\"> { this.displayBooks() } </ul> </div> ) } } export default graphql(getBooksQuery)(BookList);","title":"Handling requests from React"},{"location":"GraphQL/18-add-book-component/","text":"Add book component \u00b6 We create a new component AddBook , bring out queries to their own file. const addBookMutation = gql` mutation($name: String!, $genre: String!, $authorId: ID!) { addBook(name: $name, genre: $genre, authorId: $authorId) { name id } } ` import React, { Component } from 'react'; import { graphql } from 'react-apollo'; import { compose } from 'recompose'; import { getAuthorsQuery, addBookMutation, getBooksQuery } from '../queries/queries'; class AddBook extends Component { constructor(props) { super(props); this.state = { name: '', genre: '', authorId: '', }; } displayAuthors() { if(this.props.getAuthorsQuery.loading) { return (<option>Loading authors...</option>); } return this.props.getAuthorsQuery.authors.map(author => { return (<option key={author.id} value={author.id}>{author.name}</option>); }); } submitForm(e) { e.preventDefault(); this.props.addBookMutation({ variables: { name: this.state.name, genre: this.state.genre, authorId: this.state.authorId }, refetchQueries: [ { query: getBooksQuery } ] }); } render() { return ( <form id=\"add-book\" onSubmit={ this.submitForm.bind(this) }> <div className=\"field\"> <label>Book name:</label> <input type=\"text\" onChange={ (e) => this.setState({ name: e.target.value }) } /> </div> <div className=\"field\"> <label>Genre:</label> <input type=\"text\" onChange={ (e) => this.setState({ genre: e.target.value }) } /> </div> <div className=\"field\"> <label>Author:</label> <select onChange={ (e) => this.setState({ authorId: e.target.value }) }> <option>Select author</option> { this.displayAuthors() } </select> </div> <button>+</button> </form> ); } } export default compose( graphql(getAuthorsQuery, {name: 'getAuthorsQuery'}), graphql(addBookMutation, {name: 'addBookMutation'}) )(AddBook); Note that we are using compose since we have multiple queries bound to the component. Each has name and result will be stored at the this.props.{queryName} . Also, when calling the mutation the refetchQueries is used to refetch the list of the books in the BookList component on form submission.","title":"Add book component"},{"location":"GraphQL/18-add-book-component/#add-book-component","text":"We create a new component AddBook , bring out queries to their own file. const addBookMutation = gql` mutation($name: String!, $genre: String!, $authorId: ID!) { addBook(name: $name, genre: $genre, authorId: $authorId) { name id } } ` import React, { Component } from 'react'; import { graphql } from 'react-apollo'; import { compose } from 'recompose'; import { getAuthorsQuery, addBookMutation, getBooksQuery } from '../queries/queries'; class AddBook extends Component { constructor(props) { super(props); this.state = { name: '', genre: '', authorId: '', }; } displayAuthors() { if(this.props.getAuthorsQuery.loading) { return (<option>Loading authors...</option>); } return this.props.getAuthorsQuery.authors.map(author => { return (<option key={author.id} value={author.id}>{author.name}</option>); }); } submitForm(e) { e.preventDefault(); this.props.addBookMutation({ variables: { name: this.state.name, genre: this.state.genre, authorId: this.state.authorId }, refetchQueries: [ { query: getBooksQuery } ] }); } render() { return ( <form id=\"add-book\" onSubmit={ this.submitForm.bind(this) }> <div className=\"field\"> <label>Book name:</label> <input type=\"text\" onChange={ (e) => this.setState({ name: e.target.value }) } /> </div> <div className=\"field\"> <label>Genre:</label> <input type=\"text\" onChange={ (e) => this.setState({ genre: e.target.value }) } /> </div> <div className=\"field\"> <label>Author:</label> <select onChange={ (e) => this.setState({ authorId: e.target.value }) }> <option>Select author</option> { this.displayAuthors() } </select> </div> <button>+</button> </form> ); } } export default compose( graphql(getAuthorsQuery, {name: 'getAuthorsQuery'}), graphql(addBookMutation, {name: 'addBookMutation'}) )(AddBook); Note that we are using compose since we have multiple queries bound to the component. Each has name and result will be stored at the this.props.{queryName} . Also, when calling the mutation the refetchQueries is used to refetch the list of the books in the BookList component on form submission.","title":"Add book component"},{"location":"GraphQL/31-book-details-component/","text":"Book Details component \u00b6 We are going to add a new component for getting book details. const getBookQuery = gql` query($id: ID) { book(id: $id) { id name genre author { id name age books { name id } } } } ` import React, { Component } from 'react'; import { graphql } from 'react-apollo'; import { getBookQuery } from '../queries/queries'; class BookDetails extends Component { displayBookDetails() { const { book } = this.props.data; if(book) { return ( <div> <h2>{ book.name }</h2> <p>{ book.genre }</p> <p>{ book.author.name }</p> <p>All books by this author</p> <ul> { book.author.books.map(item => { return <li key={item.id}>{item.name}</li> }) } </ul> </div> ); } return (<div>No book selected</div>); } render() { return ( <div id=\"book-details\"> { this.displayBookDetails() } </div> ); } } export default graphql(getBookQuery, { options: (props) => { return { variables: { id: props.bookId } } } })(BookDetails); Note the export part, we are taking the prop and passing it to the graphql query as the ID param. We'll modify the BookList.js file as well, it will call the component and pass the book id to it once clicked on the title. import React, { Component } from 'react'; import { graphql } from 'react-apollo'; import { getBooksQuery } from '../queries/queries'; import BookDetails from './BookDetails'; class BookList extends Component { constructor(props) { super(props); this.state = { selected: null, } } displayBooks() { if(this.props.data.loading) { return (<div>Loading books...</div>); } return this.props.data.books.map(book => { return (<li onClick={ e => this.setState({selected: book.id}) } key={book.id}>{book.name}</li>) }); } render() { return ( <div> <ul id=\"book-list\"> { this.displayBooks() } </ul> <BookDetails bookId={ this.state.selected } /> </div> ) } } export default graphql(getBooksQuery)(BookList);","title":"Book Details component"},{"location":"GraphQL/31-book-details-component/#book-details-component","text":"We are going to add a new component for getting book details. const getBookQuery = gql` query($id: ID) { book(id: $id) { id name genre author { id name age books { name id } } } } ` import React, { Component } from 'react'; import { graphql } from 'react-apollo'; import { getBookQuery } from '../queries/queries'; class BookDetails extends Component { displayBookDetails() { const { book } = this.props.data; if(book) { return ( <div> <h2>{ book.name }</h2> <p>{ book.genre }</p> <p>{ book.author.name }</p> <p>All books by this author</p> <ul> { book.author.books.map(item => { return <li key={item.id}>{item.name}</li> }) } </ul> </div> ); } return (<div>No book selected</div>); } render() { return ( <div id=\"book-details\"> { this.displayBookDetails() } </div> ); } } export default graphql(getBookQuery, { options: (props) => { return { variables: { id: props.bookId } } } })(BookDetails); Note the export part, we are taking the prop and passing it to the graphql query as the ID param. We'll modify the BookList.js file as well, it will call the component and pass the book id to it once clicked on the title. import React, { Component } from 'react'; import { graphql } from 'react-apollo'; import { getBooksQuery } from '../queries/queries'; import BookDetails from './BookDetails'; class BookList extends Component { constructor(props) { super(props); this.state = { selected: null, } } displayBooks() { if(this.props.data.loading) { return (<div>Loading books...</div>); } return this.props.data.books.map(book => { return (<li onClick={ e => this.setState({selected: book.id}) } key={book.id}>{book.name}</li>) }); } render() { return ( <div> <ul id=\"book-list\"> { this.displayBooks() } </ul> <BookDetails bookId={ this.state.selected } /> </div> ) } } export default graphql(getBooksQuery)(BookList);","title":"Book Details component"},{"location":"Graphics/Unity/2D/","text":"2D graphics with Unity \u00b6 Sources: - Complete C# Unity Developer 2D: Learn to Code Making Games Code available at https://github.com/daviskregers/unity .","title":"2D graphics with Unity"},{"location":"Graphics/Unity/2D/#2d-graphics-with-unity","text":"Sources: - Complete C# Unity Developer 2D: Learn to Code Making Games Code available at https://github.com/daviskregers/unity .","title":"2D graphics with Unity"},{"location":"Graphics/Unity/2D/01-introduction-and-setup/01-download-unity-and-visual-studio/","text":"Download Unity and Visual Studio \u00b6 You can go to https://unity3d.com/get-unity/download and download the [[Unity]]. Choose the Unity Hub version. Once downloaded and installed, it will prompt for a license. In order to obtain it, log in or create an account with unity. Once logged in, the Activate new license button will be clickable and you will be able to obtain a license: Now you can click on the the top left arrow icon to go to the start, and go to the Installs tab. There will be a list of all unity versions installed. We can click on add button and choose the version and the additional features we want. Make sure to select the Visual Studio option. Then it will start downloading and installing unity.","title":"Download Unity and Visual Studio"},{"location":"Graphics/Unity/2D/01-introduction-and-setup/01-download-unity-and-visual-studio/#download-unity-and-visual-studio","text":"You can go to https://unity3d.com/get-unity/download and download the [[Unity]]. Choose the Unity Hub version. Once downloaded and installed, it will prompt for a license. In order to obtain it, log in or create an account with unity. Once logged in, the Activate new license button will be clickable and you will be able to obtain a license: Now you can click on the the top left arrow icon to go to the start, and go to the Installs tab. There will be a list of all unity versions installed. We can click on add button and choose the version and the additional features we want. Make sure to select the Visual Studio option. Then it will start downloading and installing unity.","title":"Download Unity and Visual Studio"},{"location":"Graphics/Unity/2D/01-introduction-and-setup/02-your-first-code/","text":"Your first code \u00b6 In this section we are going to make a Hello World application in [[Unity]] to check that everything is set up properly. How Unity and Visual Studio Relate \u00b6 The unity is the game engine that describes what everything is and how it goes together. All the code that does this is being written in [[Visual Studio]], which is an [[IDE]]. Setup \u00b6 We can go to Projects tab in [[unity]]. And create a new project with a 2D template. Once that's done, you will have a unity project opened. Now in the assets area, create a new [[C# script]], name it HelloWorld. Now, when you double click on the script, it will open up [[Visual Studio]]. Now we can modify the script: using System.Collections; using System.Collections.Generic; using UnityEngine; public class HelloWorld : MonoBehaviour { // Start is called before the first frame update void Start() { print(\"Hello World!\"); } // Update is called once per frame void Update() { } } Now we need to attach the script to an object, we can add it to the [[Main Camera object]], we have. Simply drag the script over it. Now when we open up console and click on the Play button, the Hello World script executes.","title":"Your first code"},{"location":"Graphics/Unity/2D/01-introduction-and-setup/02-your-first-code/#your-first-code","text":"In this section we are going to make a Hello World application in [[Unity]] to check that everything is set up properly.","title":"Your first code"},{"location":"Graphics/Unity/2D/01-introduction-and-setup/02-your-first-code/#how-unity-and-visual-studio-relate","text":"The unity is the game engine that describes what everything is and how it goes together. All the code that does this is being written in [[Visual Studio]], which is an [[IDE]].","title":"How Unity and Visual Studio Relate"},{"location":"Graphics/Unity/2D/01-introduction-and-setup/02-your-first-code/#setup","text":"We can go to Projects tab in [[unity]]. And create a new project with a 2D template. Once that's done, you will have a unity project opened. Now in the assets area, create a new [[C# script]], name it HelloWorld. Now, when you double click on the script, it will open up [[Visual Studio]]. Now we can modify the script: using System.Collections; using System.Collections.Generic; using UnityEngine; public class HelloWorld : MonoBehaviour { // Start is called before the first frame update void Start() { print(\"Hello World!\"); } // Update is called once per frame void Update() { } } Now we need to attach the script to an object, we can add it to the [[Main Camera object]], we have. Simply drag the script over it. Now when we open up console and click on the Play button, the Hello World script executes.","title":"Setup"},{"location":"Graphics/Unity/2D/01-introduction-and-setup/03-notes-for-mac-users/","text":"Notes for mac users \u00b6 Menus stick to the top of the screen, not the app There is an extra menu with the name of the app Hidden files are hidden by default [[Visual Studio]] community has different herritage Some [[code completion]] may be different","title":"Notes for mac users"},{"location":"Graphics/Unity/2D/01-introduction-and-setup/03-notes-for-mac-users/#notes-for-mac-users","text":"Menus stick to the top of the screen, not the app There is an extra menu with the name of the app Hidden files are hidden by default [[Visual Studio]] community has different herritage Some [[code completion]] may be different","title":"Notes for mac users"},{"location":"Graphics/Unity/2D/02-number-wizard/01-game-description/","text":"Game description \u00b6 The game basically is asking player to think of a number in a certain range like 1-1000. Then we will guess the number by doing (min+max)/2 . Then the player will indicate if the quess is lower or higher and we will adjust the min or max value. Basically we are doing binary search.","title":"Game description"},{"location":"Graphics/Unity/2D/02-number-wizard/01-game-description/#game-description","text":"The game basically is asking player to think of a number in a certain range like 1-1000. Then we will guess the number by doing (min+max)/2 . Then the player will indicate if the quess is lower or higher and we will adjust the min or max value. Basically we are doing binary search.","title":"Game description"},{"location":"Graphics/Unity/2D/02-number-wizard/02-print-to-console-with-debug-log/","text":"Print To Console With Debug.Log \u00b6 We'll make a new project. We are going to create a new C# script now and call it NumberWizard , open it up in Visual Studio. using System.Collections; using System.Collections.Generic; using UnityEngine; public class NumberWizard : MonoBehaviour { // Start is called before the first frame update void Start() { Debug.Log(\"Welcome to number wizard!\"); Debug.Log(\"Pick a number, don't tell me what it is.\"); Debug.Log(\"The highest number you can pick is 1000.\"); Debug.Log(\"The lowest number you can pick is 1.\"); } // Update is called once per frame void Update() { } } Now add the script to the camera and run it.","title":"Print To Console With Debug.Log"},{"location":"Graphics/Unity/2D/02-number-wizard/02-print-to-console-with-debug-log/#print-to-console-with-debuglog","text":"We'll make a new project. We are going to create a new C# script now and call it NumberWizard , open it up in Visual Studio. using System.Collections; using System.Collections.Generic; using UnityEngine; public class NumberWizard : MonoBehaviour { // Start is called before the first frame update void Start() { Debug.Log(\"Welcome to number wizard!\"); Debug.Log(\"Pick a number, don't tell me what it is.\"); Debug.Log(\"The highest number you can pick is 1000.\"); Debug.Log(\"The lowest number you can pick is 1.\"); } // Update is called once per frame void Update() { } } Now add the script to the camera and run it.","title":"Print To Console With Debug.Log"},{"location":"Graphics/Unity/2D/02-number-wizard/03-introducing-variables/","text":"Introducting variables \u00b6 Variables are like boxes: - They are of a particular type . - Each variable has a name. - Each variable contains data. int hitPoints = 20; float speed = 3.8f; bool isAlive = true; string name = \"Rick\"; We can use modify the previously made script to use variables for the min and max value. using System.Collections; using System.Collections.Generic; using UnityEngine; public class NumberWizard : MonoBehaviour { int min = 1; int max = 1000; // Start is called before the first frame update void Start() { Debug.Log(\"Welcome to number wizard!\"); Debug.Log(\"Pick a number, don't tell me what it is.\"); Debug.Log(\"The highest number you can pick is \" + max); Debug.Log(\"The lowest number you can pick is \" + min); } // Update is called once per frame void Update() { } }","title":"Introducting variables"},{"location":"Graphics/Unity/2D/02-number-wizard/03-introducing-variables/#introducting-variables","text":"Variables are like boxes: - They are of a particular type . - Each variable has a name. - Each variable contains data. int hitPoints = 20; float speed = 3.8f; bool isAlive = true; string name = \"Rick\"; We can use modify the previously made script to use variables for the min and max value. using System.Collections; using System.Collections.Generic; using UnityEngine; public class NumberWizard : MonoBehaviour { int min = 1; int max = 1000; // Start is called before the first frame update void Start() { Debug.Log(\"Welcome to number wizard!\"); Debug.Log(\"Pick a number, don't tell me what it is.\"); Debug.Log(\"The highest number you can pick is \" + max); Debug.Log(\"The lowest number you can pick is \" + min); } // Update is called once per frame void Update() { } }","title":"Introducting variables"},{"location":"Graphics/Unity/2D/02-number-wizard/04-respond-to-player-input/","text":"Respond to player input \u00b6 We are going to make check for player input to add the ability for the player to indicate whether the guess is higher or lower. We are going to add some more explanation text and add methods in the update loop to check for key presses. using System.Collections; using System.Collections.Generic; using UnityEngine; public class NumberWizard : MonoBehaviour { int min = 1; int max = 1000; // Start is called before the first frame update void Start() { Debug.Log(\"Welcome to number wizard!\"); Debug.Log(\"Pick a number, don't tell me what it is.\"); Debug.Log(\"The highest number you can pick is \" + max); Debug.Log(\"The lowest number you can pick is \" + min); Debug.Log(\"Tell me if your number is higher or lower than my guess\"); Debug.Log(\"Push Up = higher, Push down = lower, Push enter = Correct\"); } // Update is called once per frame void Update() { if ( Input.GetKeyDown(KeyCode.UpArrow) ) { Debug.Log(\"Up Arrow was pressed.\"); } if (Input.GetKeyDown(KeyCode.DownArrow)) { Debug.Log(\"Down Arrow was pressed.\"); } if (Input.GetKeyDown(KeyCode.Return)) { Debug.Log(\"Enter was pressed.\"); } } } When running, we get this:","title":"Respond to player input"},{"location":"Graphics/Unity/2D/02-number-wizard/04-respond-to-player-input/#respond-to-player-input","text":"We are going to make check for player input to add the ability for the player to indicate whether the guess is higher or lower. We are going to add some more explanation text and add methods in the update loop to check for key presses. using System.Collections; using System.Collections.Generic; using UnityEngine; public class NumberWizard : MonoBehaviour { int min = 1; int max = 1000; // Start is called before the first frame update void Start() { Debug.Log(\"Welcome to number wizard!\"); Debug.Log(\"Pick a number, don't tell me what it is.\"); Debug.Log(\"The highest number you can pick is \" + max); Debug.Log(\"The lowest number you can pick is \" + min); Debug.Log(\"Tell me if your number is higher or lower than my guess\"); Debug.Log(\"Push Up = higher, Push down = lower, Push enter = Correct\"); } // Update is called once per frame void Update() { if ( Input.GetKeyDown(KeyCode.UpArrow) ) { Debug.Log(\"Up Arrow was pressed.\"); } if (Input.GetKeyDown(KeyCode.DownArrow)) { Debug.Log(\"Down Arrow was pressed.\"); } if (Input.GetKeyDown(KeyCode.Return)) { Debug.Log(\"Enter was pressed.\"); } } } When running, we get this:","title":"Respond to player input"},{"location":"Graphics/Unity/2D/02-number-wizard/05-using-if-else-if-else/","text":"Using if, else if, else \u00b6 In this section we see what logical branching is, modify the source to check for only one key at the time. using System.Collections; using System.Collections.Generic; using UnityEngine; public class NumberWizard : MonoBehaviour { int min = 1; int max = 1000; int guess = 500; // Start is called before the first frame update void Start() { Debug.Log(\"Welcome to number wizard!\"); Debug.Log(\"Pick a number, don't tell me what it is.\"); Debug.Log(\"The highest number you can pick is \" + max); Debug.Log(\"The lowest number you can pick is \" + min); Debug.Log(\"Tell me if your number is higher or lower than my guess\"); Debug.Log(\"Push Up = higher, Push down = lower, Push enter = Correct\"); } // Update is called once per frame void Update() { if ( Input.GetKeyDown(KeyCode.UpArrow) ) { Debug.Log(\"Up Arrow was pressed.\"); min = guess; } else if (Input.GetKeyDown(KeyCode.DownArrow)) { Debug.Log(\"Down Arrow was pressed.\"); } else if (Input.GetKeyDown(KeyCode.Return)) { Debug.Log(\"Enter was pressed.\"); } } }","title":"Using if, else if, else"},{"location":"Graphics/Unity/2D/02-number-wizard/05-using-if-else-if-else/#using-if-else-if-else","text":"In this section we see what logical branching is, modify the source to check for only one key at the time. using System.Collections; using System.Collections.Generic; using UnityEngine; public class NumberWizard : MonoBehaviour { int min = 1; int max = 1000; int guess = 500; // Start is called before the first frame update void Start() { Debug.Log(\"Welcome to number wizard!\"); Debug.Log(\"Pick a number, don't tell me what it is.\"); Debug.Log(\"The highest number you can pick is \" + max); Debug.Log(\"The lowest number you can pick is \" + min); Debug.Log(\"Tell me if your number is higher or lower than my guess\"); Debug.Log(\"Push Up = higher, Push down = lower, Push enter = Correct\"); } // Update is called once per frame void Update() { if ( Input.GetKeyDown(KeyCode.UpArrow) ) { Debug.Log(\"Up Arrow was pressed.\"); min = guess; } else if (Input.GetKeyDown(KeyCode.DownArrow)) { Debug.Log(\"Down Arrow was pressed.\"); } else if (Input.GetKeyDown(KeyCode.Return)) { Debug.Log(\"Enter was pressed.\"); } } }","title":"Using if, else if, else"},{"location":"Graphics/Unity/2D/02-number-wizard/06-calculate-guess-variable/","text":"Calculate guess variable \u00b6 Now we are recalculating introducting the guess variable, showing it and recalculating based on player's input. using System.Collections; using System.Collections.Generic; using UnityEngine; public class NumberWizard : MonoBehaviour { int min = 1; int max = 1000; int guess = 500; // Start is called before the first frame update void Start() { Debug.Log(\"Welcome to number wizard!\"); Debug.Log(\"Pick a number, don't tell me what it is.\"); Debug.Log(\"The highest number you can pick is \" + max); Debug.Log(\"The lowest number you can pick is \" + min); Debug.Log(\"Tell me if your number is higher or lower than \" + guess); Debug.Log(\"Push Up = higher, Push down = lower, Push enter = Correct\"); guess++; } // Update is called once per frame void Update() { if ( Input.GetKeyDown(KeyCode.UpArrow) ) { min = guess; guess = (min + max) / 2; Debug.Log(\"Higher it is.\"); Debug.Log(\"Tell me if your number is higher or lower than \" + guess); } else if (Input.GetKeyDown(KeyCode.DownArrow)) { max = guess; guess = (min + max) / 2; Debug.Log(\"Lower it is.\"); Debug.Log(\"Tell me if your number is higher or lower than \" + guess); } else if (Input.GetKeyDown(KeyCode.Return)) { Debug.Log(\"Correct!\"); } } }","title":"Calculate guess variable"},{"location":"Graphics/Unity/2D/02-number-wizard/06-calculate-guess-variable/#calculate-guess-variable","text":"Now we are recalculating introducting the guess variable, showing it and recalculating based on player's input. using System.Collections; using System.Collections.Generic; using UnityEngine; public class NumberWizard : MonoBehaviour { int min = 1; int max = 1000; int guess = 500; // Start is called before the first frame update void Start() { Debug.Log(\"Welcome to number wizard!\"); Debug.Log(\"Pick a number, don't tell me what it is.\"); Debug.Log(\"The highest number you can pick is \" + max); Debug.Log(\"The lowest number you can pick is \" + min); Debug.Log(\"Tell me if your number is higher or lower than \" + guess); Debug.Log(\"Push Up = higher, Push down = lower, Push enter = Correct\"); guess++; } // Update is called once per frame void Update() { if ( Input.GetKeyDown(KeyCode.UpArrow) ) { min = guess; guess = (min + max) / 2; Debug.Log(\"Higher it is.\"); Debug.Log(\"Tell me if your number is higher or lower than \" + guess); } else if (Input.GetKeyDown(KeyCode.DownArrow)) { max = guess; guess = (min + max) / 2; Debug.Log(\"Lower it is.\"); Debug.Log(\"Tell me if your number is higher or lower than \" + guess); } else if (Input.GetKeyDown(KeyCode.Return)) { Debug.Log(\"Correct!\"); } } }","title":"Calculate guess variable"},{"location":"Graphics/Unity/2D/02-number-wizard/07-functions-and-encapsulating/","text":"Functions and encapsulating \u00b6 Now we are going to make so that the game can reset once it's finished. Also, some code refactor. using System.Collections; using System.Collections.Generic; using UnityEngine; public class NumberWizard : MonoBehaviour { int min = 1; int max = 1000; int guess = 500; // Start is called before the first frame update void Start() { StartGame(); } void StartGame() { max = 1000; min = 1; guess = 500; Debug.Log(\"Welcome to number wizard!\"); Debug.Log(\"Pick a number, don't tell me what it is.\"); Debug.Log(\"The highest number you can pick is \" + max); Debug.Log(\"The lowest number you can pick is \" + min); Debug.Log(\"Tell me if your number is higher or lower than \" + guess); Debug.Log(\"Push Up = higher, Push down = lower, Push enter = Correct\"); guess++; } // Update is called once per frame void Update() { if ( Input.GetKeyDown(KeyCode.UpArrow) ) { min = guess; Debug.Log(\"Higher it is.\"); NextGuess(); } else if (Input.GetKeyDown(KeyCode.DownArrow)) { max = guess; Debug.Log(\"Lower it is.\"); NextGuess(); } else if (Input.GetKeyDown(KeyCode.Return)) { Debug.Log(\"Correct!\"); StartGame(); } } void NextGuess() { guess = (min + max) / 2; Debug.Log(\"Tell me if your number is higher or lower than \" + guess); } }","title":"Functions and encapsulating"},{"location":"Graphics/Unity/2D/02-number-wizard/07-functions-and-encapsulating/#functions-and-encapsulating","text":"Now we are going to make so that the game can reset once it's finished. Also, some code refactor. using System.Collections; using System.Collections.Generic; using UnityEngine; public class NumberWizard : MonoBehaviour { int min = 1; int max = 1000; int guess = 500; // Start is called before the first frame update void Start() { StartGame(); } void StartGame() { max = 1000; min = 1; guess = 500; Debug.Log(\"Welcome to number wizard!\"); Debug.Log(\"Pick a number, don't tell me what it is.\"); Debug.Log(\"The highest number you can pick is \" + max); Debug.Log(\"The lowest number you can pick is \" + min); Debug.Log(\"Tell me if your number is higher or lower than \" + guess); Debug.Log(\"Push Up = higher, Push down = lower, Push enter = Correct\"); guess++; } // Update is called once per frame void Update() { if ( Input.GetKeyDown(KeyCode.UpArrow) ) { min = guess; Debug.Log(\"Higher it is.\"); NextGuess(); } else if (Input.GetKeyDown(KeyCode.DownArrow)) { max = guess; Debug.Log(\"Lower it is.\"); NextGuess(); } else if (Input.GetKeyDown(KeyCode.Return)) { Debug.Log(\"Correct!\"); StartGame(); } } void NextGuess() { guess = (min + max) / 2; Debug.Log(\"Tell me if your number is higher or lower than \" + guess); } }","title":"Functions and encapsulating"},{"location":"Graphics/Unity/2D/03-text101/01-game-design/","text":"Game design \u00b6 Code Game design \u00b6 Player experience : Feeling of discovery Core mechanic : Choose your own adventure Theme : Steampunk Core game loop : Player is shown text on what is happening in the world and given 1 to 3 choices on how to progress. Story Hooks \u00b6 Who is the player? An injured airship pilot. What is the setting? Small city under siege What is the threat? The \"Festers\" ( gangs of desert-dwelling scum) What is the goal? Get airborne Create your own CYOA design \u00b6 What is your game theme? What image(-s) sums up your game theme. (find images/pictures) Who is the player? What is the goal?","title":"Game design"},{"location":"Graphics/Unity/2D/03-text101/01-game-design/#game-design","text":"","title":"Game design"},{"location":"Graphics/Unity/2D/03-text101/01-game-design/#code-game-design","text":"Player experience : Feeling of discovery Core mechanic : Choose your own adventure Theme : Steampunk Core game loop : Player is shown text on what is happening in the world and given 1 to 3 choices on how to progress.","title":"Code Game design"},{"location":"Graphics/Unity/2D/03-text101/01-game-design/#story-hooks","text":"Who is the player? An injured airship pilot. What is the setting? Small city under siege What is the threat? The \"Festers\" ( gangs of desert-dwelling scum) What is the goal? Get airborne","title":"Story Hooks"},{"location":"Graphics/Unity/2D/03-text101/01-game-design/#create-your-own-cyoa-design","text":"What is your game theme? What image(-s) sums up your game theme. (find images/pictures) Who is the player? What is the goal?","title":"Create your own CYOA design"},{"location":"Graphics/Unity/2D/03-text101/02-creating-sprites-in-unity/","text":"Creating sprites in Unity \u00b6 A Sprite is a 2D graphic object obtained from a bitmap image. We can move, scale, rotate and do other transformations to it. On the default layout, we have following windows: - Hierarchy window - Scene window - Game window - Project window - Inspector window To better get to know them, we can add a couple of sprites to the scene. First of all, within the scene window, we can scroll in and out by using the scrollwheel or while holding ALT and right mouse button and dragging left/right. When we click on the camera icon in the scene, we'll see that it will be selected in the hierarchy window, also will shot it's properties in inspector window. The camera is called a Game Object. The Camera Preview shows what we would actually see if we would play the game, we also can click on the Game Window to see that. We can change several options in the Inspector window, like background color of the camera. When going to Assets directory in the Project window , we can right click and go to Create -> Sprites -> Square . Now, by dragging it into either hierarchy window or scene window, we will create an instance of it. Then we can use the toolbar to select tools and transform them. We can also change their properties in the Inspector window . We can also toggle between 2D and 3D environments, by using the button under the Scene window tab.","title":"Creating sprites in Unity"},{"location":"Graphics/Unity/2D/03-text101/02-creating-sprites-in-unity/#creating-sprites-in-unity","text":"A Sprite is a 2D graphic object obtained from a bitmap image. We can move, scale, rotate and do other transformations to it. On the default layout, we have following windows: - Hierarchy window - Scene window - Game window - Project window - Inspector window To better get to know them, we can add a couple of sprites to the scene. First of all, within the scene window, we can scroll in and out by using the scrollwheel or while holding ALT and right mouse button and dragging left/right. When we click on the camera icon in the scene, we'll see that it will be selected in the hierarchy window, also will shot it's properties in inspector window. The camera is called a Game Object. The Camera Preview shows what we would actually see if we would play the game, we also can click on the Game Window to see that. We can change several options in the Inspector window, like background color of the camera. When going to Assets directory in the Project window , we can right click and go to Create -> Sprites -> Square . Now, by dragging it into either hierarchy window or scene window, we will create an instance of it. Then we can use the toolbar to select tools and transform them. We can also change their properties in the Inspector window . We can also toggle between 2D and 3D environments, by using the button under the Scene window tab.","title":"Creating sprites in Unity"},{"location":"Graphics/Unity/2D/03-text101/03-ui-canvas-and-text/","text":"UI canvas & text \u00b6 Next we are creating an user interface. That is referring to buttons, text, menus. In unity, the UI lives on the canvas. Canvas is overlayed on top of the game. We can add a canvas by right clicking on the hierarchy window and going UI -> Canvas or following the same path under GameObject menu. It will create two game objects - Canvas and EventSystem . Then, when the Canvas is selected in the Hierarchy window , right click on it and add UI -> Text . Then we add an image in a same manner. Then, we can change their order, properties etc. Then we can make more customizations and create something like this:","title":"UI canvas & text"},{"location":"Graphics/Unity/2D/03-text101/03-ui-canvas-and-text/#ui-canvas-text","text":"Next we are creating an user interface. That is referring to buttons, text, menus. In unity, the UI lives on the canvas. Canvas is overlayed on top of the game. We can add a canvas by right clicking on the hierarchy window and going UI -> Canvas or following the same path under GameObject menu. It will create two game objects - Canvas and EventSystem . Then, when the Canvas is selected in the Hierarchy window , right click on it and add UI -> Text . Then we add an image in a same manner. Then, we can change their order, properties etc. Then we can make more customizations and create something like this:","title":"UI canvas &amp; text"},{"location":"Graphics/Unity/2D/03-text101/04-update-text-component/","text":"Update text component \u00b6 In this section we are going to programmatically add / change text. First, under Hierarchy window we are going right click and select Create Empty game object, call it Game . Best practice is also to reset it's coordinates to (0;0;0). Then, while in the Inspector window , we can click on Add component -> New Script and name it AdventureGame , open it in Visual Studio. We are going to use UI namespace and add text component. using System.Collections; using System.Collections.Generic; using UnityEngine; using UnityEngine.UI; public class AdventureGame : MonoBehaviour { [SerializeField] Text textComponent; // Start is called before the first frame update void Start() { } // Update is called once per frame void Update() { } } By using the [SerializeField] , it will add the field to our inspector. Then, we can click on the little circle icon and select an element we want to reference. We select Story Text (text) element. using System.Collections; using System.Collections.Generic; using UnityEngine; using UnityEngine.UI; public class AdventureGame : MonoBehaviour { [SerializeField] Text textComponent; // Start is called before the first frame update void Start() { textComponent.text = \"Hello World!\"; } // Update is called once per frame void Update() { } } Now, when we run the game, the text changes:","title":"Update text component"},{"location":"Graphics/Unity/2D/03-text101/04-update-text-component/#update-text-component","text":"In this section we are going to programmatically add / change text. First, under Hierarchy window we are going right click and select Create Empty game object, call it Game . Best practice is also to reset it's coordinates to (0;0;0). Then, while in the Inspector window , we can click on Add component -> New Script and name it AdventureGame , open it in Visual Studio. We are going to use UI namespace and add text component. using System.Collections; using System.Collections.Generic; using UnityEngine; using UnityEngine.UI; public class AdventureGame : MonoBehaviour { [SerializeField] Text textComponent; // Start is called before the first frame update void Start() { } // Update is called once per frame void Update() { } } By using the [SerializeField] , it will add the field to our inspector. Then, we can click on the little circle icon and select an element we want to reference. We select Story Text (text) element. using System.Collections; using System.Collections.Generic; using UnityEngine; using UnityEngine.UI; public class AdventureGame : MonoBehaviour { [SerializeField] Text textComponent; // Start is called before the first frame update void Start() { textComponent.text = \"Hello World!\"; } // Update is called once per frame void Update() { } } Now, when we run the game, the text changes:","title":"Update text component"},{"location":"Graphics/Unity/2D/03-text101/05-game-states/","text":"Game states \u00b6 When creating a text based game, we will need a lot of text for each situation. In order to do this, we will need a good way organize it. We can use a state machine to organize it. We can create multiple text files with text and save them into our Assets directory.","title":"Game states"},{"location":"Graphics/Unity/2D/03-text101/05-game-states/#game-states","text":"When creating a text based game, we will need a lot of text for each situation. In order to do this, we will need a good way organize it. We can use a state machine to organize it. We can create multiple text files with text and save them into our Assets directory.","title":"Game states"},{"location":"Graphics/Unity/2D/03-text101/06-scriptable-objects/","text":"Scriptable objects \u00b6 ScriptableObject is a class that lets us store data in stand alone assets. Keep mountains of data out of our scripts. It is lightweight and convenient. Used as a template for consistency. We can use them as a template for the story text. To create it, we can create a new C# script in assets directory and call it State . When opening up in Visual Studio, we change the class inheritance from MonoBehaviour to ScriptableObject . using System.Collections; using System.Collections.Generic; using UnityEngine; [CreateAssetMenu(menuName = \"State\")] public class State : ScriptableObject { [TextArea(14,10)][SerializeField] string storyText; } Now, when adding new assets, we can choose to add the template State . We create one and call it StartingState . Now we can edit it and insert the text.","title":"Scriptable objects"},{"location":"Graphics/Unity/2D/03-text101/06-scriptable-objects/#scriptable-objects","text":"ScriptableObject is a class that lets us store data in stand alone assets. Keep mountains of data out of our scripts. It is lightweight and convenient. Used as a template for consistency. We can use them as a template for the story text. To create it, we can create a new C# script in assets directory and call it State . When opening up in Visual Studio, we change the class inheritance from MonoBehaviour to ScriptableObject . using System.Collections; using System.Collections.Generic; using UnityEngine; [CreateAssetMenu(menuName = \"State\")] public class State : ScriptableObject { [TextArea(14,10)][SerializeField] string storyText; } Now, when adding new assets, we can choose to add the template State . We create one and call it StartingState . Now we can edit it and insert the text.","title":"Scriptable objects"},{"location":"Graphics/Unity/2D/03-text101/07-state-story-implementation/","text":"State Story Implementation \u00b6 We are going to create a public method in the State class we created earlier called GetStateStory . using System.Collections; using System.Collections.Generic; using UnityEngine; [CreateAssetMenu(menuName = \"State\")] public class State : ScriptableObject { [TextArea(14,10)][SerializeField] string storyText; public string GetStateStory() { return storyText; } } Then we are going to modify the AdventureGame class to use these states. using System.Collections; using System.Collections.Generic; using UnityEngine; using UnityEngine.UI; public class AdventureGame : MonoBehaviour { [SerializeField] Text textComponent; [SerializeField] State startingState; State state; // Start is called before the first frame update void Start() { state = startingState; textComponent.text = state.GetStateStory(); } // Update is called once per frame void Update() { } } Now we can drag in the starting state in the inspector and run the game.","title":"State Story Implementation"},{"location":"Graphics/Unity/2D/03-text101/07-state-story-implementation/#state-story-implementation","text":"We are going to create a public method in the State class we created earlier called GetStateStory . using System.Collections; using System.Collections.Generic; using UnityEngine; [CreateAssetMenu(menuName = \"State\")] public class State : ScriptableObject { [TextArea(14,10)][SerializeField] string storyText; public string GetStateStory() { return storyText; } } Then we are going to modify the AdventureGame class to use these states. using System.Collections; using System.Collections.Generic; using UnityEngine; using UnityEngine.UI; public class AdventureGame : MonoBehaviour { [SerializeField] Text textComponent; [SerializeField] State startingState; State state; // Start is called before the first frame update void Start() { state = startingState; textComponent.text = state.GetStateStory(); } // Update is called once per frame void Update() { } } Now we can drag in the starting state in the inspector and run the game.","title":"State Story Implementation"},{"location":"Graphics/Unity/2D/03-text101/08-creating-an-array/","text":"Creating an array \u00b6 We can modify the State to store a variable to all the next states. We'll start by creating 2 new states Room 1 and Room 2 with a dummy text in them. Then modify the State class to hold a serialized field with an array of states and a method to retrieve it. using System.Collections; using System.Collections.Generic; using UnityEngine; [CreateAssetMenu(menuName = \"State\")] public class State : ScriptableObject { [TextArea(14,10)][SerializeField] string storyText; [SerializeField] State[] nextStates; public string GetStateStory() { return storyText; } public State[] GetNextStates() { return nextStates; } } Now we can add both rooms to the starting state.","title":"Creating an array"},{"location":"Graphics/Unity/2D/03-text101/08-creating-an-array/#creating-an-array","text":"We can modify the State to store a variable to all the next states. We'll start by creating 2 new states Room 1 and Room 2 with a dummy text in them. Then modify the State class to hold a serialized field with an array of states and a method to retrieve it. using System.Collections; using System.Collections.Generic; using UnityEngine; [CreateAssetMenu(menuName = \"State\")] public class State : ScriptableObject { [TextArea(14,10)][SerializeField] string storyText; [SerializeField] State[] nextStates; public string GetStateStory() { return storyText; } public State[] GetNextStates() { return nextStates; } } Now we can add both rooms to the starting state.","title":"Creating an array"},{"location":"Graphics/Unity/2D/03-text101/09-manage-states/","text":"Manage states \u00b6 Now we can hook everything up. When a player pushes a key, we swap states. We'll modify the AdventureGame class. using System; using System.Collections; using System.Collections.Generic; using UnityEngine; using UnityEngine.UI; public class AdventureGame : MonoBehaviour { [SerializeField] Text textComponent; [SerializeField] State startingState; State state; // Start is called before the first frame update void Start() { state = startingState; textComponent.text = state.GetStateStory(); } // Update is called once per frame void Update() { ManageState(); } private void ManageState() { var nextStates = state.GetNextStates(); if(Input.GetKeyDown(KeyCode.Alpha1)) { state = nextStates[0]; } else if (Input.GetKeyDown(KeyCode.Alpha2)) { state = nextStates[1]; } else if (Input.GetKeyDown(KeyCode.Alpha3)) { state = nextStates[2]; } textComponent.text = state.GetStateStory(); } }","title":"Manage states"},{"location":"Graphics/Unity/2D/03-text101/09-manage-states/#manage-states","text":"Now we can hook everything up. When a player pushes a key, we swap states. We'll modify the AdventureGame class. using System; using System.Collections; using System.Collections.Generic; using UnityEngine; using UnityEngine.UI; public class AdventureGame : MonoBehaviour { [SerializeField] Text textComponent; [SerializeField] State startingState; State state; // Start is called before the first frame update void Start() { state = startingState; textComponent.text = state.GetStateStory(); } // Update is called once per frame void Update() { ManageState(); } private void ManageState() { var nextStates = state.GetNextStates(); if(Input.GetKeyDown(KeyCode.Alpha1)) { state = nextStates[0]; } else if (Input.GetKeyDown(KeyCode.Alpha2)) { state = nextStates[1]; } else if (Input.GetKeyDown(KeyCode.Alpha3)) { state = nextStates[2]; } textComponent.text = state.GetStateStory(); } }","title":"Manage states"},{"location":"Graphics/Unity/2D/03-text101/10-game-state-story-design/","text":"Game State Story Design \u00b6 Added a draw.io diagram in Steam-Punk-Game-Diagram.xml .","title":"Game State Story Design"},{"location":"Graphics/Unity/2D/03-text101/10-game-state-story-design/#game-state-story-design","text":"Added a draw.io diagram in Steam-Punk-Game-Diagram.xml .","title":"Game State Story Design"},{"location":"Graphics/Unity/2D/03-text101/11-organize-state-files/","text":"Organize state files \u00b6 We are now going to delete the previously created Room 1 and Room 2 . Rename the StartingState to A1. Ouside Hangar . Create folders States and Scripts . Organize files into them. Then into States folder we are going to create 3 folders - Game Over , Hangar , Intro . We move the A1. Outside Hangar into the Hangar directory. Then in Intro folder, we create a new state A0. Introduction . Add text to it and update the Game object to reference it as the starting state. Then we add the A1. Outside Hangar as a next state to it. We add X1. Game over into Game over directory. Add text. Add A1. Outside Hangar and A0. Introduction as nextState. Now we go to Hangar directory and click ctrl+d to duplicate the A1. Outside Hangar file up to A11 . Then we put all the states and texts together.","title":"Organize state files"},{"location":"Graphics/Unity/2D/03-text101/11-organize-state-files/#organize-state-files","text":"We are now going to delete the previously created Room 1 and Room 2 . Rename the StartingState to A1. Ouside Hangar . Create folders States and Scripts . Organize files into them. Then into States folder we are going to create 3 folders - Game Over , Hangar , Intro . We move the A1. Outside Hangar into the Hangar directory. Then in Intro folder, we create a new state A0. Introduction . Add text to it and update the Game object to reference it as the starting state. Then we add the A1. Outside Hangar as a next state to it. We add X1. Game over into Game over directory. Add text. Add A1. Outside Hangar and A0. Introduction as nextState. Now we go to Hangar directory and click ctrl+d to duplicate the A1. Outside Hangar file up to A11 . Then we put all the states and texts together.","title":"Organize state files"},{"location":"Graphics/Unity/2D/03-text101/12-text-mesh-pro-polish/","text":"Text Mest Pro and Polish \u00b6 We can go to Window -> TextMesh Pro -> Font Asset Creator . Import both options. We can go to https://www.dafont.com/vtc-fuzzy-punky-slippers.font . Download it and extract. Drag it into Unity assets. Drag it into the Font Source in TextMesh Pro and click on Generate Font Atlas . Save it. Then create a new UI GameObject - TextMeshPro - Text and create a replacement title for the old one, pick the downloaded font. Change colors and stuff.","title":"Text Mest Pro and Polish"},{"location":"Graphics/Unity/2D/03-text101/12-text-mesh-pro-polish/#text-mest-pro-and-polish","text":"We can go to Window -> TextMesh Pro -> Font Asset Creator . Import both options. We can go to https://www.dafont.com/vtc-fuzzy-punky-slippers.font . Download it and extract. Drag it into Unity assets. Drag it into the Font Source in TextMesh Pro and click on Generate Font Atlas . Save it. Then create a new UI GameObject - TextMeshPro - Text and create a replacement title for the old one, pick the downloaded font. Change colors and stuff.","title":"Text Mest Pro and Polish"},{"location":"Graphics/Unity/2D/03-text101/13-for-loop/","text":"For loop \u00b6 We are going to fix a bug that is in the state machine, by adding a for loop. Basically, currently we are listening to inputs 1-3 even if there are 1-2 next states. When the state does not exist, it throws an error. We fix this by modifying the ManageState method in AdventureGame class. private void ManageState() { var nextStates = state.GetNextStates(); for (int i = 0; i < nextStates.Length; i++) { if (Input.GetKeyDown(KeyCode.Alpha1 + i)) { state = nextStates[i]; } } textComponent.text = state.GetStateStory(); }","title":"For loop"},{"location":"Graphics/Unity/2D/03-text101/13-for-loop/#for-loop","text":"We are going to fix a bug that is in the state machine, by adding a for loop. Basically, currently we are listening to inputs 1-3 even if there are 1-2 next states. When the state does not exist, it throws an error. We fix this by modifying the ManageState method in AdventureGame class. private void ManageState() { var nextStates = state.GetNextStates(); for (int i = 0; i < nextStates.Length; i++) { if (Input.GetKeyDown(KeyCode.Alpha1 + i)) { state = nextStates[i]; } } textComponent.text = state.GetStateStory(); }","title":"For loop"},{"location":"Graphics/Unity/2D/03-text101/14-publish-webgl/","text":"Publish game via WebGL \u00b6 If we change our aspect ratio in the Game window to 16:9 , we'll see that the camera is smaller than the game menu. We can change canvas UI Scale Mode from Constant Pixel Size to Scale With Screen Size . And then changing the Reference resolution to 1920x1080 . This will make it so that everything will fit no matter what screen resolution player has. We can no go to File -> Build Settings , under it we can choose WebGL . If that does not show, you will need to add module to Unity. Click on Build And Run , save it somewhere. You can publish it in sharemygame.com and link it somewhere. Be ready for feedback.","title":"Publish game via WebGL"},{"location":"Graphics/Unity/2D/03-text101/14-publish-webgl/#publish-game-via-webgl","text":"If we change our aspect ratio in the Game window to 16:9 , we'll see that the camera is smaller than the game menu. We can change canvas UI Scale Mode from Constant Pixel Size to Scale With Screen Size . And then changing the Reference resolution to 1920x1080 . This will make it so that everything will fit no matter what screen resolution player has. We can no go to File -> Build Settings , under it we can choose WebGL . If that does not show, you will need to add module to Unity. Click on Build And Run , save it somewhere. You can publish it in sharemygame.com and link it somewhere. Be ready for feedback.","title":"Publish game via WebGL"},{"location":"Javascript/Bind%20click%20to%20document/","text":"Bind click to document \u00b6 document.addEventListener('click', (event) => { if (event.target.matches('a.some-class')) { event.preventDefault(); alert('do something!') } }, false);","title":"Bind click to document"},{"location":"Javascript/Bind%20click%20to%20document/#bind-click-to-document","text":"document.addEventListener('click', (event) => { if (event.target.matches('a.some-class')) { event.preventDefault(); alert('do something!') } }, false);","title":"Bind click to document"},{"location":"Linux/Enable%20numlock%20on%20startup/","text":"Enable numlock on startup \u00b6 davis@davis-arch \ue0b0 ~ \ue0b0 sudo pacman -S numlockx [sudo] password for davis: resolving dependencies... looking for conflicting packages... Packages (1) numlockx-1.2-4 Total Download Size: 0.01 MiB Total Installed Size: 0.01 MiB :: Proceed with installation? [Y/n] :: Retrieving packages... numlockx-1.2-4-x86_64 5.9 KiB 0.00B/s 00:00 [########################################] 100% (1/1) checking keys in keyring [########################################] 100% (1/1) checking package integrity [########################################] 100% (1/1) loading package files [########################################] 100% (1/1) checking for file conflicts [########################################] 100% (1/1) checking available disk space [########################################] 100% :: Processing package changes... (1/1) installing numlockx [########################################] 100% :: Running post-transaction hooks... (1/1) Arming ConditionNeedsUpdate... davis@davis-arch \ue0b0 ~ \ue0b0 vi ~/.xinitrc davis@davis-arch \ue0b0 ~ \ue0b0 cat ~/.xinitrc #! /bin/bash numlockx & exec i3 betterlockscreen -w dim source ~/.fehbg","title":"Enable numlock on startup"},{"location":"Linux/Enable%20numlock%20on%20startup/#enable-numlock-on-startup","text":"davis@davis-arch \ue0b0 ~ \ue0b0 sudo pacman -S numlockx [sudo] password for davis: resolving dependencies... looking for conflicting packages... Packages (1) numlockx-1.2-4 Total Download Size: 0.01 MiB Total Installed Size: 0.01 MiB :: Proceed with installation? [Y/n] :: Retrieving packages... numlockx-1.2-4-x86_64 5.9 KiB 0.00B/s 00:00 [########################################] 100% (1/1) checking keys in keyring [########################################] 100% (1/1) checking package integrity [########################################] 100% (1/1) loading package files [########################################] 100% (1/1) checking for file conflicts [########################################] 100% (1/1) checking available disk space [########################################] 100% :: Processing package changes... (1/1) installing numlockx [########################################] 100% :: Running post-transaction hooks... (1/1) Arming ConditionNeedsUpdate... davis@davis-arch \ue0b0 ~ \ue0b0 vi ~/.xinitrc davis@davis-arch \ue0b0 ~ \ue0b0 cat ~/.xinitrc #! /bin/bash numlockx & exec i3 betterlockscreen -w dim source ~/.fehbg","title":"Enable numlock on startup"},{"location":"Linux/Get%20Current%20Linux%20Version/","text":"Get current linux version \u00b6 Get kernel version \u00b6 uname -a Linux davis-mint 4.15.0-20-generic #21-Ubuntu SMP Tue Apr 24 06:16:15 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux cat /proc/version Linux version 4.15.0-20-generic (buildd@lgw01-amd64-039) (gcc version 7.3.0 (Ubuntu 7.3.0-16ubuntu3)) #21-Ubuntu SMP Tue Apr 24 06:16:15 UTC 2018 Get distribution version \u00b6 cat /etc/*release DISTRIB_ID=LinuxMint DISTRIB_RELEASE=19 DISTRIB_CODENAME=tara DISTRIB_DESCRIPTION=\"Linux Mint 19 Tara\" NAME=\"Linux Mint\" VERSION=\"19 (Tara)\" ID=linuxmint ID_LIKE=ubuntu PRETTY_NAME=\"Linux Mint 19\" VERSION_ID=\"19\" HOME_URL=\"https://www.linuxmint.com/\" SUPPORT_URL=\"https://forums.ubuntu.com/\" BUG_REPORT_URL=\"http://linuxmint-troubleshooting-guide.readthedocs.io/en/latest/\" PRIVACY_POLICY_URL=\"https://www.linuxmint.com/\" VERSION_CODENAME=tara UBUNTU_CODENAME=bionic cat: /etc/upstream-release: Is a directory","title":"Get current linux version"},{"location":"Linux/Get%20Current%20Linux%20Version/#get-current-linux-version","text":"","title":"Get current linux version"},{"location":"Linux/Get%20Current%20Linux%20Version/#get-kernel-version","text":"uname -a Linux davis-mint 4.15.0-20-generic #21-Ubuntu SMP Tue Apr 24 06:16:15 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux cat /proc/version Linux version 4.15.0-20-generic (buildd@lgw01-amd64-039) (gcc version 7.3.0 (Ubuntu 7.3.0-16ubuntu3)) #21-Ubuntu SMP Tue Apr 24 06:16:15 UTC 2018","title":"Get kernel version"},{"location":"Linux/Get%20Current%20Linux%20Version/#get-distribution-version","text":"cat /etc/*release DISTRIB_ID=LinuxMint DISTRIB_RELEASE=19 DISTRIB_CODENAME=tara DISTRIB_DESCRIPTION=\"Linux Mint 19 Tara\" NAME=\"Linux Mint\" VERSION=\"19 (Tara)\" ID=linuxmint ID_LIKE=ubuntu PRETTY_NAME=\"Linux Mint 19\" VERSION_ID=\"19\" HOME_URL=\"https://www.linuxmint.com/\" SUPPORT_URL=\"https://forums.ubuntu.com/\" BUG_REPORT_URL=\"http://linuxmint-troubleshooting-guide.readthedocs.io/en/latest/\" PRIVACY_POLICY_URL=\"https://www.linuxmint.com/\" VERSION_CODENAME=tara UBUNTU_CODENAME=bionic cat: /etc/upstream-release: Is a directory","title":"Get distribution version"},{"location":"Linux/IPTables/","text":"IPTables \u00b6 Sources: \u00b6 https://www.howtogeek.com/177621/the-beginners-guide-to-iptables-the-linux-firewall/ IPTables uses three different chains: input , forward and output . Input \u00b6 This chain is used to control the begaviour for incomming connections. For example, if a user attempts to ssh into the system, iptables will attempt to match the IP address and port to a rule in the input chain. Forward \u00b6 This chain is used for incoming connections that aren't actually being delivered locally. Think of a router - data is always being sent to it but rarely actually destined for the router itself, it is forwarded instead. To check whether the system uses / needs the forward chain: iptables -L -V Output \u00b6 This chain is used for outgoing connections. For example, if you try to ping a website, iptables will check it's output chain to see what the rules are regarding ping and the domain before making a decision to allow or deny the connection attempt. Policy chain default behaviour \u00b6 Before going in and configuring specific rules, you'll want to decide what you want the default behaviour of the three chains to be. To see what you policy chains are currently configured to do with unmatched traffic: iptables -L More times than not, you'll want your system to accept connections by default. Unless you've changed the policy chain rules previously, this setting should already be configured. Either way, you can do that by using these commands: iptables --policy INPUT ACCEPT iptables --policy OUTPUT ACCEPT iptables --policy FORWARD ACCEPT By defaulting to the accept rule, you can then use iptables to deny specific IP addresses or ports, while accepting all other connections. If you rather deny all connections and manually specify which ones you want to allow to connect, you should change the default policy of your chains to drop. Doing this would probably only be useful for servers that contain sensitive information and only ever have the same IP addresses connecting to them. iptables --policy INPUT DROP iptables --policy OUTPUT DROP iptables --policy FORWARD DROP Connection-specific responses \u00b6 With your default chain policies configured, you can start adding rules to iptables so it knows what to do when it encounters a connection from or to a particular IP address or port. The 3 most basic and commonly used responses are: Accept - allow the connection Drop - drop the connection, act like it never happened. This is the best if you don't want the source to realize your system exists. Reject - Don't allow the connection, but send back an error. This is best if you don't want a source to connect to your system and want to let them know that. Allowing or blocking specific connections \u00b6 If you need to insert a rule above another you can use: Connections from a single IP address \u00b6 iptables -A INPUT -s 10.10.10.10 -j DROP Connections from a range of IP addresses \u00b6 iptables -A INPUT -s 10.10.10.0/24 -j DROP or iptables -A INPUT -s 10.10.10.0/255.255.255.0 -j DROP Connections to a specific port \u00b6 iptables -A INPUT -p tcp --dport ssh -s 10.10.10.10 -j DROP Connection states \u00b6 Many protocols like SSH require communication in both directions. For example, if you want to allow ssh connections to your system, the input and output chains are going to need a rule added to them. Because of this, connection states are introducted, which gives you capability to allow two way communication but only allow one way connections to be established. In this example, ssh connections from 10.10.10.10 are permitted, and it is permitted to send back information to it, as long as the session has already been established. iptables -A INPUT -p tcp --dport ssh -s 10.10.10.10 -m state --state NEW,ESTABLISHED -j ACCEPT iptables -A OUTPUT -p tcp --sport 222 -d 10.10.10.10 -m state --state ESTABLISHED -j accept Saving changes \u00b6 The changes that you make to your iptables rules will be scrapped the next time that the iptables service gets restarted unless you execute a command to save the changes. This command can differ depending on your distribution: Ubuntu : sudo /sbin/iptables-save Red Hat / CentOS /sbin/service iptables save Or /etc/init.d/iptables save Other commands \u00b6 List the currently configured iptables rules: iptables -L Adding the -v option will give you packet and byte information, and adding -n will list everything numerically. Hostnames, protocols and networks are listed as numbers. To clear all the currently configured rules, you can issue the flush command iptables -F","title":"IPTables"},{"location":"Linux/IPTables/#iptables","text":"","title":"IPTables"},{"location":"Linux/IPTables/#sources","text":"https://www.howtogeek.com/177621/the-beginners-guide-to-iptables-the-linux-firewall/ IPTables uses three different chains: input , forward and output .","title":"Sources:"},{"location":"Linux/IPTables/#input","text":"This chain is used to control the begaviour for incomming connections. For example, if a user attempts to ssh into the system, iptables will attempt to match the IP address and port to a rule in the input chain.","title":"Input"},{"location":"Linux/IPTables/#forward","text":"This chain is used for incoming connections that aren't actually being delivered locally. Think of a router - data is always being sent to it but rarely actually destined for the router itself, it is forwarded instead. To check whether the system uses / needs the forward chain: iptables -L -V","title":"Forward"},{"location":"Linux/IPTables/#output","text":"This chain is used for outgoing connections. For example, if you try to ping a website, iptables will check it's output chain to see what the rules are regarding ping and the domain before making a decision to allow or deny the connection attempt.","title":"Output"},{"location":"Linux/IPTables/#policy-chain-default-behaviour","text":"Before going in and configuring specific rules, you'll want to decide what you want the default behaviour of the three chains to be. To see what you policy chains are currently configured to do with unmatched traffic: iptables -L More times than not, you'll want your system to accept connections by default. Unless you've changed the policy chain rules previously, this setting should already be configured. Either way, you can do that by using these commands: iptables --policy INPUT ACCEPT iptables --policy OUTPUT ACCEPT iptables --policy FORWARD ACCEPT By defaulting to the accept rule, you can then use iptables to deny specific IP addresses or ports, while accepting all other connections. If you rather deny all connections and manually specify which ones you want to allow to connect, you should change the default policy of your chains to drop. Doing this would probably only be useful for servers that contain sensitive information and only ever have the same IP addresses connecting to them. iptables --policy INPUT DROP iptables --policy OUTPUT DROP iptables --policy FORWARD DROP","title":"Policy chain default behaviour"},{"location":"Linux/IPTables/#connection-specific-responses","text":"With your default chain policies configured, you can start adding rules to iptables so it knows what to do when it encounters a connection from or to a particular IP address or port. The 3 most basic and commonly used responses are: Accept - allow the connection Drop - drop the connection, act like it never happened. This is the best if you don't want the source to realize your system exists. Reject - Don't allow the connection, but send back an error. This is best if you don't want a source to connect to your system and want to let them know that.","title":"Connection-specific responses"},{"location":"Linux/IPTables/#allowing-or-blocking-specific-connections","text":"If you need to insert a rule above another you can use:","title":"Allowing or blocking specific connections"},{"location":"Linux/IPTables/#connections-from-a-single-ip-address","text":"iptables -A INPUT -s 10.10.10.10 -j DROP","title":"Connections from a single IP address"},{"location":"Linux/IPTables/#connections-from-a-range-of-ip-addresses","text":"iptables -A INPUT -s 10.10.10.0/24 -j DROP or iptables -A INPUT -s 10.10.10.0/255.255.255.0 -j DROP","title":"Connections from a range of IP addresses"},{"location":"Linux/IPTables/#connections-to-a-specific-port","text":"iptables -A INPUT -p tcp --dport ssh -s 10.10.10.10 -j DROP","title":"Connections to a specific port"},{"location":"Linux/IPTables/#connection-states","text":"Many protocols like SSH require communication in both directions. For example, if you want to allow ssh connections to your system, the input and output chains are going to need a rule added to them. Because of this, connection states are introducted, which gives you capability to allow two way communication but only allow one way connections to be established. In this example, ssh connections from 10.10.10.10 are permitted, and it is permitted to send back information to it, as long as the session has already been established. iptables -A INPUT -p tcp --dport ssh -s 10.10.10.10 -m state --state NEW,ESTABLISHED -j ACCEPT iptables -A OUTPUT -p tcp --sport 222 -d 10.10.10.10 -m state --state ESTABLISHED -j accept","title":"Connection states"},{"location":"Linux/IPTables/#saving-changes","text":"The changes that you make to your iptables rules will be scrapped the next time that the iptables service gets restarted unless you execute a command to save the changes. This command can differ depending on your distribution: Ubuntu : sudo /sbin/iptables-save Red Hat / CentOS /sbin/service iptables save Or /etc/init.d/iptables save","title":"Saving changes"},{"location":"Linux/IPTables/#other-commands","text":"List the currently configured iptables rules: iptables -L Adding the -v option will give you packet and byte information, and adding -n will list everything numerically. Hostnames, protocols and networks are listed as numbers. To clear all the currently configured rules, you can issue the flush command iptables -F","title":"Other commands"},{"location":"Linux/Increase%20Battery%20Life/","text":"Conspect this","title":"Increase Battery Life"},{"location":"Linux/Increase%20Swap/","text":"Increase swap \u00b6 Swap memory is used where operating system temporarily stores data when it runs out of RAM memory. It uses hard drive space to save this data. sudo fallocate -l 10G /swapfile ls -lh /swapfile sudo chmod 600 /swapfile ls -lh /swapfile sudo mkswap /swapfile sudo swapon /swapfile sudo swapon --show free -h echo '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab","title":"Increase swap"},{"location":"Linux/Increase%20Swap/#increase-swap","text":"Swap memory is used where operating system temporarily stores data when it runs out of RAM memory. It uses hard drive space to save this data. sudo fallocate -l 10G /swapfile ls -lh /swapfile sudo chmod 600 /swapfile ls -lh /swapfile sudo mkswap /swapfile sudo swapon /swapfile sudo swapon --show free -h echo '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab","title":"Increase swap"},{"location":"Linux/cUrl/","text":"Learning cUrl \u00b6 Sources: \u00b6 https://www.tecmint.com/linux-curl-command-examples/ https://www.baeldung.com/curl-rest API testing \u00b6 Verbose mode \u00b6 When testing, it is a good idea to set verbose mode on: curl -v http://deiveris.lv curl -v http://deiveris.lv * Rebuilt URL to: http://deiveris.lv/ * Trying 146.185.159.146... * TCP_NODELAY set * Connected to deiveris.lv (146.185.159.146) port 80 (#0) > GET / HTTP/1.1 > Host: deiveris.lv > User-Agent: curl/7.58.0 > Accept: */* > < HTTP/1.1 301 Moved Permanently < Server: nginx/1.10.3 (Ubuntu) < Date: Thu, 07 Mar 2019 09:33:02 GMT < Content-Type: text/html < Content-Length: 194 < Connection: keep-alive < Location: https://deiveris.lv/ < <html> <head><title>301 Moved Permanently</title></head> <body bgcolor=\"white\"> <center><h1>301 Moved Permanently</h1></center> <hr><center>nginx/1.10.3 (Ubuntu)</center> </body> </html> * Connection #0 to host deiveris.lv left intact By default it outputs the response body to stdout , we can also specify to save it as a file: curl -o out.json http://domain.com/index.html HTTP Methods with cURL \u00b6 GET \u00b6 curl -v http://domain.com/path/to/something POST \u00b6 The simple way of sending data: curl -d 'id=9&name=deivs' http://domain.com:8080/path/to/something Passing a file that contains the request body: curl -d @request.json -H \"Content-Type: application/json\" http://domain.com:8080/path/to/something PUT \u00b6 This method is very similar to POST, but we use it when we want to send a new version of an existing resource. To do this, we use the -X option. curl -d @request.json -H 'Content-Type: application/json' -X PUT http://domain.com:8080/path/to/something DELETE \u00b6 We can specify to delete by using the -X option as well. curl -X DELETE http://domain.com:8080/path/to/something/9 Custom headers \u00b6 We can replace the default headers or add our own curl -H \"Host: com.domain.com\" http://domain.com curl -H \"User-Agent:\" http://domain.com Authentication \u00b6 For basic authentication we can use: curl --user username:password http://domain.com If we want to use OAuth2 , we'll need to get the access_token : { \"access_token\": \"b1094abc0-54a4-3eab-7213-877142c33fh3\", \"token_type\": \"bearer\", \"refresh_token\": \"253begef-868c-5d48-92e8-448c2ec4bd91\", \"expires_in\": 31234 } Now we can use: curl -H \"Authorization: Bearer b1094abc0-54a4-3eab-7213-877142c33fh3\" http://domain.com Examples \u00b6 Viewing version \u00b6 curl --version Download a file \u00b6 You can use -O or -o flags. The -O will save the file in the CWD with the same filename as on the remote. curl -O http://domain.com/file.tar.gz # save as $CWD/file.tar.gz The -o will require you specifying where to save the file: curl -o /tmp/renamed.tar.gz http://domain.com/file.tar.gz # save to /tmp/renamed.tmp.gz Resume an interrupted download \u00b6 If a download was interrupted for some reason, it can be resumed by using the -C - flag. curl -C - -O http://domain/file.tar.gz Download multiple files \u00b6 You can chain multiple flags of the same type: curl -O http://domain.com/index1.html -O http://domain.com/index2.html Download URLs from a file \u00b6 You can combine curl with xargs to download files from a list of URLs in a file. http://domain.com/index1.html http://domain.com/index2.html http://domain.com/index3.html xargs -n 1 curl -O < listurls.txt Use a Proxy with or without Authentication \u00b6 If you are behind a proxy server lisyening on port 8080 at proxy.domain.com : curl -x proxy.domain.com:8080 -U user:password -O http://domain.com/file.tar.gz Query HTTP Headers \u00b6 HTTP headers allow the remote web server to send additional information about itself along with the actual request. This provides the client with details on how the request is handled. curl -I http://deiveris.lv HTTP/1.1 301 Moved Permanently Server: nginx/1.10.3 (Ubuntu) Date: Thu, 07 Mar 2019 09:20:21 GMT Content-Type: text/html Content-Length: 194 Connection: keep-alive Location: https://deiveris.lv/ Make a POST request with parameters \u00b6 You can send POST data with it as well: curl --data \"firstName=John&lastName=Doe\" https://domain.info.php Download files from an FTP server with or without authentication \u00b6 curl -u username:password -O ftp://domain.com/file.tar.gz Upload files to FTP server \u00b6 curl -u username:password -Y local.tar.gz ftp://domain.com Specify User Agent \u00b6 The user agent is part of the information that is sent along with an HTTP request. This indicates which browser the client used to make the request. curl -I http://deiveris.lv --user-agent \"This is a CURL browser\" Store website cookies \u00b6 You can check what cookies are downloaded when visiting a site by using a command: curl --cookie-jar saved.txt https://domain.com Send website cookies \u00b6 You can use the stored cookies in the last section, to make subsequent requests: curl --cookie saved.txt https://domain.com Modify Name Resolution \u00b6 If you want to test a local version of domain.com before pushing it live, you can make a curl resolve: curl --resolve www.domain.com:80:localhost http://www.domain.com Limit Download rate \u00b6 You can limit your downloads. The following command will limit it to 100KB/s . curl --limit-rate 100K http://domain.com/file.tar.gz","title":"Learning cUrl"},{"location":"Linux/cUrl/#learning-curl","text":"","title":"Learning cUrl"},{"location":"Linux/cUrl/#sources","text":"https://www.tecmint.com/linux-curl-command-examples/ https://www.baeldung.com/curl-rest","title":"Sources:"},{"location":"Linux/cUrl/#api-testing","text":"","title":"API testing"},{"location":"Linux/cUrl/#verbose-mode","text":"When testing, it is a good idea to set verbose mode on: curl -v http://deiveris.lv curl -v http://deiveris.lv * Rebuilt URL to: http://deiveris.lv/ * Trying 146.185.159.146... * TCP_NODELAY set * Connected to deiveris.lv (146.185.159.146) port 80 (#0) > GET / HTTP/1.1 > Host: deiveris.lv > User-Agent: curl/7.58.0 > Accept: */* > < HTTP/1.1 301 Moved Permanently < Server: nginx/1.10.3 (Ubuntu) < Date: Thu, 07 Mar 2019 09:33:02 GMT < Content-Type: text/html < Content-Length: 194 < Connection: keep-alive < Location: https://deiveris.lv/ < <html> <head><title>301 Moved Permanently</title></head> <body bgcolor=\"white\"> <center><h1>301 Moved Permanently</h1></center> <hr><center>nginx/1.10.3 (Ubuntu)</center> </body> </html> * Connection #0 to host deiveris.lv left intact By default it outputs the response body to stdout , we can also specify to save it as a file: curl -o out.json http://domain.com/index.html","title":"Verbose mode"},{"location":"Linux/cUrl/#http-methods-with-curl","text":"","title":"HTTP Methods with cURL"},{"location":"Linux/cUrl/#get","text":"curl -v http://domain.com/path/to/something","title":"GET"},{"location":"Linux/cUrl/#post","text":"The simple way of sending data: curl -d 'id=9&name=deivs' http://domain.com:8080/path/to/something Passing a file that contains the request body: curl -d @request.json -H \"Content-Type: application/json\" http://domain.com:8080/path/to/something","title":"POST"},{"location":"Linux/cUrl/#put","text":"This method is very similar to POST, but we use it when we want to send a new version of an existing resource. To do this, we use the -X option. curl -d @request.json -H 'Content-Type: application/json' -X PUT http://domain.com:8080/path/to/something","title":"PUT"},{"location":"Linux/cUrl/#delete","text":"We can specify to delete by using the -X option as well. curl -X DELETE http://domain.com:8080/path/to/something/9","title":"DELETE"},{"location":"Linux/cUrl/#custom-headers","text":"We can replace the default headers or add our own curl -H \"Host: com.domain.com\" http://domain.com curl -H \"User-Agent:\" http://domain.com","title":"Custom headers"},{"location":"Linux/cUrl/#authentication","text":"For basic authentication we can use: curl --user username:password http://domain.com If we want to use OAuth2 , we'll need to get the access_token : { \"access_token\": \"b1094abc0-54a4-3eab-7213-877142c33fh3\", \"token_type\": \"bearer\", \"refresh_token\": \"253begef-868c-5d48-92e8-448c2ec4bd91\", \"expires_in\": 31234 } Now we can use: curl -H \"Authorization: Bearer b1094abc0-54a4-3eab-7213-877142c33fh3\" http://domain.com","title":"Authentication"},{"location":"Linux/cUrl/#examples","text":"","title":"Examples"},{"location":"Linux/cUrl/#viewing-version","text":"curl --version","title":"Viewing version"},{"location":"Linux/cUrl/#download-a-file","text":"You can use -O or -o flags. The -O will save the file in the CWD with the same filename as on the remote. curl -O http://domain.com/file.tar.gz # save as $CWD/file.tar.gz The -o will require you specifying where to save the file: curl -o /tmp/renamed.tar.gz http://domain.com/file.tar.gz # save to /tmp/renamed.tmp.gz","title":"Download a file"},{"location":"Linux/cUrl/#resume-an-interrupted-download","text":"If a download was interrupted for some reason, it can be resumed by using the -C - flag. curl -C - -O http://domain/file.tar.gz","title":"Resume an interrupted download"},{"location":"Linux/cUrl/#download-multiple-files","text":"You can chain multiple flags of the same type: curl -O http://domain.com/index1.html -O http://domain.com/index2.html","title":"Download multiple files"},{"location":"Linux/cUrl/#download-urls-from-a-file","text":"You can combine curl with xargs to download files from a list of URLs in a file. http://domain.com/index1.html http://domain.com/index2.html http://domain.com/index3.html xargs -n 1 curl -O < listurls.txt","title":"Download URLs from a file"},{"location":"Linux/cUrl/#use-a-proxy-with-or-without-authentication","text":"If you are behind a proxy server lisyening on port 8080 at proxy.domain.com : curl -x proxy.domain.com:8080 -U user:password -O http://domain.com/file.tar.gz","title":"Use a Proxy with or without Authentication"},{"location":"Linux/cUrl/#query-http-headers","text":"HTTP headers allow the remote web server to send additional information about itself along with the actual request. This provides the client with details on how the request is handled. curl -I http://deiveris.lv HTTP/1.1 301 Moved Permanently Server: nginx/1.10.3 (Ubuntu) Date: Thu, 07 Mar 2019 09:20:21 GMT Content-Type: text/html Content-Length: 194 Connection: keep-alive Location: https://deiveris.lv/","title":"Query HTTP Headers"},{"location":"Linux/cUrl/#make-a-post-request-with-parameters","text":"You can send POST data with it as well: curl --data \"firstName=John&lastName=Doe\" https://domain.info.php","title":"Make a POST request with parameters"},{"location":"Linux/cUrl/#download-files-from-an-ftp-server-with-or-without-authentication","text":"curl -u username:password -O ftp://domain.com/file.tar.gz","title":"Download files from an FTP server with or without authentication"},{"location":"Linux/cUrl/#upload-files-to-ftp-server","text":"curl -u username:password -Y local.tar.gz ftp://domain.com","title":"Upload files to FTP server"},{"location":"Linux/cUrl/#specify-user-agent","text":"The user agent is part of the information that is sent along with an HTTP request. This indicates which browser the client used to make the request. curl -I http://deiveris.lv --user-agent \"This is a CURL browser\"","title":"Specify User Agent"},{"location":"Linux/cUrl/#store-website-cookies","text":"You can check what cookies are downloaded when visiting a site by using a command: curl --cookie-jar saved.txt https://domain.com","title":"Store website cookies"},{"location":"Linux/cUrl/#send-website-cookies","text":"You can use the stored cookies in the last section, to make subsequent requests: curl --cookie saved.txt https://domain.com","title":"Send website cookies"},{"location":"Linux/cUrl/#modify-name-resolution","text":"If you want to test a local version of domain.com before pushing it live, you can make a curl resolve: curl --resolve www.domain.com:80:localhost http://www.domain.com","title":"Modify Name Resolution"},{"location":"Linux/cUrl/#limit-download-rate","text":"You can limit your downloads. The following command will limit it to 100KB/s . curl --limit-rate 100K http://domain.com/file.tar.gz","title":"Limit Download rate"},{"location":"Linux/tmux/","text":"tmux cheatsheet \u00b6 Taken from https://gist.github.com/henrik/1967800 . start new: tmux start new with session name: tmux new -s myname attach: tmux a # (or at, or attach) attach to named: tmux a -t myname list sessions: tmux ls kill session: tmux kill-session -t myname In tmux, hit the prefix ctrl+b and then: Sessions \u00b6 :new<CR> new session s list sessions $ name session Windows (tabs) \u00b6 c new window , name window w list windows f find window & kill window . move window - prompted for a new number :movew<CR> move window to the next unused number Panes (splits) \u00b6 % horizontal split \" vertical split o swap panes q show pane numbers x kill pane \u237d space - toggle between layouts Window/pane surgery \u00b6 :joinp -s :2<CR> move window 2 into a new pane in the current window :joinp -t :1<CR> move the current pane into a new pane in window 1 Move window to pane How to reorder windows Misc \u00b6 d detach t big clock ? list shortcuts : prompt Resources: cheat sheet Enable mouse scroll Notes: You can cmd+click URLs to open in iTerm.","title":"tmux cheatsheet"},{"location":"Linux/tmux/#tmux-cheatsheet","text":"Taken from https://gist.github.com/henrik/1967800 . start new: tmux start new with session name: tmux new -s myname attach: tmux a # (or at, or attach) attach to named: tmux a -t myname list sessions: tmux ls kill session: tmux kill-session -t myname In tmux, hit the prefix ctrl+b and then:","title":"tmux cheatsheet"},{"location":"Linux/tmux/#sessions","text":":new<CR> new session s list sessions $ name session","title":"Sessions"},{"location":"Linux/tmux/#windows-tabs","text":"c new window , name window w list windows f find window & kill window . move window - prompted for a new number :movew<CR> move window to the next unused number","title":"Windows (tabs)"},{"location":"Linux/tmux/#panes-splits","text":"% horizontal split \" vertical split o swap panes q show pane numbers x kill pane \u237d space - toggle between layouts","title":"Panes (splits)"},{"location":"Linux/tmux/#windowpane-surgery","text":":joinp -s :2<CR> move window 2 into a new pane in the current window :joinp -t :1<CR> move the current pane into a new pane in window 1 Move window to pane How to reorder windows","title":"Window/pane surgery"},{"location":"Linux/tmux/#misc","text":"d detach t big clock ? list shortcuts : prompt Resources: cheat sheet Enable mouse scroll Notes: You can cmd+click URLs to open in iTerm.","title":"Misc"},{"location":"Linux/xterm-termite%20unknown%20terminal%20error/","text":"xterm-termite : unknown terminal type error fix \u00b6 When Anaconda is installed and using termite, you might encounter this error 'xterm-termite': unknown terminal type. You can fix this with a symlink: ln -s /usr/share/terminfo/x/xterm-termite ~/anaconda3/share/terminfo/x","title":"`xterm-termite`: unknown terminal type error fix"},{"location":"Linux/xterm-termite%20unknown%20terminal%20error/#xterm-termite-unknown-terminal-type-error-fix","text":"When Anaconda is installed and using termite, you might encounter this error 'xterm-termite': unknown terminal type. You can fix this with a symlink: ln -s /usr/share/terminfo/x/xterm-termite ~/anaconda3/share/terminfo/x","title":"xterm-termite: unknown terminal type error fix"},{"location":"Linux/Archlinux/1_installation/","text":"Arch linux installation \u00b6 Download .iso from https://www.archlinux.org/download Then write the iso to a bootable flash, where X is the flash device identifier. (can be found using lsblk command). dd if=/path/to/image.iso of=/dev/sdX Reboot computer and boot into the arch stick. Check for efivars and internet connection. ls /sys/firmware/efi/efivars ping archlinux.org If the efivars returns a list of files instead of an error, you will need to make an EFI partition in later steps. Format drives \u00b6 fdisk -l fdisk /dev/sdX Where X is the drive you want to install the arch to. This will open an interface where you can manage drive partitions, to delete them all - enter the command d a few times. Create EFI partition (skip if no efivars found) \u00b6 Press n Select p (primary) Press enter (1) Press enter (start at the very start) Write +550M - this will create a 550M partition Create boot partition (skip if efivars found) \u00b6 Press n Select p (primary) Press enter (2) Press enter (start at the very start) Write +200M - this will create a 200M partition Create SWAP partition \u00b6 Press n Select p Enter Enter +24G -- 150% of RAM Create / root partition \u00b6 Later on switched to using root `/` and home `/home` in the same partition because sometimes by root directory would run out of space because of docker images, which sometimes becomes annoying to deal with. Could have increased the root partition, but aside from that, I don't really need that much. Press n Select p Enter Enter +50G Create /home partition \u00b6 Press n Select p Enter Enter Enter (takes up the rest of the drive) Apply changes \u00b6 There are no changes made to the drive yet. To apply them press w . Note that it will wipe your hd clean and all the data on it will be lost. Make file systems \u00b6 mkfs.ext4 /dev/sda1 or mkfs.fat -F32 /dev/sda1 # (no efivars vs efivars) mkfs.ext4 /dev/sda2 mkfs.ext4 /dev/sda4 mkswap /dev/sda3 Mount drives \u00b6 mount /dev/sda3 /mnt mkdir -p /mnt/boot /mnt/home /mnt/efi mount /dev/sda2 /mnt/boot mount /dev/sda1 /mnt/efi mount /dev/sda4 /mnt/home Install Arch \u00b6 pacstrap /mnt base base-devel Write the mounts to fstab \u00b6 genfstab /mnt -U >> /mnt/etc/fstab Change root to freshly installed arch \u00b6 arch-chroot /mnt Apply time settings & locales \u00b6 ln -sf /usr/share/zoneinfo/Europe/Riga /etc/localtime hwclock --systohc nano /etc/locale.gen # uncomment wanted locales locale-gen nano /etc/locale.conf # put LANG=en_US.UTF-8 nano /etc/hostname # put your machine hostname like `davis-arch` Set up hosts \u00b6 nano /etc/hosts 127.0.0.1 localhost ::1 localhost 127.0.1.1 davis-arch.localdomain davis-arch Change root password \u00b6 passwd Install grub \u00b6 pacman -S grub os-prober grub-install --target=i386-pc /dev/sda grub-mkconfig -o /boot/grub/grub.cfg Install network manager \u00b6 pacman -S networkmanager systemctl enable NetworkManager Exit setup \u00b6 Press CTRL+d or exit and type reboot .","title":"Arch linux installation"},{"location":"Linux/Archlinux/1_installation/#arch-linux-installation","text":"Download .iso from https://www.archlinux.org/download Then write the iso to a bootable flash, where X is the flash device identifier. (can be found using lsblk command). dd if=/path/to/image.iso of=/dev/sdX Reboot computer and boot into the arch stick. Check for efivars and internet connection. ls /sys/firmware/efi/efivars ping archlinux.org If the efivars returns a list of files instead of an error, you will need to make an EFI partition in later steps.","title":"Arch linux installation"},{"location":"Linux/Archlinux/1_installation/#format-drives","text":"fdisk -l fdisk /dev/sdX Where X is the drive you want to install the arch to. This will open an interface where you can manage drive partitions, to delete them all - enter the command d a few times.","title":"Format drives"},{"location":"Linux/Archlinux/1_installation/#create-efi-partition-skip-if-no-efivars-found","text":"Press n Select p (primary) Press enter (1) Press enter (start at the very start) Write +550M - this will create a 550M partition","title":"Create EFI partition (skip if no efivars found)"},{"location":"Linux/Archlinux/1_installation/#create-boot-partition-skip-if-efivars-found","text":"Press n Select p (primary) Press enter (2) Press enter (start at the very start) Write +200M - this will create a 200M partition","title":"Create boot partition (skip if efivars found)"},{"location":"Linux/Archlinux/1_installation/#create-swap-partition","text":"Press n Select p Enter Enter +24G -- 150% of RAM","title":"Create SWAP partition"},{"location":"Linux/Archlinux/1_installation/#create-root-partition","text":"Later on switched to using root `/` and home `/home` in the same partition because sometimes by root directory would run out of space because of docker images, which sometimes becomes annoying to deal with. Could have increased the root partition, but aside from that, I don't really need that much. Press n Select p Enter Enter +50G","title":"Create / root partition"},{"location":"Linux/Archlinux/1_installation/#create-home-partition","text":"Press n Select p Enter Enter Enter (takes up the rest of the drive)","title":"Create /home partition"},{"location":"Linux/Archlinux/1_installation/#apply-changes","text":"There are no changes made to the drive yet. To apply them press w . Note that it will wipe your hd clean and all the data on it will be lost.","title":"Apply changes"},{"location":"Linux/Archlinux/1_installation/#make-file-systems","text":"mkfs.ext4 /dev/sda1 or mkfs.fat -F32 /dev/sda1 # (no efivars vs efivars) mkfs.ext4 /dev/sda2 mkfs.ext4 /dev/sda4 mkswap /dev/sda3","title":"Make file systems"},{"location":"Linux/Archlinux/1_installation/#mount-drives","text":"mount /dev/sda3 /mnt mkdir -p /mnt/boot /mnt/home /mnt/efi mount /dev/sda2 /mnt/boot mount /dev/sda1 /mnt/efi mount /dev/sda4 /mnt/home","title":"Mount drives"},{"location":"Linux/Archlinux/1_installation/#install-arch","text":"pacstrap /mnt base base-devel","title":"Install Arch"},{"location":"Linux/Archlinux/1_installation/#write-the-mounts-to-fstab","text":"genfstab /mnt -U >> /mnt/etc/fstab","title":"Write the mounts to fstab"},{"location":"Linux/Archlinux/1_installation/#change-root-to-freshly-installed-arch","text":"arch-chroot /mnt","title":"Change root to freshly installed arch"},{"location":"Linux/Archlinux/1_installation/#apply-time-settings-locales","text":"ln -sf /usr/share/zoneinfo/Europe/Riga /etc/localtime hwclock --systohc nano /etc/locale.gen # uncomment wanted locales locale-gen nano /etc/locale.conf # put LANG=en_US.UTF-8 nano /etc/hostname # put your machine hostname like `davis-arch`","title":"Apply time settings &amp; locales"},{"location":"Linux/Archlinux/1_installation/#set-up-hosts","text":"nano /etc/hosts 127.0.0.1 localhost ::1 localhost 127.0.1.1 davis-arch.localdomain davis-arch","title":"Set up hosts"},{"location":"Linux/Archlinux/1_installation/#change-root-password","text":"passwd","title":"Change root password"},{"location":"Linux/Archlinux/1_installation/#install-grub","text":"pacman -S grub os-prober grub-install --target=i386-pc /dev/sda grub-mkconfig -o /boot/grub/grub.cfg","title":"Install grub"},{"location":"Linux/Archlinux/1_installation/#install-network-manager","text":"pacman -S networkmanager systemctl enable NetworkManager","title":"Install network manager"},{"location":"Linux/Archlinux/1_installation/#exit-setup","text":"Press CTRL+d or exit and type reboot .","title":"Exit setup"},{"location":"Linux/Archlinux/2_post_installation/","text":"Arch post installation \u00b6 Add your account \u00b6 useradd -m -G wheel,users -s /bin/bash davis Change password \u00b6 passwd davis Install sudo & add to sudoers \u00b6 pacman -S sudo visudo Add davis ALL=(ALL) ALL after the root ALL=(ALL) ALL . Install yaourt \u00b6 sudo pacman -S base-devel wget git cd /tmp git clone https://aur.archlinux.org/package-query.git cd package-query makepkg -si cd .. git clone https://aur.archlinux.org/yaourt.git cd yaourt mkpkg -si cd .. yaourt -Syu rm -rf package-query/ yaourt/ Add multilib repository \u00b6 sudo vi /etc/pacman.conf find the [multilib] part and uncomment it. sudo pacman -Syu Install i3 (from dotfiles) \u00b6 pacman -S i3-wm i3status i3blocks dmenu xorg xorg-xinit git git clone https://github.com/daviskregers/dotfiles.git cd dotfiles chmod +x sync.sh ./sync.sh chmod +x apps.sh ./apps.sh if something fails due to keys / signatures, add them gpg --recv-keys A2C794A986419D8A Install i3 (from scratch - skip if dotfiles used) \u00b6 pacman -S i3-wm dmenu xorg xorg-xinit nano .xinitrc #! /bin/bash exec i3 nano /etc/profile # autostart systemd default session on tty1 if [\"$(tty)\" == '/dev/tty1'](../../\"$(tty)\" == '/dev/tty1'.md); then exec startx fi Add sound \u00b6 pacman -S alsa-utils alsamixer yaourt -S asoundconf asoundconf list asoundconf set-default-card AIMO speaker-test -c2 Multiple monitors \u00b6 sudo pacman -S arandr arandr","title":"Arch post installation"},{"location":"Linux/Archlinux/2_post_installation/#arch-post-installation","text":"","title":"Arch post installation"},{"location":"Linux/Archlinux/2_post_installation/#add-your-account","text":"useradd -m -G wheel,users -s /bin/bash davis","title":"Add your account"},{"location":"Linux/Archlinux/2_post_installation/#change-password","text":"passwd davis","title":"Change password"},{"location":"Linux/Archlinux/2_post_installation/#install-sudo-add-to-sudoers","text":"pacman -S sudo visudo Add davis ALL=(ALL) ALL after the root ALL=(ALL) ALL .","title":"Install sudo &amp; add to sudoers"},{"location":"Linux/Archlinux/2_post_installation/#install-yaourt","text":"sudo pacman -S base-devel wget git cd /tmp git clone https://aur.archlinux.org/package-query.git cd package-query makepkg -si cd .. git clone https://aur.archlinux.org/yaourt.git cd yaourt mkpkg -si cd .. yaourt -Syu rm -rf package-query/ yaourt/","title":"Install yaourt"},{"location":"Linux/Archlinux/2_post_installation/#add-multilib-repository","text":"sudo vi /etc/pacman.conf find the [multilib] part and uncomment it. sudo pacman -Syu","title":"Add multilib repository"},{"location":"Linux/Archlinux/2_post_installation/#install-i3-from-dotfiles","text":"pacman -S i3-wm i3status i3blocks dmenu xorg xorg-xinit git git clone https://github.com/daviskregers/dotfiles.git cd dotfiles chmod +x sync.sh ./sync.sh chmod +x apps.sh ./apps.sh if something fails due to keys / signatures, add them gpg --recv-keys A2C794A986419D8A","title":"Install i3 (from dotfiles)"},{"location":"Linux/Archlinux/2_post_installation/#install-i3-from-scratch-skip-if-dotfiles-used","text":"pacman -S i3-wm dmenu xorg xorg-xinit nano .xinitrc #! /bin/bash exec i3 nano /etc/profile # autostart systemd default session on tty1 if [\"$(tty)\" == '/dev/tty1'](../../\"$(tty)\" == '/dev/tty1'.md); then exec startx fi","title":"Install i3 (from scratch - skip if dotfiles used)"},{"location":"Linux/Archlinux/2_post_installation/#add-sound","text":"pacman -S alsa-utils alsamixer yaourt -S asoundconf asoundconf list asoundconf set-default-card AIMO speaker-test -c2","title":"Add sound"},{"location":"Linux/Archlinux/2_post_installation/#multiple-monitors","text":"sudo pacman -S arandr arandr","title":"Multiple monitors"},{"location":"Linux/Archlinux/find_which_package_contains_command/","text":"Find which package contains command \u00b6 pacman -Fy # sync database pacman -Fs genfstab [root@davis-arch davis]# pacman -Fs genfstab extra/arch-install-scripts 20-1 usr/bin/genfstab","title":"Find which package contains command"},{"location":"Linux/Archlinux/find_which_package_contains_command/#find-which-package-contains-command","text":"pacman -Fy # sync database pacman -Fs genfstab [root@davis-arch davis]# pacman -Fs genfstab extra/arch-install-scripts 20-1 usr/bin/genfstab","title":"Find which package contains command"},{"location":"Linux/Archlinux/keymaps/","text":"Linux keymap \u00b6 Get list of all keycodes and their names \u00b6 xmodmap -pke Get events \u00b6 xev","title":"Linux keymap"},{"location":"Linux/Archlinux/keymaps/#linux-keymap","text":"","title":"Linux keymap"},{"location":"Linux/Archlinux/keymaps/#get-list-of-all-keycodes-and-their-names","text":"xmodmap -pke","title":"Get list of all keycodes and their names"},{"location":"Linux/Archlinux/keymaps/#get-events","text":"xev","title":"Get events"},{"location":"Linux/Dolphin/Fix%20mismatched%20background%20in%20Dolphin/","text":"When running Dolphin under something other than Plasma, it is possible the background color in the folder view pane will not match the system Qt theme. This is because Dolphin reads the folder view's background color from the [Colors:View] section in ~/.config/kdeglobals . Change the following line to the RGB value you prefer (it may be given in the form #RRGGBB or R,G,B): ~/.config/kdeglobals [Colors:View] BackgroundNormal=#2E2E2E If you get a blue border around the folder view pane (if you are in split view it will only be around the focused pane), you may get rid of it by applying the fusion-fixes.qss style sheet via the qt5ct app. This answer describes what to do to get the adwaita dark theme working for dolphin under Gnome. Alternatively, use kvantum to manage your Qt5 theming. For instructions on usage see the Kvantum project homepage.","title":"Fix mismatched background in Dolphin"},{"location":"Linux/Mint/Post%20Installation%20for%20Linux%20Mint/","text":"Post Installation for Linux Mint \u00b6 setup \u00b6 RED='\\033[0;31m' NC='\\033[0m' # No Color echo -e \"${RED} Updating system packages ... ${NC}\" sudo apt update && sudo apt upgrade -y sudo apt install -y pv ### Chrome echo -e \"${RED} Installing Google Chrome ... ${NC}\" wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add - echo 'deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main' | sudo tee /etc/apt/sources.list.d/google-chrome.list sudo apt update sudo apt install --fix-broken -y google-chrome-stable ### Dropbox echo -e \"${RED} Installing Dropbox ...${NC}\" sudo apt install --fix-broken dropbox ### Spotify echo -e \"${RED} Installing Spotify ...${NC}\" sudo rm /etc/apt/sources.list.d/spotify.list sudo sh -c 'echo \"deb http://repository.spotify.com stable non-free\" >> /etc/apt/sources.list.d/spotify.list' sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 0DF731E45CE24F27EEEB1450EFDC8610341D9410 sudo apt-get update sudo apt-get install spotify-client echo -e \"${RED} Installing GIT ...${NC}\" sudo apt install --fix-broken liberror-perl git-man git -y echo -e \"${RED} Installing Boostnote ...${NC}\" ' cd /tmp wget https://github.com/BoostIO/boost-releases/releases/download/v0.11.10/boostnote_0.11.10_amd64.deb sudo dpkg -i boostnote_0.11.10_amd64.deb ' echo -e \"${RED} Installing Visual Studio Code ...${NC}\" ' cd /tmp sudo apt install software-properties-common apt-transport-https wget -y wget -q https://packages.microsoft.com/keys/microsoft.asc -O- | sudo apt-key add - sudo add-apt-repository \"deb [arch=amd64] https://packages.microsoft.com/repos/vscode stable main\" sudo apt install code -y ' echo -e \"${RED} Installing Wine ...${NC}\" cd /tmp sudo dpkg --add-architecture i386 wget -nc https://dl.winehq.org/wine-builds/Release.key sudo apt-key add Release.key sudo apt-add-repository https://dl.winehq.org/wine-builds/ubuntu/ sudo apt update sudo apt install wine-stable winehq-stable -y echo -e \"${RED} Installing HeidiSQL ... be sure to enable https://github.com/HeidiSQL/HeidiSQL/issues/83#issuecomment-375969129 afterwards ${NC}\" cd /tmp wget https://www.heidisql.com/builds/heidisql64.r5317.exe chmod +x heidisql64.r5317.exe ./heidisql64.r5317.exe echo -e \"${RED} Installing Docker ...${NC}\" curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable\" sudo apt-get update sudo apt-get install -y docker-ce sudo systemctl status docker sudo usermod -aG docker ${USER} docker -v sudo curl -L \"https://github.com/docker/compose/releases/download/1.22.0/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose echo -e \"${RED} Installing Git Kraken ...${NC}\" cd /tmp sudo apt install libgnome-keyring-common libgnome-keyring-dev -y wget https://release.gitkraken.com/linux/gitkraken-amd64.deb sudo dpkg -i gitkraken-amd64.deb echo -e \"${RED} Installing Slack ...${NC}\" sudo apt install snapd sudo snap install slack --classic echo -e \"${RED} Installing Discord ...${NC}\" sudo apt install libgconf-2-4 libappindicator1 sudo snap install discord echo -e \"${RED} Installing Guake ...${NC}\" sudo apt install guake -y echo -e \"${RED} Installing ZSH ...${NC}\" sudo apt install zsh -y sh -c \"$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\" git clone https://github.com/tarjoilija/zgen.git \"${HOME}/.zgen\" sudo apt-get install fonts-powerline echo -e \"${RED} Installing F.LUX ...${NC}\" sudo add-apt-repository ppa:nathan-renniewaldock/flux sudo apt-get update sudo apt-get install fluxgui echo -e \"${RED} Installing Shutter ...${NC}\" sudo apt install shutter -y echo -e \"${RED} Installing Postman ...${NC}\" apt-get install libgconf-2-4 -y cd /tmp wget https://dl.pstmn.io/download/latest/linux64 -O postman.tar.gz sudo tar -xzf postman.tar.gz -C /opt rm postman.tar.gz sudo ln -s /opt/Postman/Postman /usr/bin/postman cat > ~/.local/share/applications/postman.desktop <<EOL [Desktop Entry] Encoding=UTF-8 Name=Postman Exec=postman Icon=/opt/Postman/resources/app/assets/icon.png Terminal=false Type=Application Categories=Development; EOL echo -e \"${RED} Cleanup ...${NC}\" sudo apt autoremove -y","title":"Post Installation for Linux Mint"},{"location":"Linux/Mint/Post%20Installation%20for%20Linux%20Mint/#post-installation-for-linux-mint","text":"","title":"Post Installation for Linux Mint"},{"location":"Linux/Mint/Post%20Installation%20for%20Linux%20Mint/#setup","text":"RED='\\033[0;31m' NC='\\033[0m' # No Color echo -e \"${RED} Updating system packages ... ${NC}\" sudo apt update && sudo apt upgrade -y sudo apt install -y pv ### Chrome echo -e \"${RED} Installing Google Chrome ... ${NC}\" wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add - echo 'deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main' | sudo tee /etc/apt/sources.list.d/google-chrome.list sudo apt update sudo apt install --fix-broken -y google-chrome-stable ### Dropbox echo -e \"${RED} Installing Dropbox ...${NC}\" sudo apt install --fix-broken dropbox ### Spotify echo -e \"${RED} Installing Spotify ...${NC}\" sudo rm /etc/apt/sources.list.d/spotify.list sudo sh -c 'echo \"deb http://repository.spotify.com stable non-free\" >> /etc/apt/sources.list.d/spotify.list' sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 0DF731E45CE24F27EEEB1450EFDC8610341D9410 sudo apt-get update sudo apt-get install spotify-client echo -e \"${RED} Installing GIT ...${NC}\" sudo apt install --fix-broken liberror-perl git-man git -y echo -e \"${RED} Installing Boostnote ...${NC}\" ' cd /tmp wget https://github.com/BoostIO/boost-releases/releases/download/v0.11.10/boostnote_0.11.10_amd64.deb sudo dpkg -i boostnote_0.11.10_amd64.deb ' echo -e \"${RED} Installing Visual Studio Code ...${NC}\" ' cd /tmp sudo apt install software-properties-common apt-transport-https wget -y wget -q https://packages.microsoft.com/keys/microsoft.asc -O- | sudo apt-key add - sudo add-apt-repository \"deb [arch=amd64] https://packages.microsoft.com/repos/vscode stable main\" sudo apt install code -y ' echo -e \"${RED} Installing Wine ...${NC}\" cd /tmp sudo dpkg --add-architecture i386 wget -nc https://dl.winehq.org/wine-builds/Release.key sudo apt-key add Release.key sudo apt-add-repository https://dl.winehq.org/wine-builds/ubuntu/ sudo apt update sudo apt install wine-stable winehq-stable -y echo -e \"${RED} Installing HeidiSQL ... be sure to enable https://github.com/HeidiSQL/HeidiSQL/issues/83#issuecomment-375969129 afterwards ${NC}\" cd /tmp wget https://www.heidisql.com/builds/heidisql64.r5317.exe chmod +x heidisql64.r5317.exe ./heidisql64.r5317.exe echo -e \"${RED} Installing Docker ...${NC}\" curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable\" sudo apt-get update sudo apt-get install -y docker-ce sudo systemctl status docker sudo usermod -aG docker ${USER} docker -v sudo curl -L \"https://github.com/docker/compose/releases/download/1.22.0/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose echo -e \"${RED} Installing Git Kraken ...${NC}\" cd /tmp sudo apt install libgnome-keyring-common libgnome-keyring-dev -y wget https://release.gitkraken.com/linux/gitkraken-amd64.deb sudo dpkg -i gitkraken-amd64.deb echo -e \"${RED} Installing Slack ...${NC}\" sudo apt install snapd sudo snap install slack --classic echo -e \"${RED} Installing Discord ...${NC}\" sudo apt install libgconf-2-4 libappindicator1 sudo snap install discord echo -e \"${RED} Installing Guake ...${NC}\" sudo apt install guake -y echo -e \"${RED} Installing ZSH ...${NC}\" sudo apt install zsh -y sh -c \"$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\" git clone https://github.com/tarjoilija/zgen.git \"${HOME}/.zgen\" sudo apt-get install fonts-powerline echo -e \"${RED} Installing F.LUX ...${NC}\" sudo add-apt-repository ppa:nathan-renniewaldock/flux sudo apt-get update sudo apt-get install fluxgui echo -e \"${RED} Installing Shutter ...${NC}\" sudo apt install shutter -y echo -e \"${RED} Installing Postman ...${NC}\" apt-get install libgconf-2-4 -y cd /tmp wget https://dl.pstmn.io/download/latest/linux64 -O postman.tar.gz sudo tar -xzf postman.tar.gz -C /opt rm postman.tar.gz sudo ln -s /opt/Postman/Postman /usr/bin/postman cat > ~/.local/share/applications/postman.desktop <<EOL [Desktop Entry] Encoding=UTF-8 Name=Postman Exec=postman Icon=/opt/Postman/resources/app/assets/icon.png Terminal=false Type=Application Categories=Development; EOL echo -e \"${RED} Cleanup ...${NC}\" sudo apt autoremove -y","title":"setup"},{"location":"Linux/Networking/ERR_NETWORK_CHANGED/","text":"sysctl -w net.ipv6.conf.all.disable_ipv6=1 sysctl -w net.ipv6.conf.default.disable_ipv6=1","title":"ERR NETWORK CHANGED"},{"location":"Linux/Scripts/Pull%20all%20repositories%20in%20a%20directory/","text":"Pull script \u00b6 for D in ~/Sites/*/; do echo $D cd $D && git pull done","title":"Pull script"},{"location":"Linux/Scripts/Pull%20all%20repositories%20in%20a%20directory/#pull-script","text":"for D in ~/Sites/*/; do echo $D cd $D && git pull done","title":"Pull script"},{"location":"Linux/Xen%20Server/Add%20New%20VM/","text":"Add new VM to Xen Server \u00b6 xe vm-install template=\"Debian Wheezy 7.0 (64-bit)\" new-name-label=\"PHP7\" xe cd-list UUID=d0a30d43-94b4-de03-e746-0ac8dea5e587 NAME=\"PHP7\" ISO=\"debian-7.11.0-amd64-netinst.iso\" NETWORK=43eb06af-7ab7-3c44-d20e-a3a62662042a xe vm-disk-list vm=\"$NAME\" VDI=6b5d3560-c4fe-4a58-936d-c78f1adc279c xe vm-cd-add uuid=$UUID cd-name=$ISO device=1 xe vm-param-set HVM-boot-policy=\"BIOS order\" uuid=$UUID #network 619d8790-cf2d-b6f7-ae73-114eea800c63 xe vm-memory-limits-set dynamic-max=4000MiB dynamic-min=512MiB static-max=4000MiB static-min=512MiB uuid=$UUID xe vdi-resize uuid=$VDI disk-size=200GiB xe vm-start uuid=$UUID DOMID=`list_domains | grep $UUID | awk '{ print $1 }'` xenstore-read /local/domain/$DOMID/console/vnc-port","title":"Add new VM to Xen Server"},{"location":"Linux/Xen%20Server/Add%20New%20VM/#add-new-vm-to-xen-server","text":"xe vm-install template=\"Debian Wheezy 7.0 (64-bit)\" new-name-label=\"PHP7\" xe cd-list UUID=d0a30d43-94b4-de03-e746-0ac8dea5e587 NAME=\"PHP7\" ISO=\"debian-7.11.0-amd64-netinst.iso\" NETWORK=43eb06af-7ab7-3c44-d20e-a3a62662042a xe vm-disk-list vm=\"$NAME\" VDI=6b5d3560-c4fe-4a58-936d-c78f1adc279c xe vm-cd-add uuid=$UUID cd-name=$ISO device=1 xe vm-param-set HVM-boot-policy=\"BIOS order\" uuid=$UUID #network 619d8790-cf2d-b6f7-ae73-114eea800c63 xe vm-memory-limits-set dynamic-max=4000MiB dynamic-min=512MiB static-max=4000MiB static-min=512MiB uuid=$UUID xe vdi-resize uuid=$VDI disk-size=200GiB xe vm-start uuid=$UUID DOMID=`list_domains | grep $UUID | awk '{ print $1 }'` xenstore-read /local/domain/$DOMID/console/vnc-port","title":"Add new VM to Xen Server"},{"location":"OS%20X/Fresh%20install%20mac%20os/","text":"This is something very old. Consider using [[ansible]] instead. # Show Hidden files defaults write com.apple.finder AppleShowAllFiles YES killall Finder # Install powerline fonts for zsh theme cd /tmp git clone https://github.com/powerline/fonts.git --depth=1 cd fonts ./install.sh cd ..\\ rm -rf fonts # Iterm2, homebrew, node.js, java ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" brew doctor brew update brew install node brew cask install appcleaner brew cask install iterm2 curl -L https://iterm2.com/shell_integration/install_shell_integration_and_utilities.sh | bash sudo xcodebuild -license # JAVA brew tap caskroom/versions brew cask install java8 touch ~/.android/repositories.cfg brew cask install android-sdk # SUBLIME 3 brew install caskroom/cask/brew-cask brew tap caskroom/versions brew cask install sublime-text # DOCKER brew install bash-completion brew cask install docker brew install kubectl brew cask install minikube # Apps brew cask install google-chrome brew cask install firefox brew cask install anaconda brew cask install dropbox brew cask install microsoft-office brew cask install spotify brew cask install qt-creator brew cask install sourcetree brew cask install lepton brew cask install transmit4 brew cask install sequel-pro brew cask install mamp brew cask install slack brew cask install resolutionator brew install caskroom/cask/flux brew cask install telegram brew cask install phpstorm brew cask install pycharm brew cask install imageoptim brew install imageoptim-cli # Apps from appstore brew install mas mas install 478570084 mas install 853824330 mas install 960276676 mas install 409201541 mas install 409183694 mas install 1120214373 mas install 425424353 mas install 409203825 mas install 497799835 brew install zsh zsh-completions","title":"Fresh install mac os"},{"location":"OS%20X/Patch%20PHPStorm%20to%20to%20use%20OSX%20case%20sensitive%20file%20system/","text":"Patch PHPStorm to to use OSX case sensitive file system \u00b6 By default mac is not case sensitive, but you can configure it to be during installation. PHPStorm doesn't like that. echo idea.case.sensitive.fs=true >> /Applications/PhpStorm.app/Contents/bin/idea.properties","title":"Patch PHPStorm to to use OSX case sensitive file system"},{"location":"OS%20X/Patch%20PHPStorm%20to%20to%20use%20OSX%20case%20sensitive%20file%20system/#patch-phpstorm-to-to-use-osx-case-sensitive-file-system","text":"By default mac is not case sensitive, but you can configure it to be during installation. PHPStorm doesn't like that. echo idea.case.sensitive.fs=true >> /Applications/PhpStorm.app/Contents/bin/idea.properties","title":"Patch PHPStorm to to use OSX case sensitive file system"},{"location":"PHP/Template%20IFs%20in%20string/","text":"Template IFs in string \u00b6 Given a string like: I like trains{if turtles} and turtles{/if}. We can parse it with preg_replace_callback function: $text = preg_replace_callback('/\\\\{if\\\\s(.+?)}(.+?)\\\\{\\\\/if}/s', function($matches) use ($attributes) { list($condition, $variable, $content) = $matches; if (isset($attributes[$variable]) && $attributes[$variable]) { return $content; } return \"\"; }, $text, -1);","title":"Template IFs in string"},{"location":"PHP/Template%20IFs%20in%20string/#template-ifs-in-string","text":"Given a string like: I like trains{if turtles} and turtles{/if}. We can parse it with preg_replace_callback function: $text = preg_replace_callback('/\\\\{if\\\\s(.+?)}(.+?)\\\\{\\\\/if}/s', function($matches) use ($attributes) { list($condition, $variable, $content) = $matches; if (isset($attributes[$variable]) && $attributes[$variable]) { return $content; } return \"\"; }, $text, -1);","title":"Template IFs in string"},{"location":"PHP/laravel/PHPUnit%20and%20faker%20feature%20test%20escapes/","text":"PHPUnit and faker feature test escapes \u00b6 If generating data with faker, while looking for person names, sometimes the tests may break because they contain a name that has an apostrophe in it ' . Since the laravel's {{ $user->name}} escapes the name, but the tests do not, we need to escape them in the tests too. That can be done by using $response->assertSee( htmlspecialchars( $user->getFullName(), ENT_QUOTES, 'UTF-8', false);","title":"PHPUnit and faker feature test escapes"},{"location":"PHP/laravel/PHPUnit%20and%20faker%20feature%20test%20escapes/#phpunit-and-faker-feature-test-escapes","text":"If generating data with faker, while looking for person names, sometimes the tests may break because they contain a name that has an apostrophe in it ' . Since the laravel's {{ $user->name}} escapes the name, but the tests do not, we need to escape them in the tests too. That can be done by using $response->assertSee( htmlspecialchars( $user->getFullName(), ENT_QUOTES, 'UTF-8', false);","title":"PHPUnit and faker feature test escapes"},{"location":"PHP/laravel/Storage%20mocking%20on%20older%20laravel%20versions/","text":"Storage mocking on older laravel versions \u00b6 function makeStorageMock() { $fileSystemMock = Mockery::mock(\\Illuminate\\Contracts\\Filesystem\\Filesystem::class); $storageMock = Mockery::mock('alias:' . \\Illuminate\\Support\\Facades\\Storage::class); $storageMock->shouldReceive('disk') ->with('s3') ->andReturn($fileSystemMock); return $fileSystemMock; } $storage = $this->makeStorageMock(); $storage->shouldReceive('delete') ->with($job->getStoredFilename()) ->andReturn(true) ->once();","title":"Storage mocking on older laravel versions"},{"location":"PHP/laravel/Storage%20mocking%20on%20older%20laravel%20versions/#storage-mocking-on-older-laravel-versions","text":"function makeStorageMock() { $fileSystemMock = Mockery::mock(\\Illuminate\\Contracts\\Filesystem\\Filesystem::class); $storageMock = Mockery::mock('alias:' . \\Illuminate\\Support\\Facades\\Storage::class); $storageMock->shouldReceive('disk') ->with('s3') ->andReturn($fileSystemMock); return $fileSystemMock; } $storage = $this->makeStorageMock(); $storage->shouldReceive('delete') ->with($job->getStoredFilename()) ->andReturn(true) ->once();","title":"Storage mocking on older laravel versions"},{"location":"PHP/wordpress/Adding%20a%20custom%20field%20in%20Ninja%20Forms%203/","text":"Adding a custom field in Ninja Forms 3 \u00b6 Ninja Forms 3 changed the way things works - specifically a custom field is registered - and you now have filters for a lot of stuff. So, how do you register a field? \u00b6 You'll need PHP 5.4 or newer in order to be able to run the following code. If you're on an older version you can either upgrade or use named functions instead of the anonymous ones So here is how you can register a new field: add_filter('ninja_forms_register_fields', function($fields){ $fields['awesome_custom_field'] = new AwesomeCustomField(); return $fields; }); Pay attention to the awesome_custom_field key, because is important later on! class AwesomeCustomField extends NF_Abstracts_Input { function __construct() { $this->_type = 'awesome_custom_field'; $this->_name = $this->_type; $this->_nicename = __('Our Awesome custom field'); $this->_section = 'common'; $this->_icon = 'envelope-o'; parent::__construct(); } } And that's about the minimum needed to create a custom field. Neat, right? :) But this field doesn't really do anything, so let's add some functionality. Adding custom options to a field \u00b6 class AwesomeCustomField extends NF_Abstracts_Input { public function __construct() { $this->_type = 'awesome_custom_field'; $this->_name = $this->_type; $this->_nicename = __('Our Awesome custom field'); $this->_section = 'common'; $this->_icon = 'envelope-o'; add_filter('ninja_forms_field_' . $this->_type . '_settings', [$this, 'field_settings']); parent::__construct(); } public function field_settings($field_settings) { $custom_settings = [ [ 'name' => 'max_value', 'type' => 'number', 'label' => __('First option is a number'), //for width, you have few options: // - full // - one-half // - one-third 'width' => 'full', 'group' => 'primary', 'value' => 1, 'help' => __('Here is some help tooltip. Totally optional!'), ], [ 'name' => 'second_option', // Other types includes: // - option-repeater // - textbox // - fieldset 'type' => 'toggle', 'label' => __('Second Option'), 'width' => 'one-third', 'group' => 'primary', ], [ 'name' => 'third_option', 'type' => 'select', 'options' => [ [ 'label' => __('First Option'), 'value' => 1 ], [ 'label' => __('Second Option'), 'value' => 2 ] ], 'label' => __('Third Option'), 'width' => 'full', 'group' => 'primary', ], ]; return array_merge($custom_settings, $field_settings); } } Obviously enough, you could do other stuff, like validation (by adding a validate method) or a custom admin element that is visible when you edit a submission (by adding a admin_form_element method). You can check all available methods inside of NF_Abstracts_Field class. But what is a custom field that doesn't also have a custom frontend? How to add a custom field frontend? \u00b6 class AwesomeCustomField extends NF_Abstracts_Input { protected $_templates = 'awesome_custom_field'; public function __construct() { /..../ add_filter('ninja_forms_field_template_file_paths', [$this, 'add_custom_template_path']); parent::__construct(); } public function add_custom_template_path($templates) { $templates[] = plugin_dir_path(__FILE__) . '/templates/'; return $templates; } /....../ You need to add few things on the previous class: a protected $_templates variable with the template file name; A filter ninja_forms_field_template_file_paths to register a new template directory. Finally, you add the new path. If you're doing this inside of you theme (you shouldn't!), then you will need to use get_template_directory() instead of plugin_dir_path . Attention! \u00b6 Your new template file should be named fields-CUSTOM_TEMPLATE.html . Now, your fields-awesome_custom_field.html will look something like this: <script id=\"nf-tmpl-field-awesome_custom_field\" type=\"text/template\"> // your custom field </script> You can take a look into includes/Templates/fields-*.html and see how various fields looks like. However, what we need is a way of displaying custom information. How can we pass data from the PHP to the newly created form element? \u00b6 Let's add a new filter into the constructor method: add_filter('ninja_forms_localize_field_settings_' . $this->_type, [$this, 'localize_field_settings'], 10, 2); And the new method: public function localize_field_settings($settings, $form) { $settings['custom_js_value'] = 'Hello World!'; return $settings; } In our template file, we can simply add the new key: <script id=\"nf-tmpl-field-awesome_custom_field\" type=\"text/template\"> <%= custom_js_value %> </script> And that's about all.","title":"Adding a custom field in Ninja Forms 3"},{"location":"PHP/wordpress/Adding%20a%20custom%20field%20in%20Ninja%20Forms%203/#adding-a-custom-field-in-ninja-forms-3","text":"Ninja Forms 3 changed the way things works - specifically a custom field is registered - and you now have filters for a lot of stuff.","title":"Adding a custom field in Ninja Forms 3"},{"location":"PHP/wordpress/Adding%20a%20custom%20field%20in%20Ninja%20Forms%203/#so-how-do-you-register-a-field","text":"You'll need PHP 5.4 or newer in order to be able to run the following code. If you're on an older version you can either upgrade or use named functions instead of the anonymous ones So here is how you can register a new field: add_filter('ninja_forms_register_fields', function($fields){ $fields['awesome_custom_field'] = new AwesomeCustomField(); return $fields; }); Pay attention to the awesome_custom_field key, because is important later on! class AwesomeCustomField extends NF_Abstracts_Input { function __construct() { $this->_type = 'awesome_custom_field'; $this->_name = $this->_type; $this->_nicename = __('Our Awesome custom field'); $this->_section = 'common'; $this->_icon = 'envelope-o'; parent::__construct(); } } And that's about the minimum needed to create a custom field. Neat, right? :) But this field doesn't really do anything, so let's add some functionality.","title":"So, how do you register a field?"},{"location":"PHP/wordpress/Adding%20a%20custom%20field%20in%20Ninja%20Forms%203/#adding-custom-options-to-a-field","text":"class AwesomeCustomField extends NF_Abstracts_Input { public function __construct() { $this->_type = 'awesome_custom_field'; $this->_name = $this->_type; $this->_nicename = __('Our Awesome custom field'); $this->_section = 'common'; $this->_icon = 'envelope-o'; add_filter('ninja_forms_field_' . $this->_type . '_settings', [$this, 'field_settings']); parent::__construct(); } public function field_settings($field_settings) { $custom_settings = [ [ 'name' => 'max_value', 'type' => 'number', 'label' => __('First option is a number'), //for width, you have few options: // - full // - one-half // - one-third 'width' => 'full', 'group' => 'primary', 'value' => 1, 'help' => __('Here is some help tooltip. Totally optional!'), ], [ 'name' => 'second_option', // Other types includes: // - option-repeater // - textbox // - fieldset 'type' => 'toggle', 'label' => __('Second Option'), 'width' => 'one-third', 'group' => 'primary', ], [ 'name' => 'third_option', 'type' => 'select', 'options' => [ [ 'label' => __('First Option'), 'value' => 1 ], [ 'label' => __('Second Option'), 'value' => 2 ] ], 'label' => __('Third Option'), 'width' => 'full', 'group' => 'primary', ], ]; return array_merge($custom_settings, $field_settings); } } Obviously enough, you could do other stuff, like validation (by adding a validate method) or a custom admin element that is visible when you edit a submission (by adding a admin_form_element method). You can check all available methods inside of NF_Abstracts_Field class. But what is a custom field that doesn't also have a custom frontend?","title":"Adding custom options to a field"},{"location":"PHP/wordpress/Adding%20a%20custom%20field%20in%20Ninja%20Forms%203/#how-to-add-a-custom-field-frontend","text":"class AwesomeCustomField extends NF_Abstracts_Input { protected $_templates = 'awesome_custom_field'; public function __construct() { /..../ add_filter('ninja_forms_field_template_file_paths', [$this, 'add_custom_template_path']); parent::__construct(); } public function add_custom_template_path($templates) { $templates[] = plugin_dir_path(__FILE__) . '/templates/'; return $templates; } /....../ You need to add few things on the previous class: a protected $_templates variable with the template file name; A filter ninja_forms_field_template_file_paths to register a new template directory. Finally, you add the new path. If you're doing this inside of you theme (you shouldn't!), then you will need to use get_template_directory() instead of plugin_dir_path .","title":"How to add a custom field frontend?"},{"location":"PHP/wordpress/Adding%20a%20custom%20field%20in%20Ninja%20Forms%203/#attention","text":"Your new template file should be named fields-CUSTOM_TEMPLATE.html . Now, your fields-awesome_custom_field.html will look something like this: <script id=\"nf-tmpl-field-awesome_custom_field\" type=\"text/template\"> // your custom field </script> You can take a look into includes/Templates/fields-*.html and see how various fields looks like. However, what we need is a way of displaying custom information.","title":"Attention!"},{"location":"PHP/wordpress/Adding%20a%20custom%20field%20in%20Ninja%20Forms%203/#how-can-we-pass-data-from-the-php-to-the-newly-created-form-element","text":"Let's add a new filter into the constructor method: add_filter('ninja_forms_localize_field_settings_' . $this->_type, [$this, 'localize_field_settings'], 10, 2); And the new method: public function localize_field_settings($settings, $form) { $settings['custom_js_value'] = 'Hello World!'; return $settings; } In our template file, we can simply add the new key: <script id=\"nf-tmpl-field-awesome_custom_field\" type=\"text/template\"> <%= custom_js_value %> </script> And that's about all.","title":"How can we pass data from the PHP to the newly created form element?"},{"location":"Resources%20and%20tools/chrome-extensions/","text":"Chrome extensions \u00b6 Dark reader Full Page Screen Capture LastPass: Free Password Manager No Coin - Block miners on the web! React Developer Tools Todoist New Tab uBlock Origin Validity Vimium Vue.js devtools SessionBox","title":"Chrome extensions"},{"location":"Resources%20and%20tools/chrome-extensions/#chrome-extensions","text":"Dark reader Full Page Screen Capture LastPass: Free Password Manager No Coin - Block miners on the web! React Developer Tools Todoist New Tab uBlock Origin Validity Vimium Vue.js devtools SessionBox","title":"Chrome extensions"},{"location":"Resources%20and%20tools/resources/","text":"Resources \u00b6 Everything \u00b6 The Book Of Secret Knowledge Today I Learned Awesome lists SQL \u00b6 Modern SQL Use The Index, Luke! Music \u00b6 Music for programming Terminal \u00b6 lolcat","title":"Resources"},{"location":"Resources%20and%20tools/resources/#resources","text":"","title":"Resources"},{"location":"Resources%20and%20tools/resources/#everything","text":"The Book Of Secret Knowledge Today I Learned Awesome lists","title":"Everything"},{"location":"Resources%20and%20tools/resources/#sql","text":"Modern SQL Use The Index, Luke!","title":"SQL"},{"location":"Resources%20and%20tools/resources/#music","text":"Music for programming","title":"Music"},{"location":"Resources%20and%20tools/resources/#terminal","text":"lolcat","title":"Terminal"},{"location":"Resources%20and%20tools/vscode-setup/","text":"Visual Studio Code extensions \u00b6 Beautify Dark+ Material ElixirLS: Elixir support and debugger ES7 React/Redux/GraphQL/React-Native snippets GitLens Markdown All in One markdown image paste Material Icon Theme Paste Image PHP Inteliphense PHP IntelliSense PHP Namespace Resolver Python Settings Sync SQL Formatter Vetur Vim VS DocBlockr vscode-elixir vscode-elixir-formatter Settings \u00b6 { \"window.zoomLevel\": 0, \"breadcrumbs.enabled\": true, \"editor.fontSize\": 12, \"workbench.statusBar.feedback.visible\": false, \"telemetry.enableTelemetry\": false, \"telemetry.enableCrashReporter\": false, \"workbench.iconTheme\": \"material-icon-theme\", \"workbench.colorTheme\": \"Dark+ Material\", \"php.validate.executablePath\": \"/usr/bin/php\", \"php.executablePath\": \"/usr/bin/php\", \"workbench.sideBar.location\": \"left\", \"workbench.startupEditor\": \"newUntitledFile\" }","title":"Visual Studio Code extensions"},{"location":"Resources%20and%20tools/vscode-setup/#visual-studio-code-extensions","text":"Beautify Dark+ Material ElixirLS: Elixir support and debugger ES7 React/Redux/GraphQL/React-Native snippets GitLens Markdown All in One markdown image paste Material Icon Theme Paste Image PHP Inteliphense PHP IntelliSense PHP Namespace Resolver Python Settings Sync SQL Formatter Vetur Vim VS DocBlockr vscode-elixir vscode-elixir-formatter","title":"Visual Studio Code extensions"},{"location":"Resources%20and%20tools/vscode-setup/#settings","text":"{ \"window.zoomLevel\": 0, \"breadcrumbs.enabled\": true, \"editor.fontSize\": 12, \"workbench.statusBar.feedback.visible\": false, \"telemetry.enableTelemetry\": false, \"telemetry.enableCrashReporter\": false, \"workbench.iconTheme\": \"material-icon-theme\", \"workbench.colorTheme\": \"Dark+ Material\", \"php.validate.executablePath\": \"/usr/bin/php\", \"php.executablePath\": \"/usr/bin/php\", \"workbench.sideBar.location\": \"left\", \"workbench.startupEditor\": \"newUntitledFile\" }","title":"Settings"},{"location":"Resources%20and%20tools/web-testing/","text":"Web testing tools \u00b6 SSLLabs SSL Server Test HTBridge SSL Security Test HTBridge Website Security Test WEB.DEV Measure performance Pingdom PageSpeed Test Header Security Test Content Security Policy DNS CAA Tester Resources \u00b6 Strong Ciphers for Apache, nginx and Lighttpd - cipherli.st Best nginx configuration for improved security(and performance) How to Activate HTTP/2 with TLS 1.3 Encryption in NGINX for Secure Connections without a Performance Penalty Generate Content Security Policy Set up Feature-Policy, Referrer-Policy and Content Security Policy headers in Nginx CAA Record Helper","title":"Web testing tools"},{"location":"Resources%20and%20tools/web-testing/#web-testing-tools","text":"SSLLabs SSL Server Test HTBridge SSL Security Test HTBridge Website Security Test WEB.DEV Measure performance Pingdom PageSpeed Test Header Security Test Content Security Policy DNS CAA Tester","title":"Web testing tools"},{"location":"Resources%20and%20tools/web-testing/#resources","text":"Strong Ciphers for Apache, nginx and Lighttpd - cipherli.st Best nginx configuration for improved security(and performance) How to Activate HTTP/2 with TLS 1.3 Encryption in NGINX for Secure Connections without a Performance Penalty Generate Content Security Policy Set up Feature-Policy, Referrer-Policy and Content Security Policy headers in Nginx CAA Record Helper","title":"Resources"},{"location":"Rust/","text":"Learning Rust Sources: - Rust Programming Language for Beginners Code for the notes is available at https://github.com/daviskregers/learning-rust","title":"Index"},{"location":"Rust/---%20Ownership%20in%20Rust/","text":"Ownership in Rust \u00b6 Rust language 's most unique feature is [[ownership]] feature. It enables Rust to make [[memory safety guarantees]] without needing a [[garbage collector]]. Memory Allocation Ownership rules \u00b6 Each value in Rust has a [[variable]] that is it's owner. There can only be one owner at a time, cannot point at one memory location. When the owner goes out of the [[scope]], the value will be dropped. Memory and allocation \u00b6 [[String]] type, in order to support a [[mutable]], [[growable]] piece of text, we need to allocate an amount of memory on the Heap , unknown at the [[compile time]], to hold the contents. The [[memory]] must be requested from the [[operating system]] at [[runtime]] We need a way of returning this memory to the [[operating system]] when we're done working with it. In languages with a [[garbage collector]], the [[garbage collector]] keeps track and cleans up memory that isn't being used anymore. We don't need to think about it. Without the [[garbage collector]], it is our responsibility to identify when memory is no longer being used and explicitly [[call to return]] it. Rust language takes a different path - the memory is [[automatically returned]] once the variable that owns it goes out of scope. fn main() { let s = String::from(\"hello\"); ... } When a variable goes out of scope, Rust calls a special function called [[drop]]. This function is called each time a the closing bracket. So, when using something like this fn main() { let s1 = String::from(\"Hello\"); let s2 = s1; println!(\"{} {}\", s1, s2); } It will raise an error since both of the variables point to the same location in memory. It can be fixed by deep copying the heap data of the String: fn main() { let s1 = String::from(\"Hello\"); let s2 = s1.clone(); println!(\"{} {}\", s1, s2); } The same example for the stack will work, since the size is known at the compile time. fn main() { let x = 5; let y = x; println!(\"{} {}\", x, y); } Ownership and functions \u00b6 The semantics for passing a value to a function are similar to those for assigning a value to a variable. Passing a variable to a function will move or copy, just as assignment does. So, when using something like this: def main() { let s = String::from(\"Hello\"); take(s); println!(\"{}\", s); } fn take(s1: String) { println!(\"{}\", s1) } An error will be thrown since the take() function transfers the ownership to the s1 variable and the println!(\"{}\", s); accesses data that it has no longer ownership to. Returning values can also transfer ownership. def main() { let mut s = String::from(\"Hello\"); s = take(s); println!(\"{}\", s); } fn take(s1: String) -> String { println!(\"{}\", s1) s1 } Now there won't be any errors thrown. References and borrowing \u00b6 Usiung references, we can reger a value without taking ownership of it. & ampersands are references. let s1 = String::from(\"hello\"); print(&s1); The print function will not get the ownership of the variable. We don't drop what the reference points to when it goes out of scope because we don't have ownership. So, when we use a function that accepts a reference. When the scope ends - only the reference is dropped not the data it points to. References as function parameters are known as borrowing. Using references, we cannot modify the underlying values it points to. In order to allow it to modify the data, we must explicitly specify a mutable reference: print(&mut s); fn main() { let mut s = String::from(\"Hello\"); print(&mut s); println!(\"{}\", s) } fn print(s1: &mut String) { println!(\"{}\", s1); s1.push_str(\"World\") } Rules of references \u00b6 You can create any number of immutable references You can create only one mutable references in a scope. You cannot have an immutable reference if your scope uses a mutable reference. Data race \u00b6 A data race is similar to a race condition and happens when these 3 behaviours occur: Two or more pointers access the same data at the same time. At least one of the pointers is being used to write the data. There's no mechanism being used to synchronize access to the data. Data races cause undefined behaviour and can be difficult to diagnose when you're trying to track them down at runtime. Rust prevents this problem from happening because it won't even compile code with data races. Dangling references \u00b6 Pointer that references a location in memory that doesn't exist is called a dangling reference. fn main() { let s = dangle(); } fn dangle() -> &String { String d = String::from(\"hello\"); &d } // d goes out of scope here This code will not compile in rust, an error will be thrown that this function's return type contains a borrowed value, but there is no value for it to be borrowed from since the value is dropped at the end of the scope. Slices in Rust \u00b6 Slices let you reference a contiguous sequence of elements. let a = String::from(\"Hello World\"); let r1 = &a[0..5]; let r2 = &a[0..=5]; let r3 = &a[ .. 5]; let r4 = &a[0 ..]; let r5 = &a[..];","title":"Ownership in Rust"},{"location":"Rust/---%20Ownership%20in%20Rust/#ownership-in-rust","text":"Rust language 's most unique feature is [[ownership]] feature. It enables Rust to make [[memory safety guarantees]] without needing a [[garbage collector]]. Memory Allocation","title":"Ownership in Rust"},{"location":"Rust/---%20Ownership%20in%20Rust/#ownership-rules","text":"Each value in Rust has a [[variable]] that is it's owner. There can only be one owner at a time, cannot point at one memory location. When the owner goes out of the [[scope]], the value will be dropped.","title":"Ownership rules"},{"location":"Rust/---%20Ownership%20in%20Rust/#memory-and-allocation","text":"[[String]] type, in order to support a [[mutable]], [[growable]] piece of text, we need to allocate an amount of memory on the Heap , unknown at the [[compile time]], to hold the contents. The [[memory]] must be requested from the [[operating system]] at [[runtime]] We need a way of returning this memory to the [[operating system]] when we're done working with it. In languages with a [[garbage collector]], the [[garbage collector]] keeps track and cleans up memory that isn't being used anymore. We don't need to think about it. Without the [[garbage collector]], it is our responsibility to identify when memory is no longer being used and explicitly [[call to return]] it. Rust language takes a different path - the memory is [[automatically returned]] once the variable that owns it goes out of scope. fn main() { let s = String::from(\"hello\"); ... } When a variable goes out of scope, Rust calls a special function called [[drop]]. This function is called each time a the closing bracket. So, when using something like this fn main() { let s1 = String::from(\"Hello\"); let s2 = s1; println!(\"{} {}\", s1, s2); } It will raise an error since both of the variables point to the same location in memory. It can be fixed by deep copying the heap data of the String: fn main() { let s1 = String::from(\"Hello\"); let s2 = s1.clone(); println!(\"{} {}\", s1, s2); } The same example for the stack will work, since the size is known at the compile time. fn main() { let x = 5; let y = x; println!(\"{} {}\", x, y); }","title":"Memory and allocation"},{"location":"Rust/---%20Ownership%20in%20Rust/#ownership-and-functions","text":"The semantics for passing a value to a function are similar to those for assigning a value to a variable. Passing a variable to a function will move or copy, just as assignment does. So, when using something like this: def main() { let s = String::from(\"Hello\"); take(s); println!(\"{}\", s); } fn take(s1: String) { println!(\"{}\", s1) } An error will be thrown since the take() function transfers the ownership to the s1 variable and the println!(\"{}\", s); accesses data that it has no longer ownership to. Returning values can also transfer ownership. def main() { let mut s = String::from(\"Hello\"); s = take(s); println!(\"{}\", s); } fn take(s1: String) -> String { println!(\"{}\", s1) s1 } Now there won't be any errors thrown.","title":"Ownership and functions"},{"location":"Rust/---%20Ownership%20in%20Rust/#references-and-borrowing","text":"Usiung references, we can reger a value without taking ownership of it. & ampersands are references. let s1 = String::from(\"hello\"); print(&s1); The print function will not get the ownership of the variable. We don't drop what the reference points to when it goes out of scope because we don't have ownership. So, when we use a function that accepts a reference. When the scope ends - only the reference is dropped not the data it points to. References as function parameters are known as borrowing. Using references, we cannot modify the underlying values it points to. In order to allow it to modify the data, we must explicitly specify a mutable reference: print(&mut s); fn main() { let mut s = String::from(\"Hello\"); print(&mut s); println!(\"{}\", s) } fn print(s1: &mut String) { println!(\"{}\", s1); s1.push_str(\"World\") }","title":"References and borrowing"},{"location":"Rust/---%20Ownership%20in%20Rust/#rules-of-references","text":"You can create any number of immutable references You can create only one mutable references in a scope. You cannot have an immutable reference if your scope uses a mutable reference.","title":"Rules of references"},{"location":"Rust/---%20Ownership%20in%20Rust/#data-race","text":"A data race is similar to a race condition and happens when these 3 behaviours occur: Two or more pointers access the same data at the same time. At least one of the pointers is being used to write the data. There's no mechanism being used to synchronize access to the data. Data races cause undefined behaviour and can be difficult to diagnose when you're trying to track them down at runtime. Rust prevents this problem from happening because it won't even compile code with data races.","title":"Data race"},{"location":"Rust/---%20Ownership%20in%20Rust/#dangling-references","text":"Pointer that references a location in memory that doesn't exist is called a dangling reference. fn main() { let s = dangle(); } fn dangle() -> &String { String d = String::from(\"hello\"); &d } // d goes out of scope here This code will not compile in rust, an error will be thrown that this function's return type contains a borrowed value, but there is no value for it to be borrowed from since the value is dropped at the end of the scope.","title":"Dangling references"},{"location":"Rust/---%20Ownership%20in%20Rust/#slices-in-rust","text":"Slices let you reference a contiguous sequence of elements. let a = String::from(\"Hello World\"); let r1 = &a[0..5]; let r2 = &a[0..=5]; let r3 = &a[ .. 5]; let r4 = &a[0 ..]; let r5 = &a[..];","title":"Slices in Rust"},{"location":"Rust/02-install-rust/","text":"Installing rust \u00b6 You can visit the Install Rust site for instructions. curl https://sh.rustup.rs -sSf | sh \u2718 davis@davis-arch \ue0b0 ~ \ue0b0 curl https://sh.rustup.rs -sSf | sh info: downloading installer Welcome to Rust! This will download and install the official compiler for the Rust programming language, and its package manager, Cargo. It will add the cargo, rustc, rustup and other commands to Cargo's bin directory, located at: /home/davis/.cargo/bin This path will then be added to your PATH environment variable by modifying the profile files located at: /home/davis/.profile /home/davis/.zprofile /home/davis/.bash_profile You can uninstall at any time with rustup self uninstall and these changes will be reverted. Current installation options: default host triple: x86_64-unknown-linux-gnu default toolchain: stable modify PATH variable: yes 1) Proceed with installation (default) 2) Customize installation 3) Cancel installation source $HOME/.cargo/env To verify that everything is working, you can make a new hello.rs file with the following contents: fn main() { println!(\"Hello World\"); } and compile it: rustc hello.rs Then, finally run it: davis@davis-arch \ue0b0 ~/projects/rust \ue0b0 ./hello Hello World You can also use online resources to compile and run it online like repl.it or Rust playground .","title":"Installing rust"},{"location":"Rust/02-install-rust/#installing-rust","text":"You can visit the Install Rust site for instructions. curl https://sh.rustup.rs -sSf | sh \u2718 davis@davis-arch \ue0b0 ~ \ue0b0 curl https://sh.rustup.rs -sSf | sh info: downloading installer Welcome to Rust! This will download and install the official compiler for the Rust programming language, and its package manager, Cargo. It will add the cargo, rustc, rustup and other commands to Cargo's bin directory, located at: /home/davis/.cargo/bin This path will then be added to your PATH environment variable by modifying the profile files located at: /home/davis/.profile /home/davis/.zprofile /home/davis/.bash_profile You can uninstall at any time with rustup self uninstall and these changes will be reverted. Current installation options: default host triple: x86_64-unknown-linux-gnu default toolchain: stable modify PATH variable: yes 1) Proceed with installation (default) 2) Customize installation 3) Cancel installation source $HOME/.cargo/env To verify that everything is working, you can make a new hello.rs file with the following contents: fn main() { println!(\"Hello World\"); } and compile it: rustc hello.rs Then, finally run it: davis@davis-arch \ue0b0 ~/projects/rust \ue0b0 ./hello Hello World You can also use online resources to compile and run it online like repl.it or Rust playground .","title":"Installing rust"},{"location":"Rust/05-mutability/","text":"Mutability and intro into strings \u00b6 By default variables in immutable - once the value is bound to a name, you can't change that value. fn main() { let a = 10; a = 20; println!(\"{}\", a) } When compiling this program, it will throw an error saying error: re-assignment of immutable variable . However, sometimes the mutability can become very useful, it can be added by specifying mut keyword: fn main() { let mut a = 10; a = 20; println!(\"{}\", a) } There is also a variable type of Constant. They are bound to a name and are not allowed to change - you cannot add the mut keyword. Constants can be defined using const keyword, data type must be annotated. Can be defined in any scope, including the global scope. By convention, the constant names must be defined in all caps. const MAX:u8 = 132; fn main() { let mut a = 10; a = 20; println!(\"{} {}\", a, MAX) }","title":"Mutability and intro into strings"},{"location":"Rust/05-mutability/#mutability-and-intro-into-strings","text":"By default variables in immutable - once the value is bound to a name, you can't change that value. fn main() { let a = 10; a = 20; println!(\"{}\", a) } When compiling this program, it will throw an error saying error: re-assignment of immutable variable . However, sometimes the mutability can become very useful, it can be added by specifying mut keyword: fn main() { let mut a = 10; a = 20; println!(\"{}\", a) } There is also a variable type of Constant. They are bound to a name and are not allowed to change - you cannot add the mut keyword. Constants can be defined using const keyword, data type must be annotated. Can be defined in any scope, including the global scope. By convention, the constant names must be defined in all caps. const MAX:u8 = 132; fn main() { let mut a = 10; a = 20; println!(\"{} {}\", a, MAX) }","title":"Mutability and intro into strings"},{"location":"Rust/09-shadowing/","text":"Shadowing \u00b6 Using shadowing, you can define a new variable with the same name as a previous variable. fn main() { let a = 10; println!(\"{}\", a); let a = 20; println!(\"{}\", a); } Shadowing is different than marking a variable as mutable mut - you can also change it's data types: fn main() { let b:u32 = 128; println!(\"{}\", b); let b:char = 'c'; println!(\"{}\", b); } When using mutuable variables - the data type will be only limited to u32 .","title":"Shadowing"},{"location":"Rust/09-shadowing/#shadowing","text":"Using shadowing, you can define a new variable with the same name as a previous variable. fn main() { let a = 10; println!(\"{}\", a); let a = 20; println!(\"{}\", a); } Shadowing is different than marking a variable as mutable mut - you can also change it's data types: fn main() { let b:u32 = 128; println!(\"{}\", b); let b:char = 'c'; println!(\"{}\", b); } When using mutuable variables - the data type will be only limited to u32 .","title":"Shadowing"},{"location":"Rust/11-taking-input/","text":"Taking input in Rust \u00b6 In order to ask user to input a string, you can use something like this: use std::io; fn main() { let mut a=String::new(); println!(\"Enter a String!\"); io::stdin().read_line(&mut a).expect(\"Failed\"); println!(\" {} \", a); } The io::stdin().read_line(&mut a).expect(\"Failed\"); line will read in a line from stdio and put it into the a variable. By default it will put Failed . In order to use numbers: use std::io; fn main() { let mut a=String::new(); println!(\"Enter a number!\"); io::stdin().read_line(&mut a).expect(\"Failed\"); let a:i32 = a.trim().parse().expect(\"Failed\"); println!(\" {} \", a); } The .trim() function will trim the whitespace.","title":"Taking input in Rust"},{"location":"Rust/11-taking-input/#taking-input-in-rust","text":"In order to ask user to input a string, you can use something like this: use std::io; fn main() { let mut a=String::new(); println!(\"Enter a String!\"); io::stdin().read_line(&mut a).expect(\"Failed\"); println!(\" {} \", a); } The io::stdin().read_line(&mut a).expect(\"Failed\"); line will read in a line from stdio and put it into the a variable. By default it will put Failed . In order to use numbers: use std::io; fn main() { let mut a=String::new(); println!(\"Enter a number!\"); io::stdin().read_line(&mut a).expect(\"Failed\"); let a:i32 = a.trim().parse().expect(\"Failed\"); println!(\" {} \", a); } The .trim() function will trim the whitespace.","title":"Taking input in Rust"},{"location":"Rust/12-branching/","text":"Branching in Rust \u00b6 In branching, we use a conditional statement that runs a different set of statements depending on an expression given. We can use If-else for this: fn main() { let a = 10; if a%2 == 0 { println!(\"Event\"); } else { println!(\"Odd\"); } } You can also use Else If ladder that will execute a collection of Else If conditions, provided that previous condition returned false . if(condition1) { // statement } else if(condition2) { // statement - condition1 = false } else if(condition3) { // statement - condition1 = condition2 = false } else if(condition4) { // statement - condition1 = condition2 = condition3 = false } else { // statement - condition1 = condition2 = condition3 = condition4 = false } use std::io; fn main() { let mut a=String::new(); println!(\"Enter a number!\"); io::stdin().read_line(&mut a).expect(\"Failed\"); let a:i32 = a.trim().parse().expect(\"Failed\"); if a% 2 ==0 && a < 0 { println!(\"Number is even and negative\") } else if a %2 == 0 && a == 0 { println!(\"Number is even and zero\") } else if a %2 == 0 && a > 0 { println!(\"Number is even and positive\") } else if a < 0 { println!(\"Number is odd and negative\") } else { println!(\"Number is odd and positive\") } } You can also use If Else in assignment operations by using let before the first if statement and returning use std::io; fn main() { let mut a=String::new(); println!(\"Enter a number!\"); io::stdin().read_line(&mut a).expect(\"Failed\"); let a:i32 = a.trim().parse().expect(\"Failed\"); let result = if a% 2 ==0 && a < 0 { // .. doing something .. \"Number is even and negative\" } else if a %2 == 0 && a == 0 { // .. doing something .. \"Number is even and zero\" } else if a %2 == 0 && a > 0 { // .. doing something .. \"Number is even and positive\" } else if a < 0 { // .. doing something .. \"Number is odd and negative\" } else { // .. doing something .. \"Number is odd and positive\" }; println!(\"{}\", result); } Make sure to leave a semicolor after the last statement.","title":"Branching in Rust"},{"location":"Rust/12-branching/#branching-in-rust","text":"In branching, we use a conditional statement that runs a different set of statements depending on an expression given. We can use If-else for this: fn main() { let a = 10; if a%2 == 0 { println!(\"Event\"); } else { println!(\"Odd\"); } } You can also use Else If ladder that will execute a collection of Else If conditions, provided that previous condition returned false . if(condition1) { // statement } else if(condition2) { // statement - condition1 = false } else if(condition3) { // statement - condition1 = condition2 = false } else if(condition4) { // statement - condition1 = condition2 = condition3 = false } else { // statement - condition1 = condition2 = condition3 = condition4 = false } use std::io; fn main() { let mut a=String::new(); println!(\"Enter a number!\"); io::stdin().read_line(&mut a).expect(\"Failed\"); let a:i32 = a.trim().parse().expect(\"Failed\"); if a% 2 ==0 && a < 0 { println!(\"Number is even and negative\") } else if a %2 == 0 && a == 0 { println!(\"Number is even and zero\") } else if a %2 == 0 && a > 0 { println!(\"Number is even and positive\") } else if a < 0 { println!(\"Number is odd and negative\") } else { println!(\"Number is odd and positive\") } } You can also use If Else in assignment operations by using let before the first if statement and returning use std::io; fn main() { let mut a=String::new(); println!(\"Enter a number!\"); io::stdin().read_line(&mut a).expect(\"Failed\"); let a:i32 = a.trim().parse().expect(\"Failed\"); let result = if a% 2 ==0 && a < 0 { // .. doing something .. \"Number is even and negative\" } else if a %2 == 0 && a == 0 { // .. doing something .. \"Number is even and zero\" } else if a %2 == 0 && a > 0 { // .. doing something .. \"Number is even and positive\" } else if a < 0 { // .. doing something .. \"Number is odd and negative\" } else { // .. doing something .. \"Number is odd and positive\" }; println!(\"{}\", result); } Make sure to leave a semicolor after the last statement.","title":"Branching in Rust"},{"location":"Rust/15-tuple-and-array/","text":"Tuples and arrays in Rust \u00b6 Tuple \u00b6 Tuple is a comound data type. You can store a list of different data type elements in it. Tuples have a fixed length: once declared, they cannot change it's length. let tup: (i32, f64, u8) = (326, 4.9, 22) When you assign a tuple to a variable - it is known as destructing. Tuple indexes start at 0. fn main() { let a: (i32, bool, f64) = (220, true, 8.5); print_tuple(a); } fn print_tuple(x : (i32, bool, f64)) { let (a, y, z) = x; println!(\"{}, {}, {}\", a, y, z); } Arrays \u00b6 Array is a collection of values, must be in the same type and the length is fixed. let a = [1,2,3,4,5]; let a: [i32;5] = [1,2,3,4,5]; let a: [i32;5] = [0;5] // Will return 5 zero values. The values can be accessed just like in other programming languages using variable[index] . fn main() { let a: [i32; 5] = [3;5]; print_array(a); } fn print_array(x: [i32;5]) { for n in x.iter() { println! (\"{}\", n); } }","title":"Tuples and arrays in Rust"},{"location":"Rust/15-tuple-and-array/#tuples-and-arrays-in-rust","text":"","title":"Tuples and arrays in Rust"},{"location":"Rust/15-tuple-and-array/#tuple","text":"Tuple is a comound data type. You can store a list of different data type elements in it. Tuples have a fixed length: once declared, they cannot change it's length. let tup: (i32, f64, u8) = (326, 4.9, 22) When you assign a tuple to a variable - it is known as destructing. Tuple indexes start at 0. fn main() { let a: (i32, bool, f64) = (220, true, 8.5); print_tuple(a); } fn print_tuple(x : (i32, bool, f64)) { let (a, y, z) = x; println!(\"{}, {}, {}\", a, y, z); }","title":"Tuple"},{"location":"Rust/15-tuple-and-array/#arrays","text":"Array is a collection of values, must be in the same type and the length is fixed. let a = [1,2,3,4,5]; let a: [i32;5] = [1,2,3,4,5]; let a: [i32;5] = [0;5] // Will return 5 zero values. The values can be accessed just like in other programming languages using variable[index] . fn main() { let a: [i32; 5] = [3;5]; print_array(a); } fn print_array(x: [i32;5]) { for n in x.iter() { println! (\"{}\", n); } }","title":"Arrays"},{"location":"Rust/18-structures/","text":"Structures \u00b6 A struct or structure is a custom data type that lets you to name and package together multiple related values. Structs are similar to tuples, the pieces of a struct can be different types, but unlike the tuples - each piece of data is predefined so it's clear what each value means. Define struct \u00b6 struct User { username: String, email: String, age: i8 } Instantiate struct \u00b6 You can instantiate the structs using following: let user = User { email: String::from(\"email@example.org\"), username: String::from(\"example-username\"), age: 21 }; Debug struct \u00b6 In order to be able to use something like this: println!(\"{:?}\", user) You will need to add the debug functionality for the struct: #[derive(Debug)] struct User { email: String, username: String, age: i8 } Access values \u00b6 To get a specific value from a struct, we use dot notation like user.email . In order to change values the instance needs to be mutable, making only certain fields mutable is not supported. user.age = 23; Return instance from a function \u00b6 Returning instance from a function fn build_user(age: i32) -> User { User { age: age, } } If the fields variable name matches the attribute name, we can use shorthand syntax: fn build_user(age: i32, name: String) -> User { User { age, name } } Update Structs \u00b6 Updating structs: let u1 = User { email: String::from(\"email@example.org\"), username: String::from(\"example-username\"), age: 21 } let user2 = User { email: String::from(\"email2@example.org\"), ..user }; println!(\"{:?}\", &user2); Methods \u00b6 Methods are similar to functions - they're declared with the fn keywords and their name, but they are declared in the context of a struct. Their first parameter is always self, which represents the instance of the struct. struct Rectangle { width: u32, height: u32, } impl Rectangle { fn area(&self) -> u32 { self.width * self.height } fn can_hold(&self, other: &Rectangle) -> bool { self.width > other.width && self.height > other.height } } Associated function \u00b6 When we define a function within impl block that does not take the self as a parameter, we call them associated functions. They are still functions not methods, because they don't have an instance of a struct to work with. These functions are often used for constructors that will return a new instance of a struct. impl Rectangle { fn square(size: u32) -> Rectangle { Rectangle { width: size, height: size } } }","title":"Structures"},{"location":"Rust/18-structures/#structures","text":"A struct or structure is a custom data type that lets you to name and package together multiple related values. Structs are similar to tuples, the pieces of a struct can be different types, but unlike the tuples - each piece of data is predefined so it's clear what each value means.","title":"Structures"},{"location":"Rust/18-structures/#define-struct","text":"struct User { username: String, email: String, age: i8 }","title":"Define struct"},{"location":"Rust/18-structures/#instantiate-struct","text":"You can instantiate the structs using following: let user = User { email: String::from(\"email@example.org\"), username: String::from(\"example-username\"), age: 21 };","title":"Instantiate struct"},{"location":"Rust/18-structures/#debug-struct","text":"In order to be able to use something like this: println!(\"{:?}\", user) You will need to add the debug functionality for the struct: #[derive(Debug)] struct User { email: String, username: String, age: i8 }","title":"Debug struct"},{"location":"Rust/18-structures/#access-values","text":"To get a specific value from a struct, we use dot notation like user.email . In order to change values the instance needs to be mutable, making only certain fields mutable is not supported. user.age = 23;","title":"Access values"},{"location":"Rust/18-structures/#return-instance-from-a-function","text":"Returning instance from a function fn build_user(age: i32) -> User { User { age: age, } } If the fields variable name matches the attribute name, we can use shorthand syntax: fn build_user(age: i32, name: String) -> User { User { age, name } }","title":"Return instance from a function"},{"location":"Rust/18-structures/#update-structs","text":"Updating structs: let u1 = User { email: String::from(\"email@example.org\"), username: String::from(\"example-username\"), age: 21 } let user2 = User { email: String::from(\"email2@example.org\"), ..user }; println!(\"{:?}\", &user2);","title":"Update Structs"},{"location":"Rust/18-structures/#methods","text":"Methods are similar to functions - they're declared with the fn keywords and their name, but they are declared in the context of a struct. Their first parameter is always self, which represents the instance of the struct. struct Rectangle { width: u32, height: u32, } impl Rectangle { fn area(&self) -> u32 { self.width * self.height } fn can_hold(&self, other: &Rectangle) -> bool { self.width > other.width && self.height > other.height } }","title":"Methods"},{"location":"Rust/18-structures/#associated-function","text":"When we define a function within impl block that does not take the self as a parameter, we call them associated functions. They are still functions not methods, because they don't have an instance of a struct to work with. These functions are often used for constructors that will return a new instance of a struct. impl Rectangle { fn square(size: u32) -> Rectangle { Rectangle { width: size, height: size } } }","title":"Associated function"},{"location":"Rust/19-enums/","text":"Enums \u00b6 Enum \u00b6 Enum allows you to define a type by enumerating it's possible values. For example, we work with IP addresses with 2 versions - IPv4 and IPv6. enum IpAddrKind { V4, V6 } let four = IpAddrKind::V4; let six = IpAddrKind::V6; The reason this is useful is that both of these variants are now of the same type IpAddrKind and we can define a function that takes any IpAddrKind . fn route(ip_type: IpAddrKind) {} Enum values \u00b6 Using enum as a type for fields of the structure. let home = IPAddr { kind: IPAddrKind::V4, address: String::from(\"127.0.0.1) } let loopback = IPAddr { kind: IPAddrKind::V6, address: String::from(\"::1) } Predefined values enum Fruits { Apple = 0, Mango = 10, Watermelon = 20, } fn main() { let f = Fruits::Mango; println!(\"{:?}\", f as i32); } Using different types inside enum enum IPAddr { V4(u8,u8,u8,u8), V6(String) } let home = IPAddr::V4(String::from(127,0,0,1)); let loopback = IPAddr::V6(String::from(\"::1\")); Option Enum \u00b6 Option, which is another enum defined by standard library. The option type is used in many places because it encodes the very common scenario in which a value could be something or could be nothing. This functionality can prevent bugs that are extremely common in other programming languages. Rust does not have Null types, but it does have an enum that can encode the concept of a value being present or absent. This enum is Option<T> and is defined by the standard library as follows: enum Option<T> { Some(T), None }","title":"Enums"},{"location":"Rust/19-enums/#enums","text":"","title":"Enums"},{"location":"Rust/19-enums/#enum","text":"Enum allows you to define a type by enumerating it's possible values. For example, we work with IP addresses with 2 versions - IPv4 and IPv6. enum IpAddrKind { V4, V6 } let four = IpAddrKind::V4; let six = IpAddrKind::V6; The reason this is useful is that both of these variants are now of the same type IpAddrKind and we can define a function that takes any IpAddrKind . fn route(ip_type: IpAddrKind) {}","title":"Enum"},{"location":"Rust/19-enums/#enum-values","text":"Using enum as a type for fields of the structure. let home = IPAddr { kind: IPAddrKind::V4, address: String::from(\"127.0.0.1) } let loopback = IPAddr { kind: IPAddrKind::V6, address: String::from(\"::1) } Predefined values enum Fruits { Apple = 0, Mango = 10, Watermelon = 20, } fn main() { let f = Fruits::Mango; println!(\"{:?}\", f as i32); } Using different types inside enum enum IPAddr { V4(u8,u8,u8,u8), V6(String) } let home = IPAddr::V4(String::from(127,0,0,1)); let loopback = IPAddr::V6(String::from(\"::1\"));","title":"Enum values"},{"location":"Rust/19-enums/#option-enum","text":"Option, which is another enum defined by standard library. The option type is used in many places because it encodes the very common scenario in which a value could be something or could be nothing. This functionality can prevent bugs that are extremely common in other programming languages. Rust does not have Null types, but it does have an enum that can encode the concept of a value being present or absent. This enum is Option<T> and is defined by the standard library as follows: enum Option<T> { Some(T), None }","title":"Option Enum"},{"location":"Rust/20-pattern-matching/","text":"Pattern matching \u00b6 Match Control Flow Operator \u00b6 Match allows you to compare a value against a series of patterns and then execute code based on which pattern matches. enum Coin { Penny, Nickel, Dime, Quarter } fn value_in_cents(c: Coin) -> u32 { match c { Coin::Penny => 1, Coin::Nickel => 5, Coin::Dime => 10, Coin::Quarter => 25, } } fn value_in_cents(c: Coin) { match c { Coin::Penny => { println!(\"This is a Penny and it's value is 1.\"); }, Coin::Nickel => { println!(\"This is a Nickel and it's value is 5.\"); }, Coin::Dime => { println!(\"This is a Dime and it's value is 10.\"); }, Coin::Quarter => { println!(\"This is a Quarter and it's value is 25.\"); } } } Patterns that binds to value \u00b6 enum Coin { Penny, Nickel, Dime, Quarter(State) } #[derive(Debug)] enum State { Alaska, Arizona } fn value_in_cents(c: Coin) { match c { // ... Coin::Quarter(state) => { println!(\"This is a Quarter and it's value is 25. It comes from {:?}\", state); } } } fn main() { value_in_cents(Coin::Penny); value_in_cents(Coin::Nickel); value_in_cents(Coin::Dime); value_in_cents(Coin::Quarter(State::Alaska)); value_in_cents(Coin::Quarter(State::Arizona)); } Matching Option Enum \u00b6 fn main() { let five = Some(5); let six = plus_one(five); let none = plus_one(None); println!(\"{:?} {:?} {:?}\", five, six, none); } fn plus_one(x: Option<i32>) -> Option<i32> { match x { None => None, Some(i) => Some(i+1), } } The _ placeholder \u00b6 Currently in the match case - you have to match all of the possible values it can accept. If the requirement is not met, an error will be thrown. To fix this, you can use the _ placeholder: fn plus_one(x: Option<i32>) -> Option<i32> { match x { Some(i) => Some(i+1), _ => None, } } Control flow using If Let \u00b6 fn main() { let some_u8 = Some(3); if let Some(3) = some_u8 { println!(\"Three\") } } \u00b6","title":"Pattern matching"},{"location":"Rust/20-pattern-matching/#pattern-matching","text":"","title":"Pattern matching"},{"location":"Rust/20-pattern-matching/#match-control-flow-operator","text":"Match allows you to compare a value against a series of patterns and then execute code based on which pattern matches. enum Coin { Penny, Nickel, Dime, Quarter } fn value_in_cents(c: Coin) -> u32 { match c { Coin::Penny => 1, Coin::Nickel => 5, Coin::Dime => 10, Coin::Quarter => 25, } } fn value_in_cents(c: Coin) { match c { Coin::Penny => { println!(\"This is a Penny and it's value is 1.\"); }, Coin::Nickel => { println!(\"This is a Nickel and it's value is 5.\"); }, Coin::Dime => { println!(\"This is a Dime and it's value is 10.\"); }, Coin::Quarter => { println!(\"This is a Quarter and it's value is 25.\"); } } }","title":"Match Control Flow Operator"},{"location":"Rust/20-pattern-matching/#patterns-that-binds-to-value","text":"enum Coin { Penny, Nickel, Dime, Quarter(State) } #[derive(Debug)] enum State { Alaska, Arizona } fn value_in_cents(c: Coin) { match c { // ... Coin::Quarter(state) => { println!(\"This is a Quarter and it's value is 25. It comes from {:?}\", state); } } } fn main() { value_in_cents(Coin::Penny); value_in_cents(Coin::Nickel); value_in_cents(Coin::Dime); value_in_cents(Coin::Quarter(State::Alaska)); value_in_cents(Coin::Quarter(State::Arizona)); }","title":"Patterns that binds to value"},{"location":"Rust/20-pattern-matching/#matching-option-enum","text":"fn main() { let five = Some(5); let six = plus_one(five); let none = plus_one(None); println!(\"{:?} {:?} {:?}\", five, six, none); } fn plus_one(x: Option<i32>) -> Option<i32> { match x { None => None, Some(i) => Some(i+1), } }","title":"Matching Option Enum"},{"location":"Rust/20-pattern-matching/#the-_-placeholder","text":"Currently in the match case - you have to match all of the possible values it can accept. If the requirement is not met, an error will be thrown. To fix this, you can use the _ placeholder: fn plus_one(x: Option<i32>) -> Option<i32> { match x { Some(i) => Some(i+1), _ => None, } }","title":"The _ placeholder"},{"location":"Rust/20-pattern-matching/#control-flow-using-if-let","text":"fn main() { let some_u8 = Some(3); if let Some(3) = some_u8 { println!(\"Three\") } }","title":"Control flow using If Let"},{"location":"Rust/20-pattern-matching/#_1","text":"","title":""},{"location":"Rust/22-collections/","text":"Collections \u00b6 Rust has a set of data structures called collections. Collections can have different data types and are stored in heap - data does not need to be known at compile time. Vectors \u00b6 Vectors store a collection of the same type data. For example, if one element in vector is i32 , every single element must be i32 . fn main() { let mut v = Vec::new(); v.push(20); v.push(30); v.push(40); println!(\"{:?}\", v); for i in &mut v { println!(\"{}\", i); *i*=2; println!(\"{}\", i); } let v:Vec<i32> = vec![1,2,3]; println!(\"{:?}\", v); let value = &v[0]; // will thow an error if the value does not exist println!(\"{:?}\", value); let value = &v.get(0); // Will return None if value does not exist, Some if exist println!(\"{:?}\", value); } Storing multiple types in Vector \u00b6 #[derive(Debug)] enum SpreadSheet { Integer(i32), Float(f64), Text(String), } fn main() { let row = vec![ Spreadsheet::Integer(3), Spreadsheet::Float(3.14), Spreadsheet::Text(String::from(\"Hello\")) ]; println!(\"{:?}\", row); } String collection \u00b6 fn main() { // .to_string(), .push_str, .push() let a = 1; let mut s = a.to_string(); s.push_str(\" Hello\"); s.push('O'); println!(\"{}\", s); // + operator let s1 = String::from(\"Hello\"); let s2 = String::from(\"World\"); let s3 = s1 + &s2; println!(\"{}\", s3); // format! macro let s1 = String::from(\"Hello\"); let s2 = String::from(\"World\"); let s3 = format!(\"{} {}\", s1, s2); println!(\"{}\", s3); // chars method for n in \"Hello\".chars() { println!(\"{}\", n) } } HashMap \u00b6 use std::collections::HashMap; fn main() { let mut score = HashMap::new(); score.insert(\"Blue\", 10); score.insert(\"Red\", 20); println!(\"{:?}\", score); // collect let team = vec![\"Blue\", \"Red\"]; let score = vec![10, 20]; let scores:HashMap<_,_> = team.iter().zip(score.iter()).collect(); println!(\"{:?}\", scores); // get let mut scores = HashMap::new(); scores.insert(\"Blue\", 10); scores.insert(\"Yellow\", 20); let score = scores.get(\"Yellow\"); println!(\"{:?}\", score); // Iterate let mut scores = HashMap::new(); scores.insert(\"Blue\", 10); scores.insert(\"Yellow\", 20); for (key, value) in &scores { println!(\"{} {}\", key, value); } // Updating HashMap let mut score = HashMap::new(); score.insert(\"Blue\", 10); score.insert(\"Green\", 15); score.entry(\"Blue\").or_insert(20); score.entry(\"Red\").or_insert(20); score.insert(\"Green\", 25); println!(\"{:?}\", score); }","title":"Collections"},{"location":"Rust/22-collections/#collections","text":"Rust has a set of data structures called collections. Collections can have different data types and are stored in heap - data does not need to be known at compile time.","title":"Collections"},{"location":"Rust/22-collections/#vectors","text":"Vectors store a collection of the same type data. For example, if one element in vector is i32 , every single element must be i32 . fn main() { let mut v = Vec::new(); v.push(20); v.push(30); v.push(40); println!(\"{:?}\", v); for i in &mut v { println!(\"{}\", i); *i*=2; println!(\"{}\", i); } let v:Vec<i32> = vec![1,2,3]; println!(\"{:?}\", v); let value = &v[0]; // will thow an error if the value does not exist println!(\"{:?}\", value); let value = &v.get(0); // Will return None if value does not exist, Some if exist println!(\"{:?}\", value); }","title":"Vectors"},{"location":"Rust/22-collections/#storing-multiple-types-in-vector","text":"#[derive(Debug)] enum SpreadSheet { Integer(i32), Float(f64), Text(String), } fn main() { let row = vec![ Spreadsheet::Integer(3), Spreadsheet::Float(3.14), Spreadsheet::Text(String::from(\"Hello\")) ]; println!(\"{:?}\", row); }","title":"Storing multiple types in Vector"},{"location":"Rust/22-collections/#string-collection","text":"fn main() { // .to_string(), .push_str, .push() let a = 1; let mut s = a.to_string(); s.push_str(\" Hello\"); s.push('O'); println!(\"{}\", s); // + operator let s1 = String::from(\"Hello\"); let s2 = String::from(\"World\"); let s3 = s1 + &s2; println!(\"{}\", s3); // format! macro let s1 = String::from(\"Hello\"); let s2 = String::from(\"World\"); let s3 = format!(\"{} {}\", s1, s2); println!(\"{}\", s3); // chars method for n in \"Hello\".chars() { println!(\"{}\", n) } }","title":"String collection"},{"location":"Rust/22-collections/#hashmap","text":"use std::collections::HashMap; fn main() { let mut score = HashMap::new(); score.insert(\"Blue\", 10); score.insert(\"Red\", 20); println!(\"{:?}\", score); // collect let team = vec![\"Blue\", \"Red\"]; let score = vec![10, 20]; let scores:HashMap<_,_> = team.iter().zip(score.iter()).collect(); println!(\"{:?}\", scores); // get let mut scores = HashMap::new(); scores.insert(\"Blue\", 10); scores.insert(\"Yellow\", 20); let score = scores.get(\"Yellow\"); println!(\"{:?}\", score); // Iterate let mut scores = HashMap::new(); scores.insert(\"Blue\", 10); scores.insert(\"Yellow\", 20); for (key, value) in &scores { println!(\"{} {}\", key, value); } // Updating HashMap let mut score = HashMap::new(); score.insert(\"Blue\", 10); score.insert(\"Green\", 15); score.entry(\"Blue\").or_insert(20); score.entry(\"Red\").or_insert(20); score.insert(\"Green\", 25); println!(\"{:?}\", score); }","title":"HashMap"},{"location":"Rust/24-generic-types-traits-lifetimes/","text":"Generic types, traits, lifetimes \u00b6 In every language there are tools for handling duplicate concepts. In Rust one such type is generics. Removing duplication \u00b6 Removing a duplication by extracting a function: fn main() { let list = vec![23,54,65,67] let mut largest = list[0]; for n in list { if n > largest { largest = n; } } println!(\"Largest No. {}\", largest); let list = vec![223,544,655,67] let mut largest = list[0]; for n in list { if n > largest { largest = n; } } println!(\"Largest No. {}\", largest); } fn main() { let list = vec![23,54,65,67]; let result = largest(&list); println!(\"{}\", result); let list = vec![223,544,655,67] let result = largest(&list); println!(\"{}\", result); let list = vec!['y','t','u'] let result = largest_char(&list); println!(\"{}\", result); } fn largest(list :&[i32]) -> i32 { let mut largest = list[0]; for n in list { if n > largest { largest = n; } } largest } fn largest_char(list :&[char]) -> char { let mut largest = list[0]; for n in list { if n > largest { largest = n; } } largest } Now remove the duplication of the functions with different types: fn main() { let list = vec![23,54,65,67]; let result = largest(&list); println!(\"{}\", result); let list = vec![223,544,655,67]; let result = largest(&list); println!(\"{}\", result); let list = vec!['y','t','u']; let result = largest_char(&list); println!(\"{}\", result); } fn largest<T:PartialOrd+Copy>(list :&[T]) -> T { let mut largest = list[0]; for n in list { if n > &largest { largest = *n; } } largest } Generics in structure definition \u00b6 #[derive(Debug)] struct Point<T> { x: T, y: T, } fn main() { let integer = Point{x:5, y:10}; let float = Point{x:8.0, y:9.4}; println!(\"{:?}\\n{:?}\", integer, float); } Generics in Enum definition \u00b6 enum Option<T> { Some(T), None, } enum Result<T, E> { Ok(T), Err(E) } #[derive(Debug)] struct Point<T, E> { x: T, y: T, } impl <T> Point<T> { fn x(&self) -> &T { &self.x; } } Concrete types in Generics \u00b6 impl Point<f32> { fn number(&self) -> f32 { self.x } } impl Point<i32> { fn number(&self) -> i32 { self.x } } fn main() { let n = Point{x:2.2, y: 3.14}; println!(\"{}\", n.number()); let n = Point{x:2, y: 3}; println!(\"{}\", n.number()); } Performance of code using generics \u00b6 While using generics, there is no performance impact because Rust transforms these types at the compile time. Defining traits \u00b6 trait Summary { fn summarize(&self) -> String; } struct NewsArticle { headline: String, location: String, author: String, content: String, } impl Summary for NewsArticle { fn summarize(&self) -> String { format!(\"{}, by {} ({}) \\n {}\", self.headline, self.author, self.location, self.content) } } fn main() { let news = NewsArticle { headline: String::from(\"The title\"), location: String::from(\"The location\"), author: String::from(\"The author\"), content:String::from(\"The content\"), }; println!(\"News Article \\n{}\", news.summarize()); } Default implementation \u00b6 You can define a default implementation of a trait method, if defined in the struct, it will be overwritten. trait Summary { fn summarize(&self) -> String { String::from(\"Default implementation\"); } } Lifetime Annotation syntax \u00b6 fn main() { let s1 = \"Hello\"; let s2 = \"Bye\"; let result = longest(s1, s2); println!(\"{}\", result); } fn longest(x: &str, y: &str) -> &str { if x.len() > y.len() { x } else { y } } Will throw an error - \"This function's return type contain a borrowed value, but the signature does not say wether it is borrowed from 'x' or 'y'.\" fn main() { let s1 = \"Hello\"; let s2 = \"Bye\"; let result = longest(s1, s2); println!(\"{}\", result); } fn longest<'a>(x: &'a str, y:&'a str) -> 'a str { if x.len() > y.len() { x } else { y } } struct<'a> { name: &'a String, } impl <'a> S <'a> { fn fun(&self) -> &String { self.name } } fn main() { let s = S{ name: &String::from(\"Name\"), } println!(\"{}\", s.fun()); }","title":"Generic types, traits, lifetimes"},{"location":"Rust/24-generic-types-traits-lifetimes/#generic-types-traits-lifetimes","text":"In every language there are tools for handling duplicate concepts. In Rust one such type is generics.","title":"Generic types, traits, lifetimes"},{"location":"Rust/24-generic-types-traits-lifetimes/#removing-duplication","text":"Removing a duplication by extracting a function: fn main() { let list = vec![23,54,65,67] let mut largest = list[0]; for n in list { if n > largest { largest = n; } } println!(\"Largest No. {}\", largest); let list = vec![223,544,655,67] let mut largest = list[0]; for n in list { if n > largest { largest = n; } } println!(\"Largest No. {}\", largest); } fn main() { let list = vec![23,54,65,67]; let result = largest(&list); println!(\"{}\", result); let list = vec![223,544,655,67] let result = largest(&list); println!(\"{}\", result); let list = vec!['y','t','u'] let result = largest_char(&list); println!(\"{}\", result); } fn largest(list :&[i32]) -> i32 { let mut largest = list[0]; for n in list { if n > largest { largest = n; } } largest } fn largest_char(list :&[char]) -> char { let mut largest = list[0]; for n in list { if n > largest { largest = n; } } largest } Now remove the duplication of the functions with different types: fn main() { let list = vec![23,54,65,67]; let result = largest(&list); println!(\"{}\", result); let list = vec![223,544,655,67]; let result = largest(&list); println!(\"{}\", result); let list = vec!['y','t','u']; let result = largest_char(&list); println!(\"{}\", result); } fn largest<T:PartialOrd+Copy>(list :&[T]) -> T { let mut largest = list[0]; for n in list { if n > &largest { largest = *n; } } largest }","title":"Removing duplication"},{"location":"Rust/24-generic-types-traits-lifetimes/#generics-in-structure-definition","text":"#[derive(Debug)] struct Point<T> { x: T, y: T, } fn main() { let integer = Point{x:5, y:10}; let float = Point{x:8.0, y:9.4}; println!(\"{:?}\\n{:?}\", integer, float); }","title":"Generics in structure definition"},{"location":"Rust/24-generic-types-traits-lifetimes/#generics-in-enum-definition","text":"enum Option<T> { Some(T), None, } enum Result<T, E> { Ok(T), Err(E) } #[derive(Debug)] struct Point<T, E> { x: T, y: T, } impl <T> Point<T> { fn x(&self) -> &T { &self.x; } }","title":"Generics in Enum definition"},{"location":"Rust/24-generic-types-traits-lifetimes/#concrete-types-in-generics","text":"impl Point<f32> { fn number(&self) -> f32 { self.x } } impl Point<i32> { fn number(&self) -> i32 { self.x } } fn main() { let n = Point{x:2.2, y: 3.14}; println!(\"{}\", n.number()); let n = Point{x:2, y: 3}; println!(\"{}\", n.number()); }","title":"Concrete types in Generics"},{"location":"Rust/24-generic-types-traits-lifetimes/#performance-of-code-using-generics","text":"While using generics, there is no performance impact because Rust transforms these types at the compile time.","title":"Performance of code using generics"},{"location":"Rust/24-generic-types-traits-lifetimes/#defining-traits","text":"trait Summary { fn summarize(&self) -> String; } struct NewsArticle { headline: String, location: String, author: String, content: String, } impl Summary for NewsArticle { fn summarize(&self) -> String { format!(\"{}, by {} ({}) \\n {}\", self.headline, self.author, self.location, self.content) } } fn main() { let news = NewsArticle { headline: String::from(\"The title\"), location: String::from(\"The location\"), author: String::from(\"The author\"), content:String::from(\"The content\"), }; println!(\"News Article \\n{}\", news.summarize()); }","title":"Defining traits"},{"location":"Rust/24-generic-types-traits-lifetimes/#default-implementation","text":"You can define a default implementation of a trait method, if defined in the struct, it will be overwritten. trait Summary { fn summarize(&self) -> String { String::from(\"Default implementation\"); } }","title":"Default implementation"},{"location":"Rust/24-generic-types-traits-lifetimes/#lifetime-annotation-syntax","text":"fn main() { let s1 = \"Hello\"; let s2 = \"Bye\"; let result = longest(s1, s2); println!(\"{}\", result); } fn longest(x: &str, y: &str) -> &str { if x.len() > y.len() { x } else { y } } Will throw an error - \"This function's return type contain a borrowed value, but the signature does not say wether it is borrowed from 'x' or 'y'.\" fn main() { let s1 = \"Hello\"; let s2 = \"Bye\"; let result = longest(s1, s2); println!(\"{}\", result); } fn longest<'a>(x: &'a str, y:&'a str) -> 'a str { if x.len() > y.len() { x } else { y } } struct<'a> { name: &'a String, } impl <'a> S <'a> { fn fun(&self) -> &String { self.name } } fn main() { let s = S{ name: &String::from(\"Name\"), } println!(\"{}\", s.fun()); }","title":"Lifetime Annotation syntax"},{"location":"Rust/26-closures/","text":"Closures \u00b6 Closures are anonymous functions that can capture their environment, they can be assigned to a variable: fn main() { fn add_one_v1(x:u32) -> u32 { x + 1} let add_one_v2 = |x:u32| -> u32 { x + 1 }; println!(\"{}\", add_one_v1(5)); println!(\"{}\", add_one_v2(5)); } You can capture the environment in a closure in a following way: let x = 4; let equal=|z| z == x; println!(\"{}\", equal(4)); println!(\"{}\", equal(5)); true false","title":"Closures"},{"location":"Rust/26-closures/#closures","text":"Closures are anonymous functions that can capture their environment, they can be assigned to a variable: fn main() { fn add_one_v1(x:u32) -> u32 { x + 1} let add_one_v2 = |x:u32| -> u32 { x + 1 }; println!(\"{}\", add_one_v1(5)); println!(\"{}\", add_one_v2(5)); } You can capture the environment in a closure in a following way: let x = 4; let equal=|z| z == x; println!(\"{}\", equal(4)); println!(\"{}\", equal(5)); true false","title":"Closures"},{"location":"Rust/28-cargo-and-crates/","text":"Cargo and Crates.IO \u00b6 Customizing builds with release profiles \u00b6 There are two cargo profils, a build profile, that is used when we run cargo build and release profile when we run cargo build --release . These profiles can be configured in the Cargo.toml file: [profile.dev] opt-level=0 [profile.release] opt-level=3 The opt-level stands for optimization level that rust compiler applies on the build. It ranges from 0 to 3, the next level takes more time on the build time, but the run time will be faster. daviskregers@Daviss-MacBook-Pro \ue0b0 ~/Projects/learning-rust/28-profiles \ue0b0 \ue0a0 master \ue0b0 cargo build Compiling profiles v0.1.0 (/Users/daviskregers/Projects/learning-rust/28-profiles) Finished dev [unoptimized + debuginfo] target(s) in 0.64s daviskregers@Daviss-MacBook-Pro \ue0b0 ~/Projects/learning-rust/28-profiles \ue0b0 \ue0a0 master \ue0b0 cargo build --release Compiling profiles v0.1.0 (/Users/daviskregers/Projects/learning-rust/28-profiles) Finished release [optimized] target(s) in 0.31s Making Documentation \u00b6 You can document the code using markdown in a following manner: ///Adds to 1 the number given /// ///#Example /// ///``` /// let five = 5; /// assert_eq!(6, profiles::add_one(5)); ///``` pub fn add_one(x:i32) -> i32 { x + 1 } The documentation can be build using a command: cargo doc The documentation can be opened using cargo doc --open The Examples part is also running as a doctest : cargo test Publishing to Crate.IO \u00b6 First, you need to create an account on https://crates.io/ and obtain an API key. cargo login API_KEY Now, you will need to modify the Cargo.toml file and add description and license : description = \"This is a description\" license = \"MIT\" Now, you can publish it with cargo publish Cargo Workspaces \u00b6 You can create a directory with a Cargo.toml file in it: [workspace] members=[ \"adder\", \"add-one\" ] cargo new adder cargo new add-one --lib in the adder/Cargo.toml : [dependencies] add-one={path=\"../add-one\"} In add-one/src/lib.rs : pub fn add_one(num : i32) -> i32 { num + 1 } In adder/src/main.rs : extern crate add_one; fn main() { let num = 10; println!(\"{}\", add_one::add_one(num)); } cargo build \u2718 daviskregers@Daviss-MacBook-Pro \ue0b0 ~/Projects/learning-rust/28-workspaces \ue0b0 \ue0a0 master \ue0b0 cargo build Compiling add-one v0.1.0 (/Users/daviskregers/Projects/learning-rust/28-workspaces/add-one) Compiling adder v0.1.0 (/Users/daviskregers/Projects/learning-rust/28-workspaces/adder) Finished dev [unoptimized + debuginfo] target(s) in 0.67s cargo run daviskregers@Daviss-MacBook-Pro \ue0b0 ~/Projects/learning-rust/28-workspaces \ue0b0 \ue0a0 master \ue0b0 cargo run Finished dev [unoptimized + debuginfo] target(s) in 0.05s Running `target/debug/adder` 11 You can then add tests in the same manner before: #[cfg(test)] mod tests { use super::*; #[test] fn it_works() { assert_eq!(4, add_one(3)); assert_eq!(6, add_one(5)); } } cargo test \u2718 daviskregers@Daviss-MacBook-Pro \ue0b0 ~/Projects/learning-rust/28-workspaces \ue0b0 \ue0a0 master \ue0b0 cargo test Compiling add-one v0.1.0 (/Users/daviskregers/Projects/learning-rust/28-workspaces/add-one) Compiling adder v0.1.0 (/Users/daviskregers/Projects/learning-rust/28-workspaces/adder) Finished dev [unoptimized + debuginfo] target(s) in 0.74s Running target/debug/deps/add_one-04932d8ef5f0255b running 1 test test tests::it_works ... ok test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out Running target/debug/deps/adder-0285b0883f304ad9 running 0 tests test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out Doc-tests add-one running 0 tests test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out Installing binaries from Cargo.IO \u00b6 cargo install package","title":"Cargo and Crates.IO"},{"location":"Rust/28-cargo-and-crates/#cargo-and-cratesio","text":"","title":"Cargo and Crates.IO"},{"location":"Rust/28-cargo-and-crates/#customizing-builds-with-release-profiles","text":"There are two cargo profils, a build profile, that is used when we run cargo build and release profile when we run cargo build --release . These profiles can be configured in the Cargo.toml file: [profile.dev] opt-level=0 [profile.release] opt-level=3 The opt-level stands for optimization level that rust compiler applies on the build. It ranges from 0 to 3, the next level takes more time on the build time, but the run time will be faster. daviskregers@Daviss-MacBook-Pro \ue0b0 ~/Projects/learning-rust/28-profiles \ue0b0 \ue0a0 master \ue0b0 cargo build Compiling profiles v0.1.0 (/Users/daviskregers/Projects/learning-rust/28-profiles) Finished dev [unoptimized + debuginfo] target(s) in 0.64s daviskregers@Daviss-MacBook-Pro \ue0b0 ~/Projects/learning-rust/28-profiles \ue0b0 \ue0a0 master \ue0b0 cargo build --release Compiling profiles v0.1.0 (/Users/daviskregers/Projects/learning-rust/28-profiles) Finished release [optimized] target(s) in 0.31s","title":"Customizing builds with release profiles"},{"location":"Rust/28-cargo-and-crates/#making-documentation","text":"You can document the code using markdown in a following manner: ///Adds to 1 the number given /// ///#Example /// ///``` /// let five = 5; /// assert_eq!(6, profiles::add_one(5)); ///``` pub fn add_one(x:i32) -> i32 { x + 1 } The documentation can be build using a command: cargo doc The documentation can be opened using cargo doc --open The Examples part is also running as a doctest : cargo test","title":"Making Documentation"},{"location":"Rust/28-cargo-and-crates/#publishing-to-crateio","text":"First, you need to create an account on https://crates.io/ and obtain an API key. cargo login API_KEY Now, you will need to modify the Cargo.toml file and add description and license : description = \"This is a description\" license = \"MIT\" Now, you can publish it with cargo publish","title":"Publishing to Crate.IO"},{"location":"Rust/28-cargo-and-crates/#cargo-workspaces","text":"You can create a directory with a Cargo.toml file in it: [workspace] members=[ \"adder\", \"add-one\" ] cargo new adder cargo new add-one --lib in the adder/Cargo.toml : [dependencies] add-one={path=\"../add-one\"} In add-one/src/lib.rs : pub fn add_one(num : i32) -> i32 { num + 1 } In adder/src/main.rs : extern crate add_one; fn main() { let num = 10; println!(\"{}\", add_one::add_one(num)); } cargo build \u2718 daviskregers@Daviss-MacBook-Pro \ue0b0 ~/Projects/learning-rust/28-workspaces \ue0b0 \ue0a0 master \ue0b0 cargo build Compiling add-one v0.1.0 (/Users/daviskregers/Projects/learning-rust/28-workspaces/add-one) Compiling adder v0.1.0 (/Users/daviskregers/Projects/learning-rust/28-workspaces/adder) Finished dev [unoptimized + debuginfo] target(s) in 0.67s cargo run daviskregers@Daviss-MacBook-Pro \ue0b0 ~/Projects/learning-rust/28-workspaces \ue0b0 \ue0a0 master \ue0b0 cargo run Finished dev [unoptimized + debuginfo] target(s) in 0.05s Running `target/debug/adder` 11 You can then add tests in the same manner before: #[cfg(test)] mod tests { use super::*; #[test] fn it_works() { assert_eq!(4, add_one(3)); assert_eq!(6, add_one(5)); } } cargo test \u2718 daviskregers@Daviss-MacBook-Pro \ue0b0 ~/Projects/learning-rust/28-workspaces \ue0b0 \ue0a0 master \ue0b0 cargo test Compiling add-one v0.1.0 (/Users/daviskregers/Projects/learning-rust/28-workspaces/add-one) Compiling adder v0.1.0 (/Users/daviskregers/Projects/learning-rust/28-workspaces/adder) Finished dev [unoptimized + debuginfo] target(s) in 0.74s Running target/debug/deps/add_one-04932d8ef5f0255b running 1 test test tests::it_works ... ok test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out Running target/debug/deps/adder-0285b0883f304ad9 running 0 tests test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out Doc-tests add-one running 0 tests test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out","title":"Cargo Workspaces"},{"location":"Rust/28-cargo-and-crates/#installing-binaries-from-cargoio","text":"cargo install package","title":"Installing binaries from Cargo.IO"},{"location":"Rust/29-smart-pointers/","text":"Smart pointers \u00b6 A smart pointer is a variable that holds an address to a memory. The most common type of a pointer is the reference. Smart pointers are data structures that act like references but they posses additional meta data. They own data and allow to modify them. Box \u00b6 Box points to data on a heap and points to it. You can use it when you have data of a known type but an unknown size at the build time. Then, it is used in a situation where this size is needed to be known. let b = Box::new(5); println!(\"{}\", b); When the box runs out of scope, the memory is deallocated since the box itself is allocated in stack, the data in heap. It can be used also in cases when otherwise it would throw errors about recursion: use List::{Cons, Nil}; enum List { Cons(i32, Box<List>), Nil } fn main() { let list = Box::new(Cons(1, Box::new(Cons(2, Box::new(Cons(3, Box::new(Nil))))))); } Dereference \u00b6 Using deference, you can use smart pointers like references. It is used with * operator: let x = 5; let y = &x; assert_eq!(5, x); assert_eq!(5, *y); Deref trait \u00b6 You can implement deref trait in a following manner: use std::ops::Deref; struct MyBox<T>(T); impl <T> MyBox<T> { fn new(x: T) -> MyBox<T> { MyBox(x) } } impl <T> Deref for MyBox<T> { type Target = T; fn deref(&self)->&T { &self.0 } } let x = 5; let y = MyBox::new(5); assert_eq!(5,x); assert_eq!(5, *y); // *(y.deref()) Deref coercion \u00b6 fn hello(name : &str) { println!(\"Hello, {}\", name) } let m = MyBox::new(String::from(\"Rust\")); hello(&m); Drop Trait \u00b6 Customize on how data is released when it is about to go out of scope. struct CustomSmartPointer { data: String, } impl Drop for CustomSmartPointer { fn drop(&mut self) { println!(\"Dropping Custom smart pointer with data {}\", self.data); } } let c = CustomSmartPointer{data:String::from(\"my stuff\")}; let d = CustomSmartPointer{data:String::from(\"other stuff\")}; println!(\"Custom smart pointer created\") Output: Custom smart pointer created Dropping Custom smart pointer with data other stuff Dropping Custom smart pointer with data my stuff Note that rust drops data in reverse order - the \"other stuff\" drops first. You can drop value early by using: let c = CustomSmartPointer{data:String::from(\"my stuff\")}; drop(c) Reference count \u00b6 There are situations when data has multiple owners: use List::{Cons, Nil}; enum List { Cons(i32, Box<List>), Nil } let a = Cons(5, Box::new(Cons(10, Box::new(Nil)))); let b = Cons(3, Box::new(Cons(a))); let c = Cons(4, Box::new(a)); This will throw an error about that the value has been moved. To fix this, we can use a Rc: use std::rc::Rc; enum List { Cons(i32, Rc<List>), Nil } let a = Rc::new(Cons(5, Rc::new(Cons(10, Rc::new(Nil))))); println!(\"Counter after creating a {}\", Rc::strong_count(&a)); let b = Cons(3, Rc::clone(&a)); println!(\"Counter after creating a {}\", Rc::strong_count(&a)); let c = Cons(4, Rc::clone(&a)); println!(\"Counter after creating a {}\", Rc::strong_count(&a));","title":"Smart pointers"},{"location":"Rust/29-smart-pointers/#smart-pointers","text":"A smart pointer is a variable that holds an address to a memory. The most common type of a pointer is the reference. Smart pointers are data structures that act like references but they posses additional meta data. They own data and allow to modify them.","title":"Smart pointers"},{"location":"Rust/29-smart-pointers/#box","text":"Box points to data on a heap and points to it. You can use it when you have data of a known type but an unknown size at the build time. Then, it is used in a situation where this size is needed to be known. let b = Box::new(5); println!(\"{}\", b); When the box runs out of scope, the memory is deallocated since the box itself is allocated in stack, the data in heap. It can be used also in cases when otherwise it would throw errors about recursion: use List::{Cons, Nil}; enum List { Cons(i32, Box<List>), Nil } fn main() { let list = Box::new(Cons(1, Box::new(Cons(2, Box::new(Cons(3, Box::new(Nil))))))); }","title":"Box"},{"location":"Rust/29-smart-pointers/#dereference","text":"Using deference, you can use smart pointers like references. It is used with * operator: let x = 5; let y = &x; assert_eq!(5, x); assert_eq!(5, *y);","title":"Dereference"},{"location":"Rust/29-smart-pointers/#deref-trait","text":"You can implement deref trait in a following manner: use std::ops::Deref; struct MyBox<T>(T); impl <T> MyBox<T> { fn new(x: T) -> MyBox<T> { MyBox(x) } } impl <T> Deref for MyBox<T> { type Target = T; fn deref(&self)->&T { &self.0 } } let x = 5; let y = MyBox::new(5); assert_eq!(5,x); assert_eq!(5, *y); // *(y.deref())","title":"Deref trait"},{"location":"Rust/29-smart-pointers/#deref-coercion","text":"fn hello(name : &str) { println!(\"Hello, {}\", name) } let m = MyBox::new(String::from(\"Rust\")); hello(&m);","title":"Deref coercion"},{"location":"Rust/29-smart-pointers/#drop-trait","text":"Customize on how data is released when it is about to go out of scope. struct CustomSmartPointer { data: String, } impl Drop for CustomSmartPointer { fn drop(&mut self) { println!(\"Dropping Custom smart pointer with data {}\", self.data); } } let c = CustomSmartPointer{data:String::from(\"my stuff\")}; let d = CustomSmartPointer{data:String::from(\"other stuff\")}; println!(\"Custom smart pointer created\") Output: Custom smart pointer created Dropping Custom smart pointer with data other stuff Dropping Custom smart pointer with data my stuff Note that rust drops data in reverse order - the \"other stuff\" drops first. You can drop value early by using: let c = CustomSmartPointer{data:String::from(\"my stuff\")}; drop(c)","title":"Drop Trait"},{"location":"Rust/29-smart-pointers/#reference-count","text":"There are situations when data has multiple owners: use List::{Cons, Nil}; enum List { Cons(i32, Box<List>), Nil } let a = Cons(5, Box::new(Cons(10, Box::new(Nil)))); let b = Cons(3, Box::new(Cons(a))); let c = Cons(4, Box::new(a)); This will throw an error about that the value has been moved. To fix this, we can use a Rc: use std::rc::Rc; enum List { Cons(i32, Rc<List>), Nil } let a = Rc::new(Cons(5, Rc::new(Cons(10, Rc::new(Nil))))); println!(\"Counter after creating a {}\", Rc::strong_count(&a)); let b = Cons(3, Rc::clone(&a)); println!(\"Counter after creating a {}\", Rc::strong_count(&a)); let c = Cons(4, Rc::clone(&a)); println!(\"Counter after creating a {}\", Rc::strong_count(&a));","title":"Reference count"},{"location":"Rust/30-refcell-and-interior-mutability/","text":"Refcell and interior mutability \u00b6 Using interior mutability allows you to mutate data even though it is a reference to a immutable data. Normally this is not allowed by rust, but we can use data structures to bend the rules of rust. A practical use case for interior mutability is used for mock objects while testing. pub trait Messenger { fn send(&self, msg: &str); } pub struct LimitTracker <'a, T: 'a+Messenger> { messenger: &'a T, value: usize, max: usize } impl <'a, T> LimitTracker <'a, T> where T: Messenger { pub fn new (messenger: &T, max: usize) -> LimitTracker<T> { LimitTracker { messenger, value: 0, max, } } pub fn set_value(&mut self, value: usize) { self.value = value; let percent_of_max = self.value as f64 / self.max as f64; if percent_of_max >= 0.75 && percent_of_max < 0.9 { self.messenger.send(\"Warning, you've used over 75%!\") } else if percent_of_max > 0.9 { self.messenger.send(\"Warning, you've used over 90%!\") } else if percent_of_max >= 1.0 { self.messenger.send(\"You have reached your quota!\") } } } #[cfg(test)] mod test { use super::*; struct MockMessenger { sent_messages:Vec<String>, } impl MockMessenger { fn new() -> MockMessenger { MockMessenger { sent_messages:vec![] } } } impl Messenger for MockMessenger { fn send(&self, message: &str) { self.sent_messages.push(String::from(message)); } } #[test] fn it_sends_over_75_percent_message() { let mock_messanger = MockMessenger::new(); let mut limit_tracker = LimitTracker::new(&mock_messanger, 100); limit_tracker.set_value(80); assert_eq!(mock_messanger.sent_messages.len(), 1); } } When trying to compile this, it will throw an error with a following message: \u2718 davis@davis-arch \ue0b0 ~/projects/rust/30_interior_mutability \ue0b0 \ue0a0 master \ue0b0 cargo test Compiling interior_mutability v0.1.0 (/home/davis/projects/rust/30_interior_mutability) error[E0596]: cannot borrow `self.sent_messages` as mutable, as it is behind a `&` reference --> src/lib.rs:53:13 | 52 | fn send(&self, message: &str) { | ----- help: consider changing this to be a mutable reference: `&mut self` 53 | self.sent_messages.push(String::from(message)); | ^^^^^^^^^^^^^^^^^^ `self` is a `&` reference, so the data it refers to cannot be borrowed as mutable error: aborting due to previous error For more information about this error, try `rustc --explain E0596`. error: Could not compile `interior_mutability`. warning: build failed, waiting for other jobs to finish... error: build failed We can modify it to use RefCell: use std::cell::RefCell; pub trait Messenger { fn send(&self, msg: &str); } pub struct LimitTracker <'a, T: 'a+Messenger> { messenger: &'a T, value: usize, max: usize } impl <'a, T> LimitTracker <'a, T> where T: Messenger { pub fn new (messenger: &T, max: usize) -> LimitTracker<T> { LimitTracker { messenger, value: 0, max, } } pub fn set_value(&mut self, value: usize) { self.value = value; let percent_of_max = self.value as f64 / self.max as f64; if percent_of_max >= 0.75 && percent_of_max < 0.9 { self.messenger.send(\"Warning, you've used over 75%!\") } else if percent_of_max > 0.9 { self.messenger.send(\"Warning, you've used over 90%!\") } else if percent_of_max >= 1.0 { self.messenger.send(\"You have reached your quota!\") } } } #[cfg(test)] mod test { use super::*; struct MockMessenger { sent_messages:RefCell<Vec<String>>, } impl MockMessenger { fn new() -> MockMessenger { MockMessenger { sent_messages:RefCell::new(vec![]) } } } impl Messenger for MockMessenger { fn send(&self, message: &str) { self.sent_messages.borrow_mut().push(String::from(message)); } } #[test] fn it_sends_over_75_percent_message() { let mock_messanger = MockMessenger::new(); let mut limit_tracker = LimitTracker::new(&mock_messanger, 100); limit_tracker.set_value(80); assert_eq!(mock_messanger.sent_messages.borrow().len(), 1); } } Now the test will pass: \u2718 davis@davis-arch \ue0b0 ~/projects/rust/30_interior_mutability \ue0b0 \ue0a0 master \ue0b0 cargo test Compiling interior_mutability v0.1.0 (/home/davis/projects/rust/30_interior_mutability) warning: unused import: `std::cell::RefCell` --> src/lib.rs:1:5 | 1 | use std::cell::RefCell; | ^^^^^^^^^^^^^^^^^^ | = note: #[warn(unused_imports)] on by default Finished dev [unoptimized + debuginfo] target(s) in 0.39s Running target/debug/deps/interior_mutability-c8d57b4e53a0b271 running 1 test test test::it_sends_over_75_percent_message ... ok test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out Doc-tests interior_mutability running 0 tests test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out RefCell borrow checker \u00b6 let mut one_borrow=self.sent_messages.borrow_mut(); let mut two_borrow=self.sent_messages.borrow_mut(); one_borrow.push(String::from(message)); two_borrow.push(String::from(message)); This will throw Already borrowed: BorrowMutError .","title":"Refcell and interior mutability"},{"location":"Rust/30-refcell-and-interior-mutability/#refcell-and-interior-mutability","text":"Using interior mutability allows you to mutate data even though it is a reference to a immutable data. Normally this is not allowed by rust, but we can use data structures to bend the rules of rust. A practical use case for interior mutability is used for mock objects while testing. pub trait Messenger { fn send(&self, msg: &str); } pub struct LimitTracker <'a, T: 'a+Messenger> { messenger: &'a T, value: usize, max: usize } impl <'a, T> LimitTracker <'a, T> where T: Messenger { pub fn new (messenger: &T, max: usize) -> LimitTracker<T> { LimitTracker { messenger, value: 0, max, } } pub fn set_value(&mut self, value: usize) { self.value = value; let percent_of_max = self.value as f64 / self.max as f64; if percent_of_max >= 0.75 && percent_of_max < 0.9 { self.messenger.send(\"Warning, you've used over 75%!\") } else if percent_of_max > 0.9 { self.messenger.send(\"Warning, you've used over 90%!\") } else if percent_of_max >= 1.0 { self.messenger.send(\"You have reached your quota!\") } } } #[cfg(test)] mod test { use super::*; struct MockMessenger { sent_messages:Vec<String>, } impl MockMessenger { fn new() -> MockMessenger { MockMessenger { sent_messages:vec![] } } } impl Messenger for MockMessenger { fn send(&self, message: &str) { self.sent_messages.push(String::from(message)); } } #[test] fn it_sends_over_75_percent_message() { let mock_messanger = MockMessenger::new(); let mut limit_tracker = LimitTracker::new(&mock_messanger, 100); limit_tracker.set_value(80); assert_eq!(mock_messanger.sent_messages.len(), 1); } } When trying to compile this, it will throw an error with a following message: \u2718 davis@davis-arch \ue0b0 ~/projects/rust/30_interior_mutability \ue0b0 \ue0a0 master \ue0b0 cargo test Compiling interior_mutability v0.1.0 (/home/davis/projects/rust/30_interior_mutability) error[E0596]: cannot borrow `self.sent_messages` as mutable, as it is behind a `&` reference --> src/lib.rs:53:13 | 52 | fn send(&self, message: &str) { | ----- help: consider changing this to be a mutable reference: `&mut self` 53 | self.sent_messages.push(String::from(message)); | ^^^^^^^^^^^^^^^^^^ `self` is a `&` reference, so the data it refers to cannot be borrowed as mutable error: aborting due to previous error For more information about this error, try `rustc --explain E0596`. error: Could not compile `interior_mutability`. warning: build failed, waiting for other jobs to finish... error: build failed We can modify it to use RefCell: use std::cell::RefCell; pub trait Messenger { fn send(&self, msg: &str); } pub struct LimitTracker <'a, T: 'a+Messenger> { messenger: &'a T, value: usize, max: usize } impl <'a, T> LimitTracker <'a, T> where T: Messenger { pub fn new (messenger: &T, max: usize) -> LimitTracker<T> { LimitTracker { messenger, value: 0, max, } } pub fn set_value(&mut self, value: usize) { self.value = value; let percent_of_max = self.value as f64 / self.max as f64; if percent_of_max >= 0.75 && percent_of_max < 0.9 { self.messenger.send(\"Warning, you've used over 75%!\") } else if percent_of_max > 0.9 { self.messenger.send(\"Warning, you've used over 90%!\") } else if percent_of_max >= 1.0 { self.messenger.send(\"You have reached your quota!\") } } } #[cfg(test)] mod test { use super::*; struct MockMessenger { sent_messages:RefCell<Vec<String>>, } impl MockMessenger { fn new() -> MockMessenger { MockMessenger { sent_messages:RefCell::new(vec![]) } } } impl Messenger for MockMessenger { fn send(&self, message: &str) { self.sent_messages.borrow_mut().push(String::from(message)); } } #[test] fn it_sends_over_75_percent_message() { let mock_messanger = MockMessenger::new(); let mut limit_tracker = LimitTracker::new(&mock_messanger, 100); limit_tracker.set_value(80); assert_eq!(mock_messanger.sent_messages.borrow().len(), 1); } } Now the test will pass: \u2718 davis@davis-arch \ue0b0 ~/projects/rust/30_interior_mutability \ue0b0 \ue0a0 master \ue0b0 cargo test Compiling interior_mutability v0.1.0 (/home/davis/projects/rust/30_interior_mutability) warning: unused import: `std::cell::RefCell` --> src/lib.rs:1:5 | 1 | use std::cell::RefCell; | ^^^^^^^^^^^^^^^^^^ | = note: #[warn(unused_imports)] on by default Finished dev [unoptimized + debuginfo] target(s) in 0.39s Running target/debug/deps/interior_mutability-c8d57b4e53a0b271 running 1 test test test::it_sends_over_75_percent_message ... ok test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out Doc-tests interior_mutability running 0 tests test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out","title":"Refcell and interior mutability"},{"location":"Rust/30-refcell-and-interior-mutability/#refcell-borrow-checker","text":"let mut one_borrow=self.sent_messages.borrow_mut(); let mut two_borrow=self.sent_messages.borrow_mut(); one_borrow.push(String::from(message)); two_borrow.push(String::from(message)); This will throw Already borrowed: BorrowMutError .","title":"RefCell borrow checker"},{"location":"Rust/34-join-handles/","text":"Using Join Handles \u00b6 Looking at the last section, we can see that the main thread stops and the spawned thread does not finish it's work. We can use Join Handles to ensure that all threads finish their tasks before exiting. use std::thread; use std::time::Duration; fn main() { let handle = thread::spawn(|| { for i in 1 .. 10 { println!(\"Hello from thread {}\", i); thread::sleep(Duration::from_millis(1)); } }); for i in 1..10 { println!(\"Hello from main {}\", i); thread::sleep(Duration::from_millis(1)); } handle.join().unwrap(); } Output: davis@davis-arch \ue0b0 ~/projects/rust \ue0b0 \ue0a0 master \ue0b0 ./34_join_handle Hello from main 1 Hello from thread 1 Hello from main 2 Hello from thread 2 Hello from main 3 Hello from thread 3 Hello from main 4 Hello from thread 4 Hello from main 5 Hello from thread 5 Hello from main 6 Hello from thread 6 Hello from main 7 Hello from thread 7 Hello from main 8 Hello from thread 8 Hello from main 9 Hello from thread 9","title":"Using Join Handles"},{"location":"Rust/34-join-handles/#using-join-handles","text":"Looking at the last section, we can see that the main thread stops and the spawned thread does not finish it's work. We can use Join Handles to ensure that all threads finish their tasks before exiting. use std::thread; use std::time::Duration; fn main() { let handle = thread::spawn(|| { for i in 1 .. 10 { println!(\"Hello from thread {}\", i); thread::sleep(Duration::from_millis(1)); } }); for i in 1..10 { println!(\"Hello from main {}\", i); thread::sleep(Duration::from_millis(1)); } handle.join().unwrap(); } Output: davis@davis-arch \ue0b0 ~/projects/rust \ue0b0 \ue0a0 master \ue0b0 ./34_join_handle Hello from main 1 Hello from thread 1 Hello from main 2 Hello from thread 2 Hello from main 3 Hello from thread 3 Hello from main 4 Hello from thread 4 Hello from main 5 Hello from thread 5 Hello from main 6 Hello from thread 6 Hello from main 7 Hello from thread 7 Hello from main 8 Hello from thread 8 Hello from main 9 Hello from thread 9","title":"Using Join Handles"},{"location":"Rust/36-message-passing-between-threads/","text":"Message passing between threads \u00b6 use std::sync::mpsc; // multiple producer, single consumer use std::thread; fn main() { let (sender, reciever) = mpsc::channel(); thread::spawn(move || { let val = String::from(\"hi\"); sender.send(val).unwrap(); }); let rec = reciever.recv().unwrap(); println!(\"Got {}\", rec); } Output: avis@davis-arch \ue0b0 ~/projects/rust \ue0b0 \ue0a0 master \ue0b0 ./36-message-passing-between-threads Got hi","title":"Message passing between threads"},{"location":"Rust/36-message-passing-between-threads/#message-passing-between-threads","text":"use std::sync::mpsc; // multiple producer, single consumer use std::thread; fn main() { let (sender, reciever) = mpsc::channel(); thread::spawn(move || { let val = String::from(\"hi\"); sender.send(val).unwrap(); }); let rec = reciever.recv().unwrap(); println!(\"Got {}\", rec); } Output: avis@davis-arch \ue0b0 ~/projects/rust \ue0b0 \ue0a0 master \ue0b0 ./36-message-passing-between-threads Got hi","title":"Message passing between threads"},{"location":"Rust/37-sending-multiple-values/","text":"Sending multiple values \u00b6 If we try to print the value sent in the previous section: use std::sync::mpsc; // multiple producer, single consumer use std::thread; fn main() { let (sender, reciever) = mpsc::channel(); thread::spawn(move || { let val = String::from(\"hi\"); sender.send(val).unwrap(); println!(\"{}\", val); }); let rec = reciever.recv().unwrap(); println!(\"Got {}\", rec); } While compiling, we will get an error use of moved value because the value has been sent to another thread. we can use something like this: use std::sync::mpsc; // multiple producer, single consumer use std::thread; use std::time::Duration; fn main() { let (sender, reciever) = mpsc::channel(); thread::spawn(move || { let vals = vec![\"hi\", \"from\", \"the\", \"thread\"]; for val in vals { sender.send(val).unwrap(); thread::sleep(Duration::from_secs(1)) } }); for received in reciever { println!(\"Got {}\", received); } } Output: davis@davis-arch \ue0b0 ~/projects/rust \ue0b0 \ue0a0 master \ue0b0 ./37-sending-multiple-values Got hi Got from Got the Got thread","title":"Sending multiple values"},{"location":"Rust/37-sending-multiple-values/#sending-multiple-values","text":"If we try to print the value sent in the previous section: use std::sync::mpsc; // multiple producer, single consumer use std::thread; fn main() { let (sender, reciever) = mpsc::channel(); thread::spawn(move || { let val = String::from(\"hi\"); sender.send(val).unwrap(); println!(\"{}\", val); }); let rec = reciever.recv().unwrap(); println!(\"Got {}\", rec); } While compiling, we will get an error use of moved value because the value has been sent to another thread. we can use something like this: use std::sync::mpsc; // multiple producer, single consumer use std::thread; use std::time::Duration; fn main() { let (sender, reciever) = mpsc::channel(); thread::spawn(move || { let vals = vec![\"hi\", \"from\", \"the\", \"thread\"]; for val in vals { sender.send(val).unwrap(); thread::sleep(Duration::from_secs(1)) } }); for received in reciever { println!(\"Got {}\", received); } } Output: davis@davis-arch \ue0b0 ~/projects/rust \ue0b0 \ue0a0 master \ue0b0 ./37-sending-multiple-values Got hi Got from Got the Got thread","title":"Sending multiple values"},{"location":"Rust/41-sharing-mutex-between-multiple-threads/","text":"Sharing Mutex between multiple threads \u00b6 We can use mutex to modify data in multiple threads by using Arc: use std::sync::{Mutex, Arc}; use std::thread; fn main() { let counter = Arc::new(Mutex::new(0)); let mut handles = vec![]; for _ in 0 .. 10 { let counter = Arc::clone(&counter); let handle = thread::spawn( move || { let mut num = counter.lock().unwrap(); *num += 1; }); handles.push(handle); } for handle in handles { handle.join().unwrap(); } println!(\"{}\", *counter.lock().unwrap()); } davis@davis-arch \ue0b0 ~/projects/rust \ue0b0 \ue0a0 master \ue0b0 ./41-sharing-mutex-between-multiple-threads 10","title":"Sharing Mutex between multiple threads"},{"location":"Rust/41-sharing-mutex-between-multiple-threads/#sharing-mutex-between-multiple-threads","text":"We can use mutex to modify data in multiple threads by using Arc: use std::sync::{Mutex, Arc}; use std::thread; fn main() { let counter = Arc::new(Mutex::new(0)); let mut handles = vec![]; for _ in 0 .. 10 { let counter = Arc::clone(&counter); let handle = thread::spawn( move || { let mut num = counter.lock().unwrap(); *num += 1; }); handles.push(handle); } for handle in handles { handle.join().unwrap(); } println!(\"{}\", *counter.lock().unwrap()); } davis@davis-arch \ue0b0 ~/projects/rust \ue0b0 \ue0a0 master \ue0b0 ./41-sharing-mutex-between-multiple-threads 10","title":"Sharing Mutex between multiple threads"},{"location":"Rust/Cargo%20Crate/","text":"A crate is a [[package]] of rust code https://crates.io/ with a [[binary]] or [[library]] type. The crates / dependencies can be added to the `Cargo.toml' file: [dependencies] rand = \"0.5.0\" Now, when running cargo build , it will automatically download the Dependency . Crates can be updated using the cargo update . to include the crate in the code you can use extern crate rand; use rand::Rng; fn main() { }","title":"Cargo Crate"},{"location":"Rust/Cargo/","text":"Cargo is a Rust language 's [[build system]] and [[package manager]]. This tool is used to manage rust projects. To create a new project using cargo: cargo new projectname --bin or --lib To [[compile]] the project: cargo build cargo check cargo run","title":"Cargo"},{"location":"Rust/Comments%20in%20Rust/","text":"For [[single line comment]]s we can use: // This is a single line comment For [[multiline comment]]s /* This Is a Multiline Comment */ fn main() { // a single line comment /* Multiline Comment */ }","title":"Comments in Rust"},{"location":"Rust/Concurrency/","text":"One of the rusts major goal is to handle [[concurency]] safely and efficiently, where parts of program are executed at the same time. By [[ownership]] and [[type checking]], many [[concurency errors]] are thrown at the [[compile time]]. Concurrency vs parallelism \u00b6 In concurrency , suppose we have one [[CPU core]] and three [[process]]es that are being ran at the same time. The CPU will switch between these processes using an [[algorithm]] like [[Round Robin]], saving the progress of it. In Parallelism , the CPU will have more than 1 core and it will be able to run more than 1 process at the same time while still using concurrency to switch between the processes.","title":"Concurrency"},{"location":"Rust/Concurrency/#concurrency-vs-parallelism","text":"In concurrency , suppose we have one [[CPU core]] and three [[process]]es that are being ran at the same time. The CPU will switch between these processes using an [[algorithm]] like [[Round Robin]], saving the progress of it. In Parallelism , the CPU will have more than 1 core and it will be able to run more than 1 process at the same time while still using concurrency to switch between the processes.","title":"Concurrency vs parallelism"},{"location":"Rust/Controlling%20Visibility/","text":"Controlling visibility \u00b6 When making a [[library]], usually we make a set of [[method]]s that users can use to do something. When we [[compile]] them, there will be a warning, that our project does not call them. To prevent this, we can use the [[visibility keyword]] pub , which will mark it as a public [[API]]. pub fn connect() {}","title":"Controlling Visibility"},{"location":"Rust/Controlling%20Visibility/#controlling-visibility","text":"When making a [[library]], usually we make a set of [[method]]s that users can use to do something. When we [[compile]] them, there will be a warning, that our project does not call them. To prevent this, we can use the [[visibility keyword]] pub , which will mark it as a public [[API]]. pub fn connect() {}","title":"Controlling visibility"},{"location":"Rust/Data%20types%20in%20Rust/","text":"Rust is a [[statically typed language]] - the [[compiler]] must know the [[data type]] of each [[variable]]. The compiler can usually infer what type we want to use based on the value and how we use it. We need to define type of variable in cases when many types are possible, such as when we are converting a [[String]] to a [[numeric type]]. In rust there are 2 types of data types: - [[Scalar]] - represents a single value (1, 9.4, 'c') - [[Integer]] Length Signed Unsigned 8-bit `i8` `u8` 16-bit `i16` `u16` 32-bit `i32` `u32` 64-bit `i64` `u64` 128-bit `i128` `u128` arch `isize` `usize` Floating-point number [[Boolean]] [[Character]] Enclosed with single quotes. [[Compound]] ffn main() { let a:i128 = -1; let b:u8 = 1; let c:f32=132.0048; let d:bool = true; let e:char = 'c'; println!(\" {} {} {} {} {}\", a, b, c, d, e); } davis@davis-arch \ue0b0 ~/projects/rust \ue0b0 ./datatypes -1 1 132.0048 true c","title":"Data types in Rust"},{"location":"Rust/Description%20of%20the%20Hello%20World%20program/","text":"Description of the Hello World program \u00b6 Previously we made a following \"Hello World\" program: fn main() { println!(\"Hello World\"); } The keyword fn is for declaring [[function]]s, just like in [[C language]] - it will look for the main [[function]] to execute the program. If we change the main to hello , an [[error]] will be thrown: davis@davis-arch \ue0b0 ~/projects/rust \ue0b0 rustc hello.rs error[E0601]: `main` function not found in crate `hello` | = note: consider adding a `main` function to `hello.rs` error: aborting due to previous error For more information about this error, try `rustc --explain E0601`. The println [[function]] prints a line in the [[terminal]] screen. You can also use [[placeholders]] in it. fn main() { println!(\"Hello, {}! {}\", \"World\", 2); } davis@davis-arch \ue0b0 ~/projects/rust \ue0b0 ./hello Hello, World! 2","title":"Description of the Hello World program"},{"location":"Rust/Description%20of%20the%20Hello%20World%20program/#description-of-the-hello-world-program","text":"Previously we made a following \"Hello World\" program: fn main() { println!(\"Hello World\"); } The keyword fn is for declaring [[function]]s, just like in [[C language]] - it will look for the main [[function]] to execute the program. If we change the main to hello , an [[error]] will be thrown: davis@davis-arch \ue0b0 ~/projects/rust \ue0b0 rustc hello.rs error[E0601]: `main` function not found in crate `hello` | = note: consider adding a `main` function to `hello.rs` error: aborting due to previous error For more information about this error, try `rustc --explain E0601`. The println [[function]] prints a line in the [[terminal]] screen. You can also use [[placeholders]] in it. fn main() { println!(\"Hello, {}! {}\", \"World\", 2); } davis@davis-arch \ue0b0 ~/projects/rust \ue0b0 ./hello Hello, World! 2","title":"Description of the Hello World program"},{"location":"Rust/Error%20handling%20in%20Rust/","text":"In Rust language there are two types of errors - [[Recoverable errors]] and [[Unrecoverable errors]]. use std::fs::File; use std::io::ErrorKind; use std::io::Read; fn main() { let f = File::open(\"hello.txt\"); // Err(Os { code: 2, kind: NotFound, message: \"No such file or directory\" }) println!(\"{:?}\", f); let _f1 = match f { Ok(file) => file, Err(_error) => { panic!(\"File not found\") } }; // Match error types let f = File::open(\"hello.txt\"); let _f2 = match f { Ok(file) => file, Err(ref error) if error.kind() == ErrorKind::NotFound => { match File::create(\"hello.txt\") { Ok(fc) => fc, Err(e) => { panic!(\"Not able to create file {:?}\", e) } } }, Err(error) => { panic!(\"Unable to Open File {:?}\", error); } }; // Unwrap let _f = File::open(\"abc.txt\").unwrap(); // Calls panic! for us when file not found // Expect let _f = File::open(\"abc.txt\").expect(\"failed\"); // Returns failed // Propagating errors let output = read(); match output { Ok(fi) => println!(\"{:?}\", fi), Err(e) => println!(\"{:?}\", e), } // Propagating using ? let output = read_q(); match output { Ok(fi) => println!(\"{:?}\", fi), Err(e) => println!(\"{:?}\", e), } // Throws an unrecoverable error panic!(\"Checking how panic works\"); let a = [1,2,3]; // Throws unrecoverable error - out of bounds a[99]; } fn read() -> Result<String, std::io::Error> { let f = File::open(\"hello.txt\"); let mut f = match f { Ok(file) => file, Err(e) => return Err(e), }; let mut s = String::new(); match f.read_to_string(&mut s) { Ok(_) => Ok(s), Err(e) => Err(e), } } fn read_q() -> Result<String, std::io::Error> { let mut f = File::open(\"hello.txt\")?; let mut s = String::new(); f.read_to_string(&mut s)?; Ok(s) }","title":"Error handling in Rust"},{"location":"Rust/Functions%20in%20Rust/","text":"[[Function]] definition consists of a [[function name]], [[return type]], [[parameters]] and the [[function body]]. Rust language uses [[snake case]] naming style - all letters are in lower case and words are sperated by _ . We use fn keyword to define a function. Functions can be defined anywhere in the program. fn function_name( argument list ) -> (return_type1, return_type2) { // statements } For example fn add(a:i32, b:i32) { return a +b } fn add(a:i32, b:i32) -> i32 { return a +b } fn sub_add(a:i32, b:i32) -> (i32, i32) { return (a-b, a+b) } The functions can also be nested: fn main() { fn sub_add(a:i32, b:i32) -> (i32, i32) { return (a-b, a+b) } println!(\"{:?}\", sub_add(1,2)) }","title":"Functions in Rust"},{"location":"Rust/Heap/","text":"Heap \u00b6 Heap can store data with a size unknown at [[compile time]] or a size that might change instead. For example, in the [[String]] the [[push_str]] method. The heap is less organised. The [[Operating System]] finds an empty spot somewhere in the heap that is big enough, marks it as being in use. When used, returns a [[pointer]], which addresses a location in memory. This process is called [[allocating on the heap]]. Allocating a large amount of space on the heap can take time.","title":"Heap"},{"location":"Rust/Heap/#heap","text":"Heap can store data with a size unknown at [[compile time]] or a size that might change instead. For example, in the [[String]] the [[push_str]] method. The heap is less organised. The [[Operating System]] finds an empty spot somewhere in the heap that is big enough, marks it as being in use. When used, returns a [[pointer]], which addresses a location in memory. This process is called [[allocating on the heap]]. Allocating a large amount of space on the heap can take time.","title":"Heap"},{"location":"Rust/Iterators%20in%20Rust/","text":"We can process a series of items with an [[iterator]]: fn main() { let v1 = vec![1,2,3]; let v1_iter=v1.iter(); for val in v1_iter { println!(\"{}\", val) } } We can also use let mut v1_iter2=v1.iter(); assert_eq!(v1_iter2.next(), Some(&1)); assert_eq!(v1_iter2.next(), Some(&2)); assert_eq!(v1_iter2.next(), Some(&3)); assert_eq!(v1_iter2.next(), None); You can use [[iterator]]s in following ways: let v1:Vec<i32> = vec![1,2,3]; let v2:Vec<_> = v1.iter().map(|x| x + 1).collect(); assert_eq!(v2,vec![2,3,4]); You can create your own [[iterator trait]] in a following manner: struct Counter { count: u32 } impl Counter { fn new() -> Counter { Counter{count: 0} } } impl Iterator for Counter { type Item = u32; fn next(&mut self) -> Option<Self::Item> { self.count += 1; if(self.count < 6) { Some(self.count) } else { None } } } let mut counter = Counter::new(); assert_eq!(counter.next(), Some(1)); assert_eq!(counter.next(), Some(2)); assert_eq!(counter.next(), Some(3)); assert_eq!(counter.next(), Some(4)); assert_eq!(counter.next(), Some(5)); assert_eq!(counter.next(), None); fn using_other_iterator_trait_methods() { let sum:u32 = Counter::new().zip(Counter::new().skip(1)).map(|(a,b)| a*b).filter(|x| x%3 ==0).sum(); assert_eq!(18, sum); } using_other_iterator_trait_methods();","title":"Iterators in Rust"},{"location":"Rust/Loops%20in%20Rust/","text":"In [[loops]] you repeat a set of operations until a certain given condition is no longer met. In Rust language there are 3 types of loops - loop , while and for loops. fn main() { let mut n =0; loop { if n < 5 { println!(\"{}\", n); n += 1; } else { break; } } while n <= 10 { println!(\"{}\", n); n += 1; } for n in 10 .. 21 { println!(\"{}\", n); } } The [[loop]] is a simple loop, [[breaking condition]] must be explicitly given. Best when you want to set up an [[infinite loop]]. The [[while loop]] is useful for a program to evaluate a [[condition]] within a [[loop]]. The [[for loop]] is used to [[iterate]] over a [[collection]] of items. The rust's [[for loop]] is faster than the [[while loop]] because the compiler adds extra [[runtime code]] to perform [[conditional checks]] on every element on every [[iteration]].","title":"Loops in Rust"},{"location":"Rust/Memory%20Allocation/","text":"","title":"Memory Allocation"},{"location":"Rust/Modules%20in%20Rust/","text":"A [[module]] is a collection of methods, it is defined by a keyword mod . mod client { fn connect() {} } mod network { fn connect() {} mod server { fn connect() {} } } You can also move the modules into separate files: mod client; and client.rs: fn connect() {}","title":"Modules in Rust"},{"location":"Rust/Multiple%20Owners%20by%20combining%20Rc%20and%20RefCell/","text":"When using [[Rc]] we can have multiple owners of data, but only gives us [[immutable access]] to that data, by using the [[RefCell]], we can gain mutable access to it. use std::rc::Rc; use std::cell::RefCell; fn main() { let value = Rc::new(RefCell::new(5)); let a = Rc::clone(&value); let b = Rc::clone(&value); *value.borrow_mut() += 5; println!(\"a {:?}\", a); *value.borrow_mut() += 5; println!(\"b {:?}\", b); *value.borrow_mut() += 5; println!(\"value {:?}\", value) } The output: davis@davis-arch \ue0b0 ~/projects/rust \ue0b0 \ue0a0 master \ue0b0 ./31_rc_refcell a RefCell { value: 10 } b RefCell { value: 15 } value RefCell { value: 20 }","title":"Multiple Owners by combining Rc and RefCell"},{"location":"Rust/Multiple%20producers%20by%20cloning%20transmitter/","text":"We can clone the sender to use more than one [[transmit]] of the [[channel]]. use std::thread; use std::sync::mpsc; use std::time::Duration; fn main() { let (s, r) = mpsc::channel(); let s1 = mpsc::Sender::clone(&s); thread::spawn( move || { let vals = vec![\"hi\", \"from\", \"the\", \"thread\"]; for val in vals { s.send(val).unwrap(); thread::sleep(Duration::from_secs(1)) } }); thread::spawn( move || { let vals = vec![\"more\", \"messages\", \"for\", \"you\"]; for val in vals { s1.send(val).unwrap(); thread::sleep(Duration::from_secs(1)) } }); for rec in r { println!(\"{}\", rec); } } This will produce an output like this: davis@davis-arch \ue0b0 ~/projects/rust \ue0b0 \ue0a0 master \ue0b0 ./38-multiple-producers-by-cloning-transmitter more hi messages from the for you thread","title":"Multiple producers by cloning transmitter"},{"location":"Rust/Mutex%20in%20Rust/","text":"When using [[channels]], the [[ownership]] of the data is being passed from one [[thread]] to another. We can use [[shared memory concurrency]] so we can access the data from multiple threads. We can use a [[Mutex]] for that. The rules of mutex: 1. You must obtain the [[lock]] of the [[mutex]] before accessing it's data 2. When you are done with the data, you must release the lock use std::sync::Mutex; fn main() { let m = Mutex::new(5); { let mut num = m.lock().unwrap(); *num = 6; } println!(\"m {:?}\", m); } Output: davis@davis-arch \ue0b0 ~/projects/rust \ue0b0 \ue0a0 master \ue0b0 ./40-mutex m Mutex { data: 6 }","title":"Mutex in Rust"},{"location":"Rust/Operators%20in%20Rust/","text":"Operators in Rust \u00b6 The following operators are supported in rust: + - [[addition]] - - [[subtraction]] / - [[division]] * - [[multiplication]] % - [[modulus]] > - [[greater than]] < - [[less than]] >= - [[greater or equal than]] <= - [[less or equal than]] \\== - [[equal to operation]] \\= - [[assignment operator operator]] && - [[and operator]] || - [[or operator]] ! - [[negate operator]] The [[operator]]s ++ and -- are note supported in Rust language . Instead you can use +=1 or -=1 operators. It can be used for *= etc as well. fn main() { println!(\" {} \", 1 + 1); println!(\" {} \", 1 - 1); println!(\" {} \", 3 * 2); println!(\" {} \", 4 / 2); println!(\" {} \", 8 % 3); println!(\" {} \", 8 > 3); println!(\" {} \", 8 < 3); println!(\" {} \", 8 >= 3); println!(\" {} \", 8 <= 3); println!(\" {} \", 8 == 8); println!(\" {} \", true && false); println!(\" {} \", true || false); println!(\" {} \", true && !false); } \u2718 davis@davis-arch \ue0b0 ~/projects/rust \ue0b0 ./5_operators 2 0 6 2 2 true false true false true false true true","title":"Operators in Rust"},{"location":"Rust/Operators%20in%20Rust/#operators-in-rust","text":"The following operators are supported in rust: + - [[addition]] - - [[subtraction]] / - [[division]] * - [[multiplication]] % - [[modulus]] > - [[greater than]] < - [[less than]] >= - [[greater or equal than]] <= - [[less or equal than]] \\== - [[equal to operation]] \\= - [[assignment operator operator]] && - [[and operator]] || - [[or operator]] ! - [[negate operator]] The [[operator]]s ++ and -- are note supported in Rust language . Instead you can use +=1 or -=1 operators. It can be used for *= etc as well. fn main() { println!(\" {} \", 1 + 1); println!(\" {} \", 1 - 1); println!(\" {} \", 3 * 2); println!(\" {} \", 4 / 2); println!(\" {} \", 8 % 3); println!(\" {} \", 8 > 3); println!(\" {} \", 8 < 3); println!(\" {} \", 8 >= 3); println!(\" {} \", 8 <= 3); println!(\" {} \", 8 == 8); println!(\" {} \", true && false); println!(\" {} \", true || false); println!(\" {} \", true && !false); } \u2718 davis@davis-arch \ue0b0 ~/projects/rust \ue0b0 ./5_operators 2 0 6 2 2 true false true false true false true true","title":"Operators in Rust"},{"location":"Rust/Parallelism/","text":"In Parallelism , the CPU will have more than 1 core and it will be able to run more than 1 process at the same time while still using concurrency to switch between the processes.","title":"Parallelism"},{"location":"Rust/Passing%20channel%20to%20function%20in%20Rust/","text":"Passing channel to function \u00b6 We can pass [[channel]]s to [[function]]s like this: use std::thread; use std::sync::mpsc; fn main() { let (s,r) = mpsc::channel(); let handle = thread::spawn(|| { run(s); run1(r); }); handle.join().unwrap(); } fn run (s: mpsc::Sender<i32>) { s.send(2).unwrap(); s.send(3).unwrap(); } fn run1(r : mpsc::Receiver<i32>) { let rc = r.recv().unwrap(); println!(\"{}\", rc); println!(\"{}\", r.recv().unwrap()); } Output: davis@davis-arch \ue0b0 ~/projects/rust \ue0b0 \ue0a0 master \ue0b0 ./39-passing-channels-to-functions 2 3","title":"Passing channel to function"},{"location":"Rust/Passing%20channel%20to%20function%20in%20Rust/#passing-channel-to-function","text":"We can pass [[channel]]s to [[function]]s like this: use std::thread; use std::sync::mpsc; fn main() { let (s,r) = mpsc::channel(); let handle = thread::spawn(|| { run(s); run1(r); }); handle.join().unwrap(); } fn run (s: mpsc::Sender<i32>) { s.send(2).unwrap(); s.send(3).unwrap(); } fn run1(r : mpsc::Receiver<i32>) { let rc = r.recv().unwrap(); println!(\"{}\", rc); println!(\"{}\", r.recv().unwrap()); } Output: davis@davis-arch \ue0b0 ~/projects/rust \ue0b0 \ue0a0 master \ue0b0 ./39-passing-channels-to-functions 2 3","title":"Passing channel to function"},{"location":"Rust/Pattern%20matching%20in%20Rust/","text":"Rust language also supports [[pattern matching]]. Match \u00b6 match is_tuesday { true => println!(\"Using purple as background color\"), false => println!(\"Using orange as background color\"), _ => println!(\"Using black as background color\") }; match x { 1|2 => println!(\"hey!\"), 3...5 => println!(\"hey?\"), _ => println!(\"bye ... \") } If let \u00b6 if let Some(color) = favorite_color { println!(\"Using your favorite color {}\", favorite_color) } else if is_tuesday { println!(\"Tuesday is green day\"); } else if let Ok(age) = age { if age > 30 { println!(\"Using purple as background color\"); } else { println!(\"Using orange as background color\"); } } else { println!(\"Using blue as background color\"); } While let and for let patterns \u00b6 let mut stack = Vec::new(); stack.push(1); stack.push(2); stack.push(3); while let Some(top) = stack.pop() { println!(\"{}\", top); } The [[loop]] stops when the pop returns None . For let \u00b6 let v = vec![\"a\", \"b\", \"c\"]; for (index, value) in v.iter().enumerate() { println!(\"{} is at index {}\", value, index) } Assignment \u00b6 let (x, y, z) = (1,2,3); println(\"{} {} {}\", x,y,z); Refutable and irrefutable patterns \u00b6 [[Patterns]] that will match for any value are called [[Refutable]], patterns that can fail for some values are called [[Irrefutable]]. Ref and Ref mut \u00b6 let name = Some(String::from(\"Bob\")) ; match name { Some(ref name) => println!(\"Found name {}\", name), None => (), } println!(\"{:?}\", name); Output: ` davis@davis-arch \ue0b0 ~/projects/rust \ue0b0 \ue0a0 master \ue0b0 ./42-pattern-matching Using orange as background color Using purple as background color 3 2 1 a is at index 0 b is at index 1 c is at index 2 1 2 3 hey! Some(\"John\")","title":"Pattern matching in Rust"},{"location":"Rust/Pattern%20matching%20in%20Rust/#match","text":"match is_tuesday { true => println!(\"Using purple as background color\"), false => println!(\"Using orange as background color\"), _ => println!(\"Using black as background color\") }; match x { 1|2 => println!(\"hey!\"), 3...5 => println!(\"hey?\"), _ => println!(\"bye ... \") }","title":"Match"},{"location":"Rust/Pattern%20matching%20in%20Rust/#if-let","text":"if let Some(color) = favorite_color { println!(\"Using your favorite color {}\", favorite_color) } else if is_tuesday { println!(\"Tuesday is green day\"); } else if let Ok(age) = age { if age > 30 { println!(\"Using purple as background color\"); } else { println!(\"Using orange as background color\"); } } else { println!(\"Using blue as background color\"); }","title":"If let"},{"location":"Rust/Pattern%20matching%20in%20Rust/#while-let-and-for-let-patterns","text":"let mut stack = Vec::new(); stack.push(1); stack.push(2); stack.push(3); while let Some(top) = stack.pop() { println!(\"{}\", top); } The [[loop]] stops when the pop returns None .","title":"While let and for let patterns"},{"location":"Rust/Pattern%20matching%20in%20Rust/#for-let","text":"let v = vec![\"a\", \"b\", \"c\"]; for (index, value) in v.iter().enumerate() { println!(\"{} is at index {}\", value, index) }","title":"For let"},{"location":"Rust/Pattern%20matching%20in%20Rust/#assignment","text":"let (x, y, z) = (1,2,3); println(\"{} {} {}\", x,y,z);","title":"Assignment"},{"location":"Rust/Pattern%20matching%20in%20Rust/#refutable-and-irrefutable-patterns","text":"[[Patterns]] that will match for any value are called [[Refutable]], patterns that can fail for some values are called [[Irrefutable]].","title":"Refutable and irrefutable patterns"},{"location":"Rust/Pattern%20matching%20in%20Rust/#ref-and-ref-mut","text":"let name = Some(String::from(\"Bob\")) ; match name { Some(ref name) => println!(\"Found name {}\", name), None => (), } println!(\"{:?}\", name); Output: ` davis@davis-arch \ue0b0 ~/projects/rust \ue0b0 \ue0a0 master \ue0b0 ./42-pattern-matching Using orange as background color Using purple as background color 3 2 1 a is at index 0 b is at index 1 c is at index 2 1 2 3 hey! Some(\"John\")","title":"Ref and Ref mut"},{"location":"Rust/Referring%20names%20to%20different%20modules/","text":"Normally, when using Modules in Rust we need to do something like this: pub mod a { pub mod series { pub mod of { pub fn nested_module() { } } } } fn main() { a::series::of::nested_module(); } We can fix this by using a use keyword. use a::series::of::nested_module; fn main() { nested_module(); } We can also use the use keyword with enums: enum TrafficLight{ Red, Yellow, Green } use TrafficLight::{Red,Yellow} fn main() { let red = Red; let yellow = Yellow; let green = TrafficLight::Green; }","title":"Referring names to different modules"},{"location":"Rust/Running%20Rust%20Tests/","text":"Running tests \u00b6 The cargo test compiles a binary that is configured to run all the tests in parallel. You can provide some configuration like not to run tests in parallel: cargo test -- --test-threads=1 To see output of the program: cargo test -- --nocapture Running a subset of tests: cargo test test_name cargo test check_guess You can run multiple tests by providing a common phrase in test names, for example this_test_will_pass and this_test_will_fail : cargo test this To ignore tests, you can add: #[test] #[ignore] fn ignored_test() { assert!(true) }","title":"Running Rust Tests"},{"location":"Rust/Running%20Rust%20Tests/#running-tests","text":"The cargo test compiles a binary that is configured to run all the tests in parallel. You can provide some configuration like not to run tests in parallel: cargo test -- --test-threads=1 To see output of the program: cargo test -- --nocapture Running a subset of tests: cargo test test_name cargo test check_guess You can run multiple tests by providing a common phrase in test names, for example this_test_will_pass and this_test_will_fail : cargo test this To ignore tests, you can add: #[test] #[ignore] fn ignored_test() { assert!(true) }","title":"Running tests"},{"location":"Rust/Rust%20Test%20Organization/","text":"Test organization \u00b6 In rust, the tests are organized into unit tests and integration tests. Unit tests are small, used to test single units in isolation. Integrated tests uses library as any other external library would. Unit testing \u00b6 We will put unit tests, we will put them in each file of the src/ directory, in a separate module tests : #[cfg(test)] mod tests { use super::*; #[test] fn it_works() { assert_eq!(2+2, 4) } } The #[cfg(test)] part tells the compiler not to include this module in the built version. Integration tests \u00b6 In rust integration tests we will save in tests/ directory, call use extern package where the package is project name, use methods under the package. Then everything works the same. extern crate package; mod tests { #[test] fn larger_can_hold_smaller() { let larger = package::Rectangle{length: 8, width: 7}; let smaller = package::Rectangle{ length: 5, width: 1}; assert!(larger.can_hold(&smaller)) } #[test] fn smaller_cannot_hold_larger() { let larger = package::Rectangle{length: 8, width: 7}; let smaller = package::Rectangle{ length: 5, width: 1}; assert!(!smaller.can_hold(&larger)); } #[test] fn test_add_two() { assert_eq!(4, package::add_two(2)); } #[test] fn greeting_contains_name() { let result = package::greeting(\"Rob\"); assert!(result.contains(\"Rob fail\"), \"Greeting dod not contain name, value was {}\", result); } #[test] #[should_panic] fn check_guess() { package::Guess::new(0); } #[test] #[ignore] fn ignored_test() { assert!(true) } }","title":"Rust Test Organization"},{"location":"Rust/Rust%20Test%20Organization/#test-organization","text":"In rust, the tests are organized into unit tests and integration tests. Unit tests are small, used to test single units in isolation. Integrated tests uses library as any other external library would.","title":"Test organization"},{"location":"Rust/Rust%20Test%20Organization/#unit-testing","text":"We will put unit tests, we will put them in each file of the src/ directory, in a separate module tests : #[cfg(test)] mod tests { use super::*; #[test] fn it_works() { assert_eq!(2+2, 4) } } The #[cfg(test)] part tells the compiler not to include this module in the built version.","title":"Unit testing"},{"location":"Rust/Rust%20Test%20Organization/#integration-tests","text":"In rust integration tests we will save in tests/ directory, call use extern package where the package is project name, use methods under the package. Then everything works the same. extern crate package; mod tests { #[test] fn larger_can_hold_smaller() { let larger = package::Rectangle{length: 8, width: 7}; let smaller = package::Rectangle{ length: 5, width: 1}; assert!(larger.can_hold(&smaller)) } #[test] fn smaller_cannot_hold_larger() { let larger = package::Rectangle{length: 8, width: 7}; let smaller = package::Rectangle{ length: 5, width: 1}; assert!(!smaller.can_hold(&larger)); } #[test] fn test_add_two() { assert_eq!(4, package::add_two(2)); } #[test] fn greeting_contains_name() { let result = package::greeting(\"Rob\"); assert!(result.contains(\"Rob fail\"), \"Greeting dod not contain name, value was {}\", result); } #[test] #[should_panic] fn check_guess() { package::Guess::new(0); } #[test] #[ignore] fn ignored_test() { assert!(true) } }","title":"Integration tests"},{"location":"Rust/Rust%20language/","text":"Rust is a [[system level]] [[programming language]]. Why Rust? \u00b6 Performance \u00b6 Rust is blazingly fast and memory-efficient: with no [[runtime]] or [[garbage collector]], it can power [[performance-critical service]]s, run on [[embedded devices]], and easily integrate with other languages. Reliability \u00b6 Rust\u2019s rich [[type system]] and [[ownership model]] guarantee [[memory-safety]] and [[thread-safety]] \u2014 and enable you to eliminate many classes of bugs at [[compile-time]]. Productivity \u00b6 Rust has great documentation, a friendly compiler with useful error messages, and top-notch tooling \u2014 an integrated [[package manager]] and [[build tool]], smart [[multi-editor support]] with auto-completion and [[type inspection]]s, an auto-formatter, and more. Basically, Rust is new age [[C++]] that has built with [[security]] in mind when using access to the [[memory]].","title":"Rust language"},{"location":"Rust/Rust%20language/#why-rust","text":"","title":"Why Rust?"},{"location":"Rust/Rust%20language/#performance","text":"Rust is blazingly fast and memory-efficient: with no [[runtime]] or [[garbage collector]], it can power [[performance-critical service]]s, run on [[embedded devices]], and easily integrate with other languages.","title":"Performance"},{"location":"Rust/Rust%20language/#reliability","text":"Rust\u2019s rich [[type system]] and [[ownership model]] guarantee [[memory-safety]] and [[thread-safety]] \u2014 and enable you to eliminate many classes of bugs at [[compile-time]].","title":"Reliability"},{"location":"Rust/Rust%20language/#productivity","text":"Rust has great documentation, a friendly compiler with useful error messages, and top-notch tooling \u2014 an integrated [[package manager]] and [[build tool]], smart [[multi-editor support]] with auto-completion and [[type inspection]]s, an auto-formatter, and more. Basically, Rust is new age [[C++]] that has built with [[security]] in mind when using access to the [[memory]].","title":"Productivity"},{"location":"Rust/Stack/","text":"The stack stores values in the order it gets them and removes them in the opposite order - [[LIFO (Last In First Out)]]. Adding an item in the stack is called as [[Push]]. Removing an item in stack is called as [[Pop]]. Stack must take up a known, fixed size. The stack is fast because of the way it accesses the data but in Heap you have to follow a [[pointer]] to get there. Also, the data on the stack is in a known, fixed size.","title":"Stack"},{"location":"Rust/Strings%20in%20Rust/","text":"Strings \u00b6 Rust has only one string type in the core language, which is [[string slice]] &str . The String type is provided by Rust's standard library rather than coded into the core language. It is [[growable]], [[mutable]] and UTF-8 encoded type. The growable means that you can add another string to it, for example adding a string \"World\" to \"Hello, \". This cannot be done when using the string slices. The mutable means - you can [[mutate the memory location]]. This cannot be done when using the string slices. fn main() { let mut a = String::new(); a = String::from(\"Hello\"); a.push_str(\", World!\"); println!(\" {} \", a) }","title":"Strings"},{"location":"Rust/Strings%20in%20Rust/#strings","text":"Rust has only one string type in the core language, which is [[string slice]] &str . The String type is provided by Rust's standard library rather than coded into the core language. It is [[growable]], [[mutable]] and UTF-8 encoded type. The growable means that you can add another string to it, for example adding a string \"World\" to \"Hello, \". This cannot be done when using the string slices. The mutable means - you can [[mutate the memory location]]. This cannot be done when using the string slices. fn main() { let mut a = String::new(); a = String::from(\"Hello\"); a.push_str(\", World!\"); println!(\" {} \", a) }","title":"Strings"},{"location":"Rust/Testing%20in%20Rust/","text":"Writing automated tests in Rust \u00b6 Rust provides support for [[automated tests]]. When using cargo , you can use command cargo test It will compile and run the tests. You can add tests in the .rs files using following syntax: struct Rectangle { length: u32, width: u32 } impl Rectangle { fn can_hold(&self, other: &Rectangle) -> bool { return self.length > other.length && self.width > other.width; } } fn add_two(a:i32) -> i32 { return a + 2; } #[cfg(test)] mod tests { use super::*; #[test] fn larger_can_hold_smaller() { let larger = Rectangle{length: 8, width: 7}; let smaller = Rectangle{ length: 5, width: 1}; assert!(larger.can_hold(&smaller)) } #[test] fn smaller_cannot_hold_larger() { let larger = Rectangle{length: 8, width: 7}; let smaller = Rectangle{ length: 5, width: 1}; assert!(!smaller.can_hold(&larger)); } #[test] fn test_add_two() { assert_eq!(4, add_two(2)); } } Now, when running the cargo test : daviskregers@Daviss-MacBook-Pro \ue0b0 ~/Projects/learning-rust/25-tests \ue0b0 \ue0a0 master \ue0b0 cargo test Finished dev [unoptimized + debuginfo] target(s) in 0.04s Running target/debug/deps/tests-394413f4ff767751 running 3 tests test tests::larger_can_hold_smaller ... ok test tests::test_add_two ... ok test tests::smaller_cannot_hold_larger ... ok test result: ok. 3 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out Doc-tests tests running 0 tests test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out You can use following [[macros]]: - assert! - assert!(true) - assert_eq! - assert_eq!(4, 2 + 2) You can define [[custom failure messages]] like so: assert!(result.contains(\"Rob\"), \"Greeting dod not contain name, value was {}\"); The output will be: ---- tests::greeting_contains_name stdout ---- thread 'tests::greeting_contains_name' panicked at 'Greeting dod not contain name, value was Hello Rob!', src/lib.rs:46:9 note: Run with `RUST_BACKTRACE=1` for a backtrace. You can also assert that the test will cause a panic using the #[should_panic] : struct Guess { value: i32, } impl Guess { fn new(value: i32) -> Guess { if value < 1 || value > 100 { panic!(\"Guess must be between 1 to 100, got {}\", value) } Guess { value } } } #[cfg(test)] mod tests { use super::*; #[test] #[should_panic] fn check_guess() { Guess::new(0); } } test tests::check_guess ... ok","title":"Writing automated tests in Rust"},{"location":"Rust/Testing%20in%20Rust/#writing-automated-tests-in-rust","text":"Rust provides support for [[automated tests]]. When using cargo , you can use command cargo test It will compile and run the tests. You can add tests in the .rs files using following syntax: struct Rectangle { length: u32, width: u32 } impl Rectangle { fn can_hold(&self, other: &Rectangle) -> bool { return self.length > other.length && self.width > other.width; } } fn add_two(a:i32) -> i32 { return a + 2; } #[cfg(test)] mod tests { use super::*; #[test] fn larger_can_hold_smaller() { let larger = Rectangle{length: 8, width: 7}; let smaller = Rectangle{ length: 5, width: 1}; assert!(larger.can_hold(&smaller)) } #[test] fn smaller_cannot_hold_larger() { let larger = Rectangle{length: 8, width: 7}; let smaller = Rectangle{ length: 5, width: 1}; assert!(!smaller.can_hold(&larger)); } #[test] fn test_add_two() { assert_eq!(4, add_two(2)); } } Now, when running the cargo test : daviskregers@Daviss-MacBook-Pro \ue0b0 ~/Projects/learning-rust/25-tests \ue0b0 \ue0a0 master \ue0b0 cargo test Finished dev [unoptimized + debuginfo] target(s) in 0.04s Running target/debug/deps/tests-394413f4ff767751 running 3 tests test tests::larger_can_hold_smaller ... ok test tests::test_add_two ... ok test tests::smaller_cannot_hold_larger ... ok test result: ok. 3 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out Doc-tests tests running 0 tests test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out You can use following [[macros]]: - assert! - assert!(true) - assert_eq! - assert_eq!(4, 2 + 2) You can define [[custom failure messages]] like so: assert!(result.contains(\"Rob\"), \"Greeting dod not contain name, value was {}\"); The output will be: ---- tests::greeting_contains_name stdout ---- thread 'tests::greeting_contains_name' panicked at 'Greeting dod not contain name, value was Hello Rob!', src/lib.rs:46:9 note: Run with `RUST_BACKTRACE=1` for a backtrace. You can also assert that the test will cause a panic using the #[should_panic] : struct Guess { value: i32, } impl Guess { fn new(value: i32) -> Guess { if value < 1 || value > 100 { panic!(\"Guess must be between 1 to 100, got {}\", value) } Guess { value } } } #[cfg(test)] mod tests { use super::*; #[test] #[should_panic] fn check_guess() { Guess::new(0); } } test tests::check_guess ... ok","title":"Writing automated tests in Rust"},{"location":"Rust/Typecasting%20in%20Rust/","text":"By default, [[typecasting]] is not supported in [[Rust]]. fn main() { let a:i32 = 10; let b:i64 = a; } Will throw an error mismatched types . This can be fixed by using something like this: fn main() { let a:i32 = 10; let b:i64 = a as i64 + 10; } or fn main() { let a:i32 = 10; let b:i64 = a.into() + 10; }","title":"Typecasting in Rust"},{"location":"Rust/Using%20Move%20Closure%20with%20Threads/","text":"Using Move Closure with Threads \u00b6 We can use [[Move Closure]] to move data from one to another [[thread]]. use std::thread; fn main() { let v = vec![1,2,3]; let handle = thread::spawn(||{ println!(\"{:?}\", v) }); handle.join().unwrap(); } This will throw a [[compile error]] closure may outlive the current function, but it borrows v, which is owned by current function. This means, that the thread may live as long as when the reference to v is no longer valid. By using move keyword, we can force the [[closure]] to move the [[ownership]] of the values: use std::thread; fn main() { let v = vec![1,2,3]; let handle = thread::spawn(move || { println!(\"{:?}\", v) }); handle.join().unwrap(); } \u2718 davis@davis-arch \ue0b0 ~/projects/rust \ue0b0 \ue0a0 master \ue0b0 ./35-move-closure-with-threads [1, 2, 3]","title":"Using Move Closure with Threads"},{"location":"Rust/Using%20Move%20Closure%20with%20Threads/#using-move-closure-with-threads","text":"We can use [[Move Closure]] to move data from one to another [[thread]]. use std::thread; fn main() { let v = vec![1,2,3]; let handle = thread::spawn(||{ println!(\"{:?}\", v) }); handle.join().unwrap(); } This will throw a [[compile error]] closure may outlive the current function, but it borrows v, which is owned by current function. This means, that the thread may live as long as when the reference to v is no longer valid. By using move keyword, we can force the [[closure]] to move the [[ownership]] of the values: use std::thread; fn main() { let v = vec![1,2,3]; let handle = thread::spawn(move || { println!(\"{:?}\", v) }); handle.join().unwrap(); } \u2718 davis@davis-arch \ue0b0 ~/projects/rust \ue0b0 \ue0a0 master \ue0b0 ./35-move-closure-with-threads [1, 2, 3]","title":"Using Move Closure with Threads"},{"location":"Rust/Using%20threads%20in%20Rust/","text":"We can use [[thread]]s to run parts of program at the same time to ensure that the program does it's tasks quicker. But now, it cannot ensure the order of the tasks that will be ran. use std::thread; use std::time::Duration; fn main() { thread::spawn(|| { for i in 1 .. 10 { println!(\"Hello from thread {}\", i); thread::sleep(Duration::from_millis(1)); } }); for i in 1..5 { println!(\"Hello from main {}\", i); thread::sleep(Duration::from_millis(1)); } } The output: davis@davis-arch \ue0b0 ~/projects/rust \ue0b0 \ue0a0 master \ue0b0 ./33_threads Hello from main 1 Hello from thread 1 Hello from main 2 Hello from thread 2 Hello from main 3 Hello from thread 3 Hello from main 4 Hello from thread 4 You can notice the [[thread]] only continues to iteration 4, that is because the [[main thread]] stops.","title":"Using threads in Rust"},{"location":"Terms/Stale%20data/","text":"In computer processing, if a processor changes the value of an operand and then, at a subsequent time, fetches the operand and obtains the old rather than the new value of the operand, then it is said to have seen stale data.","title":"Stale data"},{"location":"Testing/Test%20Layers/","text":"There are several layers of tests Unit tests (test a single unit of code) . - Integration tests ==(tests the data flow between units)(Should be written by architect)==. - Acceptance tests ==(tests the end product)(Should be written by QA)==.","title":"Test Layers"},{"location":"Tools/Brandmark%20Logo%20Maker/","text":"Make logos with AI","title":"Brandmark Logo Maker"},{"location":"Tools/Chrome/Chrome%20save%20pinned%20tabs/","text":"This tool saves pinned tabs and re-opens it when chrome is started.","title":"Chrome save pinned tabs"},{"location":"Tools/Home/MiTemperature2/","text":"You can use this tool to connect to Xiaomi thermometers via [[bluetooth]] and record their values. To get the thermometer [[mac address]]es, add them to the home app, under the sensor -> about you will see the address. This will support sending the measurements to [[MQTT]] as well as other options.","title":"MiTemperature2"},{"location":"Tools/Keyboard/Colemak/","text":"The [[QWERTY]] layout was designed in the 19th century. Colemak is a modern alternative to the [[QWERTY]] and [[Dvorak]] layouts, designed for efficient and ergonomic touch typing in English. Learning Colemak is a one-time investment that will allow you to enjoy faster and pain-free typing for the rest of your life. Colemak is now the 3rd most popular [[keyboard layout]] for touch typing in English, after QWERTY and Dvorak and comes pre-installed on Mac and Linux systems.","title":"Colemak"},{"location":"Tools/Microservices/Tilt/","text":"Tilt makes it easy to develop [[micro-service]]s locally as well as on Kubernetes . It provides with smart rebuilds and live updating.","title":"Tilt"},{"location":"Tools/Monitoring/Prometheus/MQTTGateway%20for%20Prometheus/","text":"You can use this tool to subscribe to [[MQTT]] queues and publish their records as [[Prometheus]] metrics.","title":"MQTTGateway for Prometheus"},{"location":"Tools/Screenshot/Shutter/","text":"sudo apt install shutter Replace default screenshot software with shutter \u00b6 In mint you can go to \"Keyboard settings -> Custom shortcuts\" and add a new shortcut for the \"Print Screen\" button. It will execute shutter -s command which stands for selected area. Annotating screenshots & disabled features \u00b6 sudo add-apt-repository ppa:linuxuprising/shutter sudo apt update sudo apt install gnome-web-photo sudo apt install libgoo-canvas-perl And restart shutter and there will be an editing tool available, where you can anotate.","title":"Shutter"},{"location":"Tools/Screenshot/Shutter/#replace-default-screenshot-software-with-shutter","text":"In mint you can go to \"Keyboard settings -> Custom shortcuts\" and add a new shortcut for the \"Print Screen\" button. It will execute shutter -s command which stands for selected area.","title":"Replace default screenshot software with shutter"},{"location":"Tools/Screenshot/Shutter/#annotating-screenshots-disabled-features","text":"sudo add-apt-repository ppa:linuxuprising/shutter sudo apt update sudo apt install gnome-web-photo sudo apt install libgoo-canvas-perl And restart shutter and there will be an editing tool available, where you can anotate.","title":"Annotating screenshots &amp; disabled features"},{"location":"Tools/Testing/Httpie/","text":"Httpie is an API testing tool, similar to Postman but it also includes a command line version as well. Installing \u00b6 Mac \u00b6 brew install httpie Ubuntu / Debian \u00b6 apt install httpie Archlinux \u00b6 pacman -S httpie Usage \u00b6 You can refer to documentation here . Basically, you can use: $ http PUT pie.dev/put X-API-Token:123 name=John","title":"Httpie"},{"location":"Tools/Testing/Httpie/#installing","text":"","title":"Installing"},{"location":"Tools/Testing/Httpie/#mac","text":"brew install httpie","title":"Mac"},{"location":"Tools/Testing/Httpie/#ubuntu-debian","text":"apt install httpie","title":"Ubuntu / Debian"},{"location":"Tools/Testing/Httpie/#archlinux","text":"pacman -S httpie","title":"Archlinux"},{"location":"Tools/Testing/Httpie/#usage","text":"You can refer to documentation here . Basically, you can use: $ http PUT pie.dev/put X-API-Token:123 name=John","title":"Usage"},{"location":"Tools/Testing/Postman/","text":"Postman is an [[API]] platform for building and using APIs. Postman simplifies each step of the [[API lifecycle]] and streamlines collaboration so you can create better APIs\u2014faster.","title":"Postman"},{"location":"Vim/Configuring%20VIM%20as%20an%20IDE/","text":"Configuring VIM as IDE \u00b6 pacman -S vim Consider using [[Neovim]] though, as it support lua scripts and is easier to work with. Installing a plugin \u00b6 To install any plugin, we must perform the following steps: Create .vim/bundle directory in user's home directory Copy plugin inside the directory Set runtime path in vim mkdir -p ~/.vim/bundle cd ~/.vim/bundle/ git clone https://github.com/sjl/badwolf.git echo \"set runtimepath^ = ~/.vim/bundle/badwolf\" > ~/.vimrc Now plugin is installed, we can use badwolf color scheme as follows: :colorscheme badwolf Using Vundle as a plugin manager \u00b6 Install git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim Configure .vimrc set nocompatible \" be iMproved, required filetype off \" required \" set the runtime path to include Vundle and initialize set rtp+=~/.vim/bundle/Vundle.vim call vundle#begin() \" alternatively, pass a path where Vundle should install plugins \"call vundle#begin('~/some/path/here') \" let Vundle manage Vundle, required Plugin 'VundleVim/Vundle.vim' \" The following are examples of different formats supported. \" Keep Plugin commands between vundle#begin/end. \" plugin on GitHub repo Plugin 'tpope/vim-fugitive' \" plugin from http://vim-scripts.org/vim/scripts.html \" Plugin 'L9' \" Git plugin not hosted on GitHub Plugin 'git://git.wincent.com/command-t.git' \" git repos on your local machine (i.e. when working on your own plugin) Plugin 'file:///home/gmarik/path/to/plugin' \" The sparkup vim script is in a subdirectory of this repo called vim. \" Pass the path to set the runtimepath properly. Plugin 'rstacruz/sparkup', {'rtp': 'vim/'} \" Install L9 and avoid a Naming conflict if you've already installed a \" different version somewhere else. \" Plugin 'ascenator/L9', {'name': 'newL9'} \" All of your Plugins must be added before the following line call vundle#end() \" required filetype plugin indent on \" required \" To ignore plugin indent changes, instead use: \"filetype plugin on \" \" Brief help \" :PluginList - lists configured plugins \" :PluginInstall - installs plugins; append `!` to update or just :PluginUpdate \" :PluginSearch foo - searches for foo; append `!` to refresh local cache \" :PluginClean - confirms removal of unused plugins; append `!` to auto-approve removal \" \" see :h vundle for more details or wiki for FAQ \" Put your non-Plugin stuff after this line Using Vim as IDE \u00b6 We can configure VIM to use it as an IDE. Syntax highlightimg \u00b6 :syntax on Smart indentation \u00b6 set autoident set smartindent set cident Bounce \u00b6 If you are using a programming language which uses curly braces to combine multiple statements then the % key will be your friend. This key will jump between the start and the end of curly braces quickly. Execute shell commands \u00b6 To execute single command from Vim editor user :!<command> :!pwd If you want to execute muiltiple commands: :shell Sources \u00b6 https://www.tutorialspoint.com/vim/vim_plug_ins.htm https://www.tutorialspoint.com/vim/vim_using_vim_as_ide.htm https://github.com/VundleVim/Vundle.vim https://coderoncode.com/tools/2017/04/16/vim-the-perfect-ide.html","title":"Configuring VIM as IDE"},{"location":"Vim/Configuring%20VIM%20as%20an%20IDE/#configuring-vim-as-ide","text":"pacman -S vim Consider using [[Neovim]] though, as it support lua scripts and is easier to work with.","title":"Configuring VIM as IDE"},{"location":"Vim/Configuring%20VIM%20as%20an%20IDE/#installing-a-plugin","text":"To install any plugin, we must perform the following steps: Create .vim/bundle directory in user's home directory Copy plugin inside the directory Set runtime path in vim mkdir -p ~/.vim/bundle cd ~/.vim/bundle/ git clone https://github.com/sjl/badwolf.git echo \"set runtimepath^ = ~/.vim/bundle/badwolf\" > ~/.vimrc Now plugin is installed, we can use badwolf color scheme as follows: :colorscheme badwolf","title":"Installing a plugin"},{"location":"Vim/Configuring%20VIM%20as%20an%20IDE/#using-vundle-as-a-plugin-manager","text":"Install git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim Configure .vimrc set nocompatible \" be iMproved, required filetype off \" required \" set the runtime path to include Vundle and initialize set rtp+=~/.vim/bundle/Vundle.vim call vundle#begin() \" alternatively, pass a path where Vundle should install plugins \"call vundle#begin('~/some/path/here') \" let Vundle manage Vundle, required Plugin 'VundleVim/Vundle.vim' \" The following are examples of different formats supported. \" Keep Plugin commands between vundle#begin/end. \" plugin on GitHub repo Plugin 'tpope/vim-fugitive' \" plugin from http://vim-scripts.org/vim/scripts.html \" Plugin 'L9' \" Git plugin not hosted on GitHub Plugin 'git://git.wincent.com/command-t.git' \" git repos on your local machine (i.e. when working on your own plugin) Plugin 'file:///home/gmarik/path/to/plugin' \" The sparkup vim script is in a subdirectory of this repo called vim. \" Pass the path to set the runtimepath properly. Plugin 'rstacruz/sparkup', {'rtp': 'vim/'} \" Install L9 and avoid a Naming conflict if you've already installed a \" different version somewhere else. \" Plugin 'ascenator/L9', {'name': 'newL9'} \" All of your Plugins must be added before the following line call vundle#end() \" required filetype plugin indent on \" required \" To ignore plugin indent changes, instead use: \"filetype plugin on \" \" Brief help \" :PluginList - lists configured plugins \" :PluginInstall - installs plugins; append `!` to update or just :PluginUpdate \" :PluginSearch foo - searches for foo; append `!` to refresh local cache \" :PluginClean - confirms removal of unused plugins; append `!` to auto-approve removal \" \" see :h vundle for more details or wiki for FAQ \" Put your non-Plugin stuff after this line","title":"Using Vundle as a plugin manager"},{"location":"Vim/Configuring%20VIM%20as%20an%20IDE/#using-vim-as-ide","text":"We can configure VIM to use it as an IDE.","title":"Using Vim as IDE"},{"location":"Vim/Configuring%20VIM%20as%20an%20IDE/#syntax-highlightimg","text":":syntax on","title":"Syntax highlightimg"},{"location":"Vim/Configuring%20VIM%20as%20an%20IDE/#smart-indentation","text":"set autoident set smartindent set cident","title":"Smart indentation"},{"location":"Vim/Configuring%20VIM%20as%20an%20IDE/#bounce","text":"If you are using a programming language which uses curly braces to combine multiple statements then the % key will be your friend. This key will jump between the start and the end of curly braces quickly.","title":"Bounce"},{"location":"Vim/Configuring%20VIM%20as%20an%20IDE/#execute-shell-commands","text":"To execute single command from Vim editor user :!<command> :!pwd If you want to execute muiltiple commands: :shell","title":"Execute shell commands"},{"location":"Vim/Configuring%20VIM%20as%20an%20IDE/#sources","text":"https://www.tutorialspoint.com/vim/vim_plug_ins.htm https://www.tutorialspoint.com/vim/vim_using_vim_as_ide.htm https://github.com/VundleVim/Vundle.vim https://coderoncode.com/tools/2017/04/16/vim-the-perfect-ide.html","title":"Sources"},{"location":"Vim/Vim%20motions/","text":"cip - change in paragraph =ap - format around paragraph gv - open up the same selection as the previous one gV - 3gI - open 3/n cursors in insert mode. (Opening this and filling a will result in 3 a's). _ - go to the beginning of the line A - append to the end of the line:w makeprj auto reload on changes? Ctrl+o or ctrl+c instead of esc fo+, or . c not d. (cw) vi{ va{ marks m1 m2 '1 '2 alt + j/k (vim mapping in config) jumplist","title":"Vim motions"},{"location":"Windows/Backup%20iOS%20devices%20from%20iTunes%20on%20Windows/","text":"Backup iOS devices from iTunes on Windows \u00b6 By default, when making a backup on windows for an apple device, it will save it to a directory like this: C:\\Users\\Davis\\Apple\\MobileSync\\Backup Since I have another hard drive that stores backups and synces it elsewhere, I want to move it. That can be done by using something similar to symlink (linux): mklink /J \"C:\\Users\\Davis\\Apple\\MobileSync\" \"X:\\Backups\\D\u0101vis Backup\\MobileSync\"","title":"Backup iOS devices from iTunes on Windows"},{"location":"Windows/Backup%20iOS%20devices%20from%20iTunes%20on%20Windows/#backup-ios-devices-from-itunes-on-windows","text":"By default, when making a backup on windows for an apple device, it will save it to a directory like this: C:\\Users\\Davis\\Apple\\MobileSync\\Backup Since I have another hard drive that stores backups and synces it elsewhere, I want to move it. That can be done by using something similar to symlink (linux): mklink /J \"C:\\Users\\Davis\\Apple\\MobileSync\" \"X:\\Backups\\D\u0101vis Backup\\MobileSync\"","title":"Backup iOS devices from iTunes on Windows"}]}